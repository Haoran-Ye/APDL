2022-12-09 00:17:59 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 00:17:59 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 00:17:59 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 00:17:59 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 00:17:59 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 00:17:59 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 00:34:18 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 00:34:18 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 00:34:18 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 00:34:18 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 00:34:18 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 00:34:18 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 00:34:18 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 00:34:19 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 00:34:19 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 00:34:19 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 00:34:19 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 00:34:25 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 00:34:25 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 00:34:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:34:25 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 00:34:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:34:25 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 00:34:25 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 00:34:25 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:34:25 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:34:25 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 00:34:25 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 00:34:25 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 00:34:25 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 00:34:26 | INFO | fairseq.trainer | begin training epoch 1
2022-12-09 00:34:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-09 00:34:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-09 00:34:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-09 00:34:36 | INFO | train_inner | epoch 001:    103 / 1102 loss=27.124, nll_loss=24.939, ppl=3.21564e+07, wps=36950, ups=10.28, wpb=3587.8, bsz=141.5, num_updates=100, lr=1.25975e-05, gnorm=8.24, loss_scale=16, train_wall=10, gb_free=19.8, wall=11
2022-12-09 00:34:46 | INFO | train_inner | epoch 001:    203 / 1102 loss=22.862, nll_loss=21.439, ppl=2.8428e+06, wps=37071.2, ups=10.42, wpb=3557.1, bsz=141.2, num_updates=200, lr=2.5095e-05, gnorm=3.079, loss_scale=16, train_wall=9, gb_free=19.8, wall=20
2022-12-09 00:34:55 | INFO | train_inner | epoch 001:    303 / 1102 loss=20.993, nll_loss=19.554, ppl=769891, wps=36497.6, ups=10.39, wpb=3513.7, bsz=152.5, num_updates=300, lr=3.75925e-05, gnorm=2.796, loss_scale=16, train_wall=9, gb_free=19.7, wall=30
2022-12-09 00:35:05 | INFO | train_inner | epoch 001:    403 / 1102 loss=19.967, nll_loss=18.367, ppl=337965, wps=36668.6, ups=10.28, wpb=3567.4, bsz=144.8, num_updates=400, lr=5.009e-05, gnorm=3.109, loss_scale=16, train_wall=9, gb_free=19.9, wall=40
2022-12-09 00:35:15 | INFO | train_inner | epoch 001:    503 / 1102 loss=19.534, nll_loss=17.842, ppl=234978, wps=37824.9, ups=10.37, wpb=3649.1, bsz=156.3, num_updates=500, lr=6.25875e-05, gnorm=3.038, loss_scale=16, train_wall=9, gb_free=19.6, wall=49
2022-12-09 00:35:24 | INFO | train_inner | epoch 001:    603 / 1102 loss=19.434, nll_loss=17.74, ppl=218962, wps=37460.8, ups=10.49, wpb=3570.7, bsz=148.2, num_updates=600, lr=7.5085e-05, gnorm=2.834, loss_scale=16, train_wall=9, gb_free=19.5, wall=59
2022-12-09 00:35:34 | INFO | train_inner | epoch 001:    703 / 1102 loss=19.311, nll_loss=17.597, ppl=198260, wps=35924.5, ups=10.2, wpb=3521.4, bsz=134.9, num_updates=700, lr=8.75825e-05, gnorm=2.942, loss_scale=16, train_wall=10, gb_free=19.9, wall=69
2022-12-09 00:35:44 | INFO | train_inner | epoch 001:    803 / 1102 loss=18.932, nll_loss=17.139, ppl=144280, wps=37559.7, ups=10.35, wpb=3628.5, bsz=143.4, num_updates=800, lr=0.00010008, gnorm=3.165, loss_scale=16, train_wall=9, gb_free=19.6, wall=78
2022-12-09 00:35:54 | INFO | train_inner | epoch 001:    903 / 1102 loss=18.629, nll_loss=16.79, ppl=113322, wps=36351.9, ups=9.99, wpb=3640.6, bsz=141.5, num_updates=900, lr=0.000112578, gnorm=2.676, loss_scale=16, train_wall=10, gb_free=19.5, wall=88
2022-12-09 00:35:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-09 00:36:04 | INFO | train_inner | epoch 001:   1004 / 1102 loss=18.323, nll_loss=16.435, ppl=88570.9, wps=35900.9, ups=10.03, wpb=3579.3, bsz=140, num_updates=1000, lr=0.000125075, gnorm=2.765, loss_scale=8, train_wall=10, gb_free=19.6, wall=98
2022-12-09 00:36:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 00:38:31 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 00:38:31 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 00:38:31 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 00:38:31 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 00:38:31 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 00:38:31 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 00:38:32 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 00:38:32 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 00:38:32 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 00:38:32 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 00:38:32 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 00:38:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 00:38:37 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 00:38:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:38:37 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 00:38:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:38:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 00:38:37 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 00:38:37 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:38:37 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:38:37 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 00:38:37 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 00:38:37 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 00:38:37 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 00:38:37 | INFO | fairseq.trainer | begin training epoch 1
2022-12-09 00:38:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-09 00:38:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-09 00:38:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-09 00:38:48 | INFO | train_inner | epoch 001:    103 / 1102 loss=27.124, nll_loss=24.939, ppl=3.21564e+07, wps=36197.8, ups=10.07, wpb=3587.8, bsz=141.5, num_updates=100, lr=1.25975e-05, gnorm=8.24, loss_scale=16, train_wall=10, gb_free=19.8, wall=11
2022-12-09 00:38:58 | INFO | train_inner | epoch 001:    203 / 1102 loss=22.862, nll_loss=21.439, ppl=2.8428e+06, wps=36230.2, ups=10.19, wpb=3557.1, bsz=141.2, num_updates=200, lr=2.5095e-05, gnorm=3.079, loss_scale=16, train_wall=10, gb_free=19.8, wall=21
2022-12-09 00:40:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 00:40:46 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 00:40:46 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 00:40:46 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 00:40:46 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 00:40:46 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 00:40:47 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 00:40:47 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 00:40:47 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 00:40:47 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 00:40:47 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 00:40:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 00:40:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 00:40:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:40:47 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 00:40:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:40:47 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 00:40:47 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 00:40:47 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:40:47 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:40:47 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 00:40:47 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 00:40:47 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 00:40:47 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 00:42:35 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 00:42:36 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 00:42:36 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 00:42:36 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 00:42:36 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 00:42:36 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 00:42:36 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 00:42:36 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 00:42:36 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 00:42:36 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 00:42:36 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 00:42:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 00:42:36 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 00:42:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:42:36 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 00:42:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:42:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 00:42:36 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 00:42:36 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:42:36 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:42:36 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 00:42:36 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 00:42:36 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 00:42:36 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 00:45:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 00:45:21 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 00:45:21 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 00:45:21 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 00:45:21 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 00:45:21 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 00:45:22 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 00:45:22 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 00:45:22 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 00:45:22 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 00:45:22 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 00:45:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 00:45:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 00:45:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:45:22 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 00:45:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 00:45:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 00:45:22 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 00:45:22 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:45:22 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 00:45:22 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 00:45:22 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 00:45:22 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 00:45:22 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 11:28:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 11:28:05 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 11:28:05 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 11:28:05 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 11:28:05 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 11:28:05 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 11:28:53 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 11:28:53 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 11:28:53 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 11:28:53 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 11:28:53 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 11:28:53 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 11:28:54 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 11:28:54 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 11:28:54 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 11:28:54 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 11:28:54 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 11:28:58 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 11:28:58 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 11:28:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:28:58 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 11:28:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:28:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 11:28:58 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 11:28:58 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 11:28:58 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 11:28:58 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 11:28:58 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 11:28:58 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 11:28:58 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 11:29:47 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 11:29:47 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 11:29:47 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 11:29:47 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 11:29:47 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 11:29:47 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 11:29:48 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 11:29:48 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 11:29:48 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 11:29:48 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 11:29:48 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 11:29:51 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 11:29:51 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 11:29:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:29:51 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 11:29:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:29:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 11:29:51 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 11:29:51 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 11:29:51 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 11:29:51 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 11:29:51 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 11:29:51 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 11:29:51 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 11:29:52 | INFO | fairseq.trainer | begin training epoch 1
2022-12-09 11:29:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-09 11:29:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-09 11:29:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-09 11:30:04 | INFO | train_inner | epoch 001:    103 / 1102 loss=27.124, nll_loss=24.939, ppl=3.21564e+07, wps=32144.6, ups=8.94, wpb=3587.8, bsz=141.5, num_updates=100, lr=1.25975e-05, gnorm=8.24, loss_scale=16, train_wall=12, gb_free=19.6, wall=13
2022-12-09 11:30:15 | INFO | train_inner | epoch 001:    203 / 1102 loss=22.862, nll_loss=21.439, ppl=2.84277e+06, wps=32231.4, ups=9.06, wpb=3557.1, bsz=141.2, num_updates=200, lr=2.5095e-05, gnorm=3.079, loss_scale=16, train_wall=11, gb_free=19.6, wall=24
2022-12-09 11:30:26 | INFO | train_inner | epoch 001:    303 / 1102 loss=20.993, nll_loss=19.554, ppl=769884, wps=32006.4, ups=9.11, wpb=3513.7, bsz=152.5, num_updates=300, lr=3.75925e-05, gnorm=2.795, loss_scale=16, train_wall=11, gb_free=19.5, wall=35
2022-12-09 11:30:37 | INFO | train_inner | epoch 001:    403 / 1102 loss=19.967, nll_loss=18.366, ppl=337949, wps=32288.2, ups=9.05, wpb=3567.4, bsz=144.8, num_updates=400, lr=5.009e-05, gnorm=3.109, loss_scale=16, train_wall=11, gb_free=19.6, wall=46
2022-12-09 11:30:48 | INFO | train_inner | epoch 001:    503 / 1102 loss=19.534, nll_loss=17.842, ppl=235001, wps=33056, ups=9.06, wpb=3649.1, bsz=156.3, num_updates=500, lr=6.25875e-05, gnorm=3.04, loss_scale=16, train_wall=11, gb_free=19.4, wall=57
2022-12-09 11:30:59 | INFO | train_inner | epoch 001:    603 / 1102 loss=19.434, nll_loss=17.74, ppl=218911, wps=32376.4, ups=9.07, wpb=3570.7, bsz=148.2, num_updates=600, lr=7.5085e-05, gnorm=2.829, loss_scale=16, train_wall=11, gb_free=19.3, wall=68
2022-12-09 11:31:10 | INFO | train_inner | epoch 001:    703 / 1102 loss=19.312, nll_loss=17.597, ppl=198313, wps=32007.6, ups=9.09, wpb=3521.4, bsz=134.9, num_updates=700, lr=8.75825e-05, gnorm=2.942, loss_scale=16, train_wall=11, gb_free=19.6, wall=79
2022-12-09 11:31:21 | INFO | train_inner | epoch 001:    803 / 1102 loss=18.931, nll_loss=17.138, ppl=144245, wps=32804.7, ups=9.04, wpb=3628.5, bsz=143.4, num_updates=800, lr=0.00010008, gnorm=3.161, loss_scale=16, train_wall=11, gb_free=19.5, wall=90
2022-12-09 11:31:32 | INFO | train_inner | epoch 001:    903 / 1102 loss=18.631, nll_loss=16.791, ppl=113421, wps=32676.1, ups=8.98, wpb=3640.6, bsz=141.5, num_updates=900, lr=0.000112578, gnorm=2.678, loss_scale=16, train_wall=11, gb_free=19.3, wall=101
2022-12-09 11:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-09 11:31:44 | INFO | train_inner | epoch 001:   1004 / 1102 loss=18.324, nll_loss=16.436, ppl=88656.7, wps=32202.4, ups=9, wpb=3579.3, bsz=140, num_updates=1000, lr=0.000125075, gnorm=2.763, loss_scale=8, train_wall=11, gb_free=19.4, wall=112
2022-12-09 11:31:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:33:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 11:33:26 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 11:33:26 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 11:33:26 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 11:33:26 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 11:33:26 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 11:33:27 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 11:33:27 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 11:33:27 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 11:33:27 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 11:33:27 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 11:33:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 11:33:30 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 11:33:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:33:30 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 11:33:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:33:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 11:33:30 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 11:33:30 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 11:33:30 | INFO | fairseq.trainer | No existing checkpoint found iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 11:33:30 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-09 11:33:30 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 11:33:30 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 11:33:30 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 11:33:30 | INFO | fairseq.trainer | begin training epoch 1
2022-12-09 11:33:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-09 11:33:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-09 11:33:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-09 11:33:42 | INFO | train_inner | epoch 001:    103 / 1102 loss=27.124, nll_loss=24.939, ppl=3.21564e+07, wps=32850.2, ups=9.14, wpb=3587.8, bsz=141.5, num_updates=100, lr=1.25975e-05, gnorm=8.24, loss_scale=16, train_wall=12, gb_free=19.6, wall=12
2022-12-09 11:33:53 | INFO | train_inner | epoch 001:    203 / 1102 loss=22.862, nll_loss=21.439, ppl=2.84277e+06, wps=32692.8, ups=9.19, wpb=3557.1, bsz=141.2, num_updates=200, lr=2.5095e-05, gnorm=3.079, loss_scale=16, train_wall=11, gb_free=19.6, wall=23
2022-12-09 11:34:04 | INFO | train_inner | epoch 001:    303 / 1102 loss=20.993, nll_loss=19.554, ppl=769884, wps=31698.7, ups=9.02, wpb=3513.7, bsz=152.5, num_updates=300, lr=3.75925e-05, gnorm=2.795, loss_scale=16, train_wall=11, gb_free=19.5, wall=34
2022-12-09 11:34:15 | INFO | train_inner | epoch 001:    403 / 1102 loss=19.967, nll_loss=18.366, ppl=337949, wps=32099.9, ups=9, wpb=3567.4, bsz=144.8, num_updates=400, lr=5.009e-05, gnorm=3.109, loss_scale=16, train_wall=11, gb_free=19.6, wall=45
2022-12-09 11:34:26 | INFO | train_inner | epoch 001:    503 / 1102 loss=19.534, nll_loss=17.842, ppl=235001, wps=33095, ups=9.07, wpb=3649.1, bsz=156.3, num_updates=500, lr=6.25875e-05, gnorm=3.04, loss_scale=16, train_wall=11, gb_free=19.4, wall=56
2022-12-09 11:34:37 | INFO | train_inner | epoch 001:    603 / 1102 loss=19.434, nll_loss=17.74, ppl=218911, wps=32746.2, ups=9.17, wpb=3570.7, bsz=148.2, num_updates=600, lr=7.5085e-05, gnorm=2.829, loss_scale=16, train_wall=11, gb_free=19.3, wall=67
2022-12-09 11:34:48 | INFO | train_inner | epoch 001:    703 / 1102 loss=19.312, nll_loss=17.597, ppl=198313, wps=31748.5, ups=9.02, wpb=3521.4, bsz=134.9, num_updates=700, lr=8.75825e-05, gnorm=2.942, loss_scale=16, train_wall=11, gb_free=19.6, wall=78
2022-12-09 11:34:59 | INFO | train_inner | epoch 001:    803 / 1102 loss=18.931, nll_loss=17.138, ppl=144245, wps=32668.4, ups=9, wpb=3628.5, bsz=143.4, num_updates=800, lr=0.00010008, gnorm=3.161, loss_scale=16, train_wall=11, gb_free=19.5, wall=89
2022-12-09 11:35:10 | INFO | train_inner | epoch 001:    903 / 1102 loss=18.631, nll_loss=16.791, ppl=113421, wps=32798.7, ups=9.01, wpb=3640.6, bsz=141.5, num_updates=900, lr=0.000112578, gnorm=2.678, loss_scale=16, train_wall=11, gb_free=19.3, wall=101
2022-12-09 11:35:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-09 11:35:21 | INFO | train_inner | epoch 001:   1004 / 1102 loss=18.324, nll_loss=16.436, ppl=88656.7, wps=32425.3, ups=9.06, wpb=3579.3, bsz=140, num_updates=1000, lr=0.000125075, gnorm=2.763, loss_scale=8, train_wall=11, gb_free=19.4, wall=112
2022-12-09 11:35:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:36:18 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.641 | nll_loss 7.856 | ppl 231.62 | bleu 1.32 | wps 3959 | wpb 2835.3 | bsz 115.6 | num_updates 1098
2022-12-09 11:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1098 updates
2022-12-09 11:36:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint1.pt
2022-12-09 11:36:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint1.pt (epoch 1 @ 1098 updates, score 1.32) (writing took 1.1842379057779908 seconds)
2022-12-09 11:36:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-12-09 11:36:19 | INFO | train | epoch 001 | loss 20.282 | nll_loss 18.535 | ppl 379721 | wps 23433.6 | ups 6.54 | wpb 3582.4 | bsz 145 | num_updates 1098 | lr 0.000137323 | gnorm 3.395 | loss_scale 8 | train_wall 119 | gb_free 19.7 | wall 169
2022-12-09 11:36:19 | INFO | fairseq.trainer | begin training epoch 2
2022-12-09 11:36:19 | INFO | train_inner | epoch 002:      2 / 1102 loss=18.009, nll_loss=16.063, ppl=68481.8, wps=6194.5, ups=1.72, wpb=3595.1, bsz=150.2, num_updates=1100, lr=0.000137573, gnorm=2.685, loss_scale=8, train_wall=11, gb_free=19.3, wall=170
2022-12-09 11:36:30 | INFO | train_inner | epoch 002:    102 / 1102 loss=17.927, nll_loss=15.979, ppl=64573.7, wps=33043, ups=9.16, wpb=3607, bsz=132.8, num_updates=1200, lr=0.00015007, gnorm=2.491, loss_scale=8, train_wall=11, gb_free=19.4, wall=181
2022-12-09 11:36:41 | INFO | train_inner | epoch 002:    202 / 1102 loss=17.596, nll_loss=15.577, ppl=48864.8, wps=32703.7, ups=9.12, wpb=3586.5, bsz=146, num_updates=1300, lr=0.000162568, gnorm=2.649, loss_scale=8, train_wall=11, gb_free=19.8, wall=192
2022-12-09 11:36:52 | INFO | train_inner | epoch 002:    302 / 1102 loss=17.257, nll_loss=15.179, ppl=37086.2, wps=33039.6, ups=9.14, wpb=3613.1, bsz=143.1, num_updates=1400, lr=0.000175065, gnorm=2.379, loss_scale=8, train_wall=11, gb_free=19.5, wall=202
2022-12-09 11:37:03 | INFO | train_inner | epoch 002:    402 / 1102 loss=17.19, nll_loss=15.114, ppl=35470.5, wps=31756.3, ups=9.12, wpb=3483.5, bsz=131.5, num_updates=1500, lr=0.000187563, gnorm=2.291, loss_scale=8, train_wall=11, gb_free=19.3, wall=213
2022-12-09 11:37:14 | INFO | train_inner | epoch 002:    502 / 1102 loss=16.785, nll_loss=14.652, ppl=25746, wps=31819.7, ups=9, wpb=3535.3, bsz=157, num_updates=1600, lr=0.00020006, gnorm=2.647, loss_scale=8, train_wall=11, gb_free=19.6, wall=225
2022-12-09 11:37:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 11:37:43 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 11:37:43 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 11:37:43 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 11:37:43 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 11:37:43 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 11:37:44 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 11:37:44 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 11:37:44 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 11:37:44 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 11:37:44 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 11:37:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 11:37:47 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 11:37:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:37:47 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 11:37:47 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 11:37:47 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 11:37:47 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 11:37:47 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 11:37:48 | INFO | fairseq.trainer | Loaded checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt (epoch 2 @ 1098 updates)
2022-12-09 11:37:48 | INFO | fairseq.trainer | loading train data for epoch 2
2022-12-09 11:37:48 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 11:37:48 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 11:37:48 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 11:37:48 | INFO | fairseq.trainer | begin training epoch 2
2022-12-09 11:37:49 | INFO | train_inner | epoch 002:      2 / 1102 loss=18.074, nll_loss=16.145, ppl=72485.7, wps=33745.3, ups=8.42, wpb=3820.5, bsz=128, num_updates=1100, lr=0.000137573, gnorm=1.973, loss_scale=8, train_wall=1, gb_free=19.3, wall=2
2022-12-09 11:38:00 | INFO | train_inner | epoch 002:    102 / 1102 loss=17.927, nll_loss=15.979, ppl=64570.1, wps=33093.2, ups=9.17, wpb=3607, bsz=132.8, num_updates=1200, lr=0.00015007, gnorm=2.491, loss_scale=8, train_wall=11, gb_free=19.4, wall=13
2022-12-09 11:38:11 | INFO | train_inner | epoch 002:    202 / 1102 loss=17.595, nll_loss=15.576, ppl=48845.6, wps=32796.5, ups=9.14, wpb=3586.5, bsz=146, num_updates=1300, lr=0.000162568, gnorm=2.648, loss_scale=8, train_wall=11, gb_free=19.8, wall=24
2022-12-09 11:38:22 | INFO | train_inner | epoch 002:    302 / 1102 loss=17.257, nll_loss=15.179, ppl=37104.2, wps=33178.5, ups=9.18, wpb=3613.1, bsz=143.1, num_updates=1400, lr=0.000175065, gnorm=2.383, loss_scale=8, train_wall=11, gb_free=19.5, wall=35
2022-12-09 11:38:33 | INFO | train_inner | epoch 002:    402 / 1102 loss=17.19, nll_loss=15.114, ppl=35472.7, wps=32108.4, ups=9.22, wpb=3483.5, bsz=131.5, num_updates=1500, lr=0.000187563, gnorm=2.292, loss_scale=8, train_wall=11, gb_free=19.3, wall=46
2022-12-09 11:38:44 | INFO | train_inner | epoch 002:    502 / 1102 loss=16.786, nll_loss=14.654, ppl=25773.6, wps=32309.1, ups=9.14, wpb=3535.3, bsz=157, num_updates=1600, lr=0.00020006, gnorm=2.649, loss_scale=8, train_wall=11, gb_free=19.6, wall=56
2022-12-09 11:38:55 | INFO | train_inner | epoch 002:    602 / 1102 loss=16.681, nll_loss=14.556, ppl=24079.5, wps=32606.1, ups=9.14, wpb=3566.5, bsz=138, num_updates=1700, lr=0.000212558, gnorm=2.195, loss_scale=8, train_wall=11, gb_free=19.7, wall=67
2022-12-09 11:39:06 | INFO | train_inner | epoch 002:    702 / 1102 loss=16.42, nll_loss=14.26, ppl=19623.6, wps=32761.3, ups=9.08, wpb=3609.9, bsz=156, num_updates=1800, lr=0.000225055, gnorm=2.207, loss_scale=8, train_wall=11, gb_free=19.4, wall=78
2022-12-09 11:39:17 | INFO | train_inner | epoch 002:    802 / 1102 loss=16.474, nll_loss=14.334, ppl=20646.4, wps=32815.5, ups=9.19, wpb=3569.8, bsz=139.8, num_updates=1900, lr=0.000237553, gnorm=2.136, loss_scale=8, train_wall=11, gb_free=19.6, wall=89
2022-12-09 11:39:28 | INFO | train_inner | epoch 002:    902 / 1102 loss=16.168, nll_loss=13.988, ppl=16247.8, wps=33248.3, ups=9.12, wpb=3646.2, bsz=152.1, num_updates=2000, lr=0.00025005, gnorm=1.996, loss_scale=8, train_wall=11, gb_free=19.5, wall=100
2022-12-09 11:39:38 | INFO | train_inner | epoch 002:   1002 / 1102 loss=15.978, nll_loss=13.77, ppl=13968.5, wps=33924.1, ups=9.5, wpb=3572, bsz=160.2, num_updates=2100, lr=0.000262548, gnorm=2.137, loss_scale=8, train_wall=10, gb_free=19.4, wall=111
2022-12-09 11:39:49 | INFO | train_inner | epoch 002:   1102 / 1102 loss=16.047, nll_loss=13.864, ppl=14905.2, wps=33738.6, ups=9.31, wpb=3624.9, bsz=143.4, num_updates=2200, lr=0.000275045, gnorm=2.116, loss_scale=8, train_wall=11, gb_free=20.2, wall=122
2022-12-09 11:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:40:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.72 | nll_loss 6.798 | ppl 111.25 | bleu 1.75 | wps 3815.7 | wpb 2835.3 | bsz 115.6 | num_updates 2200 | best_bleu 1.75
2022-12-09 11:40:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2200 updates
2022-12-09 11:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint2.pt
2022-12-09 11:40:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint2.pt (epoch 2 @ 2200 updates, score 1.75) (writing took 1.6360909435898066 seconds)
2022-12-09 11:40:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-12-09 11:40:38 | INFO | train | epoch 002 | loss 16.776 | nll_loss 14.662 | ppl 25930.6 | wps 23404.4 | ups 6.53 | wpb 3583.6 | bsz 145.4 | num_updates 2200 | lr 0.000275045 | gnorm 2.295 | loss_scale 8 | train_wall 118 | gb_free 20.2 | wall 170
2022-12-09 11:40:38 | INFO | fairseq.trainer | begin training epoch 3
2022-12-09 11:40:49 | INFO | train_inner | epoch 003:    100 / 1102 loss=15.597, nll_loss=13.333, ppl=10320.9, wps=6053.3, ups=1.67, wpb=3625.9, bsz=166.3, num_updates=2300, lr=0.000287543, gnorm=2.073, loss_scale=8, train_wall=11, gb_free=19.3, wall=181
2022-12-09 11:41:00 | INFO | train_inner | epoch 003:    200 / 1102 loss=15.861, nll_loss=13.64, ppl=12766.8, wps=33079.9, ups=9.35, wpb=3539.5, bsz=140.4, num_updates=2400, lr=0.00030004, gnorm=2.033, loss_scale=8, train_wall=10, gb_free=19.6, wall=192
2022-12-09 11:41:10 | INFO | train_inner | epoch 003:    300 / 1102 loss=15.375, nll_loss=13.06, ppl=8542.23, wps=32676.5, ups=9.14, wpb=3575.8, bsz=162.1, num_updates=2500, lr=0.000312538, gnorm=2.006, loss_scale=8, train_wall=11, gb_free=19.4, wall=203
2022-12-09 11:41:21 | INFO | train_inner | epoch 003:    400 / 1102 loss=15.591, nll_loss=13.295, ppl=10054.1, wps=32656.6, ups=9.18, wpb=3557.8, bsz=128.4, num_updates=2600, lr=0.000325035, gnorm=1.902, loss_scale=8, train_wall=11, gb_free=19.5, wall=214
2022-12-09 11:41:32 | INFO | train_inner | epoch 003:    500 / 1102 loss=15.443, nll_loss=13.12, ppl=8901.65, wps=33274.5, ups=9.17, wpb=3629.3, bsz=135.7, num_updates=2700, lr=0.000337533, gnorm=1.843, loss_scale=8, train_wall=11, gb_free=19.7, wall=225
2022-12-09 11:41:43 | INFO | train_inner | epoch 003:    600 / 1102 loss=15.265, nll_loss=12.89, ppl=7590.46, wps=32908, ups=9.12, wpb=3607.9, bsz=133.3, num_updates=2800, lr=0.00035003, gnorm=1.869, loss_scale=8, train_wall=11, gb_free=19.3, wall=236
2022-12-09 11:41:54 | INFO | train_inner | epoch 003:    700 / 1102 loss=14.997, nll_loss=12.565, ppl=6060.05, wps=32735.7, ups=9.15, wpb=3578.7, bsz=146.1, num_updates=2900, lr=0.000362528, gnorm=2.033, loss_scale=8, train_wall=11, gb_free=19.5, wall=247
2022-12-09 11:42:05 | INFO | train_inner | epoch 003:    800 / 1102 loss=14.904, nll_loss=12.436, ppl=5542.49, wps=33261.8, ups=9.3, wpb=3576.7, bsz=144.6, num_updates=3000, lr=0.000375025, gnorm=2.025, loss_scale=8, train_wall=11, gb_free=19.3, wall=258
2022-12-09 11:42:16 | INFO | train_inner | epoch 003:    900 / 1102 loss=14.56, nll_loss=12.013, ppl=4134.48, wps=32861, ups=9.17, wpb=3582.2, bsz=147.6, num_updates=3100, lr=0.000387523, gnorm=1.935, loss_scale=8, train_wall=11, gb_free=19.5, wall=268
2022-12-09 11:42:27 | INFO | train_inner | epoch 003:   1000 / 1102 loss=14.342, nll_loss=11.74, ppl=3419.37, wps=32897.6, ups=9.23, wpb=3564, bsz=148.7, num_updates=3200, lr=0.00040002, gnorm=2.003, loss_scale=8, train_wall=11, gb_free=19.5, wall=279
2022-12-09 11:42:37 | INFO | train_inner | epoch 003:   1100 / 1102 loss=14.184, nll_loss=11.548, ppl=2994.5, wps=33502.3, ups=9.36, wpb=3578.2, bsz=147.7, num_updates=3300, lr=0.000412518, gnorm=1.968, loss_scale=8, train_wall=10, gb_free=19.6, wall=290
2022-12-09 11:42:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:43:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.662 | nll_loss 5.599 | ppl 48.46 | bleu 6.26 | wps 4454.9 | wpb 2835.3 | bsz 115.6 | num_updates 3302 | best_bleu 6.26
2022-12-09 11:43:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3302 updates
2022-12-09 11:43:19 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint3.pt
2022-12-09 11:43:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint3.pt (epoch 3 @ 3302 updates, score 6.26) (writing took 1.5710928095504642 seconds)
2022-12-09 11:43:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-12-09 11:43:19 | INFO | train | epoch 003 | loss 15.102 | nll_loss 12.695 | ppl 6629.68 | wps 24422.9 | ups 6.82 | wpb 3583.6 | bsz 145.4 | num_updates 3302 | lr 0.000412767 | gnorm 1.971 | loss_scale 8 | train_wall 117 | gb_free 19.5 | wall 332
2022-12-09 11:43:20 | INFO | fairseq.trainer | begin training epoch 4
2022-12-09 11:43:30 | INFO | train_inner | epoch 004:     98 / 1102 loss=13.778, nll_loss=11.045, ppl=2113.29, wps=6858.9, ups=1.88, wpb=3645.7, bsz=160.1, num_updates=3400, lr=0.000425015, gnorm=1.98, loss_scale=8, train_wall=11, gb_free=19.6, wall=343
2022-12-09 11:43:41 | INFO | train_inner | epoch 004:    198 / 1102 loss=13.862, nll_loss=11.134, ppl=2247.62, wps=33654.6, ups=9.29, wpb=3624.3, bsz=137, num_updates=3500, lr=0.000437513, gnorm=1.85, loss_scale=8, train_wall=11, gb_free=19.4, wall=354
2022-12-09 11:43:52 | INFO | train_inner | epoch 004:    298 / 1102 loss=13.696, nll_loss=10.923, ppl=1941.86, wps=33399.3, ups=9.29, wpb=3596.7, bsz=147.4, num_updates=3600, lr=0.00045001, gnorm=2.015, loss_scale=8, train_wall=11, gb_free=20.4, wall=365
2022-12-09 11:44:03 | INFO | train_inner | epoch 004:    398 / 1102 loss=13.399, nll_loss=10.565, ppl=1515.18, wps=32690.4, ups=9.24, wpb=3536.7, bsz=146.2, num_updates=3700, lr=0.000462508, gnorm=1.905, loss_scale=8, train_wall=11, gb_free=19.3, wall=375
2022-12-09 11:44:14 | INFO | train_inner | epoch 004:    498 / 1102 loss=13.289, nll_loss=10.425, ppl=1374.4, wps=33415.2, ups=9.38, wpb=3562.9, bsz=144.2, num_updates=3800, lr=0.000475005, gnorm=1.977, loss_scale=8, train_wall=10, gb_free=19.5, wall=386
2022-12-09 11:44:24 | INFO | train_inner | epoch 004:    598 / 1102 loss=13.21, nll_loss=10.314, ppl=1272.94, wps=32842.8, ups=9.18, wpb=3578.8, bsz=146.1, num_updates=3900, lr=0.000487503, gnorm=2.067, loss_scale=8, train_wall=11, gb_free=19.6, wall=397
2022-12-09 11:44:35 | INFO | train_inner | epoch 004:    698 / 1102 loss=13.013, nll_loss=10.075, ppl=1078.87, wps=33357.9, ups=9.29, wpb=3588.8, bsz=145.4, num_updates=4000, lr=0.0005, gnorm=1.89, loss_scale=8, train_wall=11, gb_free=19.3, wall=408
2022-12-09 11:44:46 | INFO | train_inner | epoch 004:    798 / 1102 loss=12.69, nll_loss=9.703, ppl=833.27, wps=33500.7, ups=9.38, wpb=3569.8, bsz=157.6, num_updates=4100, lr=0.000493865, gnorm=1.839, loss_scale=8, train_wall=10, gb_free=19.5, wall=418
2022-12-09 11:44:56 | INFO | train_inner | epoch 004:    898 / 1102 loss=12.742, nll_loss=9.747, ppl=859.55, wps=32845.4, ups=9.4, wpb=3494.9, bsz=136.7, num_updates=4200, lr=0.00048795, gnorm=1.83, loss_scale=8, train_wall=10, gb_free=19.3, wall=429
2022-12-09 11:45:07 | INFO | train_inner | epoch 004:    998 / 1102 loss=12.537, nll_loss=9.481, ppl=714.49, wps=33410.2, ups=9.26, wpb=3609.6, bsz=134.2, num_updates=4300, lr=0.000482243, gnorm=1.817, loss_scale=8, train_wall=11, gb_free=19.4, wall=440
2022-12-09 11:45:18 | INFO | train_inner | epoch 004:   1098 / 1102 loss=12.268, nll_loss=9.164, ppl=573.47, wps=33843, ups=9.36, wpb=3617.1, bsz=143.3, num_updates=4400, lr=0.000476731, gnorm=1.786, loss_scale=8, train_wall=10, gb_free=19.7, wall=451
2022-12-09 11:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:45:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.526 | nll_loss 4.28 | ppl 19.43 | bleu 17.01 | wps 4615 | wpb 2835.3 | bsz 115.6 | num_updates 4404 | best_bleu 17.01
2022-12-09 11:45:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4404 updates
2022-12-09 11:45:58 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint4.pt
2022-12-09 11:45:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint4.pt (epoch 4 @ 4404 updates, score 17.01) (writing took 1.5547953713685274 seconds)
2022-12-09 11:45:59 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-12-09 11:45:59 | INFO | train | epoch 004 | loss 13.13 | nll_loss 10.228 | ppl 1199.6 | wps 24753.4 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 4404 | lr 0.000476515 | gnorm 1.905 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 492
2022-12-09 11:45:59 | INFO | fairseq.trainer | begin training epoch 5
2022-12-09 11:46:10 | INFO | train_inner | epoch 005:     96 / 1102 loss=12.046, nll_loss=8.873, ppl=468.9, wps=6885.8, ups=1.93, wpb=3565.2, bsz=146.2, num_updates=4500, lr=0.000471405, gnorm=1.927, loss_scale=8, train_wall=11, gb_free=20, wall=502
2022-12-09 11:46:21 | INFO | train_inner | epoch 005:    196 / 1102 loss=11.747, nll_loss=8.532, ppl=370.13, wps=33189, ups=9.19, wpb=3610.3, bsz=159.5, num_updates=4600, lr=0.000466252, gnorm=1.792, loss_scale=8, train_wall=11, gb_free=19.6, wall=513
2022-12-09 11:46:31 | INFO | train_inner | epoch 005:    296 / 1102 loss=11.795, nll_loss=8.576, ppl=381.64, wps=33315, ups=9.38, wpb=3553.5, bsz=143.7, num_updates=4700, lr=0.000461266, gnorm=1.822, loss_scale=8, train_wall=10, gb_free=19.3, wall=524
2022-12-09 11:46:42 | INFO | train_inner | epoch 005:    396 / 1102 loss=11.619, nll_loss=8.342, ppl=324.59, wps=33756.5, ups=9.29, wpb=3634.1, bsz=138.3, num_updates=4800, lr=0.000456435, gnorm=1.774, loss_scale=8, train_wall=11, gb_free=19.3, wall=535
2022-12-09 11:46:53 | INFO | train_inner | epoch 005:    496 / 1102 loss=11.357, nll_loss=8.061, ppl=267.03, wps=32888.6, ups=9.21, wpb=3571.5, bsz=159.1, num_updates=4900, lr=0.000451754, gnorm=1.813, loss_scale=8, train_wall=11, gb_free=19.3, wall=546
2022-12-09 11:47:04 | INFO | train_inner | epoch 005:    596 / 1102 loss=11.453, nll_loss=8.15, ppl=284.05, wps=33328.8, ups=9.33, wpb=3571.4, bsz=143.3, num_updates=5000, lr=0.000447214, gnorm=1.846, loss_scale=8, train_wall=10, gb_free=19.7, wall=556
2022-12-09 11:47:14 | INFO | train_inner | epoch 005:    696 / 1102 loss=11.103, nll_loss=7.755, ppl=216, wps=33724.4, ups=9.3, wpb=3624.8, bsz=157.4, num_updates=5100, lr=0.000442807, gnorm=1.788, loss_scale=8, train_wall=11, gb_free=19.6, wall=567
2022-12-09 11:47:25 | INFO | train_inner | epoch 005:    796 / 1102 loss=11.029, nll_loss=7.677, ppl=204.71, wps=33477.4, ups=9.33, wpb=3589.7, bsz=153.9, num_updates=5200, lr=0.000438529, gnorm=1.823, loss_scale=8, train_wall=10, gb_free=19.6, wall=578
2022-12-09 11:47:36 | INFO | train_inner | epoch 005:    896 / 1102 loss=11.151, nll_loss=7.78, ppl=219.82, wps=33421.4, ups=9.43, wpb=3544.2, bsz=132.2, num_updates=5300, lr=0.000434372, gnorm=1.804, loss_scale=8, train_wall=10, gb_free=19.3, wall=588
2022-12-09 11:47:47 | INFO | train_inner | epoch 005:    996 / 1102 loss=11.175, nll_loss=7.818, ppl=225.58, wps=33312.9, ups=9.25, wpb=3601, bsz=126, num_updates=5400, lr=0.000430331, gnorm=1.826, loss_scale=8, train_wall=11, gb_free=19.9, wall=599
2022-12-09 11:47:57 | INFO | train_inner | epoch 005:   1096 / 1102 loss=10.883, nll_loss=7.486, ppl=179.31, wps=32894.2, ups=9.26, wpb=3552.8, bsz=142, num_updates=5500, lr=0.000426401, gnorm=1.824, loss_scale=8, train_wall=11, gb_free=19.4, wall=610
2022-12-09 11:47:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:48:36 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.816 | nll_loss 3.448 | ppl 10.91 | bleu 24.07 | wps 4687.6 | wpb 2835.3 | bsz 115.6 | num_updates 5506 | best_bleu 24.07
2022-12-09 11:48:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5506 updates
2022-12-09 11:48:37 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint5.pt
2022-12-09 11:48:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint5.pt (epoch 5 @ 5506 updates, score 24.07) (writing took 1.502597751095891 seconds)
2022-12-09 11:48:38 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-12-09 11:48:38 | INFO | train | epoch 005 | loss 11.391 | nll_loss 8.089 | ppl 272.32 | wps 24860.6 | ups 6.94 | wpb 3583.6 | bsz 145.4 | num_updates 5506 | lr 0.000426169 | gnorm 1.822 | loss_scale 8 | train_wall 116 | gb_free 19.8 | wall 650
2022-12-09 11:48:38 | INFO | fairseq.trainer | begin training epoch 6
2022-12-09 11:48:48 | INFO | train_inner | epoch 006:     94 / 1102 loss=10.716, nll_loss=7.275, ppl=154.83, wps=6981.8, ups=1.97, wpb=3549.7, bsz=131.4, num_updates=5600, lr=0.000422577, gnorm=1.849, loss_scale=8, train_wall=10, gb_free=19.5, wall=661
2022-12-09 11:48:59 | INFO | train_inner | epoch 006:    194 / 1102 loss=10.369, nll_loss=6.911, ppl=120.37, wps=33440.1, ups=9.23, wpb=3624, bsz=171.2, num_updates=5700, lr=0.000418854, gnorm=1.804, loss_scale=8, train_wall=11, gb_free=19.4, wall=672
2022-12-09 11:49:10 | INFO | train_inner | epoch 006:    294 / 1102 loss=10.664, nll_loss=7.218, ppl=148.85, wps=33547.7, ups=9.32, wpb=3597.8, bsz=135.3, num_updates=5800, lr=0.000415227, gnorm=1.821, loss_scale=8, train_wall=10, gb_free=20, wall=682
2022-12-09 11:49:20 | INFO | train_inner | epoch 006:    394 / 1102 loss=10.532, nll_loss=7.069, ppl=134.25, wps=32999.8, ups=9.31, wpb=3545.4, bsz=140.6, num_updates=5900, lr=0.000411693, gnorm=1.876, loss_scale=8, train_wall=11, gb_free=19.5, wall=693
2022-12-09 11:49:31 | INFO | train_inner | epoch 006:    494 / 1102 loss=10.462, nll_loss=6.998, ppl=127.83, wps=33284.4, ups=9.35, wpb=3561.7, bsz=145.2, num_updates=6000, lr=0.000408248, gnorm=1.863, loss_scale=8, train_wall=10, gb_free=20, wall=704
2022-12-09 11:49:42 | INFO | train_inner | epoch 006:    594 / 1102 loss=10.356, nll_loss=6.885, ppl=118.21, wps=33585.9, ups=9.37, wpb=3583.8, bsz=152, num_updates=6100, lr=0.000404888, gnorm=1.83, loss_scale=8, train_wall=10, gb_free=19.5, wall=714
2022-12-09 11:49:53 | INFO | train_inner | epoch 006:    694 / 1102 loss=10.423, nll_loss=6.944, ppl=123.16, wps=33239, ups=9.24, wpb=3595.7, bsz=136.9, num_updates=6200, lr=0.00040161, gnorm=1.88, loss_scale=8, train_wall=11, gb_free=19.3, wall=725
2022-12-09 11:50:03 | INFO | train_inner | epoch 006:    794 / 1102 loss=10.329, nll_loss=6.852, ppl=115.5, wps=33702.6, ups=9.36, wpb=3601.7, bsz=142, num_updates=6300, lr=0.00039841, gnorm=1.836, loss_scale=8, train_wall=10, gb_free=19.2, wall=736
2022-12-09 11:50:14 | INFO | train_inner | epoch 006:    894 / 1102 loss=10.218, nll_loss=6.735, ppl=106.5, wps=33065.5, ups=9.25, wpb=3575.3, bsz=150.5, num_updates=6400, lr=0.000395285, gnorm=1.873, loss_scale=8, train_wall=11, gb_free=19.4, wall=747
2022-12-09 11:50:25 | INFO | train_inner | epoch 006:    994 / 1102 loss=10.21, nll_loss=6.735, ppl=106.53, wps=33465.3, ups=9.27, wpb=3608.5, bsz=144.1, num_updates=6500, lr=0.000392232, gnorm=1.797, loss_scale=8, train_wall=11, gb_free=19.3, wall=758
2022-12-09 11:50:36 | INFO | train_inner | epoch 006:   1094 / 1102 loss=10.139, nll_loss=6.659, ppl=101.02, wps=32799.9, ups=9.13, wpb=3590.9, bsz=149.7, num_updates=6600, lr=0.000389249, gnorm=1.839, loss_scale=8, train_wall=11, gb_free=19.3, wall=768
2022-12-09 11:50:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:51:15 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.548 | nll_loss 3.131 | ppl 8.76 | bleu 26.71 | wps 4716.9 | wpb 2835.3 | bsz 115.6 | num_updates 6608 | best_bleu 26.71
2022-12-09 11:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6608 updates
2022-12-09 11:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint6.pt
2022-12-09 11:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint6.pt (epoch 6 @ 6608 updates, score 26.71) (writing took 1.4923545103520155 seconds)
2022-12-09 11:51:17 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-12-09 11:51:17 | INFO | train | epoch 006 | loss 10.396 | nll_loss 6.929 | ppl 121.84 | wps 24894.6 | ups 6.95 | wpb 3583.6 | bsz 145.4 | num_updates 6608 | lr 0.000389014 | gnorm 1.843 | loss_scale 8 | train_wall 116 | gb_free 19.8 | wall 809
2022-12-09 11:51:17 | INFO | fairseq.trainer | begin training epoch 7
2022-12-09 11:51:27 | INFO | train_inner | epoch 007:     92 / 1102 loss=10.094, nll_loss=6.59, ppl=96.31, wps=7083, ups=1.97, wpb=3595.3, bsz=131.8, num_updates=6700, lr=0.000386334, gnorm=1.812, loss_scale=8, train_wall=10, gb_free=19.5, wall=819
2022-12-09 11:51:37 | INFO | train_inner | epoch 007:    192 / 1102 loss=9.9, nll_loss=6.369, ppl=82.65, wps=33117.7, ups=9.3, wpb=3560.3, bsz=151.8, num_updates=6800, lr=0.000383482, gnorm=1.793, loss_scale=8, train_wall=11, gb_free=19.5, wall=830
2022-12-09 11:51:48 | INFO | train_inner | epoch 007:    292 / 1102 loss=9.81, nll_loss=6.271, ppl=77.24, wps=33422.6, ups=9.37, wpb=3566, bsz=146.5, num_updates=6900, lr=0.000380693, gnorm=1.788, loss_scale=8, train_wall=10, gb_free=19.7, wall=841
2022-12-09 11:51:59 | INFO | train_inner | epoch 007:    392 / 1102 loss=10.026, nll_loss=6.503, ppl=90.71, wps=33024.1, ups=9.33, wpb=3540, bsz=136.2, num_updates=7000, lr=0.000377964, gnorm=1.849, loss_scale=8, train_wall=10, gb_free=19.6, wall=851
2022-12-09 11:52:10 | INFO | train_inner | epoch 007:    492 / 1102 loss=9.811, nll_loss=6.291, ppl=78.28, wps=33219, ups=9.18, wpb=3617.4, bsz=148.9, num_updates=7100, lr=0.000375293, gnorm=1.791, loss_scale=8, train_wall=11, gb_free=19.7, wall=862
2022-12-09 11:52:20 | INFO | train_inner | epoch 007:    592 / 1102 loss=9.854, nll_loss=6.34, ppl=81.03, wps=33492.1, ups=9.32, wpb=3595.1, bsz=142.6, num_updates=7200, lr=0.000372678, gnorm=1.809, loss_scale=8, train_wall=10, gb_free=20.1, wall=873
2022-12-09 11:52:31 | INFO | train_inner | epoch 007:    692 / 1102 loss=9.88, nll_loss=6.362, ppl=82.23, wps=32002.7, ups=9.3, wpb=3440.7, bsz=151.8, num_updates=7300, lr=0.000370117, gnorm=1.898, loss_scale=8, train_wall=11, gb_free=19.4, wall=884
2022-12-09 11:52:42 | INFO | train_inner | epoch 007:    792 / 1102 loss=9.707, nll_loss=6.186, ppl=72.81, wps=34018, ups=9.18, wpb=3704.9, bsz=149.3, num_updates=7400, lr=0.000367607, gnorm=1.761, loss_scale=8, train_wall=11, gb_free=19.8, wall=895
2022-12-09 11:52:53 | INFO | train_inner | epoch 007:    892 / 1102 loss=9.777, nll_loss=6.26, ppl=76.63, wps=33166.5, ups=9.25, wpb=3585.6, bsz=143, num_updates=7500, lr=0.000365148, gnorm=1.805, loss_scale=8, train_wall=11, gb_free=19.6, wall=905
2022-12-09 11:53:04 | INFO | train_inner | epoch 007:    992 / 1102 loss=9.706, nll_loss=6.182, ppl=72.6, wps=33145.2, ups=9.25, wpb=3583.9, bsz=147.3, num_updates=7600, lr=0.000362738, gnorm=1.807, loss_scale=8, train_wall=11, gb_free=19.6, wall=916
2022-12-09 11:53:14 | INFO | train_inner | epoch 007:   1092 / 1102 loss=9.72, nll_loss=6.2, ppl=73.52, wps=33451.2, ups=9.26, wpb=3612.7, bsz=148, num_updates=7700, lr=0.000360375, gnorm=1.838, loss_scale=8, train_wall=11, gb_free=19.5, wall=927
2022-12-09 11:53:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:53:54 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.381 | nll_loss 2.94 | ppl 7.67 | bleu 28.58 | wps 4675.3 | wpb 2835.3 | bsz 115.6 | num_updates 7710 | best_bleu 28.58
2022-12-09 11:53:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7710 updates
2022-12-09 11:53:55 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint7.pt
2022-12-09 11:53:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint7.pt (epoch 7 @ 7710 updates, score 28.58) (writing took 1.5444858819246292 seconds)
2022-12-09 11:53:56 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-12-09 11:53:56 | INFO | train | epoch 007 | loss 9.841 | nll_loss 6.319 | ppl 79.86 | wps 24815.8 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 7710 | lr 0.000360141 | gnorm 1.813 | loss_scale 8 | train_wall 116 | gb_free 19.3 | wall 968
2022-12-09 11:53:56 | INFO | fairseq.trainer | begin training epoch 8
2022-12-09 11:54:06 | INFO | train_inner | epoch 008:     90 / 1102 loss=9.628, nll_loss=6.083, ppl=67.78, wps=7130.9, ups=1.95, wpb=3652.7, bsz=128.7, num_updates=7800, lr=0.000358057, gnorm=1.757, loss_scale=8, train_wall=11, gb_free=19.6, wall=978
2022-12-09 11:54:17 | INFO | train_inner | epoch 008:    190 / 1102 loss=9.577, nll_loss=6.032, ppl=65.44, wps=33354.8, ups=9.25, wpb=3606.9, bsz=140, num_updates=7900, lr=0.000355784, gnorm=1.819, loss_scale=8, train_wall=11, gb_free=19.6, wall=989
2022-12-09 11:54:27 | INFO | train_inner | epoch 008:    290 / 1102 loss=9.409, nll_loss=5.848, ppl=57.59, wps=33783.4, ups=9.33, wpb=3619.5, bsz=147.8, num_updates=8000, lr=0.000353553, gnorm=1.807, loss_scale=8, train_wall=10, gb_free=19.9, wall=1000
2022-12-09 11:54:38 | INFO | train_inner | epoch 008:    390 / 1102 loss=9.521, nll_loss=5.971, ppl=62.74, wps=32254.5, ups=9.16, wpb=3520.3, bsz=144.1, num_updates=8100, lr=0.000351364, gnorm=1.856, loss_scale=8, train_wall=11, gb_free=21.3, wall=1011
2022-12-09 11:54:49 | INFO | train_inner | epoch 008:    490 / 1102 loss=9.49, nll_loss=5.941, ppl=61.42, wps=32711.6, ups=9.24, wpb=3541.4, bsz=150.7, num_updates=8200, lr=0.000349215, gnorm=1.84, loss_scale=8, train_wall=11, gb_free=19.7, wall=1022
2022-12-09 11:55:00 | INFO | train_inner | epoch 008:    590 / 1102 loss=9.631, nll_loss=6.08, ppl=67.66, wps=33175.8, ups=9.26, wpb=3582.1, bsz=135.5, num_updates=8300, lr=0.000347105, gnorm=1.93, loss_scale=8, train_wall=11, gb_free=19.4, wall=1032
2022-12-09 11:55:11 | INFO | train_inner | epoch 008:    690 / 1102 loss=9.475, nll_loss=5.923, ppl=60.69, wps=33734.3, ups=9.32, wpb=3621, bsz=145.3, num_updates=8400, lr=0.000345033, gnorm=1.794, loss_scale=8, train_wall=11, gb_free=19.5, wall=1043
2022-12-09 11:55:21 | INFO | train_inner | epoch 008:    790 / 1102 loss=9.407, nll_loss=5.859, ppl=58.04, wps=33048.7, ups=9.19, wpb=3597.6, bsz=151, num_updates=8500, lr=0.000342997, gnorm=1.79, loss_scale=8, train_wall=11, gb_free=19.3, wall=1054
2022-12-09 11:55:32 | INFO | train_inner | epoch 008:    890 / 1102 loss=9.281, nll_loss=5.74, ppl=53.46, wps=32720.5, ups=9.22, wpb=3549.2, bsz=171.8, num_updates=8600, lr=0.000340997, gnorm=1.778, loss_scale=8, train_wall=11, gb_free=19.5, wall=1065
2022-12-09 11:55:43 | INFO | train_inner | epoch 008:    990 / 1102 loss=9.509, nll_loss=5.972, ppl=62.77, wps=33621.4, ups=9.37, wpb=3588.2, bsz=138.1, num_updates=8700, lr=0.000339032, gnorm=1.813, loss_scale=8, train_wall=10, gb_free=19.5, wall=1076
2022-12-09 11:55:54 | INFO | train_inner | epoch 008:   1090 / 1102 loss=9.387, nll_loss=5.848, ppl=57.59, wps=32871.4, ups=9.19, wpb=3575.3, bsz=155.7, num_updates=8800, lr=0.0003371, gnorm=1.824, loss_scale=8, train_wall=11, gb_free=19.4, wall=1086
2022-12-09 11:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:56:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.284 | nll_loss 2.833 | ppl 7.13 | bleu 29.48 | wps 4953.1 | wpb 2835.3 | bsz 115.6 | num_updates 8812 | best_bleu 29.48
2022-12-09 11:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8812 updates
2022-12-09 11:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint8.pt
2022-12-09 11:56:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint8.pt (epoch 8 @ 8812 updates, score 29.48) (writing took 1.4784113951027393 seconds)
2022-12-09 11:56:33 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-12-09 11:56:33 | INFO | train | epoch 008 | loss 9.489 | nll_loss 5.942 | ppl 61.49 | wps 25096.6 | ups 7 | wpb 3583.6 | bsz 145.4 | num_updates 8812 | lr 0.00033687 | gnorm 1.822 | loss_scale 8 | train_wall 116 | gb_free 19.3 | wall 1126
2022-12-09 11:56:33 | INFO | fairseq.trainer | begin training epoch 9
2022-12-09 11:56:43 | INFO | train_inner | epoch 009:     88 / 1102 loss=9.443, nll_loss=5.871, ppl=58.52, wps=7237.3, ups=2.04, wpb=3540.7, bsz=124, num_updates=8900, lr=0.000335201, gnorm=1.891, loss_scale=8, train_wall=10, gb_free=19.6, wall=1135
2022-12-09 11:56:54 | INFO | train_inner | epoch 009:    188 / 1102 loss=9.268, nll_loss=5.699, ppl=51.94, wps=32932.6, ups=9.17, wpb=3592.1, bsz=150.6, num_updates=9000, lr=0.000333333, gnorm=1.828, loss_scale=8, train_wall=11, gb_free=19.3, wall=1146
2022-12-09 11:57:05 | INFO | train_inner | epoch 009:    288 / 1102 loss=9.245, nll_loss=5.675, ppl=51.08, wps=32835.5, ups=9.08, wpb=3616.5, bsz=140.9, num_updates=9100, lr=0.000331497, gnorm=1.782, loss_scale=8, train_wall=11, gb_free=19.5, wall=1157
2022-12-09 11:57:15 | INFO | train_inner | epoch 009:    388 / 1102 loss=9.325, nll_loss=5.765, ppl=54.38, wps=33068.6, ups=9.27, wpb=3567.8, bsz=136.2, num_updates=9200, lr=0.00032969, gnorm=1.828, loss_scale=8, train_wall=11, gb_free=19.8, wall=1168
2022-12-09 11:57:26 | INFO | train_inner | epoch 009:    488 / 1102 loss=9.261, nll_loss=5.694, ppl=51.78, wps=32894, ups=9.3, wpb=3538.2, bsz=144.2, num_updates=9300, lr=0.000327913, gnorm=1.836, loss_scale=8, train_wall=11, gb_free=19.5, wall=1179
2022-12-09 11:57:37 | INFO | train_inner | epoch 009:    588 / 1102 loss=9.02, nll_loss=5.449, ppl=43.67, wps=32857.4, ups=9, wpb=3651.5, bsz=167.5, num_updates=9400, lr=0.000326164, gnorm=1.759, loss_scale=8, train_wall=11, gb_free=20, wall=1190
2022-12-09 11:57:48 | INFO | train_inner | epoch 009:    688 / 1102 loss=9.172, nll_loss=5.622, ppl=49.25, wps=33698.1, ups=9.18, wpb=3671.6, bsz=158.9, num_updates=9500, lr=0.000324443, gnorm=1.787, loss_scale=8, train_wall=11, gb_free=19.3, wall=1201
2022-12-09 11:57:59 | INFO | train_inner | epoch 009:    788 / 1102 loss=9.1, nll_loss=5.544, ppl=46.65, wps=32350.2, ups=9.09, wpb=3558.3, bsz=159.4, num_updates=9600, lr=0.000322749, gnorm=1.772, loss_scale=8, train_wall=11, gb_free=19.3, wall=1212
2022-12-09 11:58:10 | INFO | train_inner | epoch 009:    888 / 1102 loss=9.264, nll_loss=5.726, ppl=52.93, wps=32747.5, ups=9.14, wpb=3582.9, bsz=149.8, num_updates=9700, lr=0.000321081, gnorm=1.79, loss_scale=8, train_wall=11, gb_free=19.6, wall=1223
2022-12-09 11:58:21 | INFO | train_inner | epoch 009:    988 / 1102 loss=9.42, nll_loss=5.858, ppl=58.01, wps=32450.7, ups=9.34, wpb=3474.3, bsz=121.4, num_updates=9800, lr=0.000319438, gnorm=1.889, loss_scale=8, train_wall=10, gb_free=19.6, wall=1233
2022-12-09 11:58:32 | INFO | train_inner | epoch 009:   1088 / 1102 loss=9.23, nll_loss=5.669, ppl=50.87, wps=33498, ups=9.34, wpb=3586.4, bsz=136.4, num_updates=9900, lr=0.000317821, gnorm=1.822, loss_scale=8, train_wall=10, gb_free=19.5, wall=1244
2022-12-09 11:58:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 11:59:08 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.201 | nll_loss 2.728 | ppl 6.62 | bleu 30.72 | wps 5101.2 | wpb 2835.3 | bsz 115.6 | num_updates 9914 | best_bleu 30.72
2022-12-09 11:59:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9914 updates
2022-12-09 11:59:09 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint9.pt
2022-12-09 11:59:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint9.pt (epoch 9 @ 9914 updates, score 30.72) (writing took 1.5476765148341656 seconds)
2022-12-09 11:59:10 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-12-09 11:59:10 | INFO | train | epoch 009 | loss 9.232 | nll_loss 5.67 | ppl 50.93 | wps 25150.4 | ups 7.02 | wpb 3583.6 | bsz 145.4 | num_updates 9914 | lr 0.000317596 | gnorm 1.811 | loss_scale 8 | train_wall 117 | gb_free 19.5 | wall 1283
2022-12-09 11:59:10 | INFO | fairseq.trainer | begin training epoch 10
2022-12-09 11:59:20 | INFO | train_inner | epoch 010:     86 / 1102 loss=8.985, nll_loss=5.412, ppl=42.59, wps=7506.2, ups=2.07, wpb=3626.2, bsz=148.3, num_updates=10000, lr=0.000316228, gnorm=1.778, loss_scale=8, train_wall=11, gb_free=19.9, wall=1292
2022-12-09 11:59:31 | INFO | train_inner | epoch 010:    186 / 1102 loss=9.057, nll_loss=5.472, ppl=44.4, wps=32929.1, ups=9.23, wpb=3566.7, bsz=140.5, num_updates=10100, lr=0.000314658, gnorm=1.797, loss_scale=8, train_wall=11, gb_free=19.4, wall=1303
2022-12-09 11:59:41 | INFO | train_inner | epoch 010:    286 / 1102 loss=9.115, nll_loss=5.532, ppl=46.28, wps=33044.1, ups=9.29, wpb=3557, bsz=139, num_updates=10200, lr=0.000313112, gnorm=1.795, loss_scale=8, train_wall=11, gb_free=19.4, wall=1314
2022-12-09 11:59:52 | INFO | train_inner | epoch 010:    386 / 1102 loss=9.031, nll_loss=5.45, ppl=43.72, wps=33472.4, ups=9.3, wpb=3600.2, bsz=144, num_updates=10300, lr=0.000311588, gnorm=1.749, loss_scale=8, train_wall=11, gb_free=19.4, wall=1325
2022-12-09 12:00:03 | INFO | train_inner | epoch 010:    486 / 1102 loss=8.926, nll_loss=5.342, ppl=40.57, wps=33382, ups=9.25, wpb=3608.7, bsz=157.8, num_updates=10400, lr=0.000310087, gnorm=1.813, loss_scale=8, train_wall=11, gb_free=19.6, wall=1336
2022-12-09 12:00:14 | INFO | train_inner | epoch 010:    586 / 1102 loss=9.159, nll_loss=5.592, ppl=48.23, wps=34135.3, ups=9.52, wpb=3585.5, bsz=133.4, num_updates=10500, lr=0.000308607, gnorm=1.789, loss_scale=8, train_wall=10, gb_free=19.5, wall=1346
2022-12-09 12:00:24 | INFO | train_inner | epoch 010:    686 / 1102 loss=9.077, nll_loss=5.503, ppl=45.36, wps=33329.8, ups=9.41, wpb=3542.1, bsz=143, num_updates=10600, lr=0.000307148, gnorm=1.81, loss_scale=8, train_wall=10, gb_free=19.5, wall=1357
2022-12-09 12:00:35 | INFO | train_inner | epoch 010:    786 / 1102 loss=8.97, nll_loss=5.417, ppl=42.72, wps=32977.4, ups=9.12, wpb=3617.1, bsz=159, num_updates=10700, lr=0.000305709, gnorm=1.786, loss_scale=8, train_wall=11, gb_free=19.4, wall=1368
2022-12-09 12:00:46 | INFO | train_inner | epoch 010:    886 / 1102 loss=8.989, nll_loss=5.425, ppl=42.95, wps=33587.3, ups=9.27, wpb=3621.8, bsz=155.3, num_updates=10800, lr=0.00030429, gnorm=1.78, loss_scale=8, train_wall=11, gb_free=19.4, wall=1379
2022-12-09 12:00:57 | INFO | train_inner | epoch 010:    986 / 1102 loss=8.973, nll_loss=5.404, ppl=42.35, wps=33265.5, ups=9.39, wpb=3540.9, bsz=149.7, num_updates=10900, lr=0.000302891, gnorm=1.739, loss_scale=8, train_wall=10, gb_free=19.3, wall=1389
2022-12-09 12:01:07 | INFO | train_inner | epoch 010:   1086 / 1102 loss=9.041, nll_loss=5.463, ppl=44.11, wps=33350, ups=9.38, wpb=3554.9, bsz=134.5, num_updates=11000, lr=0.000301511, gnorm=1.791, loss_scale=8, train_wall=10, gb_free=19.7, wall=1400
2022-12-09 12:01:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:01:46 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.148 | nll_loss 2.666 | ppl 6.35 | bleu 31.21 | wps 4892 | wpb 2835.3 | bsz 115.6 | num_updates 11016 | best_bleu 31.21
2022-12-09 12:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 11016 updates
2022-12-09 12:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint10.pt
2022-12-09 12:01:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint10.pt (epoch 10 @ 11016 updates, score 31.21) (writing took 1.4766711331903934 seconds)
2022-12-09 12:01:47 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-12-09 12:01:47 | INFO | train | epoch 010 | loss 9.031 | nll_loss 5.458 | ppl 43.95 | wps 25107.5 | ups 7.01 | wpb 3583.6 | bsz 145.4 | num_updates 11016 | lr 0.000301292 | gnorm 1.785 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 1440
2022-12-09 12:01:47 | INFO | fairseq.trainer | begin training epoch 11
2022-12-09 12:01:57 | INFO | train_inner | epoch 011:     84 / 1102 loss=8.879, nll_loss=5.301, ppl=39.43, wps=7091.4, ups=2.02, wpb=3513.5, bsz=150.5, num_updates=11100, lr=0.00030015, gnorm=1.867, loss_scale=8, train_wall=11, gb_free=19.3, wall=1449
2022-12-09 12:02:08 | INFO | train_inner | epoch 011:    184 / 1102 loss=8.835, nll_loss=5.242, ppl=37.84, wps=33634.6, ups=9.24, wpb=3639.5, bsz=147, num_updates=11200, lr=0.000298807, gnorm=1.708, loss_scale=8, train_wall=11, gb_free=19.3, wall=1460
2022-12-09 12:02:19 | INFO | train_inner | epoch 011:    284 / 1102 loss=8.799, nll_loss=5.206, ppl=36.92, wps=33094.3, ups=9.14, wpb=3620.7, bsz=152.3, num_updates=11300, lr=0.000297482, gnorm=1.748, loss_scale=8, train_wall=11, gb_free=19.6, wall=1471
2022-12-09 12:02:29 | INFO | train_inner | epoch 011:    384 / 1102 loss=8.976, nll_loss=5.398, ppl=42.17, wps=33393.7, ups=9.24, wpb=3613.3, bsz=136.7, num_updates=11400, lr=0.000296174, gnorm=1.769, loss_scale=8, train_wall=11, gb_free=19.8, wall=1482
2022-12-09 12:02:40 | INFO | train_inner | epoch 011:    484 / 1102 loss=8.944, nll_loss=5.359, ppl=41.04, wps=33108.5, ups=9.24, wpb=3583.2, bsz=141.1, num_updates=11500, lr=0.000294884, gnorm=1.748, loss_scale=8, train_wall=11, gb_free=19.3, wall=1493
2022-12-09 12:02:51 | INFO | train_inner | epoch 011:    584 / 1102 loss=8.823, nll_loss=5.237, ppl=37.71, wps=33231.7, ups=9.27, wpb=3584.7, bsz=152.8, num_updates=11600, lr=0.00029361, gnorm=1.772, loss_scale=8, train_wall=11, gb_free=19.3, wall=1504
2022-12-09 12:03:02 | INFO | train_inner | epoch 011:    684 / 1102 loss=8.777, nll_loss=5.18, ppl=36.26, wps=33269.5, ups=9.37, wpb=3551.7, bsz=152.8, num_updates=11700, lr=0.000292353, gnorm=1.773, loss_scale=8, train_wall=10, gb_free=19.4, wall=1514
2022-12-09 12:03:12 | INFO | train_inner | epoch 011:    784 / 1102 loss=8.911, nll_loss=5.32, ppl=39.96, wps=32953.5, ups=9.29, wpb=3545.7, bsz=134.3, num_updates=11800, lr=0.000291111, gnorm=1.852, loss_scale=8, train_wall=11, gb_free=19.8, wall=1525
2022-12-09 12:03:23 | INFO | train_inner | epoch 011:    884 / 1102 loss=8.883, nll_loss=5.298, ppl=39.33, wps=32525.2, ups=9.17, wpb=3545.2, bsz=142.5, num_updates=11900, lr=0.000289886, gnorm=1.824, loss_scale=8, train_wall=11, gb_free=19.3, wall=1536
2022-12-09 12:03:34 | INFO | train_inner | epoch 011:    984 / 1102 loss=8.839, nll_loss=5.27, ppl=38.58, wps=33724.3, ups=9.25, wpb=3647.5, bsz=142.6, num_updates=12000, lr=0.000288675, gnorm=1.677, loss_scale=8, train_wall=11, gb_free=19.8, wall=1547
2022-12-09 12:03:45 | INFO | train_inner | epoch 011:   1084 / 1102 loss=8.924, nll_loss=5.355, ppl=40.93, wps=33444.4, ups=9.37, wpb=3567.5, bsz=145, num_updates=12100, lr=0.00028748, gnorm=1.79, loss_scale=8, train_wall=10, gb_free=19.4, wall=1557
2022-12-09 12:03:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:04:23 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.094 | nll_loss 2.606 | ppl 6.09 | bleu 31.87 | wps 4903.2 | wpb 2835.3 | bsz 115.6 | num_updates 12118 | best_bleu 31.87
2022-12-09 12:04:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 12118 updates
2022-12-09 12:04:24 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint11.pt
2022-12-09 12:04:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint11.pt (epoch 11 @ 12118 updates, score 31.87) (writing took 1.6306600216776133 seconds)
2022-12-09 12:04:25 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-12-09 12:04:25 | INFO | train | epoch 011 | loss 8.869 | nll_loss 5.284 | ppl 38.96 | wps 25054 | ups 6.99 | wpb 3583.6 | bsz 145.4 | num_updates 12118 | lr 0.000287266 | gnorm 1.777 | loss_scale 8 | train_wall 116 | gb_free 19.4 | wall 1598
2022-12-09 12:04:25 | INFO | fairseq.trainer | begin training epoch 12
2022-12-09 12:04:34 | INFO | train_inner | epoch 012:     82 / 1102 loss=8.702, nll_loss=5.097, ppl=34.22, wps=7258.4, ups=2.02, wpb=3588.4, bsz=143.4, num_updates=12200, lr=0.000286299, gnorm=1.82, loss_scale=8, train_wall=11, gb_free=19.5, wall=1607
2022-12-09 12:04:45 | INFO | train_inner | epoch 012:    182 / 1102 loss=8.747, nll_loss=5.145, ppl=35.38, wps=33745.6, ups=9.3, wpb=3627.2, bsz=135.3, num_updates=12300, lr=0.000285133, gnorm=1.744, loss_scale=8, train_wall=11, gb_free=19.4, wall=1618
2022-12-09 12:04:56 | INFO | train_inner | epoch 012:    282 / 1102 loss=8.697, nll_loss=5.114, ppl=34.64, wps=33870.5, ups=9.26, wpb=3655.9, bsz=165.4, num_updates=12400, lr=0.000283981, gnorm=1.8, loss_scale=8, train_wall=11, gb_free=19.3, wall=1628
2022-12-09 12:05:07 | INFO | train_inner | epoch 012:    382 / 1102 loss=8.77, nll_loss=5.178, ppl=36.21, wps=33432.9, ups=9.29, wpb=3599, bsz=141.2, num_updates=12500, lr=0.000282843, gnorm=1.78, loss_scale=8, train_wall=11, gb_free=19.6, wall=1639
2022-12-09 12:05:17 | INFO | train_inner | epoch 012:    482 / 1102 loss=8.771, nll_loss=5.171, ppl=36.02, wps=32670.5, ups=9.17, wpb=3561.7, bsz=140.4, num_updates=12600, lr=0.000281718, gnorm=1.717, loss_scale=8, train_wall=11, gb_free=19.5, wall=1650
2022-12-09 12:05:28 | INFO | train_inner | epoch 012:    582 / 1102 loss=8.607, nll_loss=5.009, ppl=32.19, wps=32747.1, ups=9.32, wpb=3515, bsz=158.3, num_updates=12700, lr=0.000280607, gnorm=1.747, loss_scale=8, train_wall=11, gb_free=19.7, wall=1661
2022-12-09 12:05:39 | INFO | train_inner | epoch 012:    682 / 1102 loss=8.636, nll_loss=5.049, ppl=33.11, wps=33588, ups=9.31, wpb=3607.6, bsz=159.8, num_updates=12800, lr=0.000279508, gnorm=1.731, loss_scale=8, train_wall=11, gb_free=19.7, wall=1671
2022-12-09 12:05:50 | INFO | train_inner | epoch 012:    782 / 1102 loss=8.735, nll_loss=5.146, ppl=35.41, wps=33786.4, ups=9.27, wpb=3643.4, bsz=144, num_updates=12900, lr=0.000278423, gnorm=1.716, loss_scale=8, train_wall=11, gb_free=19.5, wall=1682
2022-12-09 12:06:00 | INFO | train_inner | epoch 012:    882 / 1102 loss=8.799, nll_loss=5.218, ppl=37.21, wps=33931, ups=9.42, wpb=3603.4, bsz=134.3, num_updates=13000, lr=0.00027735, gnorm=1.805, loss_scale=8, train_wall=10, gb_free=19.7, wall=1693
2022-12-09 12:06:11 | INFO | train_inner | epoch 012:    982 / 1102 loss=8.781, nll_loss=5.19, ppl=36.51, wps=32769, ups=9.34, wpb=3508.1, bsz=139.9, num_updates=13100, lr=0.000276289, gnorm=1.829, loss_scale=8, train_wall=10, gb_free=19.2, wall=1704
2022-12-09 12:06:22 | INFO | train_inner | epoch 012:   1082 / 1102 loss=8.804, nll_loss=5.224, ppl=37.38, wps=33145.1, ups=9.36, wpb=3541.2, bsz=135.8, num_updates=13200, lr=0.000275241, gnorm=1.706, loss_scale=8, train_wall=10, gb_free=19.8, wall=1714
2022-12-09 12:06:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:07:01 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.055 | nll_loss 2.561 | ppl 5.9 | bleu 32.25 | wps 4880.1 | wpb 2835.3 | bsz 115.6 | num_updates 13220 | best_bleu 32.25
2022-12-09 12:07:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 13220 updates
2022-12-09 12:07:02 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint12.pt
2022-12-09 12:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint12.pt (epoch 12 @ 13220 updates, score 32.25) (writing took 1.7408546935766935 seconds)
2022-12-09 12:07:03 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-12-09 12:07:03 | INFO | train | epoch 012 | loss 8.732 | nll_loss 5.141 | ppl 35.28 | wps 25064 | ups 6.99 | wpb 3583.6 | bsz 145.4 | num_updates 13220 | lr 0.000275033 | gnorm 1.763 | loss_scale 8 | train_wall 116 | gb_free 19.2 | wall 1755
2022-12-09 12:07:03 | INFO | fairseq.trainer | begin training epoch 13
2022-12-09 12:07:11 | INFO | train_inner | epoch 013:     80 / 1102 loss=8.624, nll_loss=5.023, ppl=32.52, wps=7199.9, ups=2.01, wpb=3579.6, bsz=147.6, num_updates=13300, lr=0.000274204, gnorm=1.844, loss_scale=8, train_wall=10, gb_free=19.5, wall=1764
2022-12-09 12:07:22 | INFO | train_inner | epoch 013:    180 / 1102 loss=8.58, nll_loss=4.972, ppl=31.38, wps=33622.6, ups=9.2, wpb=3652.9, bsz=148.4, num_updates=13400, lr=0.000273179, gnorm=1.772, loss_scale=8, train_wall=11, gb_free=19.4, wall=1775
2022-12-09 12:07:33 | INFO | train_inner | epoch 013:    280 / 1102 loss=8.549, nll_loss=4.946, ppl=30.82, wps=33661.3, ups=9.43, wpb=3569.5, bsz=156.8, num_updates=13500, lr=0.000272166, gnorm=1.789, loss_scale=8, train_wall=10, gb_free=19.5, wall=1785
2022-12-09 12:07:44 | INFO | train_inner | epoch 013:    380 / 1102 loss=8.575, nll_loss=4.972, ppl=31.38, wps=33221.8, ups=9.33, wpb=3558.9, bsz=148.2, num_updates=13600, lr=0.000271163, gnorm=1.779, loss_scale=8, train_wall=10, gb_free=19.3, wall=1796
2022-12-09 12:07:54 | INFO | train_inner | epoch 013:    480 / 1102 loss=8.646, nll_loss=5.047, ppl=33.06, wps=33658, ups=9.4, wpb=3580.8, bsz=146.9, num_updates=13700, lr=0.000270172, gnorm=1.736, loss_scale=8, train_wall=10, gb_free=19.6, wall=1807
2022-12-09 12:08:05 | INFO | train_inner | epoch 013:    580 / 1102 loss=8.564, nll_loss=4.961, ppl=31.15, wps=33565.4, ups=9.29, wpb=3612.3, bsz=154.9, num_updates=13800, lr=0.000269191, gnorm=1.727, loss_scale=8, train_wall=11, gb_free=19.4, wall=1818
2022-12-09 12:08:16 | INFO | train_inner | epoch 013:    680 / 1102 loss=8.751, nll_loss=5.143, ppl=35.34, wps=32262.2, ups=9.12, wpb=3538.2, bsz=118.6, num_updates=13900, lr=0.000268221, gnorm=1.777, loss_scale=8, train_wall=11, gb_free=19.4, wall=1829
2022-12-09 12:08:27 | INFO | train_inner | epoch 013:    780 / 1102 loss=8.615, nll_loss=5.013, ppl=32.3, wps=33186.6, ups=9.21, wpb=3603.2, bsz=141.9, num_updates=14000, lr=0.000267261, gnorm=1.704, loss_scale=8, train_wall=11, gb_free=19.5, wall=1839
2022-12-09 12:08:37 | INFO | train_inner | epoch 013:    880 / 1102 loss=8.678, nll_loss=5.076, ppl=33.73, wps=33328, ups=9.4, wpb=3543.8, bsz=135.8, num_updates=14100, lr=0.000266312, gnorm=1.77, loss_scale=8, train_wall=10, gb_free=19.6, wall=1850
2022-12-09 12:08:48 | INFO | train_inner | epoch 013:    980 / 1102 loss=8.663, nll_loss=5.082, ppl=33.87, wps=33087, ups=9.38, wpb=3527.4, bsz=152.5, num_updates=14200, lr=0.000265372, gnorm=1.825, loss_scale=8, train_wall=10, gb_free=20.2, wall=1861
2022-12-09 12:08:59 | INFO | train_inner | epoch 013:   1080 / 1102 loss=8.602, nll_loss=5.019, ppl=32.41, wps=33585.9, ups=9.27, wpb=3624.4, bsz=150, num_updates=14300, lr=0.000264443, gnorm=1.727, loss_scale=8, train_wall=11, gb_free=19.5, wall=1871
2022-12-09 12:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:09:37 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.01 | nll_loss 2.511 | ppl 5.7 | bleu 32.76 | wps 5067.5 | wpb 2835.3 | bsz 115.6 | num_updates 14322 | best_bleu 32.76
2022-12-09 12:09:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 14322 updates
2022-12-09 12:09:38 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint13.pt
2022-12-09 12:09:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint13.pt (epoch 13 @ 14322 updates, score 32.76) (writing took 1.7693244935944676 seconds)
2022-12-09 12:09:39 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-12-09 12:09:39 | INFO | train | epoch 013 | loss 8.62 | nll_loss 5.021 | ppl 32.46 | wps 25280.2 | ups 7.05 | wpb 3583.6 | bsz 145.4 | num_updates 14322 | lr 0.00026424 | gnorm 1.767 | loss_scale 8 | train_wall 116 | gb_free 19.4 | wall 1911
2022-12-09 12:09:39 | INFO | fairseq.trainer | begin training epoch 14
2022-12-09 12:09:48 | INFO | train_inner | epoch 014:     78 / 1102 loss=8.578, nll_loss=4.977, ppl=31.5, wps=7413.8, ups=2.05, wpb=3608.7, bsz=135.6, num_updates=14400, lr=0.000263523, gnorm=1.695, loss_scale=8, train_wall=11, gb_free=19.5, wall=1920
2022-12-09 12:09:59 | INFO | train_inner | epoch 014:    178 / 1102 loss=8.392, nll_loss=4.776, ppl=27.4, wps=32719.1, ups=9.03, wpb=3621.6, bsz=153, num_updates=14500, lr=0.000262613, gnorm=1.692, loss_scale=8, train_wall=11, gb_free=19.3, wall=1931
2022-12-09 12:10:10 | INFO | train_inner | epoch 014:    278 / 1102 loss=8.41, nll_loss=4.793, ppl=27.71, wps=32596.5, ups=9.2, wpb=3542, bsz=151.8, num_updates=14600, lr=0.000261712, gnorm=1.719, loss_scale=8, train_wall=11, gb_free=19.4, wall=1942
2022-12-09 12:10:20 | INFO | train_inner | epoch 014:    378 / 1102 loss=8.518, nll_loss=4.903, ppl=29.91, wps=32939.6, ups=9.24, wpb=3563.5, bsz=149.4, num_updates=14700, lr=0.00026082, gnorm=1.794, loss_scale=8, train_wall=11, gb_free=19.7, wall=1953
2022-12-09 12:10:31 | INFO | train_inner | epoch 014:    478 / 1102 loss=8.566, nll_loss=4.954, ppl=30.99, wps=33607.2, ups=9.3, wpb=3613.6, bsz=138.6, num_updates=14800, lr=0.000259938, gnorm=1.818, loss_scale=8, train_wall=11, gb_free=19.5, wall=1964
2022-12-09 12:10:42 | INFO | train_inner | epoch 014:    578 / 1102 loss=8.672, nll_loss=5.078, ppl=33.78, wps=33018.7, ups=9.2, wpb=3590.8, bsz=132.5, num_updates=14900, lr=0.000259064, gnorm=1.699, loss_scale=8, train_wall=11, gb_free=19.3, wall=1975
2022-12-09 12:10:53 | INFO | train_inner | epoch 014:    678 / 1102 loss=8.571, nll_loss=4.971, ppl=31.36, wps=32678.5, ups=9.23, wpb=3540.7, bsz=143.8, num_updates=15000, lr=0.000258199, gnorm=1.717, loss_scale=8, train_wall=11, gb_free=19.4, wall=1985
2022-12-09 12:11:03 | INFO | train_inner | epoch 014:    778 / 1102 loss=8.561, nll_loss=4.958, ppl=31.08, wps=33085.2, ups=9.37, wpb=3532.8, bsz=140, num_updates=15100, lr=0.000257343, gnorm=1.739, loss_scale=8, train_wall=10, gb_free=19.7, wall=1996
2022-12-09 12:11:14 | INFO | train_inner | epoch 014:    878 / 1102 loss=8.476, nll_loss=4.874, ppl=29.32, wps=32776, ups=9.09, wpb=3606.3, bsz=159.3, num_updates=15200, lr=0.000256495, gnorm=1.757, loss_scale=8, train_wall=11, gb_free=19.6, wall=2007
2022-12-09 12:11:26 | INFO | train_inner | epoch 014:    978 / 1102 loss=8.575, nll_loss=4.975, ppl=31.45, wps=32551.5, ups=9.06, wpb=3592.1, bsz=141, num_updates=15300, lr=0.000255655, gnorm=1.742, loss_scale=8, train_wall=11, gb_free=19.4, wall=2018
2022-12-09 12:11:37 | INFO | train_inner | epoch 014:   1078 / 1102 loss=8.463, nll_loss=4.862, ppl=29.08, wps=32635.4, ups=9.03, wpb=3613.6, bsz=143.3, num_updates=15400, lr=0.000254824, gnorm=1.687, loss_scale=8, train_wall=11, gb_free=19.3, wall=2029
2022-12-09 12:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:12:17 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.991 | nll_loss 2.495 | ppl 5.64 | bleu 32.96 | wps 4801 | wpb 2835.3 | bsz 115.6 | num_updates 15424 | best_bleu 32.96
2022-12-09 12:12:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 15424 updates
2022-12-09 12:12:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint14.pt
2022-12-09 12:12:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint14.pt (epoch 14 @ 15424 updates, score 32.96) (writing took 1.768126968294382 seconds)
2022-12-09 12:12:19 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-12-09 12:12:19 | INFO | train | epoch 014 | loss 8.517 | nll_loss 4.912 | ppl 30.11 | wps 24711.1 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 15424 | lr 0.000254625 | gnorm 1.732 | loss_scale 8 | train_wall 117 | gb_free 19.3 | wall 2071
2022-12-09 12:12:19 | INFO | fairseq.trainer | begin training epoch 15
2022-12-09 12:12:27 | INFO | train_inner | epoch 015:     76 / 1102 loss=8.359, nll_loss=4.743, ppl=26.79, wps=7144.4, ups=1.99, wpb=3596, bsz=148.1, num_updates=15500, lr=0.000254, gnorm=1.722, loss_scale=8, train_wall=10, gb_free=19.3, wall=2080
2022-12-09 12:12:38 | INFO | train_inner | epoch 015:    176 / 1102 loss=8.397, nll_loss=4.762, ppl=27.13, wps=33028, ups=9.35, wpb=3534.1, bsz=147.1, num_updates=15600, lr=0.000253185, gnorm=1.84, loss_scale=8, train_wall=10, gb_free=19.5, wall=2090
2022-12-09 12:12:48 | INFO | train_inner | epoch 015:    276 / 1102 loss=8.392, nll_loss=4.771, ppl=27.31, wps=33308.3, ups=9.38, wpb=3549.9, bsz=147.8, num_updates=15700, lr=0.000252377, gnorm=1.719, loss_scale=8, train_wall=10, gb_free=19.7, wall=2101
2022-12-09 12:12:59 | INFO | train_inner | epoch 015:    376 / 1102 loss=8.427, nll_loss=4.813, ppl=28.1, wps=32833.6, ups=9.13, wpb=3598.1, bsz=142.8, num_updates=15800, lr=0.000251577, gnorm=1.727, loss_scale=8, train_wall=11, gb_free=19.3, wall=2112
2022-12-09 12:13:10 | INFO | train_inner | epoch 015:    476 / 1102 loss=8.481, nll_loss=4.86, ppl=29.03, wps=33061.3, ups=9.23, wpb=3580.4, bsz=146.6, num_updates=15900, lr=0.000250785, gnorm=1.893, loss_scale=8, train_wall=11, gb_free=19.5, wall=2123
2022-12-09 12:13:21 | INFO | train_inner | epoch 015:    576 / 1102 loss=8.411, nll_loss=4.8, ppl=27.86, wps=33168.3, ups=9.4, wpb=3528.8, bsz=151.5, num_updates=16000, lr=0.00025, gnorm=1.714, loss_scale=8, train_wall=10, gb_free=19.3, wall=2133
2022-12-09 12:13:31 | INFO | train_inner | epoch 015:    676 / 1102 loss=8.359, nll_loss=4.765, ppl=27.18, wps=33319.6, ups=9.26, wpb=3598, bsz=161.2, num_updates=16100, lr=0.000249222, gnorm=1.691, loss_scale=8, train_wall=11, gb_free=19.4, wall=2144
2022-12-09 12:13:42 | INFO | train_inner | epoch 015:    776 / 1102 loss=8.501, nll_loss=4.887, ppl=29.58, wps=33637.5, ups=9.31, wpb=3613, bsz=138.4, num_updates=16200, lr=0.000248452, gnorm=1.758, loss_scale=8, train_wall=11, gb_free=19.7, wall=2155
2022-12-09 12:13:53 | INFO | train_inner | epoch 015:    876 / 1102 loss=8.5, nll_loss=4.902, ppl=29.9, wps=33163.3, ups=9.16, wpb=3619.1, bsz=140.2, num_updates=16300, lr=0.000247689, gnorm=1.66, loss_scale=8, train_wall=11, gb_free=19.5, wall=2166
2022-12-09 12:14:04 | INFO | train_inner | epoch 015:    976 / 1102 loss=8.466, nll_loss=4.866, ppl=29.15, wps=32891.8, ups=9.17, wpb=3585.2, bsz=143.6, num_updates=16400, lr=0.000246932, gnorm=1.731, loss_scale=8, train_wall=11, gb_free=19.3, wall=2177
2022-12-09 12:14:15 | INFO | train_inner | epoch 015:   1076 / 1102 loss=8.4, nll_loss=4.793, ppl=27.72, wps=33530.9, ups=9.32, wpb=3596.5, bsz=146.3, num_updates=16500, lr=0.000246183, gnorm=1.74, loss_scale=8, train_wall=10, gb_free=19.3, wall=2187
2022-12-09 12:14:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:14:52 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.97 | nll_loss 2.464 | ppl 5.52 | bleu 33.36 | wps 5262.7 | wpb 2835.3 | bsz 115.6 | num_updates 16526 | best_bleu 33.36
2022-12-09 12:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 16526 updates
2022-12-09 12:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint15.pt
2022-12-09 12:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint15.pt (epoch 15 @ 16526 updates, score 33.36) (writing took 1.5761390253901482 seconds)
2022-12-09 12:14:54 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-12-09 12:14:54 | INFO | train | epoch 015 | loss 8.432 | nll_loss 4.819 | ppl 28.23 | wps 25478.1 | ups 7.11 | wpb 3583.6 | bsz 145.4 | num_updates 16526 | lr 0.000245989 | gnorm 1.752 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 2226
2022-12-09 12:14:54 | INFO | fairseq.trainer | begin training epoch 16
2022-12-09 12:15:02 | INFO | train_inner | epoch 016:     74 / 1102 loss=8.443, nll_loss=4.824, ppl=28.32, wps=7613.7, ups=2.12, wpb=3591.1, bsz=131.8, num_updates=16600, lr=0.00024544, gnorm=1.784, loss_scale=8, train_wall=11, gb_free=19.3, wall=2235
2022-12-09 12:15:13 | INFO | train_inner | epoch 016:    174 / 1102 loss=8.263, nll_loss=4.634, ppl=24.83, wps=33777.3, ups=9.41, wpb=3591, bsz=146.8, num_updates=16700, lr=0.000244704, gnorm=1.665, loss_scale=8, train_wall=10, gb_free=19.4, wall=2245
2022-12-09 12:15:23 | INFO | train_inner | epoch 016:    274 / 1102 loss=8.403, nll_loss=4.766, ppl=27.21, wps=32711.4, ups=9.2, wpb=3556.9, bsz=134.3, num_updates=16800, lr=0.000243975, gnorm=1.734, loss_scale=8, train_wall=11, gb_free=19.8, wall=2256
2022-12-09 12:15:34 | INFO | train_inner | epoch 016:    374 / 1102 loss=8.376, nll_loss=4.747, ppl=26.86, wps=33225.5, ups=9.4, wpb=3534.9, bsz=140.9, num_updates=16900, lr=0.000243252, gnorm=1.77, loss_scale=8, train_wall=10, gb_free=19.3, wall=2267
2022-12-09 12:15:45 | INFO | train_inner | epoch 016:    474 / 1102 loss=8.328, nll_loss=4.709, ppl=26.16, wps=33843.2, ups=9.45, wpb=3582.1, bsz=152.7, num_updates=17000, lr=0.000242536, gnorm=1.732, loss_scale=8, train_wall=10, gb_free=19.2, wall=2277
2022-12-09 12:15:55 | INFO | train_inner | epoch 016:    574 / 1102 loss=8.356, nll_loss=4.731, ppl=26.57, wps=33294.6, ups=9.36, wpb=3556.3, bsz=150.3, num_updates=17100, lr=0.000241825, gnorm=1.787, loss_scale=8, train_wall=10, gb_free=19.4, wall=2288
2022-12-09 12:16:06 | INFO | train_inner | epoch 016:    674 / 1102 loss=8.341, nll_loss=4.727, ppl=26.48, wps=33980.8, ups=9.31, wpb=3649.2, bsz=139.7, num_updates=17200, lr=0.000241121, gnorm=1.75, loss_scale=8, train_wall=11, gb_free=19.4, wall=2299
2022-12-09 12:16:17 | INFO | train_inner | epoch 016:    774 / 1102 loss=8.375, nll_loss=4.745, ppl=26.82, wps=33798.1, ups=9.29, wpb=3638.2, bsz=133, num_updates=17300, lr=0.000240424, gnorm=1.781, loss_scale=8, train_wall=11, gb_free=19.5, wall=2309
2022-12-09 12:16:28 | INFO | train_inner | epoch 016:    874 / 1102 loss=8.467, nll_loss=4.862, ppl=29.08, wps=33668.5, ups=9.3, wpb=3618.8, bsz=139, num_updates=17400, lr=0.000239732, gnorm=1.749, loss_scale=8, train_wall=11, gb_free=19.8, wall=2320
2022-12-09 12:16:38 | INFO | train_inner | epoch 016:    974 / 1102 loss=8.274, nll_loss=4.677, ppl=25.58, wps=32995.2, ups=9.35, wpb=3527.8, bsz=168.2, num_updates=17500, lr=0.000239046, gnorm=1.718, loss_scale=16, train_wall=10, gb_free=19.5, wall=2331
2022-12-09 12:16:49 | INFO | train_inner | epoch 016:   1074 / 1102 loss=8.262, nll_loss=4.651, ppl=25.12, wps=33545.9, ups=9.31, wpb=3604.2, bsz=157, num_updates=17600, lr=0.000238366, gnorm=1.724, loss_scale=16, train_wall=11, gb_free=19.5, wall=2342
2022-12-09 12:16:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:17:27 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.942 | nll_loss 2.432 | ppl 5.4 | bleu 33.68 | wps 5141.8 | wpb 2835.3 | bsz 115.6 | num_updates 17628 | best_bleu 33.68
2022-12-09 12:17:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 17628 updates
2022-12-09 12:17:28 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint16.pt
2022-12-09 12:17:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint16.pt (epoch 16 @ 17628 updates, score 33.68) (writing took 1.7691370351240039 seconds)
2022-12-09 12:17:29 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-12-09 12:17:29 | INFO | train | epoch 016 | loss 8.35 | nll_loss 4.732 | ppl 26.58 | wps 25409.6 | ups 7.09 | wpb 3583.6 | bsz 145.4 | num_updates 17628 | lr 0.000238176 | gnorm 1.738 | loss_scale 16 | train_wall 116 | gb_free 19.3 | wall 2382
2022-12-09 12:17:29 | INFO | fairseq.trainer | begin training epoch 17
2022-12-09 12:17:37 | INFO | train_inner | epoch 017:     72 / 1102 loss=8.324, nll_loss=4.711, ppl=26.2, wps=7507.1, ups=2.07, wpb=3618, bsz=149.5, num_updates=17700, lr=0.000237691, gnorm=1.755, loss_scale=16, train_wall=11, gb_free=19.5, wall=2390
2022-12-09 12:17:48 | INFO | train_inner | epoch 017:    172 / 1102 loss=8.16, nll_loss=4.521, ppl=22.96, wps=33702, ups=9.31, wpb=3618.7, bsz=156.3, num_updates=17800, lr=0.000237023, gnorm=1.712, loss_scale=16, train_wall=11, gb_free=19.3, wall=2401
2022-12-09 12:17:59 | INFO | train_inner | epoch 017:    272 / 1102 loss=8.261, nll_loss=4.629, ppl=24.74, wps=33480.1, ups=9.34, wpb=3585.8, bsz=140.1, num_updates=17900, lr=0.00023636, gnorm=1.746, loss_scale=16, train_wall=10, gb_free=19.3, wall=2411
2022-12-09 12:18:09 | INFO | train_inner | epoch 017:    372 / 1102 loss=8.152, nll_loss=4.537, ppl=23.22, wps=33019, ups=9.31, wpb=3544.8, bsz=169.6, num_updates=18000, lr=0.000235702, gnorm=1.68, loss_scale=16, train_wall=11, gb_free=19.5, wall=2422
2022-12-09 12:18:20 | INFO | train_inner | epoch 017:    472 / 1102 loss=8.242, nll_loss=4.603, ppl=24.31, wps=33510.8, ups=9.43, wpb=3553.5, bsz=143.5, num_updates=18100, lr=0.00023505, gnorm=1.711, loss_scale=16, train_wall=10, gb_free=19.5, wall=2433
2022-12-09 12:18:31 | INFO | train_inner | epoch 017:    572 / 1102 loss=8.347, nll_loss=4.717, ppl=26.29, wps=33439.9, ups=9.3, wpb=3596.6, bsz=142.2, num_updates=18200, lr=0.000234404, gnorm=1.777, loss_scale=16, train_wall=11, gb_free=19.4, wall=2443
2022-12-09 12:18:41 | INFO | train_inner | epoch 017:    672 / 1102 loss=8.362, nll_loss=4.732, ppl=26.57, wps=33720.4, ups=9.48, wpb=3557.8, bsz=128.3, num_updates=18300, lr=0.000233762, gnorm=1.716, loss_scale=16, train_wall=10, gb_free=19.4, wall=2454
2022-12-09 12:18:52 | INFO | train_inner | epoch 017:    772 / 1102 loss=8.441, nll_loss=4.81, ppl=28.06, wps=32968.6, ups=9.27, wpb=3555, bsz=123.8, num_updates=18400, lr=0.000233126, gnorm=1.81, loss_scale=16, train_wall=11, gb_free=19.3, wall=2465
2022-12-09 12:19:03 | INFO | train_inner | epoch 017:    872 / 1102 loss=8.282, nll_loss=4.664, ppl=25.35, wps=32497.7, ups=9.11, wpb=3566, bsz=147.2, num_updates=18500, lr=0.000232495, gnorm=1.689, loss_scale=16, train_wall=11, gb_free=19.3, wall=2476
2022-12-09 12:19:14 | INFO | train_inner | epoch 017:    972 / 1102 loss=8.278, nll_loss=4.678, ppl=25.6, wps=33395.5, ups=9.17, wpb=3641, bsz=154.2, num_updates=18600, lr=0.000231869, gnorm=1.693, loss_scale=16, train_wall=11, gb_free=19.9, wall=2487
2022-12-09 12:19:25 | INFO | train_inner | epoch 017:   1072 / 1102 loss=8.282, nll_loss=4.661, ppl=25.3, wps=32521.1, ups=9.11, wpb=3567.9, bsz=146.8, num_updates=18700, lr=0.000231249, gnorm=1.703, loss_scale=16, train_wall=11, gb_free=19.5, wall=2498
2022-12-09 12:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:20:04 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.921 | nll_loss 2.403 | ppl 5.29 | bleu 33.96 | wps 5084.9 | wpb 2835.3 | bsz 115.6 | num_updates 18730 | best_bleu 33.96
2022-12-09 12:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 18730 updates
2022-12-09 12:20:05 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint17.pt
2022-12-09 12:20:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint17.pt (epoch 17 @ 18730 updates, score 33.96) (writing took 1.7326142555102706 seconds)
2022-12-09 12:20:05 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-12-09 12:20:05 | INFO | train | epoch 017 | loss 8.283 | nll_loss 4.658 | ppl 25.24 | wps 25231.1 | ups 7.04 | wpb 3583.6 | bsz 145.4 | num_updates 18730 | lr 0.000231063 | gnorm 1.727 | loss_scale 16 | train_wall 116 | gb_free 19.6 | wall 2538
2022-12-09 12:20:06 | INFO | fairseq.trainer | begin training epoch 18
2022-12-09 12:20:13 | INFO | train_inner | epoch 018:     70 / 1102 loss=8.181, nll_loss=4.549, ppl=23.4, wps=7436.3, ups=2.07, wpb=3586.7, bsz=144.8, num_updates=18800, lr=0.000230633, gnorm=1.661, loss_scale=16, train_wall=10, gb_free=19.8, wall=2546
2022-12-09 12:20:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-09 12:20:24 | INFO | train_inner | epoch 018:    171 / 1102 loss=8.1, nll_loss=4.464, ppl=22.07, wps=33365, ups=9.17, wpb=3638.6, bsz=151.8, num_updates=18900, lr=0.000230022, gnorm=1.687, loss_scale=8, train_wall=11, gb_free=19.2, wall=2557
2022-12-09 12:20:35 | INFO | train_inner | epoch 018:    271 / 1102 loss=8.154, nll_loss=4.508, ppl=22.75, wps=33184.8, ups=9.35, wpb=3549.9, bsz=150.4, num_updates=19000, lr=0.000229416, gnorm=1.775, loss_scale=8, train_wall=10, gb_free=19.6, wall=2567
2022-12-09 12:20:45 | INFO | train_inner | epoch 018:    371 / 1102 loss=8.203, nll_loss=4.562, ppl=23.63, wps=33318.4, ups=9.36, wpb=3558.5, bsz=141.3, num_updates=19100, lr=0.000228814, gnorm=1.707, loss_scale=8, train_wall=10, gb_free=19.3, wall=2578
2022-12-09 12:20:56 | INFO | train_inner | epoch 018:    471 / 1102 loss=8.285, nll_loss=4.652, ppl=25.15, wps=33608.7, ups=9.44, wpb=3561.8, bsz=141, num_updates=19200, lr=0.000228218, gnorm=1.756, loss_scale=8, train_wall=10, gb_free=19.4, wall=2589
2022-12-09 12:21:07 | INFO | train_inner | epoch 018:    571 / 1102 loss=8.313, nll_loss=4.683, ppl=25.69, wps=33526.5, ups=9.38, wpb=3576, bsz=140.6, num_updates=19300, lr=0.000227626, gnorm=1.787, loss_scale=8, train_wall=10, gb_free=19.7, wall=2599
2022-12-09 12:21:18 | INFO | train_inner | epoch 018:    671 / 1102 loss=8.127, nll_loss=4.495, ppl=22.55, wps=32888.3, ups=9.08, wpb=3624, bsz=153.8, num_updates=19400, lr=0.000227038, gnorm=1.737, loss_scale=8, train_wall=11, gb_free=19.5, wall=2610
2022-12-09 12:21:29 | INFO | train_inner | epoch 018:    771 / 1102 loss=8.211, nll_loss=4.593, ppl=24.14, wps=33001.9, ups=9.28, wpb=3554.8, bsz=154.6, num_updates=19500, lr=0.000226455, gnorm=1.713, loss_scale=8, train_wall=11, gb_free=19.4, wall=2621
2022-12-09 12:21:39 | INFO | train_inner | epoch 018:    871 / 1102 loss=8.255, nll_loss=4.628, ppl=24.73, wps=32986, ups=9.23, wpb=3573.2, bsz=147.3, num_updates=19600, lr=0.000225877, gnorm=1.855, loss_scale=8, train_wall=11, gb_free=19.4, wall=2632
2022-12-09 12:21:50 | INFO | train_inner | epoch 018:    971 / 1102 loss=8.317, nll_loss=4.7, ppl=25.98, wps=33448.1, ups=9.35, wpb=3576.8, bsz=138.9, num_updates=19700, lr=0.000225303, gnorm=1.792, loss_scale=8, train_wall=10, gb_free=19.4, wall=2643
2022-12-09 12:22:01 | INFO | train_inner | epoch 018:   1071 / 1102 loss=8.286, nll_loss=4.654, ppl=25.17, wps=33842.4, ups=9.34, wpb=3623.3, bsz=134.7, num_updates=19800, lr=0.000224733, gnorm=1.75, loss_scale=8, train_wall=10, gb_free=19.3, wall=2653
2022-12-09 12:22:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:22:41 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.901 | nll_loss 2.389 | ppl 5.24 | bleu 34.08 | wps 4958.2 | wpb 2835.3 | bsz 115.6 | num_updates 19831 | best_bleu 34.08
2022-12-09 12:22:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 19831 updates
2022-12-09 12:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint18.pt
2022-12-09 12:22:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint18.pt (epoch 18 @ 19831 updates, score 34.08) (writing took 1.744270520284772 seconds)
2022-12-09 12:22:42 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-12-09 12:22:42 | INFO | train | epoch 018 | loss 8.222 | nll_loss 4.591 | ppl 24.1 | wps 25149.2 | ups 7.02 | wpb 3583.8 | bsz 145.5 | num_updates 19831 | lr 0.000224558 | gnorm 1.751 | loss_scale 8 | train_wall 116 | gb_free 19.4 | wall 2695
2022-12-09 12:22:42 | INFO | fairseq.trainer | begin training epoch 19
2022-12-09 12:22:50 | INFO | train_inner | epoch 019:     69 / 1102 loss=8.206, nll_loss=4.57, ppl=23.75, wps=7314.7, ups=2.03, wpb=3607.6, bsz=143.2, num_updates=19900, lr=0.000224168, gnorm=1.78, loss_scale=8, train_wall=11, gb_free=19.7, wall=2703
2022-12-09 12:23:01 | INFO | train_inner | epoch 019:    169 / 1102 loss=8.211, nll_loss=4.571, ppl=23.76, wps=33424.9, ups=9.46, wpb=3532.4, bsz=140.9, num_updates=20000, lr=0.000223607, gnorm=1.779, loss_scale=8, train_wall=10, gb_free=19.6, wall=2713
2022-12-09 12:23:11 | INFO | train_inner | epoch 019:    269 / 1102 loss=8.142, nll_loss=4.502, ppl=22.66, wps=33434.1, ups=9.26, wpb=3612.2, bsz=145.4, num_updates=20100, lr=0.00022305, gnorm=1.813, loss_scale=8, train_wall=11, gb_free=19.5, wall=2724
2022-12-09 12:23:22 | INFO | train_inner | epoch 019:    369 / 1102 loss=8.231, nll_loss=4.575, ppl=23.84, wps=33503.9, ups=9.38, wpb=3572.1, bsz=132.2, num_updates=20200, lr=0.000222497, gnorm=1.816, loss_scale=8, train_wall=10, gb_free=19.3, wall=2735
2022-12-09 12:23:33 | INFO | train_inner | epoch 019:    469 / 1102 loss=8.137, nll_loss=4.487, ppl=22.42, wps=34105.4, ups=9.36, wpb=3643, bsz=141, num_updates=20300, lr=0.000221948, gnorm=1.769, loss_scale=8, train_wall=10, gb_free=19.6, wall=2745
2022-12-09 12:23:44 | INFO | train_inner | epoch 019:    569 / 1102 loss=8.127, nll_loss=4.506, ppl=22.72, wps=33046.1, ups=9.29, wpb=3558.8, bsz=161.7, num_updates=20400, lr=0.000221404, gnorm=1.696, loss_scale=8, train_wall=11, gb_free=19.4, wall=2756
2022-12-09 12:23:54 | INFO | train_inner | epoch 019:    669 / 1102 loss=8.174, nll_loss=4.542, ppl=23.3, wps=33195.9, ups=9.32, wpb=3562.3, bsz=149.5, num_updates=20500, lr=0.000220863, gnorm=1.7, loss_scale=8, train_wall=10, gb_free=19.4, wall=2767
2022-12-09 12:24:05 | INFO | train_inner | epoch 019:    769 / 1102 loss=8.13, nll_loss=4.496, ppl=22.56, wps=33565.7, ups=9.13, wpb=3677, bsz=146, num_updates=20600, lr=0.000220326, gnorm=1.714, loss_scale=8, train_wall=11, gb_free=19.3, wall=2778
2022-12-09 12:24:16 | INFO | train_inner | epoch 019:    869 / 1102 loss=8.074, nll_loss=4.452, ppl=21.89, wps=33090.8, ups=9.24, wpb=3581.8, bsz=165, num_updates=20700, lr=0.000219793, gnorm=1.716, loss_scale=8, train_wall=11, gb_free=19.4, wall=2789
2022-12-09 12:24:27 | INFO | train_inner | epoch 019:    969 / 1102 loss=8.282, nll_loss=4.643, ppl=24.98, wps=32535, ups=9.33, wpb=3488.2, bsz=130.2, num_updates=20800, lr=0.000219265, gnorm=1.746, loss_scale=8, train_wall=10, gb_free=19.4, wall=2799
2022-12-09 12:24:37 | INFO | train_inner | epoch 019:   1069 / 1102 loss=8.172, nll_loss=4.543, ppl=23.32, wps=33801.3, ups=9.43, wpb=3585.6, bsz=148.4, num_updates=20900, lr=0.000218739, gnorm=1.7, loss_scale=8, train_wall=10, gb_free=19.5, wall=2810
2022-12-09 12:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:25:17 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.891 | nll_loss 2.372 | ppl 5.18 | bleu 34.27 | wps 5043.2 | wpb 2835.3 | bsz 115.6 | num_updates 20933 | best_bleu 34.27
2022-12-09 12:25:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 20933 updates
2022-12-09 12:25:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint19.pt
2022-12-09 12:25:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint19.pt (epoch 19 @ 20933 updates, score 34.27) (writing took 1.7252299189567566 seconds)
2022-12-09 12:25:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-12-09 12:25:18 | INFO | train | epoch 019 | loss 8.167 | nll_loss 4.53 | ppl 23.11 | wps 25300.4 | ups 7.06 | wpb 3583.6 | bsz 145.4 | num_updates 20933 | lr 0.000218567 | gnorm 1.745 | loss_scale 8 | train_wall 116 | gb_free 19.3 | wall 2851
2022-12-09 12:25:19 | INFO | fairseq.trainer | begin training epoch 20
2022-12-09 12:25:26 | INFO | train_inner | epoch 020:     67 / 1102 loss=8.062, nll_loss=4.423, ppl=21.45, wps=7404.2, ups=2.05, wpb=3604.1, bsz=153, num_updates=21000, lr=0.000218218, gnorm=1.718, loss_scale=8, train_wall=11, gb_free=19.6, wall=2859
2022-12-09 12:25:37 | INFO | train_inner | epoch 020:    167 / 1102 loss=8.097, nll_loss=4.459, ppl=22, wps=33180.8, ups=9.29, wpb=3572, bsz=143.4, num_updates=21100, lr=0.0002177, gnorm=1.697, loss_scale=8, train_wall=11, gb_free=19.8, wall=2869
2022-12-09 12:25:48 | INFO | train_inner | epoch 020:    267 / 1102 loss=8.153, nll_loss=4.497, ppl=22.58, wps=33598.8, ups=9.4, wpb=3575.8, bsz=129, num_updates=21200, lr=0.000217186, gnorm=1.757, loss_scale=8, train_wall=10, gb_free=19.3, wall=2880
2022-12-09 12:25:58 | INFO | train_inner | epoch 020:    367 / 1102 loss=8.167, nll_loss=4.512, ppl=22.82, wps=32838.8, ups=9.39, wpb=3497.4, bsz=134, num_updates=21300, lr=0.000216676, gnorm=1.749, loss_scale=8, train_wall=10, gb_free=19.8, wall=2891
2022-12-09 12:26:09 | INFO | train_inner | epoch 020:    467 / 1102 loss=8.175, nll_loss=4.539, ppl=23.24, wps=33520.5, ups=9.32, wpb=3595.8, bsz=144.8, num_updates=21400, lr=0.000216169, gnorm=1.739, loss_scale=8, train_wall=10, gb_free=19.5, wall=2901
2022-12-09 12:26:20 | INFO | train_inner | epoch 020:    567 / 1102 loss=8.15, nll_loss=4.51, ppl=22.78, wps=33340.6, ups=9.2, wpb=3625.8, bsz=141.8, num_updates=21500, lr=0.000215666, gnorm=1.73, loss_scale=8, train_wall=11, gb_free=19.5, wall=2912
2022-12-09 12:26:31 | INFO | train_inner | epoch 020:    667 / 1102 loss=8.014, nll_loss=4.378, ppl=20.8, wps=33274.8, ups=9.16, wpb=3633.8, bsz=158.5, num_updates=21600, lr=0.000215166, gnorm=1.736, loss_scale=8, train_wall=11, gb_free=19.4, wall=2923
2022-12-09 12:26:41 | INFO | train_inner | epoch 020:    767 / 1102 loss=8.101, nll_loss=4.469, ppl=22.15, wps=33617.3, ups=9.36, wpb=3592, bsz=152.3, num_updates=21700, lr=0.000214669, gnorm=1.75, loss_scale=8, train_wall=10, gb_free=19.4, wall=2934
2022-12-09 12:26:52 | INFO | train_inner | epoch 020:    867 / 1102 loss=8.142, nll_loss=4.492, ppl=22.5, wps=33430.6, ups=9.38, wpb=3564.7, bsz=141, num_updates=21800, lr=0.000214176, gnorm=1.787, loss_scale=8, train_wall=10, gb_free=19.8, wall=2945
2022-12-09 12:27:03 | INFO | train_inner | epoch 020:    967 / 1102 loss=8.123, nll_loss=4.491, ppl=22.48, wps=33251, ups=9.39, wpb=3542.3, bsz=149, num_updates=21900, lr=0.000213687, gnorm=1.726, loss_scale=8, train_wall=10, gb_free=19.2, wall=2955
2022-12-09 12:27:13 | INFO | train_inner | epoch 020:   1067 / 1102 loss=8.067, nll_loss=4.43, ppl=21.56, wps=33466.4, ups=9.3, wpb=3600.4, bsz=148.6, num_updates=22000, lr=0.000213201, gnorm=1.774, loss_scale=8, train_wall=11, gb_free=19.6, wall=2966
2022-12-09 12:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:27:53 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.875 | nll_loss 2.358 | ppl 5.13 | bleu 34.31 | wps 5103.3 | wpb 2835.3 | bsz 115.6 | num_updates 22035 | best_bleu 34.31
2022-12-09 12:27:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 22035 updates
2022-12-09 12:27:54 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint20.pt
2022-12-09 12:27:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint20.pt (epoch 20 @ 22035 updates, score 34.31) (writing took 1.6987956026569009 seconds)
2022-12-09 12:27:54 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-12-09 12:27:54 | INFO | train | epoch 020 | loss 8.112 | nll_loss 4.471 | ppl 22.18 | wps 25324.7 | ups 7.07 | wpb 3583.6 | bsz 145.4 | num_updates 22035 | lr 0.000213031 | gnorm 1.742 | loss_scale 8 | train_wall 116 | gb_free 19.3 | wall 3007
2022-12-09 12:27:54 | INFO | fairseq.trainer | begin training epoch 21
2022-12-09 12:28:02 | INFO | train_inner | epoch 021:     65 / 1102 loss=8.049, nll_loss=4.413, ppl=21.3, wps=7467.4, ups=2.07, wpb=3600.6, bsz=148.2, num_updates=22100, lr=0.000212718, gnorm=1.682, loss_scale=8, train_wall=11, gb_free=19.3, wall=3014
2022-12-09 12:28:12 | INFO | train_inner | epoch 021:    165 / 1102 loss=8.063, nll_loss=4.399, ppl=21.1, wps=33171.9, ups=9.43, wpb=3516.7, bsz=140.2, num_updates=22200, lr=0.000212238, gnorm=1.759, loss_scale=8, train_wall=10, gb_free=19.4, wall=3025
2022-12-09 12:28:23 | INFO | train_inner | epoch 021:    265 / 1102 loss=8.081, nll_loss=4.433, ppl=21.6, wps=33016.5, ups=9.27, wpb=3561.8, bsz=140.8, num_updates=22300, lr=0.000211762, gnorm=1.693, loss_scale=8, train_wall=11, gb_free=19.8, wall=3036
2022-12-09 12:28:34 | INFO | train_inner | epoch 021:    365 / 1102 loss=8.078, nll_loss=4.417, ppl=21.37, wps=32799.4, ups=9.32, wpb=3517.9, bsz=135.4, num_updates=22400, lr=0.000211289, gnorm=1.759, loss_scale=8, train_wall=10, gb_free=19.5, wall=3046
2022-12-09 12:28:44 | INFO | train_inner | epoch 021:    465 / 1102 loss=8.023, nll_loss=4.369, ppl=20.67, wps=33815.4, ups=9.45, wpb=3577.2, bsz=150.2, num_updates=22500, lr=0.000210819, gnorm=1.734, loss_scale=8, train_wall=10, gb_free=19.7, wall=3057
2022-12-09 12:28:55 | INFO | train_inner | epoch 021:    565 / 1102 loss=7.962, nll_loss=4.317, ppl=19.94, wps=33953.5, ups=9.24, wpb=3672.9, bsz=163.4, num_updates=22600, lr=0.000210352, gnorm=1.693, loss_scale=8, train_wall=11, gb_free=19.4, wall=3068
2022-12-09 12:29:06 | INFO | train_inner | epoch 021:    665 / 1102 loss=8.082, nll_loss=4.457, ppl=21.96, wps=33879.3, ups=9.3, wpb=3642.3, bsz=150.7, num_updates=22700, lr=0.000209888, gnorm=1.681, loss_scale=8, train_wall=11, gb_free=19.6, wall=3079
2022-12-09 12:29:17 | INFO | train_inner | epoch 021:    765 / 1102 loss=8.153, nll_loss=4.499, ppl=22.6, wps=32900, ups=9.37, wpb=3511.2, bsz=134.4, num_updates=22800, lr=0.000209427, gnorm=1.749, loss_scale=8, train_wall=10, gb_free=19.3, wall=3089
2022-12-09 12:29:27 | INFO | train_inner | epoch 021:    865 / 1102 loss=8.125, nll_loss=4.489, ppl=22.45, wps=33639.8, ups=9.24, wpb=3638.7, bsz=137.9, num_updates=22900, lr=0.000208969, gnorm=1.667, loss_scale=8, train_wall=11, gb_free=19.8, wall=3100
2022-12-09 12:29:38 | INFO | train_inner | epoch 021:    965 / 1102 loss=8.012, nll_loss=4.38, ppl=20.82, wps=33415, ups=9.2, wpb=3633.9, bsz=159.4, num_updates=23000, lr=0.000208514, gnorm=1.633, loss_scale=8, train_wall=11, gb_free=19.7, wall=3111
2022-12-09 12:29:49 | INFO | train_inner | epoch 021:   1065 / 1102 loss=8.077, nll_loss=4.433, ppl=21.6, wps=33531.7, ups=9.32, wpb=3596.8, bsz=141.8, num_updates=23100, lr=0.000208063, gnorm=1.753, loss_scale=8, train_wall=11, gb_free=19.2, wall=3122
2022-12-09 12:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:30:29 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.859 | nll_loss 2.339 | ppl 5.06 | bleu 34.68 | wps 5045.2 | wpb 2835.3 | bsz 115.6 | num_updates 23137 | best_bleu 34.68
2022-12-09 12:30:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 23137 updates
2022-12-09 12:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint21.pt
2022-12-09 12:30:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint21.pt (epoch 21 @ 23137 updates, score 34.68) (writing took 1.706686643883586 seconds)
2022-12-09 12:30:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-12-09 12:30:31 | INFO | train | epoch 021 | loss 8.062 | nll_loss 4.417 | ppl 21.36 | wps 25296.3 | ups 7.06 | wpb 3583.6 | bsz 145.4 | num_updates 23137 | lr 0.000207896 | gnorm 1.708 | loss_scale 8 | train_wall 116 | gb_free 19.8 | wall 3163
2022-12-09 12:30:31 | INFO | fairseq.trainer | begin training epoch 22
2022-12-09 12:30:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-09 12:30:38 | INFO | train_inner | epoch 022:     64 / 1102 loss=7.984, nll_loss=4.331, ppl=20.13, wps=7388.3, ups=2.05, wpb=3596, bsz=140.2, num_updates=23200, lr=0.000207614, gnorm=1.703, loss_scale=4, train_wall=11, gb_free=19.8, wall=3170
2022-12-09 12:30:48 | INFO | train_inner | epoch 022:    164 / 1102 loss=7.966, nll_loss=4.315, ppl=19.9, wps=33524.5, ups=9.41, wpb=3561.8, bsz=158.4, num_updates=23300, lr=0.000207168, gnorm=1.723, loss_scale=4, train_wall=10, gb_free=19.8, wall=3181
2022-12-09 12:30:59 | INFO | train_inner | epoch 022:    264 / 1102 loss=7.995, nll_loss=4.339, ppl=20.24, wps=33860.4, ups=9.52, wpb=3558.2, bsz=143.8, num_updates=23400, lr=0.000206725, gnorm=1.71, loss_scale=4, train_wall=10, gb_free=20, wall=3191
2022-12-09 12:31:10 | INFO | train_inner | epoch 022:    364 / 1102 loss=7.901, nll_loss=4.254, ppl=19.08, wps=33687, ups=9.22, wpb=3652.6, bsz=160, num_updates=23500, lr=0.000206284, gnorm=1.613, loss_scale=4, train_wall=11, gb_free=19.6, wall=3202
2022-12-09 12:31:20 | INFO | train_inner | epoch 022:    464 / 1102 loss=8.053, nll_loss=4.395, ppl=21.04, wps=33667.5, ups=9.39, wpb=3585.9, bsz=141.1, num_updates=23600, lr=0.000205847, gnorm=1.751, loss_scale=4, train_wall=10, gb_free=19.3, wall=3213
2022-12-09 12:31:31 | INFO | train_inner | epoch 022:    564 / 1102 loss=7.948, nll_loss=4.306, ppl=19.77, wps=33572.3, ups=9.33, wpb=3597.9, bsz=161.5, num_updates=23700, lr=0.000205412, gnorm=1.725, loss_scale=4, train_wall=10, gb_free=19.8, wall=3224
2022-12-09 12:31:42 | INFO | train_inner | epoch 022:    664 / 1102 loss=8.043, nll_loss=4.39, ppl=20.97, wps=33377.4, ups=9.39, wpb=3552.8, bsz=141.1, num_updates=23800, lr=0.00020498, gnorm=1.774, loss_scale=4, train_wall=10, gb_free=19.3, wall=3234
2022-12-09 12:31:52 | INFO | train_inner | epoch 022:    764 / 1102 loss=8.068, nll_loss=4.418, ppl=21.38, wps=33383.3, ups=9.43, wpb=3539.1, bsz=142.2, num_updates=23900, lr=0.000204551, gnorm=1.734, loss_scale=4, train_wall=10, gb_free=19.5, wall=3245
2022-12-09 12:32:03 | INFO | train_inner | epoch 022:    864 / 1102 loss=8.09, nll_loss=4.438, ppl=21.68, wps=33400.1, ups=9.41, wpb=3547.9, bsz=134.7, num_updates=24000, lr=0.000204124, gnorm=1.8, loss_scale=4, train_wall=10, gb_free=19.9, wall=3256
2022-12-09 12:32:14 | INFO | train_inner | epoch 022:    964 / 1102 loss=8.173, nll_loss=4.521, ppl=22.95, wps=33061.6, ups=9.27, wpb=3567.6, bsz=124.2, num_updates=24100, lr=0.0002037, gnorm=1.776, loss_scale=4, train_wall=11, gb_free=19.5, wall=3266
2022-12-09 12:32:24 | INFO | train_inner | epoch 022:   1064 / 1102 loss=8.027, nll_loss=4.391, ppl=20.98, wps=33742.5, ups=9.37, wpb=3600.2, bsz=153.1, num_updates=24200, lr=0.000203279, gnorm=1.71, loss_scale=4, train_wall=10, gb_free=19.3, wall=3277
2022-12-09 12:32:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:33:05 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.853 | nll_loss 2.327 | ppl 5.02 | bleu 34.81 | wps 4946.7 | wpb 2835.3 | bsz 115.6 | num_updates 24238 | best_bleu 34.81
2022-12-09 12:33:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 24238 updates
2022-12-09 12:33:06 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint22.pt
2022-12-09 12:33:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint22.pt (epoch 22 @ 24238 updates, score 34.81) (writing took 1.7022389210760593 seconds)
2022-12-09 12:33:07 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-12-09 12:33:07 | INFO | train | epoch 022 | loss 8.02 | nll_loss 4.37 | ppl 20.67 | wps 25281.7 | ups 7.06 | wpb 3583.3 | bsz 145.3 | num_updates 24238 | lr 0.000203119 | gnorm 1.73 | loss_scale 4 | train_wall 115 | gb_free 19.7 | wall 3319
2022-12-09 12:33:07 | INFO | fairseq.trainer | begin training epoch 23
2022-12-09 12:33:14 | INFO | train_inner | epoch 023:     62 / 1102 loss=7.936, nll_loss=4.289, ppl=19.54, wps=7363.7, ups=2.03, wpb=3625.8, bsz=151.2, num_updates=24300, lr=0.00020286, gnorm=1.721, loss_scale=4, train_wall=11, gb_free=19.2, wall=3326
2022-12-09 12:33:24 | INFO | train_inner | epoch 023:    162 / 1102 loss=7.936, nll_loss=4.28, ppl=19.43, wps=33866, ups=9.37, wpb=3613.2, bsz=149.1, num_updates=24400, lr=0.000202444, gnorm=1.733, loss_scale=4, train_wall=10, gb_free=20, wall=3337
2022-12-09 12:33:35 | INFO | train_inner | epoch 023:    262 / 1102 loss=7.939, nll_loss=4.276, ppl=19.38, wps=33555.8, ups=9.39, wpb=3573.3, bsz=149.4, num_updates=24500, lr=0.000202031, gnorm=1.789, loss_scale=4, train_wall=10, gb_free=19.4, wall=3348
2022-12-09 12:33:46 | INFO | train_inner | epoch 023:    362 / 1102 loss=8.05, nll_loss=4.384, ppl=20.88, wps=32938, ups=9.21, wpb=3577.5, bsz=129.9, num_updates=24600, lr=0.000201619, gnorm=1.743, loss_scale=4, train_wall=11, gb_free=19.6, wall=3358
2022-12-09 12:33:56 | INFO | train_inner | epoch 023:    462 / 1102 loss=7.951, nll_loss=4.309, ppl=19.82, wps=33745.2, ups=9.45, wpb=3571.2, bsz=159, num_updates=24700, lr=0.000201211, gnorm=1.733, loss_scale=4, train_wall=10, gb_free=19.5, wall=3369
2022-12-09 12:34:07 | INFO | train_inner | epoch 023:    562 / 1102 loss=7.973, nll_loss=4.301, ppl=19.71, wps=32961.9, ups=9.27, wpb=3557.3, bsz=135.8, num_updates=24800, lr=0.000200805, gnorm=1.751, loss_scale=4, train_wall=11, gb_free=19.5, wall=3380
2022-12-09 12:34:18 | INFO | train_inner | epoch 023:    662 / 1102 loss=7.97, nll_loss=4.306, ppl=19.78, wps=33271.8, ups=9.26, wpb=3594, bsz=139.3, num_updates=24900, lr=0.000200401, gnorm=1.74, loss_scale=4, train_wall=11, gb_free=19.7, wall=3391
2022-12-09 12:34:29 | INFO | train_inner | epoch 023:    762 / 1102 loss=8.025, nll_loss=4.361, ppl=20.55, wps=33094.4, ups=9.24, wpb=3583.4, bsz=134.5, num_updates=25000, lr=0.0002, gnorm=1.841, loss_scale=4, train_wall=11, gb_free=19.7, wall=3401
2022-12-09 12:34:40 | INFO | train_inner | epoch 023:    862 / 1102 loss=8.015, nll_loss=4.367, ppl=20.63, wps=33066, ups=9.27, wpb=3567.1, bsz=143.4, num_updates=25100, lr=0.000199601, gnorm=1.74, loss_scale=4, train_wall=11, gb_free=19.9, wall=3412
2022-12-09 12:34:50 | INFO | train_inner | epoch 023:    962 / 1102 loss=8.003, nll_loss=4.36, ppl=20.54, wps=33190.5, ups=9.35, wpb=3549.6, bsz=153.2, num_updates=25200, lr=0.000199205, gnorm=1.725, loss_scale=4, train_wall=10, gb_free=19.5, wall=3423
2022-12-09 12:35:01 | INFO | train_inner | epoch 023:   1062 / 1102 loss=8.01, nll_loss=4.369, ppl=20.67, wps=33654.3, ups=9.33, wpb=3606.1, bsz=149.8, num_updates=25300, lr=0.000198811, gnorm=1.764, loss_scale=4, train_wall=10, gb_free=19.8, wall=3434
2022-12-09 12:35:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:35:40 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.849 | nll_loss 2.324 | ppl 5.01 | bleu 34.68 | wps 5163.4 | wpb 2835.3 | bsz 115.6 | num_updates 25340 | best_bleu 34.81
2022-12-09 12:35:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 25340 updates
2022-12-09 12:35:41 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint23.pt
2022-12-09 12:35:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint23.pt (epoch 23 @ 25340 updates, score 34.68) (writing took 1.257665446959436 seconds)
2022-12-09 12:35:42 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-12-09 12:35:42 | INFO | train | epoch 023 | loss 7.979 | nll_loss 4.323 | ppl 20.02 | wps 25476.8 | ups 7.11 | wpb 3583.6 | bsz 145.4 | num_updates 25340 | lr 0.000198654 | gnorm 1.756 | loss_scale 4 | train_wall 116 | gb_free 19.6 | wall 3474
2022-12-09 12:35:42 | INFO | fairseq.trainer | begin training epoch 24
2022-12-09 12:35:48 | INFO | train_inner | epoch 024:     60 / 1102 loss=7.921, nll_loss=4.249, ppl=19.01, wps=7579.7, ups=2.11, wpb=3584, bsz=142.3, num_updates=25400, lr=0.000198419, gnorm=1.809, loss_scale=4, train_wall=10, gb_free=19.8, wall=3481
2022-12-09 12:35:59 | INFO | train_inner | epoch 024:    160 / 1102 loss=7.836, nll_loss=4.171, ppl=18.02, wps=34087.9, ups=9.35, wpb=3646.7, bsz=147.3, num_updates=25500, lr=0.00019803, gnorm=1.684, loss_scale=4, train_wall=10, gb_free=19.4, wall=3492
2022-12-09 12:36:10 | INFO | train_inner | epoch 024:    260 / 1102 loss=7.947, nll_loss=4.282, ppl=19.45, wps=33018.4, ups=9.19, wpb=3590.9, bsz=142.5, num_updates=25600, lr=0.000197642, gnorm=1.77, loss_scale=4, train_wall=11, gb_free=19.8, wall=3502
2022-12-09 12:36:20 | INFO | train_inner | epoch 024:    360 / 1102 loss=7.945, nll_loss=4.283, ppl=19.47, wps=33858.5, ups=9.46, wpb=3577.5, bsz=138.4, num_updates=25700, lr=0.000197257, gnorm=1.709, loss_scale=4, train_wall=10, gb_free=19.8, wall=3513
2022-12-09 12:36:31 | INFO | train_inner | epoch 024:    460 / 1102 loss=8.042, nll_loss=4.356, ppl=20.47, wps=33479.7, ups=9.46, wpb=3539, bsz=123.8, num_updates=25800, lr=0.000196875, gnorm=1.794, loss_scale=4, train_wall=10, gb_free=19.7, wall=3524
2022-12-09 12:36:42 | INFO | train_inner | epoch 024:    560 / 1102 loss=7.81, nll_loss=4.153, ppl=17.79, wps=33536.2, ups=9.35, wpb=3585.3, bsz=159.7, num_updates=25900, lr=0.000196494, gnorm=1.7, loss_scale=4, train_wall=10, gb_free=19.3, wall=3534
2022-12-09 12:36:53 | INFO | train_inner | epoch 024:    660 / 1102 loss=7.964, nll_loss=4.316, ppl=19.91, wps=33609.8, ups=9.17, wpb=3664.1, bsz=147, num_updates=26000, lr=0.000196116, gnorm=1.69, loss_scale=4, train_wall=11, gb_free=19.9, wall=3545
2022-12-09 12:37:03 | INFO | train_inner | epoch 024:    760 / 1102 loss=8.006, nll_loss=4.344, ppl=20.31, wps=33488.1, ups=9.37, wpb=3573.3, bsz=137, num_updates=26100, lr=0.00019574, gnorm=1.761, loss_scale=4, train_wall=10, gb_free=19.5, wall=3556
2022-12-09 12:37:14 | INFO | train_inner | epoch 024:    860 / 1102 loss=7.874, nll_loss=4.228, ppl=18.74, wps=32951.9, ups=9.44, wpb=3489.5, bsz=172.8, num_updates=26200, lr=0.000195366, gnorm=1.826, loss_scale=4, train_wall=10, gb_free=19.5, wall=3566
2022-12-09 12:37:25 | INFO | train_inner | epoch 024:    960 / 1102 loss=7.952, nll_loss=4.306, ppl=19.78, wps=32752.2, ups=9.08, wpb=3607, bsz=148.4, num_updates=26300, lr=0.000194994, gnorm=1.622, loss_scale=4, train_wall=11, gb_free=19.4, wall=3577
2022-12-09 12:37:36 | INFO | train_inner | epoch 024:   1060 / 1102 loss=7.987, nll_loss=4.346, ppl=20.33, wps=33008.8, ups=9.14, wpb=3610.4, bsz=146.9, num_updates=26400, lr=0.000194625, gnorm=1.709, loss_scale=4, train_wall=11, gb_free=19.4, wall=3588
2022-12-09 12:37:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:38:14 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.842 | nll_loss 2.32 | ppl 4.99 | bleu 34.6 | wps 5303.1 | wpb 2835.3 | bsz 115.6 | num_updates 26442 | best_bleu 34.81
2022-12-09 12:38:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 26442 updates
2022-12-09 12:38:15 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint24.pt
2022-12-09 12:38:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint24.pt (epoch 24 @ 26442 updates, score 34.6) (writing took 1.2666196040809155 seconds)
2022-12-09 12:38:16 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-12-09 12:38:16 | INFO | train | epoch 024 | loss 7.939 | nll_loss 4.281 | ppl 19.44 | wps 25613.9 | ups 7.15 | wpb 3583.6 | bsz 145.4 | num_updates 26442 | lr 0.00019447 | gnorm 1.731 | loss_scale 4 | train_wall 116 | gb_free 19.9 | wall 3628
2022-12-09 12:38:16 | INFO | fairseq.trainer | begin training epoch 25
2022-12-09 12:38:22 | INFO | train_inner | epoch 025:     58 / 1102 loss=7.944, nll_loss=4.283, ppl=19.47, wps=7715.3, ups=2.15, wpb=3581, bsz=134, num_updates=26500, lr=0.000194257, gnorm=1.747, loss_scale=4, train_wall=10, gb_free=19.4, wall=3635
2022-12-09 12:38:33 | INFO | train_inner | epoch 025:    158 / 1102 loss=7.806, nll_loss=4.125, ppl=17.45, wps=33044.9, ups=9.3, wpb=3551.4, bsz=151.8, num_updates=26600, lr=0.000193892, gnorm=1.765, loss_scale=4, train_wall=11, gb_free=19.6, wall=3646
2022-12-09 12:38:44 | INFO | train_inner | epoch 025:    258 / 1102 loss=7.924, nll_loss=4.263, ppl=19.2, wps=34140, ups=9.46, wpb=3609.1, bsz=147, num_updates=26700, lr=0.000193528, gnorm=1.694, loss_scale=4, train_wall=10, gb_free=19.4, wall=3656
2022-12-09 12:38:54 | INFO | train_inner | epoch 025:    358 / 1102 loss=7.86, nll_loss=4.196, ppl=18.33, wps=33992.9, ups=9.44, wpb=3601.5, bsz=141.3, num_updates=26800, lr=0.000193167, gnorm=1.697, loss_scale=4, train_wall=10, gb_free=19.7, wall=3667
2022-12-09 12:39:05 | INFO | train_inner | epoch 025:    458 / 1102 loss=7.825, nll_loss=4.172, ppl=18.03, wps=33410.5, ups=9.23, wpb=3621.3, bsz=160, num_updates=26900, lr=0.000192807, gnorm=1.73, loss_scale=4, train_wall=11, gb_free=19.4, wall=3678
2022-12-09 12:39:16 | INFO | train_inner | epoch 025:    558 / 1102 loss=7.907, nll_loss=4.255, ppl=19.09, wps=33341.6, ups=9.28, wpb=3592, bsz=149.1, num_updates=27000, lr=0.00019245, gnorm=1.713, loss_scale=4, train_wall=11, gb_free=19.4, wall=3688
2022-12-09 12:39:26 | INFO | train_inner | epoch 025:    658 / 1102 loss=7.936, nll_loss=4.255, ppl=19.09, wps=32832.3, ups=9.33, wpb=3520.1, bsz=136.3, num_updates=27100, lr=0.000192095, gnorm=1.784, loss_scale=4, train_wall=10, gb_free=19.7, wall=3699
2022-12-09 12:39:37 | INFO | train_inner | epoch 025:    758 / 1102 loss=7.968, nll_loss=4.308, ppl=19.81, wps=33337.1, ups=9.36, wpb=3560.2, bsz=149.4, num_updates=27200, lr=0.000191741, gnorm=1.761, loss_scale=4, train_wall=10, gb_free=19.2, wall=3710
2022-12-09 12:39:48 | INFO | train_inner | epoch 025:    858 / 1102 loss=7.88, nll_loss=4.219, ppl=18.62, wps=34202.5, ups=9.44, wpb=3621.5, bsz=147.8, num_updates=27300, lr=0.00019139, gnorm=1.818, loss_scale=4, train_wall=10, gb_free=19.4, wall=3720
2022-12-09 12:39:58 | INFO | train_inner | epoch 025:    958 / 1102 loss=7.985, nll_loss=4.315, ppl=19.91, wps=33383.5, ups=9.51, wpb=3510.4, bsz=132.9, num_updates=27400, lr=0.00019104, gnorm=1.826, loss_scale=4, train_wall=10, gb_free=19.6, wall=3731
2022-12-09 12:40:09 | INFO | train_inner | epoch 025:   1058 / 1102 loss=7.998, nll_loss=4.339, ppl=20.24, wps=33226.4, ups=9.25, wpb=3592.6, bsz=135.9, num_updates=27500, lr=0.000190693, gnorm=1.69, loss_scale=4, train_wall=11, gb_free=19.6, wall=3742
2022-12-09 12:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:40:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.823 | nll_loss 2.298 | ppl 4.92 | bleu 35.08 | wps 4913 | wpb 2835.3 | bsz 115.6 | num_updates 27544 | best_bleu 35.08
2022-12-09 12:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 27544 updates
2022-12-09 12:40:51 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint25.pt
2022-12-09 12:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint25.pt (epoch 25 @ 27544 updates, score 35.08) (writing took 1.6934641981497407 seconds)
2022-12-09 12:40:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-12-09 12:40:52 | INFO | train | epoch 025 | loss 7.902 | nll_loss 4.239 | ppl 18.89 | wps 25252.8 | ups 7.05 | wpb 3583.6 | bsz 145.4 | num_updates 27544 | lr 0.00019054 | gnorm 1.744 | loss_scale 4 | train_wall 115 | gb_free 19.5 | wall 3785
2022-12-09 12:40:52 | INFO | fairseq.trainer | begin training epoch 26
2022-12-09 12:40:58 | INFO | train_inner | epoch 026:     56 / 1102 loss=7.865, nll_loss=4.202, ppl=18.41, wps=7237, ups=2.02, wpb=3575, bsz=148.4, num_updates=27600, lr=0.000190347, gnorm=1.806, loss_scale=4, train_wall=10, gb_free=19.3, wall=3791
2022-12-09 12:41:09 | INFO | train_inner | epoch 026:    156 / 1102 loss=7.839, nll_loss=4.155, ppl=17.81, wps=33662.4, ups=9.47, wpb=3554.1, bsz=138.2, num_updates=27700, lr=0.000190003, gnorm=1.773, loss_scale=4, train_wall=10, gb_free=19.4, wall=3802
2022-12-09 12:41:20 | INFO | train_inner | epoch 026:    256 / 1102 loss=7.805, nll_loss=4.135, ppl=17.57, wps=33543.6, ups=9.12, wpb=3676.2, bsz=150.5, num_updates=27800, lr=0.000189661, gnorm=1.7, loss_scale=4, train_wall=11, gb_free=19.3, wall=3813
2022-12-09 12:41:31 | INFO | train_inner | epoch 026:    356 / 1102 loss=7.842, nll_loss=4.178, ppl=18.1, wps=33955.3, ups=9.44, wpb=3598.6, bsz=155, num_updates=27900, lr=0.000189321, gnorm=1.716, loss_scale=4, train_wall=10, gb_free=19.4, wall=3823
2022-12-09 12:41:41 | INFO | train_inner | epoch 026:    456 / 1102 loss=7.95, nll_loss=4.267, ppl=19.26, wps=33786.5, ups=9.42, wpb=3585.7, bsz=127.2, num_updates=28000, lr=0.000188982, gnorm=1.816, loss_scale=4, train_wall=10, gb_free=19.2, wall=3834
2022-12-09 12:41:52 | INFO | train_inner | epoch 026:    556 / 1102 loss=7.873, nll_loss=4.208, ppl=18.48, wps=33122.3, ups=9.38, wpb=3529.4, bsz=144.3, num_updates=28100, lr=0.000188646, gnorm=1.691, loss_scale=4, train_wall=10, gb_free=19.7, wall=3844
2022-12-09 12:42:03 | INFO | train_inner | epoch 026:    656 / 1102 loss=7.845, nll_loss=4.189, ppl=18.24, wps=33896.6, ups=9.35, wpb=3626.1, bsz=153.7, num_updates=28200, lr=0.000188311, gnorm=1.774, loss_scale=4, train_wall=10, gb_free=19.5, wall=3855
2022-12-09 12:42:13 | INFO | train_inner | epoch 026:    756 / 1102 loss=7.831, nll_loss=4.16, ppl=17.87, wps=32848.1, ups=9.34, wpb=3515.8, bsz=150.2, num_updates=28300, lr=0.000187978, gnorm=1.743, loss_scale=4, train_wall=10, gb_free=19.6, wall=3866
2022-12-09 12:42:24 | INFO | train_inner | epoch 026:    856 / 1102 loss=7.948, nll_loss=4.272, ppl=19.31, wps=33632.6, ups=9.37, wpb=3589.6, bsz=135.6, num_updates=28400, lr=0.000187647, gnorm=1.797, loss_scale=4, train_wall=10, gb_free=19.4, wall=3877
2022-12-09 12:42:35 | INFO | train_inner | epoch 026:    956 / 1102 loss=7.966, nll_loss=4.307, ppl=19.79, wps=33637.3, ups=9.37, wpb=3591.6, bsz=138.3, num_updates=28500, lr=0.000187317, gnorm=1.786, loss_scale=4, train_wall=10, gb_free=19.5, wall=3887
2022-12-09 12:42:45 | INFO | train_inner | epoch 026:   1056 / 1102 loss=7.808, nll_loss=4.159, ppl=17.86, wps=33292.3, ups=9.28, wpb=3589.1, bsz=158.5, num_updates=28600, lr=0.000186989, gnorm=1.688, loss_scale=4, train_wall=11, gb_free=19.4, wall=3898
2022-12-09 12:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:43:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.816 | nll_loss 2.293 | ppl 4.9 | bleu 35.09 | wps 4822.5 | wpb 2835.3 | bsz 115.6 | num_updates 28646 | best_bleu 35.09
2022-12-09 12:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 28646 updates
2022-12-09 12:43:29 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint26.pt
2022-12-09 12:43:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint26.pt (epoch 26 @ 28646 updates, score 35.09) (writing took 1.7216022238135338 seconds)
2022-12-09 12:43:30 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-12-09 12:43:30 | INFO | train | epoch 026 | loss 7.87 | nll_loss 4.202 | ppl 18.41 | wps 25086.8 | ups 7 | wpb 3583.6 | bsz 145.4 | num_updates 28646 | lr 0.000186839 | gnorm 1.755 | loss_scale 4 | train_wall 115 | gb_free 19.5 | wall 3942
2022-12-09 12:43:30 | INFO | fairseq.trainer | begin training epoch 27
2022-12-09 12:43:36 | INFO | train_inner | epoch 027:     54 / 1102 loss=7.829, nll_loss=4.168, ppl=17.97, wps=7160.9, ups=1.99, wpb=3601.2, bsz=147.4, num_updates=28700, lr=0.000186663, gnorm=1.686, loss_scale=4, train_wall=11, gb_free=19.4, wall=3948
2022-12-09 12:43:46 | INFO | train_inner | epoch 027:    154 / 1102 loss=7.814, nll_loss=4.13, ppl=17.51, wps=33087.6, ups=9.31, wpb=3555.6, bsz=140.3, num_updates=28800, lr=0.000186339, gnorm=1.736, loss_scale=4, train_wall=10, gb_free=19.5, wall=3959
2022-12-09 12:43:57 | INFO | train_inner | epoch 027:    254 / 1102 loss=7.738, nll_loss=4.07, ppl=16.79, wps=33322.7, ups=9.25, wpb=3603.1, bsz=157.8, num_updates=28900, lr=0.000186016, gnorm=1.794, loss_scale=4, train_wall=11, gb_free=20.1, wall=3970
2022-12-09 12:44:08 | INFO | train_inner | epoch 027:    354 / 1102 loss=7.794, nll_loss=4.119, ppl=17.37, wps=33878.4, ups=9.3, wpb=3641.2, bsz=148, num_updates=29000, lr=0.000185695, gnorm=1.734, loss_scale=4, train_wall=11, gb_free=19.7, wall=3981
2022-12-09 12:44:19 | INFO | train_inner | epoch 027:    454 / 1102 loss=7.85, nll_loss=4.17, ppl=18.01, wps=33665.8, ups=9.31, wpb=3617.1, bsz=141.7, num_updates=29100, lr=0.000185376, gnorm=1.806, loss_scale=4, train_wall=11, gb_free=19.5, wall=3991
2022-12-09 12:44:29 | INFO | train_inner | epoch 027:    554 / 1102 loss=7.83, nll_loss=4.157, ppl=17.84, wps=32711.6, ups=9.44, wpb=3464.9, bsz=151.3, num_updates=29200, lr=0.000185058, gnorm=1.787, loss_scale=4, train_wall=10, gb_free=19.5, wall=4002
2022-12-09 12:44:40 | INFO | train_inner | epoch 027:    654 / 1102 loss=7.815, nll_loss=4.147, ppl=17.72, wps=33680.6, ups=9.35, wpb=3600.7, bsz=149.9, num_updates=29300, lr=0.000184742, gnorm=1.685, loss_scale=4, train_wall=10, gb_free=19.3, wall=4013
2022-12-09 12:44:51 | INFO | train_inner | epoch 027:    754 / 1102 loss=7.918, nll_loss=4.253, ppl=19.06, wps=34276.9, ups=9.41, wpb=3644.2, bsz=141.2, num_updates=29400, lr=0.000184428, gnorm=1.714, loss_scale=4, train_wall=10, gb_free=19.6, wall=4023
2022-12-09 12:45:01 | INFO | train_inner | epoch 027:    854 / 1102 loss=7.884, nll_loss=4.218, ppl=18.6, wps=33212.4, ups=9.29, wpb=3574.3, bsz=140.9, num_updates=29500, lr=0.000184115, gnorm=1.816, loss_scale=4, train_wall=11, gb_free=20.1, wall=4034
2022-12-09 12:45:12 | INFO | train_inner | epoch 027:    954 / 1102 loss=7.953, nll_loss=4.27, ppl=19.3, wps=33488.4, ups=9.36, wpb=3576.4, bsz=123.5, num_updates=29600, lr=0.000183804, gnorm=1.834, loss_scale=4, train_wall=10, gb_free=19.2, wall=4045
2022-12-09 12:45:23 | INFO | train_inner | epoch 027:   1054 / 1102 loss=7.781, nll_loss=4.131, ppl=17.52, wps=33758, ups=9.43, wpb=3578.9, bsz=165.4, num_updates=29700, lr=0.000183494, gnorm=1.75, loss_scale=4, train_wall=10, gb_free=19.4, wall=4055
2022-12-09 12:45:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:46:05 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.803 | nll_loss 2.281 | ppl 4.86 | bleu 35.09 | wps 4887.6 | wpb 2835.3 | bsz 115.6 | num_updates 29748 | best_bleu 35.09
2022-12-09 12:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 29748 updates
2022-12-09 12:46:06 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint27.pt
2022-12-09 12:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint27.pt (epoch 27 @ 29748 updates, score 35.09) (writing took 1.7162574119865894 seconds)
2022-12-09 12:46:07 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-12-09 12:46:07 | INFO | train | epoch 027 | loss 7.839 | nll_loss 4.168 | ppl 17.98 | wps 25161.1 | ups 7.02 | wpb 3583.6 | bsz 145.4 | num_updates 29748 | lr 0.000183346 | gnorm 1.762 | loss_scale 4 | train_wall 115 | gb_free 19.4 | wall 4099
2022-12-09 12:46:07 | INFO | fairseq.trainer | begin training epoch 28
2022-12-09 12:46:12 | INFO | train_inner | epoch 028:     52 / 1102 loss=7.801, nll_loss=4.144, ppl=17.68, wps=7277, ups=2.01, wpb=3621, bsz=153.5, num_updates=29800, lr=0.000183186, gnorm=1.712, loss_scale=4, train_wall=11, gb_free=19.3, wall=4105
2022-12-09 12:46:23 | INFO | train_inner | epoch 028:    152 / 1102 loss=7.677, nll_loss=4.015, ppl=16.17, wps=33737.7, ups=9.34, wpb=3613.3, bsz=165.5, num_updates=29900, lr=0.000182879, gnorm=1.702, loss_scale=4, train_wall=10, gb_free=19.4, wall=4116
2022-12-09 12:46:34 | INFO | train_inner | epoch 028:    252 / 1102 loss=7.916, nll_loss=4.227, ppl=18.72, wps=33912.6, ups=9.5, wpb=3570.5, bsz=125, num_updates=30000, lr=0.000182574, gnorm=1.769, loss_scale=4, train_wall=10, gb_free=19.6, wall=4126
2022-12-09 12:46:44 | INFO | train_inner | epoch 028:    352 / 1102 loss=7.867, nll_loss=4.188, ppl=18.23, wps=34154, ups=9.49, wpb=3599.9, bsz=133.6, num_updates=30100, lr=0.000182271, gnorm=1.714, loss_scale=4, train_wall=10, gb_free=19.9, wall=4137
2022-12-09 12:46:55 | INFO | train_inner | epoch 028:    452 / 1102 loss=7.696, nll_loss=4.032, ppl=16.35, wps=34365, ups=9.43, wpb=3646.1, bsz=161.3, num_updates=30200, lr=0.000181969, gnorm=1.681, loss_scale=4, train_wall=10, gb_free=19.4, wall=4147
2022-12-09 12:47:05 | INFO | train_inner | epoch 028:    552 / 1102 loss=7.732, nll_loss=4.048, ppl=16.55, wps=33494.2, ups=9.39, wpb=3565.4, bsz=149.9, num_updates=30300, lr=0.000181668, gnorm=1.776, loss_scale=4, train_wall=10, gb_free=20.1, wall=4158
2022-12-09 12:47:16 | INFO | train_inner | epoch 028:    652 / 1102 loss=7.836, nll_loss=4.155, ppl=17.81, wps=32936.8, ups=9.29, wpb=3544.7, bsz=143.9, num_updates=30400, lr=0.000181369, gnorm=1.783, loss_scale=4, train_wall=11, gb_free=19.6, wall=4169
2022-12-09 12:47:27 | INFO | train_inner | epoch 028:    752 / 1102 loss=7.831, nll_loss=4.143, ppl=17.66, wps=33077.6, ups=9.33, wpb=3546, bsz=131.3, num_updates=30500, lr=0.000181071, gnorm=1.846, loss_scale=4, train_wall=10, gb_free=19.5, wall=4180
2022-12-09 12:47:38 | INFO | train_inner | epoch 028:    852 / 1102 loss=7.786, nll_loss=4.115, ppl=17.32, wps=33158.3, ups=9.35, wpb=3544.9, bsz=149, num_updates=30600, lr=0.000180775, gnorm=1.785, loss_scale=4, train_wall=10, gb_free=19.9, wall=4190
2022-12-09 12:47:48 | INFO | train_inner | epoch 028:    952 / 1102 loss=7.968, nll_loss=4.279, ppl=19.42, wps=33434.8, ups=9.46, wpb=3534.5, bsz=129.8, num_updates=30700, lr=0.000180481, gnorm=2.018, loss_scale=4, train_wall=10, gb_free=19.7, wall=4201
2022-12-09 12:47:59 | INFO | train_inner | epoch 028:   1052 / 1102 loss=7.81, nll_loss=4.158, ppl=17.86, wps=33670.4, ups=9.36, wpb=3596.3, bsz=157.6, num_updates=30800, lr=0.000180187, gnorm=1.688, loss_scale=4, train_wall=10, gb_free=19.5, wall=4211
2022-12-09 12:48:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:48:41 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.805 | nll_loss 2.273 | ppl 4.83 | bleu 35.44 | wps 4968.6 | wpb 2835.3 | bsz 115.6 | num_updates 30850 | best_bleu 35.44
2022-12-09 12:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 30850 updates
2022-12-09 12:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint28.pt
2022-12-09 12:48:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint28.pt (epoch 28 @ 30850 updates, score 35.44) (writing took 1.7170685660094023 seconds)
2022-12-09 12:48:42 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-12-09 12:48:42 | INFO | train | epoch 028 | loss 7.808 | nll_loss 4.133 | ppl 17.54 | wps 25344.1 | ups 7.07 | wpb 3583.6 | bsz 145.4 | num_updates 30850 | lr 0.000180041 | gnorm 1.774 | loss_scale 4 | train_wall 115 | gb_free 19.5 | wall 4255
2022-12-09 12:48:42 | INFO | fairseq.trainer | begin training epoch 29
2022-12-09 12:48:48 | INFO | train_inner | epoch 029:     50 / 1102 loss=7.783, nll_loss=4.095, ppl=17.09, wps=7405.5, ups=2.03, wpb=3643.9, bsz=133.8, num_updates=30900, lr=0.000179896, gnorm=1.762, loss_scale=4, train_wall=11, gb_free=19.2, wall=4261
2022-12-09 12:48:59 | INFO | train_inner | epoch 029:    150 / 1102 loss=7.663, nll_loss=3.992, ppl=15.91, wps=33799.5, ups=9.32, wpb=3627.4, bsz=159.2, num_updates=31000, lr=0.000179605, gnorm=1.715, loss_scale=4, train_wall=10, gb_free=19.3, wall=4271
2022-12-09 12:49:09 | INFO | train_inner | epoch 029:    250 / 1102 loss=7.808, nll_loss=4.118, ppl=17.37, wps=32676.1, ups=9.47, wpb=3451.4, bsz=137, num_updates=31100, lr=0.000179316, gnorm=1.801, loss_scale=4, train_wall=10, gb_free=19.6, wall=4282
2022-12-09 12:49:20 | INFO | train_inner | epoch 029:    350 / 1102 loss=7.797, nll_loss=4.102, ppl=17.17, wps=32322.7, ups=9.17, wpb=3524.1, bsz=137.8, num_updates=31200, lr=0.000179029, gnorm=1.827, loss_scale=4, train_wall=11, gb_free=19.3, wall=4293
2022-12-09 12:49:31 | INFO | train_inner | epoch 029:    450 / 1102 loss=7.778, nll_loss=4.1, ppl=17.15, wps=33418.8, ups=9.16, wpb=3647.6, bsz=152, num_updates=31300, lr=0.000178743, gnorm=1.745, loss_scale=4, train_wall=11, gb_free=19.2, wall=4304
2022-12-09 12:49:42 | INFO | train_inner | epoch 029:    550 / 1102 loss=7.839, nll_loss=4.16, ppl=17.88, wps=34054.9, ups=9.45, wpb=3603.3, bsz=138.7, num_updates=31400, lr=0.000178458, gnorm=1.722, loss_scale=4, train_wall=10, gb_free=19.3, wall=4314
2022-12-09 12:49:52 | INFO | train_inner | epoch 029:    650 / 1102 loss=7.793, nll_loss=4.106, ppl=17.22, wps=33127.1, ups=9.44, wpb=3510.9, bsz=141.3, num_updates=31500, lr=0.000178174, gnorm=1.76, loss_scale=4, train_wall=10, gb_free=19.4, wall=4325
2022-12-09 12:50:03 | INFO | train_inner | epoch 029:    750 / 1102 loss=7.846, nll_loss=4.159, ppl=17.87, wps=32936.1, ups=9.16, wpb=3594.7, bsz=130, num_updates=31600, lr=0.000177892, gnorm=1.773, loss_scale=4, train_wall=11, gb_free=19.5, wall=4336
2022-12-09 12:50:14 | INFO | train_inner | epoch 029:    850 / 1102 loss=7.794, nll_loss=4.123, ppl=17.43, wps=33469.2, ups=9.36, wpb=3576, bsz=147.2, num_updates=31700, lr=0.000177611, gnorm=1.707, loss_scale=4, train_wall=10, gb_free=19.2, wall=4347
2022-12-09 12:50:25 | INFO | train_inner | epoch 029:    950 / 1102 loss=7.746, nll_loss=4.07, ppl=16.8, wps=33370.8, ups=9.25, wpb=3607.3, bsz=149.7, num_updates=31800, lr=0.000177332, gnorm=1.7, loss_scale=4, train_wall=11, gb_free=19.4, wall=4357
2022-12-09 12:50:35 | INFO | train_inner | epoch 029:   1050 / 1102 loss=7.732, nll_loss=4.067, ppl=16.76, wps=33604, ups=9.36, wpb=3590.4, bsz=168.9, num_updates=31900, lr=0.000177054, gnorm=1.781, loss_scale=4, train_wall=10, gb_free=19.4, wall=4368
2022-12-09 12:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:51:16 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.799 | nll_loss 2.272 | ppl 4.83 | bleu 35.25 | wps 5182.4 | wpb 2835.3 | bsz 115.6 | num_updates 31952 | best_bleu 35.44
2022-12-09 12:51:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 31952 updates
2022-12-09 12:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint29.pt
2022-12-09 12:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint29.pt (epoch 29 @ 31952 updates, score 35.25) (writing took 1.1785260578617454 seconds)
2022-12-09 12:51:17 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-12-09 12:51:17 | INFO | train | epoch 029 | loss 7.779 | nll_loss 4.1 | ppl 17.15 | wps 25520.7 | ups 7.12 | wpb 3583.6 | bsz 145.4 | num_updates 31952 | lr 0.000176909 | gnorm 1.747 | loss_scale 4 | train_wall 116 | gb_free 19.3 | wall 4410
2022-12-09 12:51:17 | INFO | fairseq.trainer | begin training epoch 30
2022-12-09 12:51:22 | INFO | train_inner | epoch 030:     48 / 1102 loss=7.752, nll_loss=4.089, ppl=17.02, wps=7867.3, ups=2.13, wpb=3692.8, bsz=150.5, num_updates=32000, lr=0.000176777, gnorm=1.708, loss_scale=4, train_wall=11, gb_free=19.3, wall=4415
2022-12-09 12:51:33 | INFO | train_inner | epoch 030:    148 / 1102 loss=7.72, nll_loss=4.039, ppl=16.44, wps=33754.5, ups=9.37, wpb=3603, bsz=147, num_updates=32100, lr=0.000176501, gnorm=1.796, loss_scale=4, train_wall=10, gb_free=19.4, wall=4426
2022-12-09 12:51:44 | INFO | train_inner | epoch 030:    248 / 1102 loss=7.684, nll_loss=3.995, ppl=15.94, wps=32888.1, ups=9.42, wpb=3492.4, bsz=147.8, num_updates=32200, lr=0.000176227, gnorm=1.765, loss_scale=4, train_wall=10, gb_free=19.3, wall=4436
2022-12-09 12:51:54 | INFO | train_inner | epoch 030:    348 / 1102 loss=7.699, nll_loss=4.011, ppl=16.12, wps=33754.9, ups=9.36, wpb=3605.7, bsz=156.2, num_updates=32300, lr=0.000175954, gnorm=1.782, loss_scale=4, train_wall=10, gb_free=19.3, wall=4447
2022-12-09 12:52:05 | INFO | train_inner | epoch 030:    448 / 1102 loss=7.797, nll_loss=4.109, ppl=17.25, wps=32932.6, ups=9.26, wpb=3558.3, bsz=137.9, num_updates=32400, lr=0.000175682, gnorm=1.752, loss_scale=4, train_wall=11, gb_free=19.5, wall=4458
2022-12-09 12:52:16 | INFO | train_inner | epoch 030:    548 / 1102 loss=7.77, nll_loss=4.086, ppl=16.99, wps=33899.8, ups=9.26, wpb=3662.2, bsz=140.2, num_updates=32500, lr=0.000175412, gnorm=1.731, loss_scale=4, train_wall=11, gb_free=19.3, wall=4469
2022-12-09 12:52:27 | INFO | train_inner | epoch 030:    648 / 1102 loss=7.858, nll_loss=4.173, ppl=18.04, wps=33416.8, ups=9.23, wpb=3619.7, bsz=131.4, num_updates=32600, lr=0.000175142, gnorm=1.824, loss_scale=4, train_wall=11, gb_free=19.4, wall=4479
2022-12-09 12:52:37 | INFO | train_inner | epoch 030:    748 / 1102 loss=7.776, nll_loss=4.087, ppl=17, wps=33187.1, ups=9.38, wpb=3536.6, bsz=140.9, num_updates=32700, lr=0.000174874, gnorm=1.852, loss_scale=4, train_wall=10, gb_free=19.8, wall=4490
2022-12-09 12:52:48 | INFO | train_inner | epoch 030:    848 / 1102 loss=7.677, nll_loss=4.013, ppl=16.14, wps=33604.9, ups=9.26, wpb=3627.7, bsz=167, num_updates=32800, lr=0.000174608, gnorm=1.633, loss_scale=4, train_wall=11, gb_free=19.5, wall=4501
2022-12-09 12:52:59 | INFO | train_inner | epoch 030:    948 / 1102 loss=7.714, nll_loss=4.029, ppl=16.32, wps=33224.2, ups=9.4, wpb=3536.2, bsz=146.8, num_updates=32900, lr=0.000174342, gnorm=1.709, loss_scale=4, train_wall=10, gb_free=19.7, wall=4512
2022-12-09 12:53:10 | INFO | train_inner | epoch 030:   1048 / 1102 loss=7.817, nll_loss=4.136, ppl=17.58, wps=32925.7, ups=9.23, wpb=3567, bsz=140.2, num_updates=33000, lr=0.000174078, gnorm=1.785, loss_scale=4, train_wall=11, gb_free=19.8, wall=4522
2022-12-09 12:53:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:53:52 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.783 | nll_loss 2.253 | ppl 4.77 | bleu 35.49 | wps 4902 | wpb 2835.3 | bsz 115.6 | num_updates 33054 | best_bleu 35.49
2022-12-09 12:53:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 33054 updates
2022-12-09 12:53:53 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint30.pt
2022-12-09 12:53:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint30.pt (epoch 30 @ 33054 updates, score 35.49) (writing took 1.7282680766656995 seconds)
2022-12-09 12:53:54 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-12-09 12:53:54 | INFO | train | epoch 030 | loss 7.752 | nll_loss 4.07 | ppl 16.79 | wps 25137.9 | ups 7.01 | wpb 3583.6 | bsz 145.4 | num_updates 33054 | lr 0.000173935 | gnorm 1.765 | loss_scale 4 | train_wall 116 | gb_free 19.8 | wall 4567
2022-12-09 12:53:54 | INFO | fairseq.trainer | begin training epoch 31
2022-12-09 12:54:00 | INFO | train_inner | epoch 031:     46 / 1102 loss=7.757, nll_loss=4.066, ppl=16.75, wps=7144.5, ups=2.01, wpb=3557.2, bsz=134.8, num_updates=33100, lr=0.000173814, gnorm=1.802, loss_scale=4, train_wall=11, gb_free=19.2, wall=4572
2022-12-09 12:54:10 | INFO | train_inner | epoch 031:    146 / 1102 loss=7.731, nll_loss=4.025, ppl=16.28, wps=33827.1, ups=9.43, wpb=3585.8, bsz=127.5, num_updates=33200, lr=0.000173553, gnorm=1.736, loss_scale=4, train_wall=10, gb_free=19.8, wall=4583
2022-12-09 12:54:21 | INFO | train_inner | epoch 031:    246 / 1102 loss=7.741, nll_loss=4.047, ppl=16.53, wps=32311.9, ups=9.28, wpb=3481, bsz=142.1, num_updates=33300, lr=0.000173292, gnorm=1.831, loss_scale=4, train_wall=11, gb_free=19.9, wall=4594
2022-12-09 12:54:32 | INFO | train_inner | epoch 031:    346 / 1102 loss=7.798, nll_loss=4.096, ppl=17.1, wps=33650.8, ups=9.37, wpb=3592.4, bsz=129.4, num_updates=33400, lr=0.000173032, gnorm=1.799, loss_scale=4, train_wall=10, gb_free=19.6, wall=4604
2022-12-09 12:54:42 | INFO | train_inner | epoch 031:    446 / 1102 loss=7.677, nll_loss=3.994, ppl=15.93, wps=33827.7, ups=9.41, wpb=3596, bsz=149.1, num_updates=33500, lr=0.000172774, gnorm=1.724, loss_scale=4, train_wall=10, gb_free=19.4, wall=4615
2022-12-09 12:54:53 | INFO | train_inner | epoch 031:    546 / 1102 loss=7.628, nll_loss=3.943, ppl=15.38, wps=33918.7, ups=9.29, wpb=3650.7, bsz=152.4, num_updates=33600, lr=0.000172516, gnorm=1.717, loss_scale=4, train_wall=11, gb_free=19.5, wall=4626
2022-12-09 12:55:04 | INFO | train_inner | epoch 031:    646 / 1102 loss=7.74, nll_loss=4.064, ppl=16.72, wps=33323.2, ups=9.4, wpb=3544, bsz=152.1, num_updates=33700, lr=0.00017226, gnorm=1.843, loss_scale=4, train_wall=10, gb_free=19.5, wall=4636
2022-12-09 12:55:14 | INFO | train_inner | epoch 031:    746 / 1102 loss=7.729, nll_loss=4.062, ppl=16.7, wps=33778, ups=9.26, wpb=3649.4, bsz=155.7, num_updates=33800, lr=0.000172005, gnorm=1.745, loss_scale=4, train_wall=11, gb_free=19.6, wall=4647
2022-12-09 12:55:25 | INFO | train_inner | epoch 031:    846 / 1102 loss=7.73, nll_loss=4.055, ppl=16.63, wps=33783.4, ups=9.36, wpb=3610.4, bsz=151.2, num_updates=33900, lr=0.000171751, gnorm=1.75, loss_scale=4, train_wall=10, gb_free=21.3, wall=4658
2022-12-09 12:55:36 | INFO | train_inner | epoch 031:    946 / 1102 loss=7.656, nll_loss=3.979, ppl=15.77, wps=33951.6, ups=9.39, wpb=3614.4, bsz=159, num_updates=34000, lr=0.000171499, gnorm=1.776, loss_scale=4, train_wall=10, gb_free=19.6, wall=4668
2022-12-09 12:55:46 | INFO | train_inner | epoch 031:   1046 / 1102 loss=7.832, nll_loss=4.152, ppl=17.78, wps=33107.4, ups=9.36, wpb=3538.6, bsz=139.6, num_updates=34100, lr=0.000171247, gnorm=1.884, loss_scale=4, train_wall=10, gb_free=19.9, wall=4679
2022-12-09 12:55:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:56:31 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.784 | nll_loss 2.251 | ppl 4.76 | bleu 35.69 | wps 4668.7 | wpb 2835.3 | bsz 115.6 | num_updates 34156 | best_bleu 35.69
2022-12-09 12:56:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 34156 updates
2022-12-09 12:56:32 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint31.pt
2022-12-09 12:56:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint31.pt (epoch 31 @ 34156 updates, score 35.69) (writing took 1.8696673903614283 seconds)
2022-12-09 12:56:33 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-12-09 12:56:33 | INFO | train | epoch 031 | loss 7.725 | nll_loss 4.039 | ppl 16.44 | wps 24868.9 | ups 6.94 | wpb 3583.6 | bsz 145.4 | num_updates 34156 | lr 0.000171106 | gnorm 1.78 | loss_scale 4 | train_wall 115 | gb_free 19.7 | wall 4726
2022-12-09 12:56:33 | INFO | fairseq.trainer | begin training epoch 32
2022-12-09 12:56:38 | INFO | train_inner | epoch 032:     44 / 1102 loss=7.717, nll_loss=4.03, ppl=16.33, wps=6917, ups=1.95, wpb=3549, bsz=146.6, num_updates=34200, lr=0.000170996, gnorm=1.742, loss_scale=4, train_wall=10, gb_free=19.4, wall=4730
2022-12-09 12:56:48 | INFO | train_inner | epoch 032:    144 / 1102 loss=7.536, nll_loss=3.856, ppl=14.48, wps=34972.9, ups=9.54, wpb=3664.7, bsz=175.6, num_updates=34300, lr=0.000170747, gnorm=1.711, loss_scale=4, train_wall=10, gb_free=19.5, wall=4741
2022-12-09 12:56:59 | INFO | train_inner | epoch 032:    244 / 1102 loss=7.689, nll_loss=3.987, ppl=15.85, wps=34253.5, ups=9.48, wpb=3612.3, bsz=144.4, num_updates=34400, lr=0.000170499, gnorm=1.78, loss_scale=4, train_wall=10, gb_free=19.3, wall=4751
2022-12-09 12:57:09 | INFO | train_inner | epoch 032:    344 / 1102 loss=7.651, nll_loss=3.955, ppl=15.51, wps=33698.8, ups=9.45, wpb=3567.1, bsz=142.8, num_updates=34500, lr=0.000170251, gnorm=1.747, loss_scale=4, train_wall=10, gb_free=19.4, wall=4762
2022-12-09 12:57:20 | INFO | train_inner | epoch 032:    444 / 1102 loss=7.764, nll_loss=4.07, ppl=16.8, wps=33356.6, ups=9.29, wpb=3591, bsz=131, num_updates=34600, lr=0.000170005, gnorm=1.698, loss_scale=4, train_wall=11, gb_free=19.7, wall=4773
2022-12-09 12:57:31 | INFO | train_inner | epoch 032:    544 / 1102 loss=7.762, nll_loss=4.077, ppl=16.88, wps=32792.3, ups=9.17, wpb=3574.4, bsz=138.6, num_updates=34700, lr=0.00016976, gnorm=1.744, loss_scale=4, train_wall=11, gb_free=19.5, wall=4784
2022-12-09 12:57:42 | INFO | train_inner | epoch 032:    644 / 1102 loss=7.72, nll_loss=4.033, ppl=16.37, wps=32701.8, ups=9.23, wpb=3541.5, bsz=144.2, num_updates=34800, lr=0.000169516, gnorm=1.772, loss_scale=4, train_wall=11, gb_free=19.7, wall=4794
2022-12-09 12:57:53 | INFO | train_inner | epoch 032:    744 / 1102 loss=7.699, nll_loss=3.999, ppl=15.99, wps=33035, ups=9.23, wpb=3579, bsz=139.2, num_updates=34900, lr=0.000169273, gnorm=1.788, loss_scale=4, train_wall=11, gb_free=19.3, wall=4805
2022-12-09 12:58:03 | INFO | train_inner | epoch 032:    844 / 1102 loss=7.774, nll_loss=4.085, ppl=16.97, wps=33528.2, ups=9.36, wpb=3582.1, bsz=138.3, num_updates=35000, lr=0.000169031, gnorm=1.712, loss_scale=4, train_wall=10, gb_free=19.6, wall=4816
2022-12-09 12:58:14 | INFO | train_inner | epoch 032:    944 / 1102 loss=7.756, nll_loss=4.073, ppl=16.83, wps=33681.5, ups=9.49, wpb=3547.8, bsz=140.8, num_updates=35100, lr=0.00016879, gnorm=1.736, loss_scale=4, train_wall=10, gb_free=19.4, wall=4827
2022-12-09 12:58:25 | INFO | train_inner | epoch 032:   1044 / 1102 loss=7.652, nll_loss=3.981, ppl=15.79, wps=33110.1, ups=9.23, wpb=3588.6, bsz=165.3, num_updates=35200, lr=0.00016855, gnorm=1.708, loss_scale=4, train_wall=11, gb_free=19.3, wall=4837
2022-12-09 12:58:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 12:59:08 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.792 | nll_loss 2.257 | ppl 4.78 | bleu 35.28 | wps 4806 | wpb 2835.3 | bsz 115.6 | num_updates 35258 | best_bleu 35.69
2022-12-09 12:59:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 35258 updates
2022-12-09 12:59:09 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint32.pt
2022-12-09 12:59:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint32.pt (epoch 32 @ 35258 updates, score 35.28) (writing took 1.238854667171836 seconds)
2022-12-09 12:59:10 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-12-09 12:59:10 | INFO | train | epoch 032 | loss 7.7 | nll_loss 4.012 | ppl 16.14 | wps 25195.9 | ups 7.03 | wpb 3583.6 | bsz 145.4 | num_updates 35258 | lr 0.000168411 | gnorm 1.737 | loss_scale 4 | train_wall 115 | gb_free 19.4 | wall 4882
2022-12-09 12:59:10 | INFO | fairseq.trainer | begin training epoch 33
2022-12-09 12:59:14 | INFO | train_inner | epoch 033:     42 / 1102 loss=7.694, nll_loss=4.011, ppl=16.12, wps=7256.5, ups=2.01, wpb=3606.7, bsz=140.8, num_updates=35300, lr=0.000168311, gnorm=1.731, loss_scale=4, train_wall=10, gb_free=19.4, wall=4887
2022-12-09 12:59:25 | INFO | train_inner | epoch 033:    142 / 1102 loss=7.521, nll_loss=3.839, ppl=14.31, wps=33939.2, ups=9.29, wpb=3655.1, bsz=167.9, num_updates=35400, lr=0.000168073, gnorm=1.648, loss_scale=4, train_wall=11, gb_free=19.5, wall=4898
2022-12-09 12:59:36 | INFO | train_inner | epoch 033:    242 / 1102 loss=7.606, nll_loss=3.901, ppl=14.94, wps=33253.8, ups=9.31, wpb=3571.7, bsz=145.6, num_updates=35500, lr=0.000167836, gnorm=1.753, loss_scale=4, train_wall=10, gb_free=19.5, wall=4909
2022-12-09 12:59:47 | INFO | train_inner | epoch 033:    342 / 1102 loss=7.673, nll_loss=3.966, ppl=15.63, wps=32961.8, ups=9.28, wpb=3550.5, bsz=148.7, num_updates=35600, lr=0.0001676, gnorm=1.861, loss_scale=4, train_wall=11, gb_free=19.6, wall=4919
2022-12-09 12:59:58 | INFO | train_inner | epoch 033:    442 / 1102 loss=7.542, nll_loss=3.853, ppl=14.45, wps=33476.5, ups=9.23, wpb=3625.4, bsz=159, num_updates=35700, lr=0.000167365, gnorm=1.708, loss_scale=4, train_wall=11, gb_free=19.7, wall=4930
2022-12-09 13:00:08 | INFO | train_inner | epoch 033:    542 / 1102 loss=7.783, nll_loss=4.078, ppl=16.89, wps=32831.6, ups=9.45, wpb=3474.9, bsz=131.6, num_updates=35800, lr=0.000167132, gnorm=1.887, loss_scale=4, train_wall=10, gb_free=19.4, wall=4941
2022-12-09 13:00:19 | INFO | train_inner | epoch 033:    642 / 1102 loss=7.676, nll_loss=3.989, ppl=15.88, wps=33523.5, ups=9.21, wpb=3639.5, bsz=151.4, num_updates=35900, lr=0.000166899, gnorm=1.698, loss_scale=4, train_wall=11, gb_free=19.3, wall=4952
2022-12-09 13:00:30 | INFO | train_inner | epoch 033:    742 / 1102 loss=7.721, nll_loss=4.029, ppl=16.33, wps=33438.2, ups=9.34, wpb=3581.5, bsz=140.3, num_updates=36000, lr=0.000166667, gnorm=1.766, loss_scale=4, train_wall=10, gb_free=19.3, wall=4962
2022-12-09 13:00:40 | INFO | train_inner | epoch 033:    842 / 1102 loss=7.773, nll_loss=4.092, ppl=17.06, wps=33144.5, ups=9.31, wpb=3559.2, bsz=134.8, num_updates=36100, lr=0.000166436, gnorm=1.723, loss_scale=4, train_wall=11, gb_free=19.7, wall=4973
2022-12-09 13:00:51 | INFO | train_inner | epoch 033:    942 / 1102 loss=7.642, nll_loss=3.952, ppl=15.48, wps=33425.9, ups=9.23, wpb=3620.9, bsz=147.3, num_updates=36200, lr=0.000166206, gnorm=1.728, loss_scale=4, train_wall=11, gb_free=19.3, wall=4984
2022-12-09 13:01:02 | INFO | train_inner | epoch 033:   1042 / 1102 loss=7.823, nll_loss=4.125, ppl=17.45, wps=33101.3, ups=9.27, wpb=3570.6, bsz=121.9, num_updates=36300, lr=0.000165977, gnorm=1.865, loss_scale=4, train_wall=11, gb_free=19.4, wall=4995
2022-12-09 13:01:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:01:49 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.772 | nll_loss 2.247 | ppl 4.75 | bleu 35.52 | wps 4469.9 | wpb 2835.3 | bsz 115.6 | num_updates 36360 | best_bleu 35.69
2022-12-09 13:01:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 36360 updates
2022-12-09 13:01:50 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint33.pt
2022-12-09 13:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint33.pt (epoch 33 @ 36360 updates, score 35.52) (writing took 1.2468030266463757 seconds)
2022-12-09 13:01:50 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-12-09 13:01:50 | INFO | train | epoch 033 | loss 7.677 | nll_loss 3.985 | ppl 15.84 | wps 24623.4 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 36360 | lr 0.00016584 | gnorm 1.765 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 5043
2022-12-09 13:01:50 | INFO | fairseq.trainer | begin training epoch 34
2022-12-09 13:01:55 | INFO | train_inner | epoch 034:     40 / 1102 loss=7.687, nll_loss=4.015, ppl=16.16, wps=6763.4, ups=1.9, wpb=3552.3, bsz=149.4, num_updates=36400, lr=0.000165748, gnorm=1.742, loss_scale=4, train_wall=10, gb_free=19.3, wall=5047
2022-12-09 13:02:05 | INFO | train_inner | epoch 034:    140 / 1102 loss=7.506, nll_loss=3.822, ppl=14.15, wps=33266.7, ups=9.38, wpb=3546.5, bsz=162.6, num_updates=36500, lr=0.000165521, gnorm=1.647, loss_scale=4, train_wall=10, gb_free=19.6, wall=5058
2022-12-09 13:02:16 | INFO | train_inner | epoch 034:    240 / 1102 loss=7.587, nll_loss=3.882, ppl=14.74, wps=33079.2, ups=9.27, wpb=3569.4, bsz=151.1, num_updates=36600, lr=0.000165295, gnorm=1.799, loss_scale=4, train_wall=11, gb_free=19.2, wall=5069
2022-12-09 13:02:27 | INFO | train_inner | epoch 034:    340 / 1102 loss=7.675, nll_loss=3.976, ppl=15.73, wps=33250.2, ups=9.29, wpb=3580.4, bsz=144.4, num_updates=36700, lr=0.00016507, gnorm=1.721, loss_scale=4, train_wall=11, gb_free=19.6, wall=5079
2022-12-09 13:02:38 | INFO | train_inner | epoch 034:    440 / 1102 loss=7.729, nll_loss=4.029, ppl=16.33, wps=33270.8, ups=9.21, wpb=3612.1, bsz=134.3, num_updates=36800, lr=0.000164845, gnorm=1.735, loss_scale=4, train_wall=11, gb_free=19.6, wall=5090
2022-12-09 13:02:48 | INFO | train_inner | epoch 034:    540 / 1102 loss=7.687, nll_loss=3.988, ppl=15.87, wps=33483.3, ups=9.26, wpb=3615.7, bsz=139.6, num_updates=36900, lr=0.000164622, gnorm=1.789, loss_scale=4, train_wall=11, gb_free=19.3, wall=5101
2022-12-09 13:02:59 | INFO | train_inner | epoch 034:    640 / 1102 loss=7.746, nll_loss=4.048, ppl=16.54, wps=33499.2, ups=9.34, wpb=3586.7, bsz=129.7, num_updates=37000, lr=0.000164399, gnorm=1.783, loss_scale=4, train_wall=10, gb_free=19.6, wall=5112
2022-12-09 13:03:10 | INFO | train_inner | epoch 034:    740 / 1102 loss=7.75, nll_loss=4.048, ppl=16.54, wps=33114.9, ups=9.26, wpb=3576.8, bsz=135.4, num_updates=37100, lr=0.000164177, gnorm=1.858, loss_scale=4, train_wall=11, gb_free=19.7, wall=5123
2022-12-09 13:03:21 | INFO | train_inner | epoch 034:    840 / 1102 loss=7.571, nll_loss=3.877, ppl=14.69, wps=33112.9, ups=9.23, wpb=3588.9, bsz=153.4, num_updates=37200, lr=0.000163956, gnorm=1.689, loss_scale=4, train_wall=11, gb_free=19.4, wall=5133
2022-12-09 13:03:32 | INFO | train_inner | epoch 034:    940 / 1102 loss=7.659, nll_loss=3.972, ppl=15.7, wps=33017, ups=9.29, wpb=3555.8, bsz=149, num_updates=37300, lr=0.000163737, gnorm=1.751, loss_scale=4, train_wall=11, gb_free=19.4, wall=5144
2022-12-09 13:03:42 | INFO | train_inner | epoch 034:   1040 / 1102 loss=7.625, nll_loss=3.938, ppl=15.33, wps=33184.1, ups=9.25, wpb=3589.3, bsz=156.1, num_updates=37400, lr=0.000163517, gnorm=1.775, loss_scale=4, train_wall=11, gb_free=19.2, wall=5155
2022-12-09 13:03:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:04:26 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.767 | nll_loss 2.233 | ppl 4.7 | bleu 35.91 | wps 4837.2 | wpb 2835.3 | bsz 115.6 | num_updates 37462 | best_bleu 35.91
2022-12-09 13:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 37462 updates
2022-12-09 13:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint34.pt
2022-12-09 13:04:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint34.pt (epoch 34 @ 37462 updates, score 35.91) (writing took 1.7007773965597153 seconds)
2022-12-09 13:04:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-12-09 13:04:28 | INFO | train | epoch 034 | loss 7.654 | nll_loss 3.959 | ppl 15.56 | wps 24986 | ups 6.97 | wpb 3583.6 | bsz 145.4 | num_updates 37462 | lr 0.000163382 | gnorm 1.756 | loss_scale 4 | train_wall 116 | gb_free 19.3 | wall 5201
2022-12-09 13:04:28 | INFO | fairseq.trainer | begin training epoch 35
2022-12-09 13:04:32 | INFO | train_inner | epoch 035:     38 / 1102 loss=7.651, nll_loss=3.96, ppl=15.56, wps=7246.3, ups=2, wpb=3626.9, bsz=144.2, num_updates=37500, lr=0.000163299, gnorm=1.777, loss_scale=4, train_wall=11, gb_free=19.3, wall=5205
2022-12-09 13:04:43 | INFO | train_inner | epoch 035:    138 / 1102 loss=7.547, nll_loss=3.844, ppl=14.36, wps=33235.6, ups=9.2, wpb=3614.4, bsz=146.1, num_updates=37600, lr=0.000163082, gnorm=1.669, loss_scale=4, train_wall=11, gb_free=19.5, wall=5216
2022-12-09 13:04:54 | INFO | train_inner | epoch 035:    238 / 1102 loss=7.514, nll_loss=3.822, ppl=14.14, wps=33174.5, ups=9.19, wpb=3608, bsz=163.4, num_updates=37700, lr=0.000162866, gnorm=1.711, loss_scale=4, train_wall=11, gb_free=19.4, wall=5227
2022-12-09 13:05:05 | INFO | train_inner | epoch 035:    338 / 1102 loss=7.637, nll_loss=3.95, ppl=15.45, wps=34078.5, ups=9.23, wpb=3694.1, bsz=146.6, num_updates=37800, lr=0.00016265, gnorm=1.729, loss_scale=4, train_wall=11, gb_free=19.4, wall=5238
2022-12-09 13:05:16 | INFO | train_inner | epoch 035:    438 / 1102 loss=7.773, nll_loss=4.048, ppl=16.54, wps=32953.1, ups=9.41, wpb=3503.8, bsz=117.8, num_updates=37900, lr=0.000162435, gnorm=1.854, loss_scale=4, train_wall=10, gb_free=19.5, wall=5248
2022-12-09 13:05:26 | INFO | train_inner | epoch 035:    538 / 1102 loss=7.622, nll_loss=3.932, ppl=15.26, wps=33097.2, ups=9.33, wpb=3547, bsz=154.8, num_updates=38000, lr=0.000162221, gnorm=1.761, loss_scale=4, train_wall=10, gb_free=19.7, wall=5259
2022-12-09 13:05:37 | INFO | train_inner | epoch 035:    638 / 1102 loss=7.612, nll_loss=3.924, ppl=15.18, wps=33437, ups=9.29, wpb=3598.2, bsz=154.8, num_updates=38100, lr=0.000162008, gnorm=1.727, loss_scale=4, train_wall=11, gb_free=19.7, wall=5270
2022-12-09 13:05:48 | INFO | train_inner | epoch 035:    738 / 1102 loss=7.671, nll_loss=3.962, ppl=15.58, wps=32647, ups=9.21, wpb=3542.9, bsz=139, num_updates=38200, lr=0.000161796, gnorm=1.804, loss_scale=4, train_wall=11, gb_free=19.7, wall=5281
2022-12-09 13:05:59 | INFO | train_inner | epoch 035:    838 / 1102 loss=7.685, nll_loss=3.986, ppl=15.85, wps=32682.2, ups=9.23, wpb=3542.3, bsz=138.3, num_updates=38300, lr=0.000161585, gnorm=1.798, loss_scale=4, train_wall=11, gb_free=19.2, wall=5291
2022-12-09 13:06:10 | INFO | train_inner | epoch 035:    938 / 1102 loss=7.665, nll_loss=3.963, ppl=15.59, wps=32927.4, ups=9.32, wpb=3531.7, bsz=136.2, num_updates=38400, lr=0.000161374, gnorm=1.781, loss_scale=4, train_wall=10, gb_free=19.3, wall=5302
2022-12-09 13:06:20 | INFO | train_inner | epoch 035:   1038 / 1102 loss=7.64, nll_loss=3.959, ppl=15.55, wps=33602.1, ups=9.22, wpb=3643.7, bsz=154.9, num_updates=38500, lr=0.000161165, gnorm=1.76, loss_scale=4, train_wall=11, gb_free=19.6, wall=5313
2022-12-09 13:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:07:05 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.768 | nll_loss 2.232 | ppl 4.7 | bleu 35.63 | wps 4755.2 | wpb 2835.3 | bsz 115.6 | num_updates 38564 | best_bleu 35.91
2022-12-09 13:07:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 38564 updates
2022-12-09 13:07:06 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint35.pt
2022-12-09 13:07:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint35.pt (epoch 35 @ 38564 updates, score 35.63) (writing took 1.2483479734510183 seconds)
2022-12-09 13:07:06 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-12-09 13:07:06 | INFO | train | epoch 035 | loss 7.633 | nll_loss 3.935 | ppl 15.29 | wps 24944.5 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 38564 | lr 0.000161031 | gnorm 1.762 | loss_scale 4 | train_wall 116 | gb_free 19.5 | wall 5359
2022-12-09 13:07:07 | INFO | fairseq.trainer | begin training epoch 36
2022-12-09 13:07:11 | INFO | train_inner | epoch 036:     36 / 1102 loss=7.56, nll_loss=3.865, ppl=14.57, wps=7113.9, ups=1.99, wpb=3571, bsz=153.4, num_updates=38600, lr=0.000160956, gnorm=1.808, loss_scale=4, train_wall=10, gb_free=19.5, wall=5363
2022-12-09 13:07:21 | INFO | train_inner | epoch 036:    136 / 1102 loss=7.505, nll_loss=3.801, ppl=13.94, wps=33521.7, ups=9.31, wpb=3602.5, bsz=156.6, num_updates=38700, lr=0.000160748, gnorm=1.753, loss_scale=4, train_wall=11, gb_free=19.3, wall=5374
2022-12-09 13:07:32 | INFO | train_inner | epoch 036:    236 / 1102 loss=7.545, nll_loss=3.828, ppl=14.2, wps=33142.5, ups=9.41, wpb=3521.9, bsz=147.9, num_updates=38800, lr=0.00016054, gnorm=1.768, loss_scale=4, train_wall=10, gb_free=19.3, wall=5385
2022-12-09 13:07:43 | INFO | train_inner | epoch 036:    336 / 1102 loss=7.577, nll_loss=3.875, ppl=14.67, wps=33429.3, ups=9.35, wpb=3574.6, bsz=144.6, num_updates=38900, lr=0.000160334, gnorm=1.799, loss_scale=4, train_wall=10, gb_free=19.7, wall=5395
2022-12-09 13:07:53 | INFO | train_inner | epoch 036:    436 / 1102 loss=7.712, nll_loss=4.011, ppl=16.12, wps=33486.5, ups=9.37, wpb=3573, bsz=132.9, num_updates=39000, lr=0.000160128, gnorm=1.785, loss_scale=4, train_wall=10, gb_free=19.6, wall=5406
2022-12-09 13:08:04 | INFO | train_inner | epoch 036:    536 / 1102 loss=7.581, nll_loss=3.873, ppl=14.65, wps=33771.1, ups=9.36, wpb=3609.1, bsz=143.4, num_updates=39100, lr=0.000159923, gnorm=1.775, loss_scale=4, train_wall=10, gb_free=19.5, wall=5417
2022-12-09 13:08:15 | INFO | train_inner | epoch 036:    636 / 1102 loss=7.66, nll_loss=3.941, ppl=15.36, wps=32577.4, ups=9.42, wpb=3458.6, bsz=140.5, num_updates=39200, lr=0.000159719, gnorm=1.894, loss_scale=4, train_wall=10, gb_free=19.5, wall=5427
2022-12-09 13:08:26 | INFO | train_inner | epoch 036:    736 / 1102 loss=7.553, nll_loss=3.856, ppl=14.48, wps=33585.4, ups=9.25, wpb=3631.7, bsz=160.2, num_updates=39300, lr=0.000159516, gnorm=1.704, loss_scale=4, train_wall=11, gb_free=19.3, wall=5438
2022-12-09 13:08:36 | INFO | train_inner | epoch 036:    836 / 1102 loss=7.706, nll_loss=4.001, ppl=16.01, wps=32908.6, ups=9.2, wpb=3578.5, bsz=133.7, num_updates=39400, lr=0.000159313, gnorm=1.79, loss_scale=4, train_wall=11, gb_free=19.4, wall=5449
2022-12-09 13:08:47 | INFO | train_inner | epoch 036:    936 / 1102 loss=7.706, nll_loss=4.006, ppl=16.06, wps=33732.9, ups=9.24, wpb=3650, bsz=132.8, num_updates=39500, lr=0.000159111, gnorm=1.768, loss_scale=4, train_wall=11, gb_free=19.4, wall=5460
2022-12-09 13:08:58 | INFO | train_inner | epoch 036:   1036 / 1102 loss=7.635, nll_loss=3.943, ppl=15.38, wps=33179.9, ups=9.31, wpb=3565.7, bsz=144.2, num_updates=39600, lr=0.00015891, gnorm=1.831, loss_scale=8, train_wall=11, gb_free=19.8, wall=5471
2022-12-09 13:09:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:09:43 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.758 | nll_loss 2.225 | ppl 4.68 | bleu 35.92 | wps 4792.3 | wpb 2835.3 | bsz 115.6 | num_updates 39666 | best_bleu 35.92
2022-12-09 13:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 39666 updates
2022-12-09 13:09:44 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint36.pt
2022-12-09 13:09:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint36.pt (epoch 36 @ 39666 updates, score 35.92) (writing took 1.76914347615093 seconds)
2022-12-09 13:09:45 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-12-09 13:09:45 | INFO | train | epoch 036 | loss 7.611 | nll_loss 3.91 | ppl 15.03 | wps 24974.1 | ups 6.97 | wpb 3583.6 | bsz 145.4 | num_updates 39666 | lr 0.000158778 | gnorm 1.778 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 5517
2022-12-09 13:09:45 | INFO | fairseq.trainer | begin training epoch 37
2022-12-09 13:09:49 | INFO | train_inner | epoch 037:     34 / 1102 loss=7.548, nll_loss=3.873, ppl=14.66, wps=7321.3, ups=1.97, wpb=3708.1, bsz=160.7, num_updates=39700, lr=0.00015871, gnorm=1.651, loss_scale=8, train_wall=11, gb_free=19.3, wall=5521
2022-12-09 13:09:59 | INFO | train_inner | epoch 037:    134 / 1102 loss=7.517, nll_loss=3.808, ppl=14, wps=32853.5, ups=9.32, wpb=3523.5, bsz=151.5, num_updates=39800, lr=0.000158511, gnorm=1.873, loss_scale=8, train_wall=10, gb_free=19.5, wall=5532
2022-12-09 13:10:10 | INFO | train_inner | epoch 037:    234 / 1102 loss=7.539, nll_loss=3.827, ppl=14.2, wps=32963.8, ups=9.26, wpb=3560.8, bsz=147.5, num_updates=39900, lr=0.000158312, gnorm=1.74, loss_scale=8, train_wall=11, gb_free=19.9, wall=5543
2022-12-09 13:10:21 | INFO | train_inner | epoch 037:    334 / 1102 loss=7.492, nll_loss=3.778, ppl=13.72, wps=33091.3, ups=9.33, wpb=3547.9, bsz=152.2, num_updates=40000, lr=0.000158114, gnorm=1.86, loss_scale=8, train_wall=10, gb_free=19.3, wall=5553
2022-12-09 13:10:31 | INFO | train_inner | epoch 037:    434 / 1102 loss=7.686, nll_loss=3.982, ppl=15.8, wps=33724.4, ups=9.42, wpb=3580.8, bsz=132.2, num_updates=40100, lr=0.000157917, gnorm=1.746, loss_scale=8, train_wall=10, gb_free=19.6, wall=5564
2022-12-09 13:10:42 | INFO | train_inner | epoch 037:    534 / 1102 loss=7.506, nll_loss=3.799, ppl=13.92, wps=33094.3, ups=9.25, wpb=3576, bsz=156, num_updates=40200, lr=0.00015772, gnorm=1.792, loss_scale=8, train_wall=11, gb_free=19.8, wall=5575
2022-12-09 13:10:53 | INFO | train_inner | epoch 037:    634 / 1102 loss=7.643, nll_loss=3.938, ppl=15.33, wps=32998.2, ups=9.17, wpb=3599.9, bsz=139.6, num_updates=40300, lr=0.000157524, gnorm=1.723, loss_scale=8, train_wall=11, gb_free=19.3, wall=5586
2022-12-09 13:11:04 | INFO | train_inner | epoch 037:    734 / 1102 loss=7.62, nll_loss=3.919, ppl=15.12, wps=33596.4, ups=9.29, wpb=3617.7, bsz=138.2, num_updates=40400, lr=0.000157329, gnorm=1.766, loss_scale=8, train_wall=11, gb_free=19.6, wall=5597
2022-12-09 13:11:15 | INFO | train_inner | epoch 037:    834 / 1102 loss=7.662, nll_loss=3.953, ppl=15.49, wps=32824.5, ups=9.3, wpb=3529.3, bsz=138.6, num_updates=40500, lr=0.000157135, gnorm=1.868, loss_scale=8, train_wall=11, gb_free=19.6, wall=5607
2022-12-09 13:11:26 | INFO | train_inner | epoch 037:    934 / 1102 loss=7.637, nll_loss=3.931, ppl=15.26, wps=32587.5, ups=9.14, wpb=3565.7, bsz=141.5, num_updates=40600, lr=0.000156941, gnorm=1.875, loss_scale=8, train_wall=11, gb_free=19.3, wall=5618
2022-12-09 13:11:36 | INFO | train_inner | epoch 037:   1034 / 1102 loss=7.597, nll_loss=3.916, ppl=15.09, wps=33968.3, ups=9.24, wpb=3677.4, bsz=157.2, num_updates=40700, lr=0.000156748, gnorm=1.723, loss_scale=8, train_wall=11, gb_free=19.5, wall=5629
2022-12-09 13:11:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:12:20 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.752 | nll_loss 2.22 | ppl 4.66 | bleu 35.81 | wps 4965.1 | wpb 2835.3 | bsz 115.6 | num_updates 40768 | best_bleu 35.92
2022-12-09 13:12:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 40768 updates
2022-12-09 13:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint37.pt
2022-12-09 13:12:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint37.pt (epoch 37 @ 40768 updates, score 35.81) (writing took 1.325712445192039 seconds)
2022-12-09 13:12:22 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-12-09 13:12:22 | INFO | train | epoch 037 | loss 7.591 | nll_loss 3.887 | ppl 14.8 | wps 25159.4 | ups 7.02 | wpb 3583.6 | bsz 145.4 | num_updates 40768 | lr 0.000156618 | gnorm 1.788 | loss_scale 8 | train_wall 116 | gb_free 19.7 | wall 5674
2022-12-09 13:12:22 | INFO | fairseq.trainer | begin training epoch 38
2022-12-09 13:12:25 | INFO | train_inner | epoch 038:     32 / 1102 loss=7.62, nll_loss=3.923, ppl=15.17, wps=7443.3, ups=2.05, wpb=3625.7, bsz=144, num_updates=40800, lr=0.000156556, gnorm=1.717, loss_scale=8, train_wall=11, gb_free=19.3, wall=5678
2022-12-09 13:12:36 | INFO | train_inner | epoch 038:    132 / 1102 loss=7.448, nll_loss=3.733, ppl=13.3, wps=33112.1, ups=9.32, wpb=3554.6, bsz=147.1, num_updates=40900, lr=0.000156365, gnorm=1.758, loss_scale=8, train_wall=10, gb_free=19.7, wall=5689
2022-12-09 13:12:47 | INFO | train_inner | epoch 038:    232 / 1102 loss=7.59, nll_loss=3.867, ppl=14.59, wps=33105.8, ups=9.33, wpb=3549.4, bsz=138.7, num_updates=41000, lr=0.000156174, gnorm=1.846, loss_scale=8, train_wall=10, gb_free=19.5, wall=5699
2022-12-09 13:12:57 | INFO | train_inner | epoch 038:    332 / 1102 loss=7.545, nll_loss=3.833, ppl=14.25, wps=32918.8, ups=9.25, wpb=3558.5, bsz=149, num_updates=41100, lr=0.000155984, gnorm=1.822, loss_scale=8, train_wall=11, gb_free=19.3, wall=5710
2022-12-09 13:13:08 | INFO | train_inner | epoch 038:    432 / 1102 loss=7.521, nll_loss=3.799, ppl=13.92, wps=33129.7, ups=9.3, wpb=3561.1, bsz=143, num_updates=41200, lr=0.000155794, gnorm=1.795, loss_scale=8, train_wall=11, gb_free=19.9, wall=5721
2022-12-09 13:13:19 | INFO | train_inner | epoch 038:    532 / 1102 loss=7.678, nll_loss=3.962, ppl=15.59, wps=33276.9, ups=9.43, wpb=3530, bsz=133.1, num_updates=41300, lr=0.000155606, gnorm=1.83, loss_scale=8, train_wall=10, gb_free=19.3, wall=5731
2022-12-09 13:13:30 | INFO | train_inner | epoch 038:    632 / 1102 loss=7.562, nll_loss=3.856, ppl=14.48, wps=33695.1, ups=9.25, wpb=3642.6, bsz=144.8, num_updates=41400, lr=0.000155417, gnorm=1.728, loss_scale=8, train_wall=11, gb_free=19.5, wall=5742
2022-12-09 13:13:40 | INFO | train_inner | epoch 038:    732 / 1102 loss=7.511, nll_loss=3.809, ppl=14.02, wps=33003.8, ups=9.22, wpb=3579.1, bsz=155, num_updates=41500, lr=0.00015523, gnorm=1.777, loss_scale=8, train_wall=11, gb_free=19.7, wall=5753
2022-12-09 13:13:51 | INFO | train_inner | epoch 038:    832 / 1102 loss=7.643, nll_loss=3.947, ppl=15.42, wps=33616.5, ups=9.37, wpb=3587.6, bsz=147, num_updates=41600, lr=0.000155043, gnorm=1.806, loss_scale=8, train_wall=10, gb_free=19.3, wall=5764
2022-12-09 13:14:02 | INFO | train_inner | epoch 038:    932 / 1102 loss=7.582, nll_loss=3.883, ppl=14.75, wps=33080.1, ups=9.18, wpb=3604.7, bsz=149.4, num_updates=41700, lr=0.000154857, gnorm=1.791, loss_scale=8, train_wall=11, gb_free=19.6, wall=5775
2022-12-09 13:14:13 | INFO | train_inner | epoch 038:   1032 / 1102 loss=7.679, nll_loss=3.985, ppl=15.84, wps=33645.3, ups=9.23, wpb=3643.7, bsz=141.8, num_updates=41800, lr=0.000154672, gnorm=1.758, loss_scale=8, train_wall=11, gb_free=19.3, wall=5785
2022-12-09 13:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:14:56 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.752 | nll_loss 2.218 | ppl 4.65 | bleu 35.87 | wps 5058.1 | wpb 2835.3 | bsz 115.6 | num_updates 41870 | best_bleu 35.92
2022-12-09 13:14:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 41870 updates
2022-12-09 13:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint38.pt
2022-12-09 13:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint38.pt (epoch 38 @ 41870 updates, score 35.87) (writing took 1.3033485068008304 seconds)
2022-12-09 13:14:58 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-12-09 13:14:58 | INFO | train | epoch 038 | loss 7.574 | nll_loss 3.866 | ppl 14.58 | wps 25313.6 | ups 7.06 | wpb 3583.6 | bsz 145.4 | num_updates 41870 | lr 0.000154543 | gnorm 1.79 | loss_scale 8 | train_wall 116 | gb_free 19.4 | wall 5830
2022-12-09 13:14:58 | INFO | fairseq.trainer | begin training epoch 39
2022-12-09 13:15:01 | INFO | train_inner | epoch 039:     30 / 1102 loss=7.534, nll_loss=3.825, ppl=14.17, wps=7429, ups=2.08, wpb=3578, bsz=146.9, num_updates=41900, lr=0.000154487, gnorm=1.791, loss_scale=8, train_wall=10, gb_free=19.6, wall=5834
2022-12-09 13:15:12 | INFO | train_inner | epoch 039:    130 / 1102 loss=7.559, nll_loss=3.839, ppl=14.32, wps=33333.9, ups=9.39, wpb=3551.5, bsz=129.8, num_updates=42000, lr=0.000154303, gnorm=1.813, loss_scale=8, train_wall=10, gb_free=19.2, wall=5844
2022-12-09 13:15:22 | INFO | train_inner | epoch 039:    230 / 1102 loss=7.434, nll_loss=3.721, ppl=13.19, wps=33119.9, ups=9.27, wpb=3573.9, bsz=153.4, num_updates=42100, lr=0.00015412, gnorm=1.747, loss_scale=8, train_wall=11, gb_free=19.4, wall=5855
2022-12-09 13:15:33 | INFO | train_inner | epoch 039:    330 / 1102 loss=7.478, nll_loss=3.769, ppl=13.63, wps=33638.6, ups=9.23, wpb=3644.3, bsz=155.1, num_updates=42200, lr=0.000153937, gnorm=1.73, loss_scale=8, train_wall=11, gb_free=19.5, wall=5866
2022-12-09 13:15:44 | INFO | train_inner | epoch 039:    430 / 1102 loss=7.509, nll_loss=3.799, ppl=13.92, wps=33358.1, ups=9.3, wpb=3588.4, bsz=148.2, num_updates=42300, lr=0.000153755, gnorm=1.78, loss_scale=8, train_wall=11, gb_free=19.5, wall=5877
2022-12-09 13:15:55 | INFO | train_inner | epoch 039:    530 / 1102 loss=7.612, nll_loss=3.892, ppl=14.85, wps=33299.6, ups=9.34, wpb=3564.5, bsz=135.1, num_updates=42400, lr=0.000153574, gnorm=1.853, loss_scale=8, train_wall=10, gb_free=19.5, wall=5887
2022-12-09 13:16:06 | INFO | train_inner | epoch 039:    630 / 1102 loss=7.484, nll_loss=3.787, ppl=13.81, wps=33530.4, ups=9.27, wpb=3618.5, bsz=163.3, num_updates=42500, lr=0.000153393, gnorm=1.786, loss_scale=8, train_wall=11, gb_free=20.1, wall=5898
2022-12-09 13:16:16 | INFO | train_inner | epoch 039:    730 / 1102 loss=7.578, nll_loss=3.878, ppl=14.7, wps=33615.8, ups=9.35, wpb=3596.8, bsz=154.7, num_updates=42600, lr=0.000153213, gnorm=1.77, loss_scale=8, train_wall=10, gb_free=19.3, wall=5909
2022-12-09 13:16:27 | INFO | train_inner | epoch 039:    830 / 1102 loss=7.579, nll_loss=3.87, ppl=14.62, wps=31970.2, ups=9.13, wpb=3502.5, bsz=152.2, num_updates=42700, lr=0.000153033, gnorm=1.831, loss_scale=8, train_wall=11, gb_free=19.4, wall=5920
2022-12-09 13:16:38 | INFO | train_inner | epoch 039:    930 / 1102 loss=7.661, nll_loss=3.956, ppl=15.52, wps=33320.5, ups=9.26, wpb=3598.4, bsz=134.4, num_updates=42800, lr=0.000152854, gnorm=1.803, loss_scale=8, train_wall=11, gb_free=19.4, wall=5931
2022-12-09 13:16:49 | INFO | train_inner | epoch 039:   1030 / 1102 loss=7.643, nll_loss=3.927, ppl=15.21, wps=33117, ups=9.19, wpb=3605.2, bsz=136, num_updates=42900, lr=0.000152676, gnorm=1.809, loss_scale=8, train_wall=11, gb_free=19.3, wall=5942
2022-12-09 13:16:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:17:33 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.748 | nll_loss 2.216 | ppl 4.65 | bleu 35.86 | wps 4996.4 | wpb 2835.3 | bsz 115.6 | num_updates 42972 | best_bleu 35.92
2022-12-09 13:17:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 42972 updates
2022-12-09 13:17:34 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint39.pt
2022-12-09 13:17:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint39.pt (epoch 39 @ 42972 updates, score 35.86) (writing took 1.317911459133029 seconds)
2022-12-09 13:17:34 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-12-09 13:17:34 | INFO | train | epoch 039 | loss 7.555 | nll_loss 3.844 | ppl 14.36 | wps 25236.6 | ups 7.04 | wpb 3583.6 | bsz 145.4 | num_updates 42972 | lr 0.000152548 | gnorm 1.79 | loss_scale 8 | train_wall 116 | gb_free 20.1 | wall 5987
2022-12-09 13:17:34 | INFO | fairseq.trainer | begin training epoch 40
2022-12-09 13:17:37 | INFO | train_inner | epoch 040:     28 / 1102 loss=7.534, nll_loss=3.82, ppl=14.12, wps=7370.8, ups=2.07, wpb=3566.1, bsz=148.7, num_updates=43000, lr=0.000152499, gnorm=1.811, loss_scale=8, train_wall=10, gb_free=19.4, wall=5990
2022-12-09 13:17:48 | INFO | train_inner | epoch 040:    128 / 1102 loss=7.519, nll_loss=3.792, ppl=13.85, wps=33155.2, ups=9.4, wpb=3528.3, bsz=133.7, num_updates=43100, lr=0.000152322, gnorm=1.774, loss_scale=8, train_wall=10, gb_free=19.4, wall=6001
2022-12-09 13:17:59 | INFO | train_inner | epoch 040:    228 / 1102 loss=7.514, nll_loss=3.815, ppl=14.07, wps=33563.8, ups=9.26, wpb=3623, bsz=153.8, num_updates=43200, lr=0.000152145, gnorm=1.691, loss_scale=8, train_wall=11, gb_free=19.8, wall=6011
2022-12-09 13:18:09 | INFO | train_inner | epoch 040:    328 / 1102 loss=7.451, nll_loss=3.741, ppl=13.37, wps=33562.3, ups=9.3, wpb=3608.3, bsz=153.4, num_updates=43300, lr=0.000151969, gnorm=1.785, loss_scale=8, train_wall=11, gb_free=19.6, wall=6022
2022-12-09 13:18:20 | INFO | train_inner | epoch 040:    428 / 1102 loss=7.476, nll_loss=3.76, ppl=13.55, wps=33321.5, ups=9.19, wpb=3624.3, bsz=145.3, num_updates=43400, lr=0.000151794, gnorm=1.758, loss_scale=8, train_wall=11, gb_free=19.7, wall=6033
2022-12-09 13:18:31 | INFO | train_inner | epoch 040:    528 / 1102 loss=7.576, nll_loss=3.855, ppl=14.47, wps=33421.2, ups=9.37, wpb=3566, bsz=136.1, num_updates=43500, lr=0.00015162, gnorm=1.815, loss_scale=8, train_wall=10, gb_free=19.6, wall=6044
2022-12-09 13:18:42 | INFO | train_inner | epoch 040:    628 / 1102 loss=7.595, nll_loss=3.867, ppl=14.59, wps=33039.5, ups=9.35, wpb=3534.8, bsz=133.4, num_updates=43600, lr=0.000151446, gnorm=1.842, loss_scale=8, train_wall=10, gb_free=19.6, wall=6054
2022-12-09 13:18:53 | INFO | train_inner | epoch 040:    728 / 1102 loss=7.541, nll_loss=3.841, ppl=14.33, wps=33346.2, ups=9.2, wpb=3623.4, bsz=154.4, num_updates=43700, lr=0.000151272, gnorm=1.761, loss_scale=8, train_wall=11, gb_free=19.4, wall=6065
2022-12-09 13:19:03 | INFO | train_inner | epoch 040:    828 / 1102 loss=7.598, nll_loss=3.876, ppl=14.68, wps=33111.7, ups=9.18, wpb=3607.6, bsz=135, num_updates=43800, lr=0.000151099, gnorm=1.899, loss_scale=8, train_wall=11, gb_free=19.4, wall=6076
2022-12-09 13:19:14 | INFO | train_inner | epoch 040:    928 / 1102 loss=7.574, nll_loss=3.861, ppl=14.53, wps=32981.9, ups=9.22, wpb=3578.2, bsz=140.3, num_updates=43900, lr=0.000150927, gnorm=1.837, loss_scale=8, train_wall=11, gb_free=19.4, wall=6087
2022-12-09 13:19:25 | INFO | train_inner | epoch 040:   1028 / 1102 loss=7.583, nll_loss=3.885, ppl=14.77, wps=32757.3, ups=9.13, wpb=3586.7, bsz=153.8, num_updates=44000, lr=0.000150756, gnorm=1.841, loss_scale=8, train_wall=11, gb_free=19.6, wall=6098
2022-12-09 13:19:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:20:12 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.746 | nll_loss 2.211 | ppl 4.63 | bleu 36.09 | wps 4690.6 | wpb 2835.3 | bsz 115.6 | num_updates 44074 | best_bleu 36.09
2022-12-09 13:20:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 44074 updates
2022-12-09 13:20:13 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint40.pt
2022-12-09 13:20:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint40.pt (epoch 40 @ 44074 updates, score 36.09) (writing took 1.7347557870671153 seconds)
2022-12-09 13:20:14 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-12-09 13:20:14 | INFO | train | epoch 040 | loss 7.538 | nll_loss 3.825 | ppl 14.17 | wps 24758.8 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 44074 | lr 0.000150629 | gnorm 1.806 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 6146
2022-12-09 13:20:14 | INFO | fairseq.trainer | begin training epoch 41
2022-12-09 13:20:17 | INFO | train_inner | epoch 041:     26 / 1102 loss=7.561, nll_loss=3.851, ppl=14.43, wps=6854.5, ups=1.95, wpb=3516.6, bsz=145.8, num_updates=44100, lr=0.000150585, gnorm=1.879, loss_scale=8, train_wall=11, gb_free=19.2, wall=6149
2022-12-09 13:20:27 | INFO | train_inner | epoch 041:    126 / 1102 loss=7.434, nll_loss=3.709, ppl=13.08, wps=33042.5, ups=9.26, wpb=3569.6, bsz=147, num_updates=44200, lr=0.000150414, gnorm=1.734, loss_scale=8, train_wall=11, gb_free=19.5, wall=6160
2022-12-09 13:20:38 | INFO | train_inner | epoch 041:    226 / 1102 loss=7.518, nll_loss=3.795, ppl=13.88, wps=32979.2, ups=9.29, wpb=3551.3, bsz=143.6, num_updates=44300, lr=0.000150244, gnorm=1.834, loss_scale=8, train_wall=11, gb_free=19.9, wall=6171
2022-12-09 13:20:49 | INFO | train_inner | epoch 041:    326 / 1102 loss=7.504, nll_loss=3.794, ppl=13.87, wps=33039.9, ups=9.19, wpb=3594.5, bsz=150.3, num_updates=44400, lr=0.000150075, gnorm=1.752, loss_scale=8, train_wall=11, gb_free=19.8, wall=6182
2022-12-09 13:21:00 | INFO | train_inner | epoch 041:    426 / 1102 loss=7.53, nll_loss=3.799, ppl=13.92, wps=32801.9, ups=9.17, wpb=3577.7, bsz=134.3, num_updates=44500, lr=0.000149906, gnorm=1.811, loss_scale=8, train_wall=11, gb_free=19.5, wall=6193
2022-12-09 13:21:11 | INFO | train_inner | epoch 041:    526 / 1102 loss=7.487, nll_loss=3.798, ppl=13.91, wps=33105.9, ups=8.99, wpb=3683.1, bsz=167, num_updates=44600, lr=0.000149738, gnorm=1.856, loss_scale=8, train_wall=11, gb_free=19.6, wall=6204
2022-12-09 13:21:22 | INFO | train_inner | epoch 041:    626 / 1102 loss=7.562, nll_loss=3.848, ppl=14.4, wps=32760.7, ups=9.2, wpb=3559.4, bsz=141.8, num_updates=44700, lr=0.000149571, gnorm=1.957, loss_scale=8, train_wall=11, gb_free=19.3, wall=6215
2022-12-09 13:21:33 | INFO | train_inner | epoch 041:    726 / 1102 loss=7.52, nll_loss=3.803, ppl=13.96, wps=32895.3, ups=9.15, wpb=3594.9, bsz=137.5, num_updates=44800, lr=0.000149404, gnorm=1.763, loss_scale=8, train_wall=11, gb_free=19.7, wall=6225
2022-12-09 13:21:44 | INFO | train_inner | epoch 041:    826 / 1102 loss=7.597, nll_loss=3.881, ppl=14.73, wps=33177.1, ups=9.21, wpb=3602.7, bsz=134.6, num_updates=44900, lr=0.000149237, gnorm=1.801, loss_scale=8, train_wall=11, gb_free=19.8, wall=6236
2022-12-09 13:21:55 | INFO | train_inner | epoch 041:    926 / 1102 loss=7.497, nll_loss=3.784, ppl=13.78, wps=33012.6, ups=9.23, wpb=3576.9, bsz=148, num_updates=45000, lr=0.000149071, gnorm=1.794, loss_scale=8, train_wall=11, gb_free=19.3, wall=6247
2022-12-09 13:22:05 | INFO | train_inner | epoch 041:   1026 / 1102 loss=7.542, nll_loss=3.835, ppl=14.27, wps=32936.3, ups=9.25, wpb=3560.1, bsz=152.6, num_updates=45100, lr=0.000148906, gnorm=1.79, loss_scale=8, train_wall=11, gb_free=19.7, wall=6258
2022-12-09 13:22:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:22:52 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.737 | nll_loss 2.205 | ppl 4.61 | bleu 36.1 | wps 4641 | wpb 2835.3 | bsz 115.6 | num_updates 45176 | best_bleu 36.1
2022-12-09 13:22:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 45176 updates
2022-12-09 13:22:53 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint41.pt
2022-12-09 13:22:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint41.pt (epoch 41 @ 45176 updates, score 36.1) (writing took 1.776385766454041 seconds)
2022-12-09 13:22:54 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-12-09 13:22:54 | INFO | train | epoch 041 | loss 7.521 | nll_loss 3.806 | ppl 13.99 | wps 24572.8 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 45176 | lr 0.000148781 | gnorm 1.812 | loss_scale 8 | train_wall 117 | gb_free 19.6 | wall 6307
2022-12-09 13:22:54 | INFO | fairseq.trainer | begin training epoch 42
2022-12-09 13:22:57 | INFO | train_inner | epoch 042:     24 / 1102 loss=7.484, nll_loss=3.772, ppl=13.66, wps=6920.4, ups=1.94, wpb=3576.2, bsz=151, num_updates=45200, lr=0.000148741, gnorm=1.78, loss_scale=8, train_wall=11, gb_free=19.3, wall=6310
2022-12-09 13:23:08 | INFO | train_inner | epoch 042:    124 / 1102 loss=7.485, nll_loss=3.761, ppl=13.56, wps=33288.2, ups=9.38, wpb=3549.2, bsz=144.6, num_updates=45300, lr=0.000148577, gnorm=1.832, loss_scale=8, train_wall=10, gb_free=19.8, wall=6320
2022-12-09 13:23:18 | INFO | train_inner | epoch 042:    224 / 1102 loss=7.414, nll_loss=3.722, ppl=13.19, wps=34000.2, ups=9.27, wpb=3669.2, bsz=169.8, num_updates=45400, lr=0.000148413, gnorm=1.717, loss_scale=8, train_wall=11, gb_free=19.3, wall=6331
2022-12-09 13:23:29 | INFO | train_inner | epoch 042:    324 / 1102 loss=7.565, nll_loss=3.82, ppl=14.12, wps=33063.1, ups=9.36, wpb=3530.9, bsz=128.6, num_updates=45500, lr=0.00014825, gnorm=1.828, loss_scale=8, train_wall=10, gb_free=19.6, wall=6342
2022-12-09 13:23:40 | INFO | train_inner | epoch 042:    424 / 1102 loss=7.507, nll_loss=3.772, ppl=13.66, wps=32836.6, ups=9.35, wpb=3511.6, bsz=134.2, num_updates=45600, lr=0.000148087, gnorm=1.887, loss_scale=8, train_wall=10, gb_free=19.3, wall=6352
2022-12-09 13:23:51 | INFO | train_inner | epoch 042:    524 / 1102 loss=7.459, nll_loss=3.731, ppl=13.27, wps=32918.3, ups=9.18, wpb=3587.1, bsz=141.9, num_updates=45700, lr=0.000147925, gnorm=1.794, loss_scale=8, train_wall=11, gb_free=19.8, wall=6363
2022-12-09 13:24:02 | INFO | train_inner | epoch 042:    624 / 1102 loss=7.5, nll_loss=3.787, ppl=13.8, wps=33446.1, ups=9.21, wpb=3631.2, bsz=148.6, num_updates=45800, lr=0.000147764, gnorm=1.75, loss_scale=8, train_wall=11, gb_free=19.3, wall=6374
2022-12-09 13:24:12 | INFO | train_inner | epoch 042:    724 / 1102 loss=7.529, nll_loss=3.81, ppl=14.02, wps=33291.3, ups=9.27, wpb=3589.8, bsz=142.4, num_updates=45900, lr=0.000147602, gnorm=1.834, loss_scale=8, train_wall=11, gb_free=19.6, wall=6385
2022-12-09 13:24:23 | INFO | train_inner | epoch 042:    824 / 1102 loss=7.47, nll_loss=3.764, ppl=13.59, wps=33131.5, ups=9.18, wpb=3609.5, bsz=161.4, num_updates=46000, lr=0.000147442, gnorm=1.806, loss_scale=8, train_wall=11, gb_free=19.9, wall=6396
2022-12-09 13:24:34 | INFO | train_inner | epoch 042:    924 / 1102 loss=7.533, nll_loss=3.82, ppl=14.13, wps=33305.3, ups=9.26, wpb=3595.2, bsz=140.8, num_updates=46100, lr=0.000147282, gnorm=1.784, loss_scale=8, train_wall=11, gb_free=19.3, wall=6407
2022-12-09 13:24:45 | INFO | train_inner | epoch 042:   1024 / 1102 loss=7.606, nll_loss=3.895, ppl=14.88, wps=32831.8, ups=9.22, wpb=3561.1, bsz=139.8, num_updates=46200, lr=0.000147122, gnorm=1.77, loss_scale=8, train_wall=11, gb_free=19.5, wall=6418
2022-12-09 13:24:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:25:31 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.741 | nll_loss 2.201 | ppl 4.6 | bleu 36 | wps 4781.4 | wpb 2835.3 | bsz 115.6 | num_updates 46278 | best_bleu 36.1
2022-12-09 13:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 46278 updates
2022-12-09 13:25:32 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint42.pt
2022-12-09 13:25:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint42.pt (epoch 42 @ 46278 updates, score 36.0) (writing took 1.4626016160473228 seconds)
2022-12-09 13:25:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-12-09 13:25:33 | INFO | train | epoch 042 | loss 7.504 | nll_loss 3.786 | ppl 13.79 | wps 24946.7 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 46278 | lr 0.000146998 | gnorm 1.798 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 6465
2022-12-09 13:25:33 | INFO | fairseq.trainer | begin training epoch 43
2022-12-09 13:25:35 | INFO | train_inner | epoch 043:     22 / 1102 loss=7.515, nll_loss=3.789, ppl=13.82, wps=7003.2, ups=1.99, wpb=3515, bsz=133.2, num_updates=46300, lr=0.000146964, gnorm=1.853, loss_scale=8, train_wall=10, gb_free=19.7, wall=6468
2022-12-09 13:25:46 | INFO | train_inner | epoch 043:    122 / 1102 loss=7.415, nll_loss=3.695, ppl=12.95, wps=33403.8, ups=9.26, wpb=3607.6, bsz=149.5, num_updates=46400, lr=0.000146805, gnorm=1.741, loss_scale=8, train_wall=11, gb_free=19.5, wall=6479
2022-12-09 13:25:57 | INFO | train_inner | epoch 043:    222 / 1102 loss=7.464, nll_loss=3.739, ppl=13.35, wps=33787.1, ups=9.26, wpb=3650.4, bsz=143.2, num_updates=46500, lr=0.000146647, gnorm=1.744, loss_scale=8, train_wall=11, gb_free=19.5, wall=6489
2022-12-09 13:26:08 | INFO | train_inner | epoch 043:    322 / 1102 loss=7.446, nll_loss=3.716, ppl=13.15, wps=33250.2, ups=9.24, wpb=3596.9, bsz=146.2, num_updates=46600, lr=0.00014649, gnorm=1.797, loss_scale=8, train_wall=11, gb_free=19.6, wall=6500
2022-12-09 13:26:18 | INFO | train_inner | epoch 043:    422 / 1102 loss=7.465, nll_loss=3.74, ppl=13.36, wps=33143.3, ups=9.19, wpb=3605.6, bsz=144.6, num_updates=46700, lr=0.000146333, gnorm=1.8, loss_scale=8, train_wall=11, gb_free=19.4, wall=6511
2022-12-09 13:26:29 | INFO | train_inner | epoch 043:    522 / 1102 loss=7.489, nll_loss=3.768, ppl=13.62, wps=33107.8, ups=9.3, wpb=3561.6, bsz=143.4, num_updates=46800, lr=0.000146176, gnorm=1.793, loss_scale=8, train_wall=11, gb_free=19.6, wall=6522
2022-12-09 13:26:40 | INFO | train_inner | epoch 043:    622 / 1102 loss=7.546, nll_loss=3.821, ppl=14.13, wps=33469.8, ups=9.4, wpb=3562.3, bsz=140.8, num_updates=46900, lr=0.00014602, gnorm=1.857, loss_scale=8, train_wall=10, gb_free=19.9, wall=6532
2022-12-09 13:26:51 | INFO | train_inner | epoch 043:    722 / 1102 loss=7.429, nll_loss=3.716, ppl=13.14, wps=32512.6, ups=9.23, wpb=3522.7, bsz=159.8, num_updates=47000, lr=0.000145865, gnorm=1.82, loss_scale=8, train_wall=11, gb_free=19.4, wall=6543
2022-12-09 13:27:01 | INFO | train_inner | epoch 043:    822 / 1102 loss=7.554, nll_loss=3.847, ppl=14.39, wps=33600.3, ups=9.29, wpb=3618.3, bsz=145.8, num_updates=47100, lr=0.00014571, gnorm=1.771, loss_scale=8, train_wall=11, gb_free=19.8, wall=6554
2022-12-09 13:27:12 | INFO | train_inner | epoch 043:    922 / 1102 loss=7.485, nll_loss=3.775, ppl=13.69, wps=33304.5, ups=9.28, wpb=3590, bsz=151.4, num_updates=47200, lr=0.000145556, gnorm=1.746, loss_scale=8, train_wall=11, gb_free=19.9, wall=6565
2022-12-09 13:27:23 | INFO | train_inner | epoch 043:   1022 / 1102 loss=7.52, nll_loss=3.821, ppl=14.13, wps=33076.3, ups=9.31, wpb=3554.3, bsz=156.2, num_updates=47300, lr=0.000145402, gnorm=1.799, loss_scale=8, train_wall=11, gb_free=19.8, wall=6576
2022-12-09 13:27:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:28:10 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.734 | nll_loss 2.199 | ppl 4.59 | bleu 36.19 | wps 4688.3 | wpb 2835.3 | bsz 115.6 | num_updates 47380 | best_bleu 36.19
2022-12-09 13:28:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 47380 updates
2022-12-09 13:28:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint43.pt
2022-12-09 13:28:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint43.pt (epoch 43 @ 47380 updates, score 36.19) (writing took 1.7984906723722816 seconds)
2022-12-09 13:28:12 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-12-09 13:28:12 | INFO | train | epoch 043 | loss 7.488 | nll_loss 3.769 | ppl 13.63 | wps 24754.6 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 47380 | lr 0.000145279 | gnorm 1.791 | loss_scale 8 | train_wall 116 | gb_free 19.3 | wall 6625
2022-12-09 13:28:12 | INFO | fairseq.trainer | begin training epoch 44
2022-12-09 13:28:15 | INFO | train_inner | epoch 044:     20 / 1102 loss=7.543, nll_loss=3.822, ppl=14.14, wps=7020.3, ups=1.94, wpb=3619.7, bsz=131, num_updates=47400, lr=0.000145248, gnorm=1.8, loss_scale=8, train_wall=11, gb_free=19.6, wall=6627
2022-12-09 13:28:25 | INFO | train_inner | epoch 044:    120 / 1102 loss=7.444, nll_loss=3.703, ppl=13.03, wps=33345.2, ups=9.29, wpb=3587.8, bsz=135.8, num_updates=47500, lr=0.000145095, gnorm=1.778, loss_scale=8, train_wall=11, gb_free=19.5, wall=6638
2022-12-09 13:28:36 | INFO | train_inner | epoch 044:    220 / 1102 loss=7.395, nll_loss=3.667, ppl=12.7, wps=33196.8, ups=9.31, wpb=3566.2, bsz=152.1, num_updates=47600, lr=0.000144943, gnorm=1.776, loss_scale=8, train_wall=10, gb_free=19.5, wall=6649
2022-12-09 13:28:47 | INFO | train_inner | epoch 044:    320 / 1102 loss=7.465, nll_loss=3.733, ppl=13.29, wps=33624.9, ups=9.39, wpb=3581.5, bsz=135.3, num_updates=47700, lr=0.000144791, gnorm=1.759, loss_scale=8, train_wall=10, gb_free=19.3, wall=6659
2022-12-09 13:28:57 | INFO | train_inner | epoch 044:    420 / 1102 loss=7.516, nll_loss=3.775, ppl=13.69, wps=32624.4, ups=9.38, wpb=3477.6, bsz=136.5, num_updates=47800, lr=0.000144639, gnorm=1.863, loss_scale=8, train_wall=10, gb_free=19.7, wall=6670
2022-12-09 13:29:08 | INFO | train_inner | epoch 044:    520 / 1102 loss=7.447, nll_loss=3.725, ppl=13.22, wps=33491.8, ups=9.31, wpb=3599.2, bsz=151.8, num_updates=47900, lr=0.000144488, gnorm=1.837, loss_scale=8, train_wall=11, gb_free=19.7, wall=6681
2022-12-09 13:29:19 | INFO | train_inner | epoch 044:    620 / 1102 loss=7.394, nll_loss=3.697, ppl=12.97, wps=34038.9, ups=9.26, wpb=3674.9, bsz=167.3, num_updates=48000, lr=0.000144338, gnorm=1.662, loss_scale=8, train_wall=11, gb_free=19.4, wall=6691
2022-12-09 13:29:30 | INFO | train_inner | epoch 044:    720 / 1102 loss=7.524, nll_loss=3.8, ppl=13.93, wps=33187.2, ups=9.32, wpb=3560.6, bsz=142.6, num_updates=48100, lr=0.000144187, gnorm=1.811, loss_scale=8, train_wall=10, gb_free=19.8, wall=6702
2022-12-09 13:29:41 | INFO | train_inner | epoch 044:    820 / 1102 loss=7.478, nll_loss=3.757, ppl=13.52, wps=32888, ups=9.11, wpb=3608.3, bsz=143.1, num_updates=48200, lr=0.000144038, gnorm=1.736, loss_scale=8, train_wall=11, gb_free=19.3, wall=6713
2022-12-09 13:29:51 | INFO | train_inner | epoch 044:    920 / 1102 loss=7.522, nll_loss=3.796, ppl=13.89, wps=32891.4, ups=9.2, wpb=3575.1, bsz=134.8, num_updates=48300, lr=0.000143889, gnorm=1.777, loss_scale=8, train_wall=11, gb_free=19.5, wall=6724
2022-12-09 13:30:02 | INFO | train_inner | epoch 044:   1020 / 1102 loss=7.541, nll_loss=3.831, ppl=14.23, wps=33121.1, ups=9.31, wpb=3555.8, bsz=149, num_updates=48400, lr=0.00014374, gnorm=1.854, loss_scale=8, train_wall=10, gb_free=19.5, wall=6735
2022-12-09 13:30:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:30:48 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.746 | nll_loss 2.209 | ppl 4.62 | bleu 35.75 | wps 4912.2 | wpb 2835.3 | bsz 115.6 | num_updates 48482 | best_bleu 36.19
2022-12-09 13:30:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 48482 updates
2022-12-09 13:30:49 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint44.pt
2022-12-09 13:30:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint44.pt (epoch 44 @ 48482 updates, score 35.75) (writing took 1.2594168968498707 seconds)
2022-12-09 13:30:49 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-12-09 13:30:49 | INFO | train | epoch 044 | loss 7.475 | nll_loss 3.753 | ppl 13.48 | wps 25158.5 | ups 7.02 | wpb 3583.6 | bsz 145.4 | num_updates 48482 | lr 0.000143618 | gnorm 1.781 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 6782
2022-12-09 13:30:49 | INFO | fairseq.trainer | begin training epoch 45
2022-12-09 13:30:51 | INFO | train_inner | epoch 045:     18 / 1102 loss=7.484, nll_loss=3.771, ppl=13.66, wps=7385.7, ups=2.04, wpb=3620.9, bsz=145.2, num_updates=48500, lr=0.000143592, gnorm=1.715, loss_scale=8, train_wall=11, gb_free=19.5, wall=6784
2022-12-09 13:31:02 | INFO | train_inner | epoch 045:    118 / 1102 loss=7.356, nll_loss=3.629, ppl=12.37, wps=33330.6, ups=9.3, wpb=3585.5, bsz=151.8, num_updates=48600, lr=0.000143444, gnorm=1.752, loss_scale=8, train_wall=11, gb_free=19.4, wall=6795
2022-12-09 13:31:13 | INFO | train_inner | epoch 045:    218 / 1102 loss=7.405, nll_loss=3.67, ppl=12.73, wps=32934.2, ups=9.21, wpb=3576.6, bsz=147.5, num_updates=48700, lr=0.000143296, gnorm=1.833, loss_scale=8, train_wall=11, gb_free=19.4, wall=6805
2022-12-09 13:31:24 | INFO | train_inner | epoch 045:    318 / 1102 loss=7.478, nll_loss=3.73, ppl=13.27, wps=32558.1, ups=9.29, wpb=3504.2, bsz=127.8, num_updates=48800, lr=0.00014315, gnorm=1.837, loss_scale=8, train_wall=11, gb_free=19.5, wall=6816
2022-12-09 13:31:34 | INFO | train_inner | epoch 045:    418 / 1102 loss=7.466, nll_loss=3.731, ppl=13.28, wps=33599.8, ups=9.34, wpb=3597.8, bsz=135.9, num_updates=48900, lr=0.000143003, gnorm=1.781, loss_scale=8, train_wall=10, gb_free=19.7, wall=6827
2022-12-09 13:31:45 | INFO | train_inner | epoch 045:    518 / 1102 loss=7.522, nll_loss=3.802, ppl=13.95, wps=33486.1, ups=9.25, wpb=3620.2, bsz=146.6, num_updates=49000, lr=0.000142857, gnorm=1.855, loss_scale=8, train_wall=11, gb_free=19.3, wall=6838
2022-12-09 13:31:56 | INFO | train_inner | epoch 045:    618 / 1102 loss=7.473, nll_loss=3.736, ppl=13.32, wps=33009, ups=9.3, wpb=3549.5, bsz=137.9, num_updates=49100, lr=0.000142712, gnorm=1.827, loss_scale=8, train_wall=11, gb_free=19.7, wall=6848
2022-12-09 13:32:07 | INFO | train_inner | epoch 045:    718 / 1102 loss=7.465, nll_loss=3.739, ppl=13.35, wps=33311.7, ups=9.25, wpb=3603, bsz=144.2, num_updates=49200, lr=0.000142566, gnorm=1.796, loss_scale=8, train_wall=11, gb_free=19.4, wall=6859
2022-12-09 13:32:17 | INFO | train_inner | epoch 045:    818 / 1102 loss=7.571, nll_loss=3.841, ppl=14.33, wps=32632, ups=9.26, wpb=3522.6, bsz=130.2, num_updates=49300, lr=0.000142422, gnorm=1.866, loss_scale=8, train_wall=11, gb_free=19.5, wall=6870
2022-12-09 13:32:28 | INFO | train_inner | epoch 045:    918 / 1102 loss=7.495, nll_loss=3.785, ppl=13.79, wps=33289.2, ups=9.19, wpb=3621.8, bsz=147.4, num_updates=49400, lr=0.000142278, gnorm=1.746, loss_scale=8, train_wall=11, gb_free=19.2, wall=6881
2022-12-09 13:32:39 | INFO | train_inner | epoch 045:   1018 / 1102 loss=7.41, nll_loss=3.71, ppl=13.09, wps=33464, ups=9.16, wpb=3653.8, bsz=166.8, num_updates=49500, lr=0.000142134, gnorm=1.799, loss_scale=8, train_wall=11, gb_free=19.6, wall=6892
2022-12-09 13:32:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:33:25 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.729 | nll_loss 2.191 | ppl 4.57 | bleu 36.25 | wps 4933.8 | wpb 2835.3 | bsz 115.6 | num_updates 49584 | best_bleu 36.25
2022-12-09 13:33:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 49584 updates
2022-12-09 13:33:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint45.pt
2022-12-09 13:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint45.pt (epoch 45 @ 49584 updates, score 36.25) (writing took 1.6586200799793005 seconds)
2022-12-09 13:33:27 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-12-09 13:33:27 | INFO | train | epoch 045 | loss 7.455 | nll_loss 3.731 | ppl 13.28 | wps 25040.4 | ups 6.99 | wpb 3583.6 | bsz 145.4 | num_updates 49584 | lr 0.000142013 | gnorm 1.803 | loss_scale 8 | train_wall 117 | gb_free 19.7 | wall 6939
2022-12-09 13:33:27 | INFO | fairseq.trainer | begin training epoch 46
2022-12-09 13:33:29 | INFO | train_inner | epoch 046:     16 / 1102 loss=7.352, nll_loss=3.652, ppl=12.57, wps=7260.6, ups=2.02, wpb=3593.7, bsz=171.1, num_updates=49600, lr=0.00014199, gnorm=1.732, loss_scale=8, train_wall=11, gb_free=19.6, wall=6941
2022-12-09 13:33:40 | INFO | train_inner | epoch 046:    116 / 1102 loss=7.348, nll_loss=3.624, ppl=12.33, wps=33241.9, ups=9.29, wpb=3578.5, bsz=153.1, num_updates=49700, lr=0.000141848, gnorm=1.771, loss_scale=8, train_wall=11, gb_free=19.9, wall=6952
2022-12-09 13:33:50 | INFO | train_inner | epoch 046:    216 / 1102 loss=7.475, nll_loss=3.733, ppl=13.3, wps=33692.2, ups=9.42, wpb=3577, bsz=127.8, num_updates=49800, lr=0.000141705, gnorm=1.89, loss_scale=8, train_wall=10, gb_free=19.5, wall=6963
2022-12-09 13:34:01 | INFO | train_inner | epoch 046:    316 / 1102 loss=7.373, nll_loss=3.637, ppl=12.44, wps=33190.2, ups=9.3, wpb=3568.7, bsz=144, num_updates=49900, lr=0.000141563, gnorm=1.738, loss_scale=8, train_wall=11, gb_free=19.4, wall=6974
2022-12-09 13:34:12 | INFO | train_inner | epoch 046:    416 / 1102 loss=7.368, nll_loss=3.642, ppl=12.48, wps=33475.3, ups=9.21, wpb=3633.5, bsz=154.6, num_updates=50000, lr=0.000141421, gnorm=1.769, loss_scale=8, train_wall=11, gb_free=19.4, wall=6984
2022-12-09 13:34:23 | INFO | train_inner | epoch 046:    516 / 1102 loss=7.363, nll_loss=3.642, ppl=12.48, wps=33652.6, ups=9.29, wpb=3622.4, bsz=157.4, num_updates=50100, lr=0.00014128, gnorm=1.842, loss_scale=8, train_wall=11, gb_free=19.6, wall=6995
2022-12-09 13:34:33 | INFO | train_inner | epoch 046:    616 / 1102 loss=7.423, nll_loss=3.709, ppl=13.08, wps=33015.2, ups=9.22, wpb=3579.4, bsz=160.8, num_updates=50200, lr=0.000141139, gnorm=1.769, loss_scale=8, train_wall=11, gb_free=19.6, wall=7006
2022-12-09 13:34:44 | INFO | train_inner | epoch 046:    716 / 1102 loss=7.527, nll_loss=3.785, ppl=13.79, wps=33377.9, ups=9.33, wpb=3576.6, bsz=122.8, num_updates=50300, lr=0.000140999, gnorm=1.869, loss_scale=8, train_wall=10, gb_free=19.7, wall=7017
2022-12-09 13:34:55 | INFO | train_inner | epoch 046:    816 / 1102 loss=7.473, nll_loss=3.736, ppl=13.33, wps=33361.6, ups=9.36, wpb=3565, bsz=142.2, num_updates=50400, lr=0.000140859, gnorm=1.903, loss_scale=8, train_wall=10, gb_free=19.5, wall=7027
2022-12-09 13:35:06 | INFO | train_inner | epoch 046:    916 / 1102 loss=7.522, nll_loss=3.8, ppl=13.93, wps=32935.7, ups=9.27, wpb=3551.6, bsz=147, num_updates=50500, lr=0.00014072, gnorm=1.896, loss_scale=8, train_wall=11, gb_free=19.8, wall=7038
2022-12-09 13:35:16 | INFO | train_inner | epoch 046:   1016 / 1102 loss=7.524, nll_loss=3.801, ppl=13.94, wps=33454.5, ups=9.34, wpb=3581.2, bsz=138.6, num_updates=50600, lr=0.00014058, gnorm=1.809, loss_scale=8, train_wall=10, gb_free=19.3, wall=7049
2022-12-09 13:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:36:02 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.727 | nll_loss 2.19 | ppl 4.56 | bleu 36.31 | wps 4939.9 | wpb 2835.3 | bsz 115.6 | num_updates 50686 | best_bleu 36.31
2022-12-09 13:36:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 50686 updates
2022-12-09 13:36:03 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint46.pt
2022-12-09 13:36:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint46.pt (epoch 46 @ 50686 updates, score 36.31) (writing took 1.6141921104863286 seconds)
2022-12-09 13:36:04 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-12-09 13:36:04 | INFO | train | epoch 046 | loss 7.443 | nll_loss 3.716 | ppl 13.14 | wps 25157 | ups 7.02 | wpb 3583.6 | bsz 145.4 | num_updates 50686 | lr 0.000140461 | gnorm 1.822 | loss_scale 8 | train_wall 116 | gb_free 19.3 | wall 7096
2022-12-09 13:36:04 | INFO | fairseq.trainer | begin training epoch 47
2022-12-09 13:36:06 | INFO | train_inner | epoch 047:     14 / 1102 loss=7.504, nll_loss=3.79, ppl=13.83, wps=7206.1, ups=2.03, wpb=3556.7, bsz=147.6, num_updates=50700, lr=0.000140442, gnorm=1.811, loss_scale=8, train_wall=11, gb_free=19.6, wall=7098
2022-12-09 13:36:16 | INFO | train_inner | epoch 047:    114 / 1102 loss=7.373, nll_loss=3.643, ppl=12.49, wps=33530.8, ups=9.34, wpb=3588.9, bsz=153.7, num_updates=50800, lr=0.000140303, gnorm=1.809, loss_scale=8, train_wall=10, gb_free=19.3, wall=7109
2022-12-09 13:36:27 | INFO | train_inner | epoch 047:    214 / 1102 loss=7.404, nll_loss=3.655, ppl=12.59, wps=32716.5, ups=9.27, wpb=3531, bsz=138.9, num_updates=50900, lr=0.000140165, gnorm=1.85, loss_scale=8, train_wall=11, gb_free=19.4, wall=7120
2022-12-09 13:36:38 | INFO | train_inner | epoch 047:    314 / 1102 loss=7.413, nll_loss=3.676, ppl=12.79, wps=33194.3, ups=9.28, wpb=3578.7, bsz=148, num_updates=51000, lr=0.000140028, gnorm=1.811, loss_scale=8, train_wall=11, gb_free=19.5, wall=7131
2022-12-09 13:36:49 | INFO | train_inner | epoch 047:    414 / 1102 loss=7.411, nll_loss=3.67, ppl=12.73, wps=33468.2, ups=9.41, wpb=3555.8, bsz=140.6, num_updates=51100, lr=0.000139891, gnorm=1.867, loss_scale=8, train_wall=10, gb_free=19.6, wall=7141
2022-12-09 13:36:59 | INFO | train_inner | epoch 047:    514 / 1102 loss=7.412, nll_loss=3.685, ppl=12.86, wps=33267.3, ups=9.33, wpb=3567.5, bsz=147.4, num_updates=51200, lr=0.000139754, gnorm=1.817, loss_scale=8, train_wall=10, gb_free=19.4, wall=7152
2022-12-09 13:37:10 | INFO | train_inner | epoch 047:    614 / 1102 loss=7.403, nll_loss=3.675, ppl=12.77, wps=33396.8, ups=9.29, wpb=3594.2, bsz=144.8, num_updates=51300, lr=0.000139618, gnorm=1.717, loss_scale=8, train_wall=11, gb_free=19.5, wall=7163
2022-12-09 13:37:21 | INFO | train_inner | epoch 047:    714 / 1102 loss=7.408, nll_loss=3.686, ppl=12.87, wps=33350.6, ups=9.14, wpb=3649.7, bsz=150.2, num_updates=51400, lr=0.000139482, gnorm=1.762, loss_scale=8, train_wall=11, gb_free=19.7, wall=7174
2022-12-09 13:37:32 | INFO | train_inner | epoch 047:    814 / 1102 loss=7.434, nll_loss=3.716, ppl=13.14, wps=33402.5, ups=9.36, wpb=3567.6, bsz=152.9, num_updates=51500, lr=0.000139347, gnorm=1.88, loss_scale=8, train_wall=10, gb_free=19.3, wall=7184
2022-12-09 13:37:42 | INFO | train_inner | epoch 047:    914 / 1102 loss=7.549, nll_loss=3.82, ppl=14.12, wps=32921.4, ups=9.26, wpb=3553.8, bsz=134.2, num_updates=51600, lr=0.000139212, gnorm=1.847, loss_scale=8, train_wall=11, gb_free=19.4, wall=7195
2022-12-09 13:37:53 | INFO | train_inner | epoch 047:   1014 / 1102 loss=7.416, nll_loss=3.688, ppl=12.88, wps=33015.8, ups=9.18, wpb=3597.7, bsz=152.2, num_updates=51700, lr=0.000139077, gnorm=1.912, loss_scale=8, train_wall=11, gb_free=19.4, wall=7206
2022-12-09 13:38:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:38:40 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.723 | nll_loss 2.189 | ppl 4.56 | bleu 36.27 | wps 4896.8 | wpb 2835.3 | bsz 115.6 | num_updates 51788 | best_bleu 36.31
2022-12-09 13:38:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 51788 updates
2022-12-09 13:38:41 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint47.pt
2022-12-09 13:38:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint47.pt (epoch 47 @ 51788 updates, score 36.27) (writing took 1.1301131583750248 seconds)
2022-12-09 13:38:41 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-12-09 13:38:41 | INFO | train | epoch 047 | loss 7.43 | nll_loss 3.7 | ppl 13 | wps 25132.3 | ups 7.01 | wpb 3583.6 | bsz 145.4 | num_updates 51788 | lr 0.000138959 | gnorm 1.827 | loss_scale 8 | train_wall 116 | gb_free 19.4 | wall 7253
2022-12-09 13:38:41 | INFO | fairseq.trainer | begin training epoch 48
2022-12-09 13:38:42 | INFO | train_inner | epoch 048:     12 / 1102 loss=7.483, nll_loss=3.763, ppl=13.58, wps=7429.2, ups=2.03, wpb=3650.8, bsz=140.4, num_updates=51800, lr=0.000138943, gnorm=1.799, loss_scale=8, train_wall=11, gb_free=19.6, wall=7255
2022-12-09 13:38:53 | INFO | train_inner | epoch 048:    112 / 1102 loss=7.339, nll_loss=3.6, ppl=12.13, wps=33150, ups=9.36, wpb=3542.6, bsz=141.4, num_updates=51900, lr=0.000138809, gnorm=1.788, loss_scale=8, train_wall=10, gb_free=19.7, wall=7266
2022-12-09 13:39:04 | INFO | train_inner | epoch 048:    212 / 1102 loss=7.396, nll_loss=3.665, ppl=12.68, wps=32875, ups=9.28, wpb=3542.8, bsz=146.2, num_updates=52000, lr=0.000138675, gnorm=1.839, loss_scale=8, train_wall=11, gb_free=19.3, wall=7277
2022-12-09 13:39:15 | INFO | train_inner | epoch 048:    312 / 1102 loss=7.447, nll_loss=3.712, ppl=13.11, wps=33066.7, ups=9.16, wpb=3607.9, bsz=137.5, num_updates=52100, lr=0.000138542, gnorm=1.902, loss_scale=8, train_wall=11, gb_free=19.6, wall=7287
2022-12-09 13:39:26 | INFO | train_inner | epoch 048:    412 / 1102 loss=7.405, nll_loss=3.688, ppl=12.89, wps=33802.2, ups=9.3, wpb=3632.8, bsz=152.2, num_updates=52200, lr=0.000138409, gnorm=1.724, loss_scale=8, train_wall=11, gb_free=19.5, wall=7298
2022-12-09 13:39:36 | INFO | train_inner | epoch 048:    512 / 1102 loss=7.388, nll_loss=3.642, ppl=12.48, wps=32781.6, ups=9.29, wpb=3529.3, bsz=139, num_updates=52300, lr=0.000138277, gnorm=1.832, loss_scale=8, train_wall=11, gb_free=19.3, wall=7309
2022-12-09 13:39:47 | INFO | train_inner | epoch 048:    612 / 1102 loss=7.437, nll_loss=3.709, ppl=13.08, wps=33145.6, ups=9.2, wpb=3602.4, bsz=144.2, num_updates=52400, lr=0.000138145, gnorm=1.779, loss_scale=8, train_wall=11, gb_free=20, wall=7320
2022-12-09 13:39:58 | INFO | train_inner | epoch 048:    712 / 1102 loss=7.435, nll_loss=3.702, ppl=13.02, wps=32889.2, ups=9.31, wpb=3532.5, bsz=147.4, num_updates=52500, lr=0.000138013, gnorm=1.872, loss_scale=8, train_wall=11, gb_free=19.4, wall=7331
2022-12-09 13:40:09 | INFO | train_inner | epoch 048:    812 / 1102 loss=7.434, nll_loss=3.695, ppl=12.95, wps=33549.4, ups=9.27, wpb=3620.7, bsz=142.5, num_updates=52600, lr=0.000137882, gnorm=1.861, loss_scale=8, train_wall=11, gb_free=19.7, wall=7341
2022-12-09 13:40:20 | INFO | train_inner | epoch 048:    912 / 1102 loss=7.583, nll_loss=3.851, ppl=14.43, wps=33373.1, ups=9.12, wpb=3660.5, bsz=126.3, num_updates=52700, lr=0.000137751, gnorm=1.829, loss_scale=8, train_wall=11, gb_free=19.5, wall=7352
2022-12-09 13:40:31 | INFO | train_inner | epoch 048:   1012 / 1102 loss=7.376, nll_loss=3.657, ppl=12.62, wps=33133.6, ups=9.17, wpb=3613.5, bsz=163.8, num_updates=52800, lr=0.00013762, gnorm=1.791, loss_scale=8, train_wall=11, gb_free=19.5, wall=7363
2022-12-09 13:40:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:41:18 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.724 | nll_loss 2.188 | ppl 4.56 | bleu 36.19 | wps 4832.3 | wpb 2835.3 | bsz 115.6 | num_updates 52890 | best_bleu 36.31
2022-12-09 13:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 52890 updates
2022-12-09 13:41:19 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint48.pt
2022-12-09 13:41:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint48.pt (epoch 48 @ 52890 updates, score 36.19) (writing took 1.4151946986094117 seconds)
2022-12-09 13:41:19 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-12-09 13:41:19 | INFO | train | epoch 048 | loss 7.417 | nll_loss 3.686 | ppl 12.87 | wps 24958.3 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 52890 | lr 0.000137503 | gnorm 1.822 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 7412
2022-12-09 13:41:19 | INFO | fairseq.trainer | begin training epoch 49
2022-12-09 13:41:20 | INFO | train_inner | epoch 049:     10 / 1102 loss=7.371, nll_loss=3.641, ppl=12.48, wps=7062.7, ups=2.01, wpb=3519.2, bsz=153.6, num_updates=52900, lr=0.00013749, gnorm=1.852, loss_scale=8, train_wall=10, gb_free=19.5, wall=7413
2022-12-09 13:41:31 | INFO | train_inner | epoch 049:    110 / 1102 loss=7.321, nll_loss=3.593, ppl=12.06, wps=33196, ups=9.3, wpb=3569.1, bsz=153.5, num_updates=53000, lr=0.000137361, gnorm=1.82, loss_scale=8, train_wall=11, gb_free=20, wall=7424
2022-12-09 13:41:42 | INFO | train_inner | epoch 049:    210 / 1102 loss=7.146, nll_loss=3.422, ppl=10.72, wps=32940, ups=9.17, wpb=3592.3, bsz=186.4, num_updates=53100, lr=0.000137231, gnorm=1.705, loss_scale=8, train_wall=11, gb_free=19.3, wall=7435
2022-12-09 13:41:53 | INFO | train_inner | epoch 049:    310 / 1102 loss=7.426, nll_loss=3.683, ppl=12.85, wps=33488.7, ups=9.36, wpb=3579, bsz=133.8, num_updates=53200, lr=0.000137102, gnorm=1.829, loss_scale=8, train_wall=10, gb_free=19.3, wall=7445
2022-12-09 13:42:04 | INFO | train_inner | epoch 049:    410 / 1102 loss=7.409, nll_loss=3.661, ppl=12.65, wps=33651.4, ups=9.33, wpb=3608, bsz=133.5, num_updates=53300, lr=0.000136973, gnorm=1.888, loss_scale=8, train_wall=10, gb_free=19.3, wall=7456
2022-12-09 13:42:14 | INFO | train_inner | epoch 049:    510 / 1102 loss=7.386, nll_loss=3.643, ppl=12.49, wps=32958.8, ups=9.22, wpb=3575.3, bsz=146.4, num_updates=53400, lr=0.000136845, gnorm=1.851, loss_scale=8, train_wall=11, gb_free=19.5, wall=7467
2022-12-09 13:42:25 | INFO | train_inner | epoch 049:    610 / 1102 loss=7.389, nll_loss=3.653, ppl=12.58, wps=32941.5, ups=9.28, wpb=3550.8, bsz=146.7, num_updates=53500, lr=0.000136717, gnorm=1.919, loss_scale=8, train_wall=11, gb_free=19.6, wall=7478
2022-12-09 13:42:36 | INFO | train_inner | epoch 049:    710 / 1102 loss=7.397, nll_loss=3.664, ppl=12.67, wps=32991.8, ups=9.26, wpb=3564.5, bsz=150.4, num_updates=53600, lr=0.00013659, gnorm=1.768, loss_scale=8, train_wall=11, gb_free=19.3, wall=7489
2022-12-09 13:42:47 | INFO | train_inner | epoch 049:    810 / 1102 loss=7.478, nll_loss=3.748, ppl=13.43, wps=33293, ups=9.23, wpb=3606.1, bsz=135, num_updates=53700, lr=0.000136462, gnorm=1.767, loss_scale=8, train_wall=11, gb_free=19.3, wall=7499
2022-12-09 13:42:58 | INFO | train_inner | epoch 049:    910 / 1102 loss=7.522, nll_loss=3.788, ppl=13.81, wps=33320.5, ups=9.25, wpb=3603.9, bsz=124.2, num_updates=53800, lr=0.000136335, gnorm=1.842, loss_scale=8, train_wall=11, gb_free=19.3, wall=7510
2022-12-09 13:43:08 | INFO | train_inner | epoch 049:   1010 / 1102 loss=7.473, nll_loss=3.749, ppl=13.44, wps=33515, ups=9.31, wpb=3599.7, bsz=146.6, num_updates=53900, lr=0.000136209, gnorm=1.806, loss_scale=8, train_wall=10, gb_free=19.2, wall=7521
2022-12-09 13:43:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:43:57 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.721 | nll_loss 2.185 | ppl 4.55 | bleu 36.24 | wps 4641.5 | wpb 2835.3 | bsz 115.6 | num_updates 53992 | best_bleu 36.31
2022-12-09 13:43:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 53992 updates
2022-12-09 13:43:58 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint49.pt
2022-12-09 13:43:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint49.pt (epoch 49 @ 53992 updates, score 36.24) (writing took 1.1506362296640873 seconds)
2022-12-09 13:43:59 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-12-09 13:43:59 | INFO | train | epoch 049 | loss 7.403 | nll_loss 3.67 | ppl 12.73 | wps 24771.5 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 53992 | lr 0.000136093 | gnorm 1.818 | loss_scale 8 | train_wall 116 | gb_free 19.6 | wall 7571
2022-12-09 13:43:59 | INFO | fairseq.trainer | begin training epoch 50
2022-12-09 13:44:00 | INFO | train_inner | epoch 050:      8 / 1102 loss=7.474, nll_loss=3.759, ppl=13.54, wps=6949.5, ups=1.95, wpb=3561.3, bsz=147, num_updates=54000, lr=0.000136083, gnorm=1.791, loss_scale=8, train_wall=11, gb_free=19.6, wall=7572
2022-12-09 13:44:10 | INFO | train_inner | epoch 050:    108 / 1102 loss=7.404, nll_loss=3.666, ppl=12.69, wps=33230.4, ups=9.28, wpb=3579.4, bsz=137.3, num_updates=54100, lr=0.000135957, gnorm=1.756, loss_scale=8, train_wall=11, gb_free=19.3, wall=7583
2022-12-09 13:44:21 | INFO | train_inner | epoch 050:    208 / 1102 loss=7.243, nll_loss=3.505, ppl=11.36, wps=33034.7, ups=9.15, wpb=3609.6, bsz=159.7, num_updates=54200, lr=0.000135831, gnorm=1.765, loss_scale=8, train_wall=11, gb_free=19.4, wall=7594
2022-12-09 13:44:32 | INFO | train_inner | epoch 050:    308 / 1102 loss=7.379, nll_loss=3.645, ppl=12.51, wps=33245.5, ups=9.32, wpb=3568.8, bsz=151.2, num_updates=54300, lr=0.000135706, gnorm=1.817, loss_scale=8, train_wall=10, gb_free=19.8, wall=7605
2022-12-09 13:44:43 | INFO | train_inner | epoch 050:    408 / 1102 loss=7.31, nll_loss=3.582, ppl=11.98, wps=33737, ups=9.22, wpb=3660.3, bsz=158.3, num_updates=54400, lr=0.000135582, gnorm=1.769, loss_scale=8, train_wall=11, gb_free=19.3, wall=7615
2022-12-09 13:44:54 | INFO | train_inner | epoch 050:    508 / 1102 loss=7.451, nll_loss=3.714, ppl=13.13, wps=33285.4, ups=9.32, wpb=3572.4, bsz=132.1, num_updates=54500, lr=0.000135457, gnorm=1.817, loss_scale=8, train_wall=10, gb_free=19.8, wall=7626
2022-12-09 13:45:04 | INFO | train_inner | epoch 050:    608 / 1102 loss=7.381, nll_loss=3.633, ppl=12.41, wps=33191.3, ups=9.36, wpb=3545.3, bsz=142.2, num_updates=54600, lr=0.000135333, gnorm=1.865, loss_scale=8, train_wall=10, gb_free=19.5, wall=7637
2022-12-09 13:45:15 | INFO | train_inner | epoch 050:    708 / 1102 loss=7.396, nll_loss=3.669, ppl=12.72, wps=33625.1, ups=9.23, wpb=3644.5, bsz=155.4, num_updates=54700, lr=0.000135209, gnorm=1.862, loss_scale=8, train_wall=11, gb_free=19.5, wall=7648
2022-12-09 13:45:26 | INFO | train_inner | epoch 050:    808 / 1102 loss=7.49, nll_loss=3.74, ppl=13.36, wps=32853.6, ups=9.28, wpb=3541.3, bsz=128.7, num_updates=54800, lr=0.000135086, gnorm=1.909, loss_scale=8, train_wall=11, gb_free=19.4, wall=7659
2022-12-09 13:45:37 | INFO | train_inner | epoch 050:    908 / 1102 loss=7.401, nll_loss=3.655, ppl=12.59, wps=32762.5, ups=9.24, wpb=3544.5, bsz=137.5, num_updates=54900, lr=0.000134963, gnorm=1.894, loss_scale=8, train_wall=11, gb_free=19.8, wall=7669
2022-12-09 13:45:48 | INFO | train_inner | epoch 050:   1008 / 1102 loss=7.44, nll_loss=3.713, ppl=13.11, wps=33362.1, ups=9.18, wpb=3632.9, bsz=146.4, num_updates=55000, lr=0.00013484, gnorm=1.754, loss_scale=8, train_wall=11, gb_free=19.3, wall=7680
2022-12-09 13:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:46:36 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.71 | nll_loss 2.177 | ppl 4.52 | bleu 36.34 | wps 4775.7 | wpb 2835.3 | bsz 115.6 | num_updates 55094 | best_bleu 36.34
2022-12-09 13:46:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 55094 updates
2022-12-09 13:46:36 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint50.pt
2022-12-09 13:46:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint50.pt (epoch 50 @ 55094 updates, score 36.34) (writing took 1.6376523189246655 seconds)
2022-12-09 13:46:37 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-12-09 13:46:37 | INFO | train | epoch 050 | loss 7.389 | nll_loss 3.653 | ppl 12.58 | wps 24895.5 | ups 6.95 | wpb 3583.6 | bsz 145.4 | num_updates 55094 | lr 0.000134725 | gnorm 1.821 | loss_scale 8 | train_wall 116 | gb_free 19.6 | wall 7730
2022-12-09 13:46:37 | INFO | fairseq.trainer | begin training epoch 51
2022-12-09 13:46:38 | INFO | train_inner | epoch 051:      6 / 1102 loss=7.396, nll_loss=3.668, ppl=12.71, wps=7016.4, ups=1.99, wpb=3533.4, bsz=147.4, num_updates=55100, lr=0.000134718, gnorm=1.835, loss_scale=8, train_wall=11, gb_free=19.3, wall=7731
2022-12-09 13:46:49 | INFO | train_inner | epoch 051:    106 / 1102 loss=7.32, nll_loss=3.574, ppl=11.91, wps=33785.1, ups=9.42, wpb=3586.3, bsz=146.8, num_updates=55200, lr=0.000134595, gnorm=1.861, loss_scale=8, train_wall=10, gb_free=19.5, wall=7741
2022-12-09 13:46:59 | INFO | train_inner | epoch 051:    206 / 1102 loss=7.29, nll_loss=3.534, ppl=11.59, wps=33677.8, ups=9.53, wpb=3533.5, bsz=142.2, num_updates=55300, lr=0.000134474, gnorm=1.856, loss_scale=8, train_wall=10, gb_free=19.8, wall=7752
2022-12-09 13:47:10 | INFO | train_inner | epoch 051:    306 / 1102 loss=7.429, nll_loss=3.672, ppl=12.75, wps=33285.4, ups=9.39, wpb=3545.1, bsz=129.9, num_updates=55400, lr=0.000134352, gnorm=1.912, loss_scale=8, train_wall=10, gb_free=19.5, wall=7762
2022-12-09 13:47:21 | INFO | train_inner | epoch 051:    406 / 1102 loss=7.327, nll_loss=3.579, ppl=11.95, wps=32920.7, ups=9.27, wpb=3550.4, bsz=148.5, num_updates=55500, lr=0.000134231, gnorm=1.886, loss_scale=8, train_wall=11, gb_free=19.4, wall=7773
2022-12-09 13:47:31 | INFO | train_inner | epoch 051:    506 / 1102 loss=7.342, nll_loss=3.61, ppl=12.21, wps=32905.1, ups=9.16, wpb=3591.1, bsz=151.5, num_updates=55600, lr=0.00013411, gnorm=1.76, loss_scale=8, train_wall=11, gb_free=19.7, wall=7784
2022-12-09 13:47:42 | INFO | train_inner | epoch 051:    606 / 1102 loss=7.419, nll_loss=3.677, ppl=12.79, wps=33556.4, ups=9.24, wpb=3630.1, bsz=142.2, num_updates=55700, lr=0.00013399, gnorm=1.83, loss_scale=8, train_wall=11, gb_free=19.4, wall=7795
2022-12-09 13:47:53 | INFO | train_inner | epoch 051:    706 / 1102 loss=7.413, nll_loss=3.665, ppl=12.68, wps=32763.5, ups=9.21, wpb=3557.7, bsz=136.2, num_updates=55800, lr=0.00013387, gnorm=1.826, loss_scale=8, train_wall=11, gb_free=19.3, wall=7806
2022-12-09 13:48:04 | INFO | train_inner | epoch 051:    806 / 1102 loss=7.356, nll_loss=3.631, ppl=12.39, wps=33159, ups=9.21, wpb=3599.1, bsz=158.1, num_updates=55900, lr=0.00013375, gnorm=1.799, loss_scale=8, train_wall=11, gb_free=19.2, wall=7817
2022-12-09 13:48:15 | INFO | train_inner | epoch 051:    906 / 1102 loss=7.502, nll_loss=3.765, ppl=13.6, wps=33257.3, ups=9.32, wpb=3568.8, bsz=138.9, num_updates=56000, lr=0.000133631, gnorm=1.845, loss_scale=16, train_wall=10, gb_free=19.4, wall=7827
2022-12-09 13:48:25 | INFO | train_inner | epoch 051:   1006 / 1102 loss=7.413, nll_loss=3.68, ppl=12.82, wps=33451.5, ups=9.32, wpb=3588.2, bsz=146.2, num_updates=56100, lr=0.000133511, gnorm=1.822, loss_scale=16, train_wall=10, gb_free=19.4, wall=7838
2022-12-09 13:48:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:49:13 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.722 | nll_loss 2.18 | ppl 4.53 | bleu 36.45 | wps 4897.9 | wpb 2835.3 | bsz 115.6 | num_updates 56196 | best_bleu 36.45
2022-12-09 13:49:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 56196 updates
2022-12-09 13:49:14 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint51.pt
2022-12-09 13:49:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint51.pt (epoch 51 @ 56196 updates, score 36.45) (writing took 1.6406045770272613 seconds)
2022-12-09 13:49:14 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-12-09 13:49:14 | INFO | train | epoch 051 | loss 7.379 | nll_loss 3.64 | ppl 12.47 | wps 25114 | ups 7.01 | wpb 3583.6 | bsz 145.4 | num_updates 56196 | lr 0.000133397 | gnorm 1.831 | loss_scale 16 | train_wall 116 | gb_free 19.4 | wall 7887
2022-12-09 13:49:14 | INFO | fairseq.trainer | begin training epoch 52
2022-12-09 13:49:15 | INFO | train_inner | epoch 052:      4 / 1102 loss=7.354, nll_loss=3.649, ppl=12.55, wps=7391.4, ups=2.01, wpb=3670.3, bsz=160.1, num_updates=56200, lr=0.000133393, gnorm=1.746, loss_scale=16, train_wall=11, gb_free=19.4, wall=7888
2022-12-09 13:49:26 | INFO | train_inner | epoch 052:    104 / 1102 loss=7.275, nll_loss=3.53, ppl=11.55, wps=33065.6, ups=9.22, wpb=3584.4, bsz=149, num_updates=56300, lr=0.000133274, gnorm=1.8, loss_scale=16, train_wall=11, gb_free=19.7, wall=7899
2022-12-09 13:49:37 | INFO | train_inner | epoch 052:    204 / 1102 loss=7.31, nll_loss=3.566, ppl=11.84, wps=32865.6, ups=9.27, wpb=3546.4, bsz=146.9, num_updates=56400, lr=0.000133156, gnorm=1.837, loss_scale=16, train_wall=11, gb_free=19.5, wall=7909
2022-12-09 13:49:48 | INFO | train_inner | epoch 052:    304 / 1102 loss=7.288, nll_loss=3.561, ppl=11.8, wps=33146.1, ups=9.21, wpb=3598.6, bsz=159, num_updates=56500, lr=0.000133038, gnorm=1.742, loss_scale=16, train_wall=11, gb_free=19.6, wall=7920
2022-12-09 13:49:58 | INFO | train_inner | epoch 052:    404 / 1102 loss=7.403, nll_loss=3.666, ppl=12.7, wps=33881.2, ups=9.32, wpb=3636.9, bsz=137.7, num_updates=56600, lr=0.00013292, gnorm=1.794, loss_scale=16, train_wall=10, gb_free=19.5, wall=7931
2022-12-09 13:50:09 | INFO | train_inner | epoch 052:    504 / 1102 loss=7.388, nll_loss=3.633, ppl=12.41, wps=32770.8, ups=9.42, wpb=3477.2, bsz=138.4, num_updates=56700, lr=0.000132803, gnorm=1.915, loss_scale=16, train_wall=10, gb_free=19.5, wall=7942
2022-12-09 13:50:20 | INFO | train_inner | epoch 052:    604 / 1102 loss=7.423, nll_loss=3.689, ppl=12.9, wps=33294.2, ups=9.16, wpb=3633.6, bsz=138.5, num_updates=56800, lr=0.000132686, gnorm=1.771, loss_scale=16, train_wall=11, gb_free=19.4, wall=7952
2022-12-09 13:50:31 | INFO | train_inner | epoch 052:    704 / 1102 loss=7.443, nll_loss=3.696, ppl=12.96, wps=32869.5, ups=9.26, wpb=3550.5, bsz=135.1, num_updates=56900, lr=0.00013257, gnorm=1.895, loss_scale=16, train_wall=11, gb_free=19.4, wall=7963
2022-12-09 13:50:42 | INFO | train_inner | epoch 052:    804 / 1102 loss=7.317, nll_loss=3.584, ppl=11.99, wps=32939.1, ups=9.07, wpb=3629.9, bsz=156.2, num_updates=57000, lr=0.000132453, gnorm=1.82, loss_scale=16, train_wall=11, gb_free=19.6, wall=7974
2022-12-09 13:50:53 | INFO | train_inner | epoch 052:    904 / 1102 loss=7.393, nll_loss=3.657, ppl=12.61, wps=33115.9, ups=9.23, wpb=3589.5, bsz=147, num_updates=57100, lr=0.000132337, gnorm=1.897, loss_scale=16, train_wall=11, gb_free=19.2, wall=7985
2022-12-09 13:51:03 | INFO | train_inner | epoch 052:   1004 / 1102 loss=7.369, nll_loss=3.629, ppl=12.37, wps=33018.6, ups=9.19, wpb=3592.3, bsz=144.7, num_updates=57200, lr=0.000132221, gnorm=1.854, loss_scale=16, train_wall=11, gb_free=19.5, wall=7996
2022-12-09 13:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 13:51:51 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.712 | nll_loss 2.174 | ppl 4.51 | bleu 36.5 | wps 4896.5 | wpb 2835.3 | bsz 115.6 | num_updates 57298 | best_bleu 36.5
2022-12-09 13:51:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 57298 updates
2022-12-09 14:57:16 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 14:57:16 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 14:57:16 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 14:57:16 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 14:57:16 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 14:57:16 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 14:57:17 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 14:57:17 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 14:57:17 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 14:57:17 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 14:57:17 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 14:57:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 14:57:21 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 14:57:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 14:57:21 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 14:57:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 14:57:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 14:57:21 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 14:57:21 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 14:57:21 | INFO | fairseq.trainer | Loaded checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt (epoch 52 @ 56196 updates)
2022-12-09 14:57:21 | INFO | fairseq.trainer | loading train data for epoch 52
2022-12-09 14:57:21 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 14:57:21 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 14:57:21 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 14:57:21 | INFO | fairseq.trainer | begin training epoch 52
2022-12-09 14:57:22 | INFO | train_inner | epoch 052:      4 / 1102 loss=7.281, nll_loss=3.528, ppl=11.53, wps=29149, ups=9.23, wpb=3289.5, bsz=144, num_updates=56200, lr=0.000133393, gnorm=1.848, loss_scale=16, train_wall=1, gb_free=19.4, wall=2
2022-12-09 14:57:33 | INFO | train_inner | epoch 052:    104 / 1102 loss=7.275, nll_loss=3.53, ppl=11.55, wps=33489.8, ups=9.34, wpb=3584.4, bsz=149, num_updates=56300, lr=0.000133274, gnorm=1.814, loss_scale=16, train_wall=10, gb_free=19.7, wall=12
2022-12-09 14:58:03 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 14:58:03 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 14:58:03 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 14:58:03 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 14:58:03 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 14:58:03 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 14:58:04 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 14:58:04 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 14:58:04 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 14:58:04 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 14:58:04 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 14:58:08 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 14:58:08 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 14:58:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 14:58:08 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 14:58:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 14:58:08 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 14:58:08 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 14:58:08 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 14:58:08 | INFO | fairseq.trainer | Loaded checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt (epoch 52 @ 56196 updates)
2022-12-09 14:58:08 | INFO | fairseq.trainer | loading train data for epoch 52
2022-12-09 14:58:08 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 14:58:08 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 14:58:08 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 14:58:09 | INFO | fairseq.trainer | begin training epoch 52
2022-12-09 14:58:10 | INFO | train_inner | epoch 052:      4 / 1102 loss=7.281, nll_loss=3.528, ppl=11.53, wps=29599.7, ups=9.37, wpb=3289.5, bsz=144, num_updates=56200, lr=0.000133393, gnorm=1.848, loss_scale=16, train_wall=1, gb_free=19.4, wall=2
2022-12-09 14:58:21 | INFO | train_inner | epoch 052:    104 / 1102 loss=7.275, nll_loss=3.53, ppl=11.55, wps=33083.6, ups=9.23, wpb=3584.4, bsz=149, num_updates=56300, lr=0.000133274, gnorm=1.814, loss_scale=16, train_wall=11, gb_free=19.7, wall=13
2022-12-09 14:58:31 | INFO | train_inner | epoch 052:    204 / 1102 loss=7.31, nll_loss=3.566, ppl=11.84, wps=32682.5, ups=9.22, wpb=3546.4, bsz=146.9, num_updates=56400, lr=0.000133156, gnorm=1.84, loss_scale=16, train_wall=11, gb_free=19.5, wall=24
2022-12-09 14:58:42 | INFO | train_inner | epoch 052:    304 / 1102 loss=7.288, nll_loss=3.561, ppl=11.8, wps=33052.1, ups=9.18, wpb=3598.6, bsz=159, num_updates=56500, lr=0.000133038, gnorm=1.754, loss_scale=16, train_wall=11, gb_free=19.6, wall=35
2022-12-09 14:58:53 | INFO | train_inner | epoch 052:    404 / 1102 loss=7.402, nll_loss=3.665, ppl=12.69, wps=33711.7, ups=9.27, wpb=3636.9, bsz=137.7, num_updates=56600, lr=0.00013292, gnorm=1.782, loss_scale=16, train_wall=11, gb_free=19.5, wall=46
2022-12-09 14:59:04 | INFO | train_inner | epoch 052:    504 / 1102 loss=7.388, nll_loss=3.633, ppl=12.41, wps=32751.9, ups=9.42, wpb=3477.2, bsz=138.4, num_updates=56700, lr=0.000132803, gnorm=1.902, loss_scale=16, train_wall=10, gb_free=19.5, wall=56
2022-12-09 14:59:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-09 14:59:15 | INFO | train_inner | epoch 052:    605 / 1102 loss=7.427, nll_loss=3.693, ppl=12.94, wps=33353, ups=9.15, wpb=3643.4, bsz=140.2, num_updates=56800, lr=0.000132686, gnorm=1.774, loss_scale=8, train_wall=11, gb_free=19.5, wall=67
2022-12-09 14:59:25 | INFO | train_inner | epoch 052:    705 / 1102 loss=7.441, nll_loss=3.694, ppl=12.94, wps=33189.5, ups=9.35, wpb=3549.1, bsz=134.8, num_updates=56900, lr=0.00013257, gnorm=1.884, loss_scale=8, train_wall=10, gb_free=19.5, wall=78
2022-12-09 14:59:36 | INFO | train_inner | epoch 052:    805 / 1102 loss=7.319, nll_loss=3.581, ppl=11.97, wps=33423.4, ups=9.21, wpb=3630.7, bsz=155.8, num_updates=57000, lr=0.000132453, gnorm=1.793, loss_scale=8, train_wall=11, gb_free=19.5, wall=89
2022-12-09 14:59:47 | INFO | train_inner | epoch 052:    905 / 1102 loss=7.39, nll_loss=3.652, ppl=12.57, wps=33169.4, ups=9.24, wpb=3589.6, bsz=146.7, num_updates=57100, lr=0.000132337, gnorm=1.849, loss_scale=8, train_wall=11, gb_free=19.3, wall=99
2022-12-09 14:59:58 | INFO | train_inner | epoch 052:   1005 / 1102 loss=7.371, nll_loss=3.632, ppl=12.39, wps=33099.9, ups=9.22, wpb=3591.1, bsz=144.5, num_updates=57200, lr=0.000132221, gnorm=1.802, loss_scale=8, train_wall=11, gb_free=19.4, wall=110
2022-12-09 15:00:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:00:47 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.715 | nll_loss 2.175 | ppl 4.51 | bleu 36.29 | wps 4726 | wpb 2835.3 | bsz 115.6 | num_updates 57297 | best_bleu 36.45
2022-12-09 15:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 57297 updates
2022-12-09 15:00:48 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint52.pt
2022-12-09 15:00:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint52.pt (epoch 52 @ 57297 updates, score 36.29) (writing took 1.3984055342152715 seconds)
2022-12-09 15:00:48 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-12-09 15:00:48 | INFO | train | epoch 052 | loss 7.366 | nll_loss 3.626 | ppl 12.35 | wps 24859.1 | ups 6.94 | wpb 3584.1 | bsz 145.5 | num_updates 57297 | lr 0.00013211 | gnorm 1.818 | loss_scale 8 | train_wall 117 | gb_free 19.4 | wall 160
2022-12-09 15:00:48 | INFO | fairseq.trainer | begin training epoch 53
2022-12-09 15:00:49 | INFO | train_inner | epoch 053:      3 / 1102 loss=7.404, nll_loss=3.674, ppl=12.77, wps=7079, ups=1.97, wpb=3596.5, bsz=148.2, num_updates=57300, lr=0.000132106, gnorm=1.798, loss_scale=8, train_wall=11, gb_free=19.4, wall=161
2022-12-09 15:00:59 | INFO | train_inner | epoch 053:    103 / 1102 loss=7.29, nll_loss=3.533, ppl=11.57, wps=32827.2, ups=9.39, wpb=3496.7, bsz=143.3, num_updates=57400, lr=0.000131991, gnorm=1.866, loss_scale=8, train_wall=10, gb_free=19.7, wall=172
2022-12-09 15:01:10 | INFO | train_inner | epoch 053:    203 / 1102 loss=7.271, nll_loss=3.53, ppl=11.55, wps=33614.3, ups=9.33, wpb=3603.4, bsz=156.5, num_updates=57500, lr=0.000131876, gnorm=1.822, loss_scale=8, train_wall=10, gb_free=20, wall=182
2022-12-09 15:01:21 | INFO | train_inner | epoch 053:    303 / 1102 loss=7.343, nll_loss=3.59, ppl=12.04, wps=33168.2, ups=9.25, wpb=3584.8, bsz=143.1, num_updates=57600, lr=0.000131762, gnorm=1.867, loss_scale=8, train_wall=11, gb_free=19.5, wall=193
2022-12-09 15:01:32 | INFO | train_inner | epoch 053:    403 / 1102 loss=7.294, nll_loss=3.56, ppl=11.8, wps=33723.8, ups=9.34, wpb=3610.6, bsz=148.6, num_updates=57700, lr=0.000131647, gnorm=1.738, loss_scale=8, train_wall=10, gb_free=19.8, wall=204
2022-12-09 15:01:42 | INFO | train_inner | epoch 053:    503 / 1102 loss=7.419, nll_loss=3.662, ppl=12.66, wps=32683.7, ups=9.22, wpb=3544.2, bsz=130.5, num_updates=57800, lr=0.000131533, gnorm=1.913, loss_scale=8, train_wall=11, gb_free=19.7, wall=215
2022-12-09 15:01:53 | INFO | train_inner | epoch 053:    603 / 1102 loss=7.389, nll_loss=3.637, ppl=12.44, wps=32761.9, ups=9.15, wpb=3580.1, bsz=135.4, num_updates=57900, lr=0.00013142, gnorm=1.873, loss_scale=8, train_wall=11, gb_free=19.9, wall=226
2022-12-09 15:02:04 | INFO | train_inner | epoch 053:    703 / 1102 loss=7.356, nll_loss=3.62, ppl=12.3, wps=33279, ups=9.17, wpb=3627.2, bsz=144.2, num_updates=58000, lr=0.000131306, gnorm=1.774, loss_scale=8, train_wall=11, gb_free=19.6, wall=237
2022-12-09 15:02:15 | INFO | train_inner | epoch 053:    803 / 1102 loss=7.328, nll_loss=3.604, ppl=12.16, wps=33117.6, ups=9.15, wpb=3618.6, bsz=163.2, num_updates=58100, lr=0.000131193, gnorm=1.797, loss_scale=8, train_wall=11, gb_free=19.3, wall=248
2022-12-09 15:02:26 | INFO | train_inner | epoch 053:    903 / 1102 loss=7.441, nll_loss=3.691, ppl=12.91, wps=32798.9, ups=9.15, wpb=3585.5, bsz=130.2, num_updates=58200, lr=0.000131081, gnorm=1.835, loss_scale=8, train_wall=11, gb_free=19.4, wall=259
2022-12-09 15:02:37 | INFO | train_inner | epoch 053:   1003 / 1102 loss=7.414, nll_loss=3.686, ppl=12.87, wps=32480.1, ups=9.16, wpb=3546.5, bsz=149.4, num_updates=58300, lr=0.000130968, gnorm=1.858, loss_scale=8, train_wall=11, gb_free=19.3, wall=269
2022-12-09 15:02:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:03:27 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.719 | nll_loss 2.179 | ppl 4.53 | bleu 36.28 | wps 4599.8 | wpb 2835.3 | bsz 115.6 | num_updates 58399 | best_bleu 36.45
2022-12-09 15:03:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 58399 updates
2022-12-09 15:03:28 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint53.pt
2022-12-09 15:03:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint53.pt (epoch 53 @ 58399 updates, score 36.28) (writing took 1.1442555272951722 seconds)
2022-12-09 15:03:28 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-12-09 15:03:28 | INFO | train | epoch 053 | loss 7.352 | nll_loss 3.611 | ppl 12.22 | wps 24621.4 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 58399 | lr 0.000130857 | gnorm 1.825 | loss_scale 8 | train_wall 117 | gb_free 19.3 | wall 321
2022-12-09 15:03:28 | INFO | fairseq.trainer | begin training epoch 54
2022-12-09 15:03:29 | INFO | train_inner | epoch 054:      1 / 1102 loss=7.342, nll_loss=3.622, ppl=12.31, wps=6981.8, ups=1.93, wpb=3617.4, bsz=154.1, num_updates=58400, lr=0.000130856, gnorm=1.742, loss_scale=8, train_wall=11, gb_free=19.3, wall=321
2022-12-09 15:03:40 | INFO | train_inner | epoch 054:    101 / 1102 loss=7.2, nll_loss=3.46, ppl=11, wps=33193.1, ups=9.27, wpb=3580.9, bsz=163.6, num_updates=58500, lr=0.000130744, gnorm=1.791, loss_scale=8, train_wall=11, gb_free=19.8, wall=332
2022-12-09 15:03:51 | INFO | train_inner | epoch 054:    201 / 1102 loss=7.295, nll_loss=3.545, ppl=11.68, wps=32847.9, ups=9.2, wpb=3572, bsz=142, num_updates=58600, lr=0.000130632, gnorm=1.83, loss_scale=8, train_wall=11, gb_free=19.5, wall=343
2022-12-09 15:04:01 | INFO | train_inner | epoch 054:    301 / 1102 loss=7.375, nll_loss=3.607, ppl=12.18, wps=33044.7, ups=9.23, wpb=3580.8, bsz=130.2, num_updates=58700, lr=0.000130521, gnorm=1.862, loss_scale=8, train_wall=11, gb_free=19.3, wall=354
2022-12-09 15:04:12 | INFO | train_inner | epoch 054:    401 / 1102 loss=7.387, nll_loss=3.641, ppl=12.48, wps=32966.5, ups=9.27, wpb=3556.3, bsz=139.1, num_updates=58800, lr=0.00013041, gnorm=1.853, loss_scale=8, train_wall=11, gb_free=19.4, wall=365
2022-12-09 15:04:23 | INFO | train_inner | epoch 054:    501 / 1102 loss=7.416, nll_loss=3.661, ppl=12.65, wps=32525.6, ups=9.19, wpb=3540.7, bsz=130.2, num_updates=58900, lr=0.000130299, gnorm=1.877, loss_scale=8, train_wall=11, gb_free=19.3, wall=375
2022-12-09 15:04:34 | INFO | train_inner | epoch 054:    601 / 1102 loss=7.332, nll_loss=3.577, ppl=11.93, wps=32534.6, ups=9.09, wpb=3579.3, bsz=135.3, num_updates=59000, lr=0.000130189, gnorm=1.792, loss_scale=8, train_wall=11, gb_free=19.6, wall=386
2022-12-09 15:04:45 | INFO | train_inner | epoch 054:    701 / 1102 loss=7.294, nll_loss=3.563, ppl=11.81, wps=33217.4, ups=9.11, wpb=3645.8, bsz=156, num_updates=59100, lr=0.000130079, gnorm=1.763, loss_scale=8, train_wall=11, gb_free=19.6, wall=397
2022-12-09 15:04:56 | INFO | train_inner | epoch 054:    801 / 1102 loss=7.284, nll_loss=3.556, ppl=11.76, wps=32874.7, ups=9.08, wpb=3620.2, bsz=165.7, num_updates=59200, lr=0.000129969, gnorm=1.786, loss_scale=8, train_wall=11, gb_free=19.3, wall=408
2022-12-09 15:05:07 | INFO | train_inner | epoch 054:    901 / 1102 loss=7.383, nll_loss=3.642, ppl=12.48, wps=32483.5, ups=9.13, wpb=3558, bsz=147.8, num_updates=59300, lr=0.000129859, gnorm=1.878, loss_scale=8, train_wall=11, gb_free=19.6, wall=419
2022-12-09 15:05:18 | INFO | train_inner | epoch 054:   1001 / 1102 loss=7.424, nll_loss=3.685, ppl=12.86, wps=32671.9, ups=9.16, wpb=3568.2, bsz=142.5, num_updates=59400, lr=0.00012975, gnorm=1.857, loss_scale=8, train_wall=11, gb_free=19.4, wall=430
2022-12-09 15:05:29 | INFO | train_inner | epoch 054:   1101 / 1102 loss=7.381, nll_loss=3.648, ppl=12.54, wps=32526.7, ups=9.01, wpb=3609.9, bsz=145.2, num_updates=59500, lr=0.000129641, gnorm=1.785, loss_scale=8, train_wall=11, gb_free=19.8, wall=441
2022-12-09 15:05:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:06:09 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.723 | nll_loss 2.18 | ppl 4.53 | bleu 36.2 | wps 4561.9 | wpb 2835.3 | bsz 115.6 | num_updates 59501 | best_bleu 36.45
2022-12-09 15:06:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 59501 updates
2022-12-09 15:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint54.pt
2022-12-09 15:06:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint54.pt (epoch 54 @ 59501 updates, score 36.2) (writing took 1.1147402469068766 seconds)
2022-12-09 15:06:10 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-12-09 15:06:10 | INFO | train | epoch 054 | loss 7.342 | nll_loss 3.598 | ppl 12.11 | wps 24454.7 | ups 6.82 | wpb 3583.6 | bsz 145.4 | num_updates 59501 | lr 0.00012964 | gnorm 1.824 | loss_scale 8 | train_wall 118 | gb_free 19.4 | wall 482
2022-12-09 15:06:10 | INFO | fairseq.trainer | begin training epoch 55
2022-12-09 15:06:21 | INFO | train_inner | epoch 055:     99 / 1102 loss=7.237, nll_loss=3.489, ppl=11.23, wps=6962.6, ups=1.92, wpb=3621.9, bsz=144.3, num_updates=59600, lr=0.000129532, gnorm=1.821, loss_scale=8, train_wall=11, gb_free=19.7, wall=493
2022-12-09 15:06:32 | INFO | train_inner | epoch 055:    199 / 1102 loss=7.293, nll_loss=3.544, ppl=11.66, wps=33025.5, ups=9.32, wpb=3543.7, bsz=141.8, num_updates=59700, lr=0.000129423, gnorm=1.828, loss_scale=8, train_wall=10, gb_free=19.7, wall=504
2022-12-09 15:06:43 | INFO | train_inner | epoch 055:    299 / 1102 loss=7.345, nll_loss=3.592, ppl=12.06, wps=33006.8, ups=9.25, wpb=3566.8, bsz=145.4, num_updates=59800, lr=0.000129315, gnorm=1.921, loss_scale=8, train_wall=11, gb_free=19.5, wall=515
2022-12-09 15:06:53 | INFO | train_inner | epoch 055:    399 / 1102 loss=7.336, nll_loss=3.586, ppl=12.01, wps=32868.2, ups=9.19, wpb=3575.4, bsz=145.2, num_updates=59900, lr=0.000129207, gnorm=1.893, loss_scale=8, train_wall=11, gb_free=19.3, wall=526
2022-12-09 15:07:04 | INFO | train_inner | epoch 055:    499 / 1102 loss=7.366, nll_loss=3.61, ppl=12.21, wps=33212.1, ups=9.16, wpb=3627.2, bsz=133.2, num_updates=60000, lr=0.000129099, gnorm=1.805, loss_scale=8, train_wall=11, gb_free=19.3, wall=537
2022-12-09 15:07:15 | INFO | train_inner | epoch 055:    599 / 1102 loss=7.339, nll_loss=3.592, ppl=12.06, wps=33027.4, ups=9.13, wpb=3619.3, bsz=143.8, num_updates=60100, lr=0.000128992, gnorm=1.863, loss_scale=8, train_wall=11, gb_free=19.5, wall=548
2022-12-09 15:07:26 | INFO | train_inner | epoch 055:    699 / 1102 loss=7.372, nll_loss=3.627, ppl=12.36, wps=32612.8, ups=9.09, wpb=3588.5, bsz=140.2, num_updates=60200, lr=0.000128885, gnorm=1.815, loss_scale=8, train_wall=11, gb_free=19.9, wall=559
2022-12-09 15:07:37 | INFO | train_inner | epoch 055:    799 / 1102 loss=7.284, nll_loss=3.542, ppl=11.65, wps=32057.1, ups=9.18, wpb=3491.2, bsz=160.1, num_updates=60300, lr=0.000128778, gnorm=1.92, loss_scale=8, train_wall=11, gb_free=19.5, wall=570
2022-12-09 15:07:48 | INFO | train_inner | epoch 055:    899 / 1102 loss=7.338, nll_loss=3.596, ppl=12.09, wps=32741.6, ups=9.13, wpb=3587.4, bsz=145.5, num_updates=60400, lr=0.000128671, gnorm=1.975, loss_scale=8, train_wall=11, gb_free=19.8, wall=581
2022-12-09 15:07:59 | INFO | train_inner | epoch 055:    999 / 1102 loss=7.396, nll_loss=3.651, ppl=12.56, wps=32776.9, ups=9.19, wpb=3565.2, bsz=139.5, num_updates=60500, lr=0.000128565, gnorm=1.969, loss_scale=8, train_wall=11, gb_free=19.7, wall=591
2022-12-09 15:08:10 | INFO | train_inner | epoch 055:   1099 / 1102 loss=7.349, nll_loss=3.613, ppl=12.24, wps=33299.5, ups=9.16, wpb=3633.7, bsz=159.7, num_updates=60600, lr=0.000128459, gnorm=1.86, loss_scale=8, train_wall=11, gb_free=19.5, wall=602
2022-12-09 15:08:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:08:51 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.709 | nll_loss 2.171 | ppl 4.5 | bleu 36.44 | wps 4424.8 | wpb 2835.3 | bsz 115.6 | num_updates 60603 | best_bleu 36.45
2022-12-09 15:08:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 60603 updates
2022-12-09 15:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint55.pt
2022-12-09 15:08:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint55.pt (epoch 55 @ 60603 updates, score 36.44) (writing took 1.1287121074274182 seconds)
2022-12-09 15:08:52 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-12-09 15:08:52 | INFO | train | epoch 055 | loss 7.332 | nll_loss 3.586 | ppl 12.01 | wps 24317.1 | ups 6.79 | wpb 3583.6 | bsz 145.4 | num_updates 60603 | lr 0.000128456 | gnorm 1.879 | loss_scale 8 | train_wall 117 | gb_free 19.4 | wall 645
2022-12-09 15:08:52 | INFO | fairseq.trainer | begin training epoch 56
2022-12-09 15:09:03 | INFO | train_inner | epoch 056:     97 / 1102 loss=7.399, nll_loss=3.66, ppl=12.64, wps=6735.1, ups=1.89, wpb=3564.2, bsz=132.9, num_updates=60700, lr=0.000128353, gnorm=1.846, loss_scale=8, train_wall=10, gb_free=19.8, wall=655
2022-12-09 15:09:13 | INFO | train_inner | epoch 056:    197 / 1102 loss=7.272, nll_loss=3.508, ppl=11.38, wps=33945.5, ups=9.47, wpb=3585.7, bsz=139.1, num_updates=60800, lr=0.000128247, gnorm=1.819, loss_scale=8, train_wall=10, gb_free=19.5, wall=666
2022-12-09 15:09:24 | INFO | train_inner | epoch 056:    297 / 1102 loss=7.343, nll_loss=3.592, ppl=12.06, wps=33621.9, ups=9.44, wpb=3562, bsz=139.9, num_updates=60900, lr=0.000128142, gnorm=1.858, loss_scale=8, train_wall=10, gb_free=19.5, wall=676
2022-12-09 15:09:35 | INFO | train_inner | epoch 056:    397 / 1102 loss=7.357, nll_loss=3.603, ppl=12.15, wps=33430.6, ups=9.36, wpb=3570.6, bsz=143.2, num_updates=61000, lr=0.000128037, gnorm=1.847, loss_scale=8, train_wall=10, gb_free=19.7, wall=687
2022-12-09 15:09:45 | INFO | train_inner | epoch 056:    497 / 1102 loss=7.282, nll_loss=3.521, ppl=11.48, wps=33058.7, ups=9.36, wpb=3530.2, bsz=146.5, num_updates=61100, lr=0.000127932, gnorm=1.921, loss_scale=8, train_wall=10, gb_free=19.3, wall=698
2022-12-09 15:09:56 | INFO | train_inner | epoch 056:    597 / 1102 loss=7.295, nll_loss=3.546, ppl=11.68, wps=33656.8, ups=9.36, wpb=3595.6, bsz=143.4, num_updates=61200, lr=0.000127827, gnorm=1.743, loss_scale=8, train_wall=10, gb_free=19.6, wall=708
2022-12-09 15:10:07 | INFO | train_inner | epoch 056:    697 / 1102 loss=7.269, nll_loss=3.52, ppl=11.48, wps=33016.7, ups=9.27, wpb=3561.7, bsz=150.2, num_updates=61300, lr=0.000127723, gnorm=1.84, loss_scale=8, train_wall=11, gb_free=19.4, wall=719
2022-12-09 15:10:18 | INFO | train_inner | epoch 056:    797 / 1102 loss=7.3, nll_loss=3.555, ppl=11.75, wps=33013.9, ups=9.35, wpb=3531, bsz=151.8, num_updates=61400, lr=0.000127619, gnorm=1.936, loss_scale=8, train_wall=10, gb_free=19.6, wall=730
2022-12-09 15:10:28 | INFO | train_inner | epoch 056:    897 / 1102 loss=7.306, nll_loss=3.574, ppl=11.91, wps=33581.4, ups=9.21, wpb=3647.4, bsz=163.3, num_updates=61500, lr=0.000127515, gnorm=1.847, loss_scale=8, train_wall=11, gb_free=19.7, wall=741
2022-12-09 15:10:39 | INFO | train_inner | epoch 056:    997 / 1102 loss=7.336, nll_loss=3.604, ppl=12.16, wps=34115.1, ups=9.33, wpb=3656.3, bsz=151.7, num_updates=61600, lr=0.000127412, gnorm=1.808, loss_scale=8, train_wall=10, gb_free=19.7, wall=752
2022-12-09 15:10:50 | INFO | train_inner | epoch 056:   1097 / 1102 loss=7.359, nll_loss=3.612, ppl=12.22, wps=33433.1, ups=9.28, wpb=3604.3, bsz=137.3, num_updates=61700, lr=0.000127309, gnorm=1.952, loss_scale=8, train_wall=11, gb_free=19.4, wall=762
2022-12-09 15:10:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:11:30 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.707 | nll_loss 2.166 | ppl 4.49 | bleu 36.51 | wps 4551.3 | wpb 2835.3 | bsz 115.6 | num_updates 61705 | best_bleu 36.51
2022-12-09 15:11:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 61705 updates
2022-12-09 15:11:31 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint56.pt
2022-12-09 15:11:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint56.pt (epoch 56 @ 61705 updates, score 36.51) (writing took 1.5620452631264925 seconds)
2022-12-09 15:11:32 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-12-09 15:11:32 | INFO | train | epoch 056 | loss 7.319 | nll_loss 3.572 | ppl 11.89 | wps 24778.8 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 61705 | lr 0.000127303 | gnorm 1.856 | loss_scale 8 | train_wall 115 | gb_free 19.5 | wall 804
2022-12-09 15:11:32 | INFO | fairseq.trainer | begin training epoch 57
2022-12-09 15:11:42 | INFO | train_inner | epoch 057:     95 / 1102 loss=7.322, nll_loss=3.55, ppl=11.71, wps=6840.7, ups=1.92, wpb=3568.8, bsz=126.9, num_updates=61800, lr=0.000127205, gnorm=1.915, loss_scale=8, train_wall=10, gb_free=19.4, wall=814
2022-12-09 15:11:53 | INFO | train_inner | epoch 057:    195 / 1102 loss=7.189, nll_loss=3.434, ppl=10.81, wps=33395.3, ups=9.35, wpb=3573.4, bsz=155.6, num_updates=61900, lr=0.000127103, gnorm=1.901, loss_scale=8, train_wall=10, gb_free=19.5, wall=825
2022-12-09 15:12:03 | INFO | train_inner | epoch 057:    295 / 1102 loss=7.264, nll_loss=3.519, ppl=11.46, wps=33240.8, ups=9.37, wpb=3545.9, bsz=147.6, num_updates=62000, lr=0.000127, gnorm=1.776, loss_scale=8, train_wall=10, gb_free=19.5, wall=836
2022-12-09 15:12:14 | INFO | train_inner | epoch 057:    395 / 1102 loss=7.338, nll_loss=3.574, ppl=11.91, wps=33468.6, ups=9.32, wpb=3590.9, bsz=135, num_updates=62100, lr=0.000126898, gnorm=1.874, loss_scale=8, train_wall=10, gb_free=19.6, wall=847
2022-12-09 15:12:25 | INFO | train_inner | epoch 057:    495 / 1102 loss=7.354, nll_loss=3.594, ppl=12.08, wps=32947.3, ups=9.15, wpb=3601.1, bsz=133.1, num_updates=62200, lr=0.000126796, gnorm=1.849, loss_scale=8, train_wall=11, gb_free=19.3, wall=858
2022-12-09 15:12:36 | INFO | train_inner | epoch 057:    595 / 1102 loss=7.226, nll_loss=3.483, ppl=11.18, wps=33078.8, ups=9.21, wpb=3593.2, bsz=159, num_updates=62300, lr=0.000126694, gnorm=1.852, loss_scale=8, train_wall=11, gb_free=19.4, wall=868
2022-12-09 15:12:47 | INFO | train_inner | epoch 057:    695 / 1102 loss=7.316, nll_loss=3.584, ppl=11.99, wps=33165.7, ups=9.21, wpb=3599.6, bsz=158.6, num_updates=62400, lr=0.000126592, gnorm=1.776, loss_scale=8, train_wall=11, gb_free=19.4, wall=879
2022-12-09 15:12:58 | INFO | train_inner | epoch 057:    795 / 1102 loss=7.379, nll_loss=3.622, ppl=12.31, wps=32626.9, ups=9.15, wpb=3565, bsz=143.4, num_updates=62500, lr=0.000126491, gnorm=1.925, loss_scale=8, train_wall=11, gb_free=19.4, wall=890
2022-12-09 15:13:09 | INFO | train_inner | epoch 057:    895 / 1102 loss=7.38, nll_loss=3.634, ppl=12.41, wps=33299.8, ups=9.2, wpb=3620.4, bsz=137, num_updates=62600, lr=0.00012639, gnorm=1.881, loss_scale=8, train_wall=11, gb_free=19.5, wall=901
2022-12-09 15:13:20 | INFO | train_inner | epoch 057:    995 / 1102 loss=7.345, nll_loss=3.601, ppl=12.13, wps=32776.8, ups=9.12, wpb=3593.8, bsz=143.2, num_updates=62700, lr=0.000126289, gnorm=1.839, loss_scale=8, train_wall=11, gb_free=19.7, wall=912
2022-12-09 15:13:31 | INFO | train_inner | epoch 057:   1095 / 1102 loss=7.304, nll_loss=3.568, ppl=11.86, wps=32440.3, ups=9.07, wpb=3574.9, bsz=157.7, num_updates=62800, lr=0.000126189, gnorm=1.929, loss_scale=8, train_wall=11, gb_free=19.1, wall=923
2022-12-09 15:13:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:14:10 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.711 | nll_loss 2.169 | ppl 4.5 | bleu 36.57 | wps 4643.4 | wpb 2835.3 | bsz 115.6 | num_updates 62807 | best_bleu 36.57
2022-12-09 15:14:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 62807 updates
2022-12-09 15:14:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint57.pt
2022-12-09 15:14:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint57.pt (epoch 57 @ 62807 updates, score 36.57) (writing took 1.6780966660007834 seconds)
2022-12-09 15:14:12 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-12-09 15:14:12 | INFO | train | epoch 057 | loss 7.31 | nll_loss 3.56 | ppl 11.79 | wps 24632.9 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 62807 | lr 0.000126182 | gnorm 1.865 | loss_scale 8 | train_wall 117 | gb_free 19.3 | wall 964
2022-12-09 15:14:12 | INFO | fairseq.trainer | begin training epoch 58
2022-12-09 15:14:23 | INFO | train_inner | epoch 058:     93 / 1102 loss=7.097, nll_loss=3.355, ppl=10.23, wps=6883.1, ups=1.92, wpb=3583, bsz=174.5, num_updates=62900, lr=0.000126088, gnorm=1.807, loss_scale=8, train_wall=11, gb_free=19.4, wall=975
2022-12-09 15:14:34 | INFO | train_inner | epoch 058:    193 / 1102 loss=7.29, nll_loss=3.55, ppl=11.71, wps=32843.1, ups=8.98, wpb=3656.3, bsz=148.5, num_updates=63000, lr=0.000125988, gnorm=1.795, loss_scale=8, train_wall=11, gb_free=19.7, wall=986
2022-12-09 15:14:45 | INFO | train_inner | epoch 058:    293 / 1102 loss=7.264, nll_loss=3.521, ppl=11.48, wps=32568, ups=9.04, wpb=3602.7, bsz=153.1, num_updates=63100, lr=0.000125888, gnorm=1.841, loss_scale=8, train_wall=11, gb_free=19.8, wall=997
2022-12-09 15:14:56 | INFO | train_inner | epoch 058:    393 / 1102 loss=7.27, nll_loss=3.52, ppl=11.47, wps=32854.2, ups=9.11, wpb=3605.4, bsz=145.4, num_updates=63200, lr=0.000125789, gnorm=1.826, loss_scale=8, train_wall=11, gb_free=19.3, wall=1008
2022-12-09 15:15:07 | INFO | train_inner | epoch 058:    493 / 1102 loss=7.304, nll_loss=3.533, ppl=11.57, wps=32126.3, ups=9.14, wpb=3515.7, bsz=137.5, num_updates=63300, lr=0.000125689, gnorm=1.879, loss_scale=8, train_wall=11, gb_free=19.5, wall=1019
2022-12-09 15:15:18 | INFO | train_inner | epoch 058:    593 / 1102 loss=7.328, nll_loss=3.584, ppl=11.99, wps=32429.4, ups=9.03, wpb=3590.4, bsz=149.5, num_updates=63400, lr=0.00012559, gnorm=1.862, loss_scale=8, train_wall=11, gb_free=19.5, wall=1030
2022-12-09 15:15:29 | INFO | train_inner | epoch 058:    693 / 1102 loss=7.355, nll_loss=3.611, ppl=12.22, wps=32880.3, ups=9.07, wpb=3624.8, bsz=144.7, num_updates=63500, lr=0.000125491, gnorm=1.827, loss_scale=8, train_wall=11, gb_free=19.6, wall=1041
2022-12-09 15:15:40 | INFO | train_inner | epoch 058:    793 / 1102 loss=7.346, nll_loss=3.586, ppl=12.01, wps=32765.1, ups=9.1, wpb=3601, bsz=131.8, num_updates=63600, lr=0.000125392, gnorm=1.865, loss_scale=8, train_wall=11, gb_free=19.2, wall=1052
2022-12-09 15:15:51 | INFO | train_inner | epoch 058:    893 / 1102 loss=7.274, nll_loss=3.522, ppl=11.49, wps=32593.7, ups=9.13, wpb=3568.8, bsz=149.9, num_updates=63700, lr=0.000125294, gnorm=1.848, loss_scale=8, train_wall=11, gb_free=19.6, wall=1063
2022-12-09 15:16:02 | INFO | train_inner | epoch 058:    993 / 1102 loss=7.358, nll_loss=3.597, ppl=12.1, wps=32472.2, ups=9.18, wpb=3538.3, bsz=135, num_updates=63800, lr=0.000125196, gnorm=1.909, loss_scale=8, train_wall=11, gb_free=19.5, wall=1074
2022-12-09 15:16:13 | INFO | train_inner | epoch 058:   1093 / 1102 loss=7.382, nll_loss=3.629, ppl=12.37, wps=32255.5, ups=9.1, wpb=3544.5, bsz=137.8, num_updates=63900, lr=0.000125098, gnorm=1.888, loss_scale=8, train_wall=11, gb_free=19.5, wall=1085
2022-12-09 15:16:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:16:56 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.704 | nll_loss 2.165 | ppl 4.49 | bleu 36.58 | wps 4266.6 | wpb 2835.3 | bsz 115.6 | num_updates 63909 | best_bleu 36.58
2022-12-09 15:16:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 63909 updates
2022-12-09 15:16:57 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint58.pt
2022-12-09 15:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint58.pt (epoch 58 @ 63909 updates, score 36.58) (writing took 1.5180752277374268 seconds)
2022-12-09 15:16:57 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-12-09 15:16:57 | INFO | train | epoch 058 | loss 7.3 | nll_loss 3.548 | ppl 11.7 | wps 23872 | ups 6.66 | wpb 3583.6 | bsz 145.4 | num_updates 63909 | lr 0.000125089 | gnorm 1.851 | loss_scale 8 | train_wall 119 | gb_free 19.4 | wall 1130
2022-12-09 15:16:57 | INFO | fairseq.trainer | begin training epoch 59
2022-12-09 15:17:08 | INFO | train_inner | epoch 059:     91 / 1102 loss=7.232, nll_loss=3.468, ppl=11.06, wps=6456.6, ups=1.82, wpb=3544.2, bsz=138.4, num_updates=64000, lr=0.000125, gnorm=1.817, loss_scale=8, train_wall=11, gb_free=19.4, wall=1140
2022-12-09 15:17:19 | INFO | train_inner | epoch 059:    191 / 1102 loss=7.212, nll_loss=3.454, ppl=10.96, wps=32416.2, ups=9.13, wpb=3549.5, bsz=147.4, num_updates=64100, lr=0.000124902, gnorm=1.803, loss_scale=8, train_wall=11, gb_free=19.2, wall=1151
2022-12-09 15:17:30 | INFO | train_inner | epoch 059:    291 / 1102 loss=7.272, nll_loss=3.52, ppl=11.48, wps=32805.8, ups=9.06, wpb=3619, bsz=154.1, num_updates=64200, lr=0.000124805, gnorm=1.838, loss_scale=8, train_wall=11, gb_free=19.3, wall=1162
2022-12-09 15:17:41 | INFO | train_inner | epoch 059:    391 / 1102 loss=7.234, nll_loss=3.467, ppl=11.06, wps=32268.1, ups=9.16, wpb=3523, bsz=149, num_updates=64300, lr=0.000124708, gnorm=1.859, loss_scale=8, train_wall=11, gb_free=20, wall=1173
2022-12-09 15:17:51 | INFO | train_inner | epoch 059:    491 / 1102 loss=7.29, nll_loss=3.532, ppl=11.57, wps=33090, ups=9.17, wpb=3608.2, bsz=137.7, num_updates=64400, lr=0.000124611, gnorm=1.827, loss_scale=8, train_wall=11, gb_free=19.4, wall=1184
2022-12-09 15:18:02 | INFO | train_inner | epoch 059:    591 / 1102 loss=7.358, nll_loss=3.61, ppl=12.21, wps=33575.1, ups=9.18, wpb=3657.3, bsz=134.8, num_updates=64500, lr=0.000124515, gnorm=1.816, loss_scale=8, train_wall=11, gb_free=19.4, wall=1195
2022-12-09 15:18:13 | INFO | train_inner | epoch 059:    691 / 1102 loss=7.317, nll_loss=3.57, ppl=11.88, wps=32937, ups=9.2, wpb=3581.4, bsz=148.8, num_updates=64600, lr=0.000124418, gnorm=1.9, loss_scale=8, train_wall=11, gb_free=19.3, wall=1206
2022-12-09 15:18:24 | INFO | train_inner | epoch 059:    791 / 1102 loss=7.384, nll_loss=3.625, ppl=12.33, wps=33079.8, ups=9.2, wpb=3597.4, bsz=136.3, num_updates=64700, lr=0.000124322, gnorm=1.936, loss_scale=8, train_wall=11, gb_free=19.7, wall=1216
2022-12-09 15:18:35 | INFO | train_inner | epoch 059:    891 / 1102 loss=7.298, nll_loss=3.559, ppl=11.78, wps=32371.6, ups=9.04, wpb=3580.8, bsz=150.2, num_updates=64800, lr=0.000124226, gnorm=1.801, loss_scale=8, train_wall=11, gb_free=19.5, wall=1227
2022-12-09 15:18:46 | INFO | train_inner | epoch 059:    991 / 1102 loss=7.307, nll_loss=3.557, ppl=11.77, wps=32928.8, ups=9.15, wpb=3597.8, bsz=141.9, num_updates=64900, lr=0.00012413, gnorm=1.872, loss_scale=8, train_wall=11, gb_free=19.3, wall=1238
2022-12-09 15:18:57 | INFO | train_inner | epoch 059:   1091 / 1102 loss=7.284, nll_loss=3.534, ppl=11.58, wps=32342.9, ups=9.15, wpb=3534.6, bsz=151.6, num_updates=65000, lr=0.000124035, gnorm=1.85, loss_scale=8, train_wall=11, gb_free=19.3, wall=1249
2022-12-09 15:18:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:19:39 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.7 | nll_loss 2.163 | ppl 4.48 | bleu 36.67 | wps 4407.6 | wpb 2835.3 | bsz 115.6 | num_updates 65011 | best_bleu 36.67
2022-12-09 15:19:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 65011 updates
2022-12-09 15:19:40 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint59.pt
2022-12-09 15:19:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint59.pt (epoch 59 @ 65011 updates, score 36.67) (writing took 1.738630572333932 seconds)
2022-12-09 15:19:41 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-12-09 15:19:41 | INFO | train | epoch 059 | loss 7.289 | nll_loss 3.536 | ppl 11.6 | wps 24170.9 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 65011 | lr 0.000124024 | gnorm 1.846 | loss_scale 8 | train_wall 118 | gb_free 19.1 | wall 1293
2022-12-09 15:19:41 | INFO | fairseq.trainer | begin training epoch 60
2022-12-09 15:19:51 | INFO | train_inner | epoch 060:     89 / 1102 loss=7.219, nll_loss=3.442, ppl=10.87, wps=6573.2, ups=1.86, wpb=3533.7, bsz=138, num_updates=65100, lr=0.000123939, gnorm=1.904, loss_scale=8, train_wall=10, gb_free=19.5, wall=1303
2022-12-09 15:20:02 | INFO | train_inner | epoch 060:    189 / 1102 loss=7.192, nll_loss=3.44, ppl=10.85, wps=32507.2, ups=9.04, wpb=3595.9, bsz=147.4, num_updates=65200, lr=0.000123844, gnorm=1.813, loss_scale=8, train_wall=11, gb_free=19.2, wall=1314
2022-12-09 15:20:13 | INFO | train_inner | epoch 060:    289 / 1102 loss=7.22, nll_loss=3.467, ppl=11.06, wps=32468.1, ups=9.09, wpb=3571.4, bsz=154.1, num_updates=65300, lr=0.000123749, gnorm=1.877, loss_scale=8, train_wall=11, gb_free=19.6, wall=1325
2022-12-09 15:20:24 | INFO | train_inner | epoch 060:    389 / 1102 loss=7.32, nll_loss=3.545, ppl=11.67, wps=32349.6, ups=9.12, wpb=3548.9, bsz=132.6, num_updates=65400, lr=0.000123655, gnorm=1.9, loss_scale=8, train_wall=11, gb_free=19.6, wall=1336
2022-12-09 15:20:35 | INFO | train_inner | epoch 060:    489 / 1102 loss=7.214, nll_loss=3.463, ppl=11.03, wps=32986, ups=9.1, wpb=3625.4, bsz=153, num_updates=65500, lr=0.00012356, gnorm=1.838, loss_scale=8, train_wall=11, gb_free=19.7, wall=1347
2022-12-09 15:20:46 | INFO | train_inner | epoch 060:    589 / 1102 loss=7.29, nll_loss=3.528, ppl=11.54, wps=33293.3, ups=9.11, wpb=3655.9, bsz=140.6, num_updates=65600, lr=0.000123466, gnorm=1.806, loss_scale=8, train_wall=11, gb_free=19.3, wall=1358
2022-12-09 15:20:57 | INFO | train_inner | epoch 060:    689 / 1102 loss=7.298, nll_loss=3.543, ppl=11.65, wps=32387.3, ups=9.09, wpb=3563.3, bsz=146.2, num_updates=65700, lr=0.000123372, gnorm=1.864, loss_scale=8, train_wall=11, gb_free=19.5, wall=1369
2022-12-09 15:21:08 | INFO | train_inner | epoch 060:    789 / 1102 loss=7.29, nll_loss=3.551, ppl=11.72, wps=33149.7, ups=9.04, wpb=3668.9, bsz=155, num_updates=65800, lr=0.000123278, gnorm=1.809, loss_scale=8, train_wall=11, gb_free=19.5, wall=1380
2022-12-09 15:21:19 | INFO | train_inner | epoch 060:    889 / 1102 loss=7.377, nll_loss=3.642, ppl=12.48, wps=33002.8, ups=9.17, wpb=3598.2, bsz=147.8, num_updates=65900, lr=0.000123185, gnorm=1.841, loss_scale=8, train_wall=11, gb_free=19.6, wall=1391
2022-12-09 15:21:30 | INFO | train_inner | epoch 060:    989 / 1102 loss=7.329, nll_loss=3.574, ppl=11.91, wps=32641.2, ups=9.23, wpb=3538.2, bsz=152.6, num_updates=66000, lr=0.000123091, gnorm=1.941, loss_scale=8, train_wall=11, gb_free=19.5, wall=1402
2022-12-09 15:21:40 | INFO | train_inner | epoch 060:   1089 / 1102 loss=7.326, nll_loss=3.563, ppl=11.81, wps=32622.5, ups=9.19, wpb=3549.2, bsz=136.6, num_updates=66100, lr=0.000122998, gnorm=1.948, loss_scale=8, train_wall=11, gb_free=19.4, wall=1413
2022-12-09 15:21:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:22:23 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.703 | nll_loss 2.165 | ppl 4.49 | bleu 36.52 | wps 4408.4 | wpb 2835.3 | bsz 115.6 | num_updates 66113 | best_bleu 36.67
2022-12-09 15:22:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 66113 updates
2022-12-09 15:22:24 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint60.pt
2022-12-09 15:22:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint60.pt (epoch 60 @ 66113 updates, score 36.52) (writing took 1.0821099560707808 seconds)
2022-12-09 15:22:24 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-12-09 15:22:24 | INFO | train | epoch 060 | loss 7.28 | nll_loss 3.524 | ppl 11.5 | wps 24211.2 | ups 6.76 | wpb 3583.6 | bsz 145.4 | num_updates 66113 | lr 0.000122986 | gnorm 1.869 | loss_scale 8 | train_wall 118 | gb_free 19.6 | wall 1456
2022-12-09 15:22:24 | INFO | fairseq.trainer | begin training epoch 61
2022-12-09 15:22:34 | INFO | train_inner | epoch 061:     87 / 1102 loss=7.204, nll_loss=3.457, ppl=10.98, wps=6808.6, ups=1.87, wpb=3634.2, bsz=152.7, num_updates=66200, lr=0.000122905, gnorm=1.843, loss_scale=8, train_wall=11, gb_free=19.7, wall=1466
2022-12-09 15:22:45 | INFO | train_inner | epoch 061:    187 / 1102 loss=7.222, nll_loss=3.458, ppl=10.99, wps=32844.6, ups=9.28, wpb=3539.7, bsz=148.6, num_updates=66300, lr=0.000122813, gnorm=1.845, loss_scale=8, train_wall=11, gb_free=19.4, wall=1477
2022-12-09 15:22:55 | INFO | train_inner | epoch 061:    287 / 1102 loss=7.306, nll_loss=3.538, ppl=11.62, wps=33340.1, ups=9.21, wpb=3621.2, bsz=134.5, num_updates=66400, lr=0.00012272, gnorm=1.887, loss_scale=8, train_wall=11, gb_free=19.5, wall=1488
2022-12-09 15:23:06 | INFO | train_inner | epoch 061:    387 / 1102 loss=7.286, nll_loss=3.529, ppl=11.55, wps=33157, ups=9.18, wpb=3612.5, bsz=144.7, num_updates=66500, lr=0.000122628, gnorm=1.81, loss_scale=8, train_wall=11, gb_free=19.5, wall=1499
2022-12-09 15:23:17 | INFO | train_inner | epoch 061:    487 / 1102 loss=7.217, nll_loss=3.47, ppl=11.08, wps=32771.8, ups=9.2, wpb=3562, bsz=157.6, num_updates=66600, lr=0.000122536, gnorm=1.873, loss_scale=8, train_wall=11, gb_free=20, wall=1510
2022-12-09 15:23:28 | INFO | train_inner | epoch 061:    587 / 1102 loss=7.215, nll_loss=3.47, ppl=11.08, wps=32844.4, ups=9.14, wpb=3594.1, bsz=156.2, num_updates=66700, lr=0.000122444, gnorm=1.82, loss_scale=8, train_wall=11, gb_free=19.5, wall=1521
2022-12-09 15:23:39 | INFO | train_inner | epoch 061:    687 / 1102 loss=7.18, nll_loss=3.434, ppl=10.81, wps=32175.4, ups=9.05, wpb=3554.4, bsz=163.7, num_updates=66800, lr=0.000122352, gnorm=1.91, loss_scale=8, train_wall=11, gb_free=19.5, wall=1532
2022-12-09 15:23:50 | INFO | train_inner | epoch 061:    787 / 1102 loss=7.356, nll_loss=3.589, ppl=12.04, wps=32595.6, ups=9.17, wpb=3554.9, bsz=135.1, num_updates=66900, lr=0.000122261, gnorm=2.013, loss_scale=8, train_wall=11, gb_free=20.1, wall=1542
2022-12-09 15:24:01 | INFO | train_inner | epoch 061:    887 / 1102 loss=7.323, nll_loss=3.562, ppl=11.81, wps=33063, ups=9.18, wpb=3601.5, bsz=136.2, num_updates=67000, lr=0.000122169, gnorm=1.891, loss_scale=8, train_wall=11, gb_free=19.5, wall=1553
2022-12-09 15:24:12 | INFO | train_inner | epoch 061:    987 / 1102 loss=7.281, nll_loss=3.523, ppl=11.49, wps=32890.6, ups=9.16, wpb=3589.8, bsz=138.8, num_updates=67100, lr=0.000122078, gnorm=1.893, loss_scale=8, train_wall=11, gb_free=19.7, wall=1564
2022-12-09 15:24:23 | INFO | train_inner | epoch 061:   1087 / 1102 loss=7.365, nll_loss=3.6, ppl=12.13, wps=32799, ups=9.26, wpb=3542.5, bsz=137.7, num_updates=67200, lr=0.000121988, gnorm=1.947, loss_scale=8, train_wall=11, gb_free=19.6, wall=1575
2022-12-09 15:24:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:25:06 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.699 | nll_loss 2.159 | ppl 4.47 | bleu 36.54 | wps 4346.5 | wpb 2835.3 | bsz 115.6 | num_updates 67215 | best_bleu 36.67
2022-12-09 15:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 67215 updates
2022-12-09 15:25:07 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint61.pt
2022-12-09 15:25:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint61.pt (epoch 61 @ 67215 updates, score 36.54) (writing took 1.2822852116078138 seconds)
2022-12-09 15:25:07 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-12-09 15:25:07 | INFO | train | epoch 061 | loss 7.271 | nll_loss 3.513 | ppl 11.42 | wps 24202.1 | ups 6.75 | wpb 3583.6 | bsz 145.4 | num_updates 67215 | lr 0.000121974 | gnorm 1.884 | loss_scale 8 | train_wall 117 | gb_free 19.5 | wall 1619
2022-12-09 15:25:07 | INFO | fairseq.trainer | begin training epoch 62
2022-12-09 15:25:17 | INFO | train_inner | epoch 062:     85 / 1102 loss=7.175, nll_loss=3.42, ppl=10.7, wps=6616.7, ups=1.85, wpb=3580.2, bsz=151.2, num_updates=67300, lr=0.000121897, gnorm=1.816, loss_scale=8, train_wall=11, gb_free=19.2, wall=1629
2022-12-09 15:25:28 | INFO | train_inner | epoch 062:    185 / 1102 loss=7.286, nll_loss=3.533, ppl=11.58, wps=33473.8, ups=9.18, wpb=3648.2, bsz=141.5, num_updates=67400, lr=0.000121806, gnorm=1.793, loss_scale=8, train_wall=11, gb_free=19.4, wall=1640
2022-12-09 15:25:39 | INFO | train_inner | epoch 062:    285 / 1102 loss=7.211, nll_loss=3.451, ppl=10.93, wps=32965.2, ups=9.11, wpb=3619.2, bsz=147.4, num_updates=67500, lr=0.000121716, gnorm=1.893, loss_scale=8, train_wall=11, gb_free=19.5, wall=1651
2022-12-09 15:25:50 | INFO | train_inner | epoch 062:    385 / 1102 loss=7.283, nll_loss=3.525, ppl=11.51, wps=33199.7, ups=9.17, wpb=3622.3, bsz=148.1, num_updates=67600, lr=0.000121626, gnorm=1.856, loss_scale=8, train_wall=11, gb_free=19.4, wall=1662
2022-12-09 15:26:01 | INFO | train_inner | epoch 062:    485 / 1102 loss=7.343, nll_loss=3.569, ppl=11.87, wps=32509.3, ups=9.17, wpb=3545.2, bsz=126.6, num_updates=67700, lr=0.000121536, gnorm=1.888, loss_scale=8, train_wall=11, gb_free=19.9, wall=1673
2022-12-09 15:26:12 | INFO | train_inner | epoch 062:    585 / 1102 loss=7.304, nll_loss=3.547, ppl=11.69, wps=32856.5, ups=9.06, wpb=3626.3, bsz=139.9, num_updates=67800, lr=0.000121447, gnorm=1.84, loss_scale=8, train_wall=11, gb_free=19.4, wall=1684
2022-12-09 15:26:22 | INFO | train_inner | epoch 062:    685 / 1102 loss=7.22, nll_loss=3.458, ppl=10.99, wps=32458.1, ups=9.15, wpb=3546.4, bsz=153.1, num_updates=67900, lr=0.000121357, gnorm=1.865, loss_scale=8, train_wall=11, gb_free=19.7, wall=1695
2022-12-09 15:26:34 | INFO | train_inner | epoch 062:    785 / 1102 loss=7.178, nll_loss=3.425, ppl=10.74, wps=32087.6, ups=9.05, wpb=3546.6, bsz=161.6, num_updates=68000, lr=0.000121268, gnorm=1.844, loss_scale=8, train_wall=11, gb_free=19.4, wall=1706
2022-12-09 15:26:45 | INFO | train_inner | epoch 062:    885 / 1102 loss=7.319, nll_loss=3.547, ppl=11.68, wps=32311, ups=9.09, wpb=3555.9, bsz=129.4, num_updates=68100, lr=0.000121179, gnorm=1.889, loss_scale=8, train_wall=11, gb_free=19.8, wall=1717
2022-12-09 15:26:55 | INFO | train_inner | epoch 062:    985 / 1102 loss=7.284, nll_loss=3.519, ppl=11.46, wps=32277.7, ups=9.18, wpb=3514.6, bsz=139.6, num_updates=68200, lr=0.00012109, gnorm=2.045, loss_scale=8, train_wall=11, gb_free=19.5, wall=1728
2022-12-09 15:27:07 | INFO | train_inner | epoch 062:   1085 / 1102 loss=7.263, nll_loss=3.52, ppl=11.47, wps=32630.3, ups=9, wpb=3623.9, bsz=153, num_updates=68300, lr=0.000121001, gnorm=1.863, loss_scale=8, train_wall=11, gb_free=19.3, wall=1739
2022-12-09 15:27:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:27:45 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.694 | nll_loss 2.154 | ppl 4.45 | bleu 36.49 | wps 4983.6 | wpb 2835.3 | bsz 115.6 | num_updates 68317 | best_bleu 36.67
2022-12-09 15:27:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 68317 updates
2022-12-09 15:27:45 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint62.pt
2022-12-09 15:27:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint62.pt (epoch 62 @ 68317 updates, score 36.49) (writing took 1.0667315432801843 seconds)
2022-12-09 15:27:46 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-12-09 15:27:46 | INFO | train | epoch 062 | loss 7.259 | nll_loss 3.501 | ppl 11.32 | wps 24886.7 | ups 6.94 | wpb 3583.6 | bsz 145.4 | num_updates 68317 | lr 0.000120986 | gnorm 1.871 | loss_scale 8 | train_wall 118 | gb_free 19.5 | wall 1778
2022-12-09 15:27:46 | INFO | fairseq.trainer | begin training epoch 63
2022-12-09 15:27:55 | INFO | train_inner | epoch 063:     83 / 1102 loss=7.266, nll_loss=3.524, ppl=11.5, wps=7441.4, ups=2.06, wpb=3617.4, bsz=150.6, num_updates=68400, lr=0.000120913, gnorm=1.776, loss_scale=8, train_wall=11, gb_free=19.3, wall=1788
2022-12-09 15:28:06 | INFO | train_inner | epoch 063:    183 / 1102 loss=7.208, nll_loss=3.444, ppl=10.88, wps=32994.3, ups=9.2, wpb=3588.1, bsz=145.7, num_updates=68500, lr=0.000120824, gnorm=1.869, loss_scale=8, train_wall=11, gb_free=19.6, wall=1798
2022-12-09 15:28:17 | INFO | train_inner | epoch 063:    283 / 1102 loss=7.256, nll_loss=3.484, ppl=11.19, wps=32648.1, ups=9.14, wpb=3572.9, bsz=137.3, num_updates=68600, lr=0.000120736, gnorm=1.847, loss_scale=8, train_wall=11, gb_free=19.3, wall=1809
2022-12-09 15:28:28 | INFO | train_inner | epoch 063:    383 / 1102 loss=7.205, nll_loss=3.435, ppl=10.81, wps=32855.2, ups=9.16, wpb=3587, bsz=143.2, num_updates=68700, lr=0.000120648, gnorm=1.898, loss_scale=8, train_wall=11, gb_free=19.5, wall=1820
2022-12-09 15:28:39 | INFO | train_inner | epoch 063:    483 / 1102 loss=7.255, nll_loss=3.499, ppl=11.3, wps=33399.3, ups=9.14, wpb=3654.7, bsz=144.6, num_updates=68800, lr=0.000120561, gnorm=1.832, loss_scale=8, train_wall=11, gb_free=19.4, wall=1831
2022-12-09 15:28:50 | INFO | train_inner | epoch 063:    583 / 1102 loss=7.3, nll_loss=3.531, ppl=11.56, wps=32281.2, ups=9.13, wpb=3535.3, bsz=140.7, num_updates=68900, lr=0.000120473, gnorm=1.992, loss_scale=8, train_wall=11, gb_free=19.8, wall=1842
2022-12-09 15:29:01 | INFO | train_inner | epoch 063:    683 / 1102 loss=7.217, nll_loss=3.457, ppl=10.98, wps=31851.5, ups=9.05, wpb=3518.8, bsz=159.8, num_updates=69000, lr=0.000120386, gnorm=1.907, loss_scale=8, train_wall=11, gb_free=19.6, wall=1853
2022-12-09 15:29:12 | INFO | train_inner | epoch 063:    783 / 1102 loss=7.226, nll_loss=3.473, ppl=11.11, wps=33132, ups=9.14, wpb=3626.8, bsz=153.7, num_updates=69100, lr=0.000120299, gnorm=1.896, loss_scale=8, train_wall=11, gb_free=19.7, wall=1864
2022-12-09 15:29:23 | INFO | train_inner | epoch 063:    883 / 1102 loss=7.283, nll_loss=3.521, ppl=11.48, wps=32414.7, ups=9.17, wpb=3536.2, bsz=146.7, num_updates=69200, lr=0.000120212, gnorm=1.971, loss_scale=8, train_wall=11, gb_free=19.3, wall=1875
2022-12-09 15:29:34 | INFO | train_inner | epoch 063:    983 / 1102 loss=7.313, nll_loss=3.55, ppl=11.71, wps=32187.9, ups=9.13, wpb=3524.2, bsz=137.7, num_updates=69300, lr=0.000120125, gnorm=1.918, loss_scale=8, train_wall=11, gb_free=19.5, wall=1886
2022-12-09 15:29:45 | INFO | train_inner | epoch 063:   1083 / 1102 loss=7.272, nll_loss=3.52, ppl=11.47, wps=33168.6, ups=9.06, wpb=3662.8, bsz=141.7, num_updates=69400, lr=0.000120038, gnorm=1.911, loss_scale=8, train_wall=11, gb_free=19.3, wall=1897
2022-12-09 15:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:30:25 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.705 | nll_loss 2.159 | ppl 4.47 | bleu 36.44 | wps 4695 | wpb 2835.3 | bsz 115.6 | num_updates 69419 | best_bleu 36.67
2022-12-09 15:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 69419 updates
2022-12-09 15:30:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint63.pt
2022-12-09 15:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint63.pt (epoch 63 @ 69419 updates, score 36.44) (writing took 1.0658059269189835 seconds)
2022-12-09 15:30:26 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-12-09 15:30:26 | INFO | train | epoch 063 | loss 7.252 | nll_loss 3.491 | ppl 11.24 | wps 24586.8 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 69419 | lr 0.000120022 | gnorm 1.892 | loss_scale 8 | train_wall 118 | gb_free 19.4 | wall 1939
2022-12-09 15:30:26 | INFO | fairseq.trainer | begin training epoch 64
2022-12-09 15:30:35 | INFO | train_inner | epoch 064:     81 / 1102 loss=7.221, nll_loss=3.444, ppl=10.88, wps=6983.8, ups=1.97, wpb=3547.8, bsz=136.7, num_updates=69500, lr=0.000119952, gnorm=1.868, loss_scale=8, train_wall=11, gb_free=19.8, wall=1948
2022-12-09 15:30:46 | INFO | train_inner | epoch 064:    181 / 1102 loss=7.239, nll_loss=3.47, ppl=11.08, wps=32759.6, ups=9.16, wpb=3575.3, bsz=140.2, num_updates=69600, lr=0.000119866, gnorm=1.883, loss_scale=8, train_wall=11, gb_free=19.3, wall=1959
2022-12-09 15:30:57 | INFO | train_inner | epoch 064:    281 / 1102 loss=7.274, nll_loss=3.505, ppl=11.35, wps=32503, ups=9.19, wpb=3537.6, bsz=139.6, num_updates=69700, lr=0.00011978, gnorm=1.912, loss_scale=8, train_wall=11, gb_free=19.6, wall=1970
2022-12-09 15:31:08 | INFO | train_inner | epoch 064:    381 / 1102 loss=7.253, nll_loss=3.476, ppl=11.13, wps=33036.1, ups=9.21, wpb=3587.2, bsz=134.4, num_updates=69800, lr=0.000119694, gnorm=1.906, loss_scale=8, train_wall=11, gb_free=19.6, wall=1981
2022-12-09 15:31:19 | INFO | train_inner | epoch 064:    481 / 1102 loss=7.186, nll_loss=3.435, ppl=10.81, wps=33102, ups=9.08, wpb=3645, bsz=160, num_updates=69900, lr=0.000119608, gnorm=1.811, loss_scale=8, train_wall=11, gb_free=19.8, wall=1992
2022-12-09 15:31:30 | INFO | train_inner | epoch 064:    581 / 1102 loss=7.261, nll_loss=3.483, ppl=11.18, wps=32606.1, ups=9.26, wpb=3521.9, bsz=138, num_updates=70000, lr=0.000119523, gnorm=1.97, loss_scale=8, train_wall=11, gb_free=19.3, wall=2002
2022-12-09 15:31:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-09 15:31:41 | INFO | train_inner | epoch 064:    682 / 1102 loss=7.192, nll_loss=3.44, ppl=10.85, wps=32664.5, ups=9, wpb=3628.4, bsz=154.4, num_updates=70100, lr=0.000119438, gnorm=1.824, loss_scale=4, train_wall=11, gb_free=19.5, wall=2013
2022-12-09 15:31:52 | INFO | train_inner | epoch 064:    782 / 1102 loss=7.283, nll_loss=3.508, ppl=11.38, wps=32456.3, ups=9.05, wpb=3586.4, bsz=135, num_updates=70200, lr=0.000119352, gnorm=1.916, loss_scale=4, train_wall=11, gb_free=19.3, wall=2025
2022-12-09 15:32:03 | INFO | train_inner | epoch 064:    882 / 1102 loss=7.196, nll_loss=3.443, ppl=10.88, wps=32766.7, ups=9.18, wpb=3567.4, bsz=155.3, num_updates=70300, lr=0.000119268, gnorm=1.877, loss_scale=4, train_wall=11, gb_free=19.3, wall=2035
2022-12-09 15:32:14 | INFO | train_inner | epoch 064:    982 / 1102 loss=7.344, nll_loss=3.582, ppl=11.98, wps=31705.5, ups=9.08, wpb=3489.9, bsz=138.4, num_updates=70400, lr=0.000119183, gnorm=1.951, loss_scale=4, train_wall=11, gb_free=19.6, wall=2046
2022-12-09 15:32:25 | INFO | train_inner | epoch 064:   1082 / 1102 loss=7.222, nll_loss=3.49, ppl=11.23, wps=33133.7, ups=8.96, wpb=3697.8, bsz=162.2, num_updates=70500, lr=0.000119098, gnorm=1.755, loss_scale=4, train_wall=11, gb_free=19.7, wall=2058
2022-12-09 15:32:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:33:08 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.707 | nll_loss 2.164 | ppl 4.48 | bleu 36.38 | wps 4516.4 | wpb 2835.3 | bsz 115.6 | num_updates 70520 | best_bleu 36.67
2022-12-09 15:33:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 70520 updates
2022-12-09 15:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint64.pt
2022-12-09 15:33:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint64.pt (epoch 64 @ 70520 updates, score 36.38) (writing took 1.1298308186233044 seconds)
2022-12-09 15:33:09 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-12-09 15:33:09 | INFO | train | epoch 064 | loss 7.242 | nll_loss 3.48 | ppl 11.16 | wps 24315.3 | ups 6.79 | wpb 3583.3 | bsz 145.4 | num_updates 70520 | lr 0.000119081 | gnorm 1.879 | loss_scale 4 | train_wall 118 | gb_free 19.8 | wall 2101
2022-12-09 15:33:09 | INFO | fairseq.trainer | begin training epoch 65
2022-12-09 15:33:18 | INFO | train_inner | epoch 065:     80 / 1102 loss=7.165, nll_loss=3.409, ppl=10.62, wps=6936.9, ups=1.9, wpb=3652.2, bsz=152, num_updates=70600, lr=0.000119014, gnorm=1.842, loss_scale=4, train_wall=11, gb_free=20.1, wall=2110
2022-12-09 15:33:29 | INFO | train_inner | epoch 065:    180 / 1102 loss=7.155, nll_loss=3.402, ppl=10.57, wps=33419.2, ups=9.17, wpb=3645.6, bsz=153.3, num_updates=70700, lr=0.00011893, gnorm=1.811, loss_scale=4, train_wall=11, gb_free=19.5, wall=2121
2022-12-09 15:33:40 | INFO | train_inner | epoch 065:    280 / 1102 loss=7.197, nll_loss=3.428, ppl=10.76, wps=32303.8, ups=9.19, wpb=3515.8, bsz=150.7, num_updates=70800, lr=0.000118846, gnorm=2.004, loss_scale=4, train_wall=11, gb_free=19.4, wall=2132
2022-12-09 15:33:51 | INFO | train_inner | epoch 065:    380 / 1102 loss=7.268, nll_loss=3.487, ppl=11.21, wps=33274.9, ups=9.18, wpb=3622.8, bsz=131.4, num_updates=70900, lr=0.000118762, gnorm=1.896, loss_scale=4, train_wall=11, gb_free=19.7, wall=2143
2022-12-09 15:34:01 | INFO | train_inner | epoch 065:    480 / 1102 loss=7.206, nll_loss=3.43, ppl=10.78, wps=32390.2, ups=9.2, wpb=3520.1, bsz=144.7, num_updates=71000, lr=0.000118678, gnorm=1.989, loss_scale=4, train_wall=11, gb_free=19.4, wall=2154
2022-12-09 15:34:12 | INFO | train_inner | epoch 065:    580 / 1102 loss=7.27, nll_loss=3.496, ppl=11.28, wps=32676.9, ups=9.23, wpb=3539.8, bsz=133.2, num_updates=71100, lr=0.000118595, gnorm=1.896, loss_scale=4, train_wall=11, gb_free=19.3, wall=2165
2022-12-09 15:34:23 | INFO | train_inner | epoch 065:    680 / 1102 loss=7.227, nll_loss=3.478, ppl=11.14, wps=32574.6, ups=9.09, wpb=3583.1, bsz=153.5, num_updates=71200, lr=0.000118511, gnorm=1.92, loss_scale=4, train_wall=11, gb_free=19.6, wall=2176
2022-12-09 15:34:34 | INFO | train_inner | epoch 065:    780 / 1102 loss=7.252, nll_loss=3.494, ppl=11.26, wps=32288, ups=9.1, wpb=3547.3, bsz=150.6, num_updates=71300, lr=0.000118428, gnorm=1.884, loss_scale=4, train_wall=11, gb_free=19.7, wall=2187
2022-12-09 15:34:45 | INFO | train_inner | epoch 065:    880 / 1102 loss=7.295, nll_loss=3.519, ppl=11.46, wps=32724.1, ups=9.14, wpb=3578.8, bsz=132, num_updates=71400, lr=0.000118345, gnorm=1.912, loss_scale=4, train_wall=11, gb_free=19.3, wall=2198
2022-12-09 15:34:56 | INFO | train_inner | epoch 065:    980 / 1102 loss=7.306, nll_loss=3.537, ppl=11.61, wps=32995.8, ups=9.14, wpb=3609.6, bsz=141, num_updates=71500, lr=0.000118262, gnorm=1.948, loss_scale=4, train_wall=11, gb_free=19.3, wall=2208
2022-12-09 15:35:07 | INFO | train_inner | epoch 065:   1080 / 1102 loss=7.2, nll_loss=3.46, ppl=11.01, wps=32805.9, ups=9.1, wpb=3604.8, bsz=165.4, num_updates=71600, lr=0.00011818, gnorm=1.963, loss_scale=4, train_wall=11, gb_free=19.5, wall=2219
2022-12-09 15:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:35:49 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.695 | nll_loss 2.153 | ppl 4.45 | bleu 36.74 | wps 4571.9 | wpb 2835.3 | bsz 115.6 | num_updates 71622 | best_bleu 36.74
2022-12-09 15:35:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 71622 updates
2022-12-09 15:35:50 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint65.pt
2022-12-09 15:35:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint65.pt (epoch 65 @ 71622 updates, score 36.74) (writing took 1.497312230989337 seconds)
2022-12-09 15:35:51 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-12-09 15:35:51 | INFO | train | epoch 065 | loss 7.235 | nll_loss 3.472 | ppl 11.09 | wps 24395 | ups 6.81 | wpb 3583.6 | bsz 145.4 | num_updates 71622 | lr 0.000118162 | gnorm 1.917 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 2263
2022-12-09 15:35:51 | INFO | fairseq.trainer | begin training epoch 66
2022-12-09 15:35:59 | INFO | train_inner | epoch 066:     78 / 1102 loss=7.245, nll_loss=3.482, ppl=11.18, wps=6796.6, ups=1.91, wpb=3554.6, bsz=140, num_updates=71700, lr=0.000118097, gnorm=1.928, loss_scale=4, train_wall=11, gb_free=19.4, wall=2272
2022-12-09 15:36:10 | INFO | train_inner | epoch 066:    178 / 1102 loss=7.167, nll_loss=3.4, ppl=10.55, wps=32925.3, ups=9.25, wpb=3559.2, bsz=152.5, num_updates=71800, lr=0.000118015, gnorm=1.836, loss_scale=4, train_wall=11, gb_free=19.3, wall=2283
2022-12-09 15:36:21 | INFO | train_inner | epoch 066:    278 / 1102 loss=7.093, nll_loss=3.339, ppl=10.12, wps=32758.1, ups=9.07, wpb=3609.9, bsz=172.6, num_updates=71900, lr=0.000117933, gnorm=1.793, loss_scale=4, train_wall=11, gb_free=19.9, wall=2294
2022-12-09 15:36:32 | INFO | train_inner | epoch 066:    378 / 1102 loss=7.225, nll_loss=3.457, ppl=10.98, wps=32882.8, ups=9.29, wpb=3538.1, bsz=144.8, num_updates=72000, lr=0.000117851, gnorm=1.904, loss_scale=4, train_wall=11, gb_free=19.9, wall=2304
2022-12-09 15:36:43 | INFO | train_inner | epoch 066:    478 / 1102 loss=7.263, nll_loss=3.496, ppl=11.29, wps=32580, ups=9.16, wpb=3557.8, bsz=146.3, num_updates=72100, lr=0.000117769, gnorm=1.888, loss_scale=4, train_wall=11, gb_free=19.7, wall=2315
2022-12-09 15:36:54 | INFO | train_inner | epoch 066:    578 / 1102 loss=7.213, nll_loss=3.447, ppl=10.9, wps=33599.4, ups=9.25, wpb=3631.4, bsz=148.2, num_updates=72200, lr=0.000117688, gnorm=1.828, loss_scale=4, train_wall=11, gb_free=19.9, wall=2326
2022-12-09 15:37:05 | INFO | train_inner | epoch 066:    678 / 1102 loss=7.268, nll_loss=3.505, ppl=11.35, wps=33384.8, ups=9.21, wpb=3626.3, bsz=136, num_updates=72300, lr=0.000117606, gnorm=1.873, loss_scale=4, train_wall=11, gb_free=19.4, wall=2337
2022-12-09 15:37:16 | INFO | train_inner | epoch 066:    778 / 1102 loss=7.223, nll_loss=3.46, ppl=11.01, wps=33056.4, ups=9.1, wpb=3632.9, bsz=146.8, num_updates=72400, lr=0.000117525, gnorm=1.856, loss_scale=4, train_wall=11, gb_free=19.5, wall=2348
2022-12-09 15:37:26 | INFO | train_inner | epoch 066:    878 / 1102 loss=7.304, nll_loss=3.55, ppl=11.71, wps=33058.1, ups=9.15, wpb=3613, bsz=147, num_updates=72500, lr=0.000117444, gnorm=1.929, loss_scale=4, train_wall=11, gb_free=19.6, wall=2359
2022-12-09 15:37:37 | INFO | train_inner | epoch 066:    978 / 1102 loss=7.226, nll_loss=3.462, ppl=11.02, wps=32838.9, ups=9.12, wpb=3599.4, bsz=139.4, num_updates=72600, lr=0.000117363, gnorm=1.855, loss_scale=4, train_wall=11, gb_free=19.7, wall=2370
2022-12-09 15:37:48 | INFO | train_inner | epoch 066:   1078 / 1102 loss=7.249, nll_loss=3.471, ppl=11.09, wps=32517.1, ups=9.14, wpb=3558.2, bsz=134.2, num_updates=72700, lr=0.000117282, gnorm=1.908, loss_scale=4, train_wall=11, gb_free=19.1, wall=2381
2022-12-09 15:37:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:38:32 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.702 | nll_loss 2.161 | ppl 4.47 | bleu 36.57 | wps 4383.1 | wpb 2835.3 | bsz 115.6 | num_updates 72724 | best_bleu 36.74
2022-12-09 15:38:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 72724 updates
2022-12-09 15:38:33 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint66.pt
2022-12-09 15:38:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint66.pt (epoch 66 @ 72724 updates, score 36.57) (writing took 1.3243415532633662 seconds)
2022-12-09 15:38:33 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-12-09 15:38:33 | INFO | train | epoch 066 | loss 7.225 | nll_loss 3.46 | ppl 11.01 | wps 24242.7 | ups 6.76 | wpb 3583.6 | bsz 145.4 | num_updates 72724 | lr 0.000117263 | gnorm 1.876 | loss_scale 4 | train_wall 117 | gb_free 20 | wall 2426
2022-12-09 15:38:33 | INFO | fairseq.trainer | begin training epoch 67
2022-12-09 15:38:42 | INFO | train_inner | epoch 067:     76 / 1102 loss=7.284, nll_loss=3.494, ppl=11.27, wps=6550.8, ups=1.86, wpb=3515.7, bsz=123, num_updates=72800, lr=0.000117202, gnorm=1.975, loss_scale=4, train_wall=11, gb_free=19.3, wall=2434
2022-12-09 15:38:53 | INFO | train_inner | epoch 067:    176 / 1102 loss=7.219, nll_loss=3.452, ppl=10.95, wps=32987, ups=9.18, wpb=3592.8, bsz=141.7, num_updates=72900, lr=0.000117121, gnorm=1.858, loss_scale=4, train_wall=11, gb_free=19.4, wall=2445
2022-12-09 15:39:04 | INFO | train_inner | epoch 067:    276 / 1102 loss=7.173, nll_loss=3.397, ppl=10.53, wps=33186.3, ups=9.31, wpb=3566.1, bsz=144.2, num_updates=73000, lr=0.000117041, gnorm=2.081, loss_scale=4, train_wall=11, gb_free=19.3, wall=2456
2022-12-09 15:39:14 | INFO | train_inner | epoch 067:    376 / 1102 loss=7.19, nll_loss=3.421, ppl=10.71, wps=32943.2, ups=9.31, wpb=3537.9, bsz=144.3, num_updates=73100, lr=0.000116961, gnorm=1.959, loss_scale=4, train_wall=10, gb_free=19.8, wall=2467
2022-12-09 15:39:25 | INFO | train_inner | epoch 067:    476 / 1102 loss=7.275, nll_loss=3.493, ppl=11.26, wps=32990.8, ups=9.18, wpb=3593.7, bsz=128.4, num_updates=73200, lr=0.000116881, gnorm=1.921, loss_scale=4, train_wall=11, gb_free=19.4, wall=2478
2022-12-09 15:39:36 | INFO | train_inner | epoch 067:    576 / 1102 loss=7.137, nll_loss=3.365, ppl=10.3, wps=32235.1, ups=9.13, wpb=3529, bsz=153.2, num_updates=73300, lr=0.000116801, gnorm=1.92, loss_scale=4, train_wall=11, gb_free=19.8, wall=2489
2022-12-09 15:39:47 | INFO | train_inner | epoch 067:    676 / 1102 loss=7.273, nll_loss=3.512, ppl=11.41, wps=33203.7, ups=9.13, wpb=3637.4, bsz=141.3, num_updates=73400, lr=0.000116722, gnorm=1.811, loss_scale=4, train_wall=11, gb_free=19.3, wall=2500
2022-12-09 15:39:58 | INFO | train_inner | epoch 067:    776 / 1102 loss=7.223, nll_loss=3.462, ppl=11.02, wps=33281.4, ups=9.23, wpb=3605.9, bsz=150.4, num_updates=73500, lr=0.000116642, gnorm=1.849, loss_scale=4, train_wall=11, gb_free=19.9, wall=2510
2022-12-09 15:40:09 | INFO | train_inner | epoch 067:    876 / 1102 loss=7.246, nll_loss=3.493, ppl=11.26, wps=33053.4, ups=9.18, wpb=3600.7, bsz=152.7, num_updates=73600, lr=0.000116563, gnorm=1.886, loss_scale=4, train_wall=11, gb_free=19.5, wall=2521
2022-12-09 15:40:20 | INFO | train_inner | epoch 067:    976 / 1102 loss=7.195, nll_loss=3.44, ppl=10.85, wps=33167, ups=9.08, wpb=3651.8, bsz=161.3, num_updates=73700, lr=0.000116484, gnorm=1.909, loss_scale=4, train_wall=11, gb_free=19.2, wall=2532
2022-12-09 15:40:31 | INFO | train_inner | epoch 067:   1076 / 1102 loss=7.243, nll_loss=3.481, ppl=11.17, wps=32611.6, ups=9.21, wpb=3539.6, bsz=145.1, num_updates=73800, lr=0.000116405, gnorm=1.874, loss_scale=4, train_wall=11, gb_free=19.7, wall=2543
2022-12-09 15:40:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:41:19 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.699 | nll_loss 2.153 | ppl 4.45 | bleu 36.6 | wps 4024.8 | wpb 2835.3 | bsz 115.6 | num_updates 73826 | best_bleu 36.74
2022-12-09 15:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 73826 updates
2022-12-09 15:41:19 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint67.pt
2022-12-09 15:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint67.pt (epoch 67 @ 73826 updates, score 36.6) (writing took 1.2320586433634162 seconds)
2022-12-09 15:41:20 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-12-09 15:41:20 | INFO | train | epoch 067 | loss 7.218 | nll_loss 3.451 | ppl 10.93 | wps 23739.8 | ups 6.62 | wpb 3583.6 | bsz 145.4 | num_updates 73826 | lr 0.000116385 | gnorm 1.906 | loss_scale 4 | train_wall 117 | gb_free 19.6 | wall 2592
2022-12-09 15:41:20 | INFO | fairseq.trainer | begin training epoch 68
2022-12-09 15:41:28 | INFO | train_inner | epoch 068:     74 / 1102 loss=7.168, nll_loss=3.396, ppl=10.53, wps=6283.7, ups=1.74, wpb=3612.4, bsz=139.1, num_updates=73900, lr=0.000116326, gnorm=1.839, loss_scale=4, train_wall=11, gb_free=19.4, wall=2601
2022-12-09 15:41:39 | INFO | train_inner | epoch 068:    174 / 1102 loss=7.128, nll_loss=3.352, ppl=10.21, wps=32856.5, ups=9.17, wpb=3582.9, bsz=150.2, num_updates=74000, lr=0.000116248, gnorm=1.944, loss_scale=4, train_wall=11, gb_free=19.5, wall=2612
2022-12-09 15:41:50 | INFO | train_inner | epoch 068:    274 / 1102 loss=7.154, nll_loss=3.386, ppl=10.45, wps=33180.1, ups=9.26, wpb=3581.7, bsz=147.9, num_updates=74100, lr=0.000116169, gnorm=1.842, loss_scale=4, train_wall=11, gb_free=20, wall=2622
2022-12-09 15:42:01 | INFO | train_inner | epoch 068:    374 / 1102 loss=7.218, nll_loss=3.447, ppl=10.9, wps=33122.1, ups=9.23, wpb=3588.9, bsz=139.4, num_updates=74200, lr=0.000116091, gnorm=1.861, loss_scale=4, train_wall=11, gb_free=19.2, wall=2633
2022-12-09 15:42:12 | INFO | train_inner | epoch 068:    474 / 1102 loss=7.279, nll_loss=3.519, ppl=11.46, wps=32870.9, ups=9.22, wpb=3567.1, bsz=140.6, num_updates=74300, lr=0.000116013, gnorm=1.908, loss_scale=4, train_wall=11, gb_free=19.5, wall=2644
2022-12-09 15:42:23 | INFO | train_inner | epoch 068:    574 / 1102 loss=7.228, nll_loss=3.457, ppl=10.98, wps=32198.3, ups=9.1, wpb=3537.1, bsz=149.7, num_updates=74400, lr=0.000115935, gnorm=1.915, loss_scale=4, train_wall=11, gb_free=19.2, wall=2655
2022-12-09 15:42:34 | INFO | train_inner | epoch 068:    674 / 1102 loss=7.181, nll_loss=3.429, ppl=10.77, wps=32850.7, ups=9.05, wpb=3629.5, bsz=159.6, num_updates=74500, lr=0.000115857, gnorm=1.799, loss_scale=4, train_wall=11, gb_free=20.1, wall=2666
2022-12-09 15:42:45 | INFO | train_inner | epoch 068:    774 / 1102 loss=7.194, nll_loss=3.436, ppl=10.83, wps=32637.3, ups=9.17, wpb=3560.4, bsz=152.9, num_updates=74600, lr=0.000115779, gnorm=1.877, loss_scale=4, train_wall=11, gb_free=19.5, wall=2677
2022-12-09 15:42:56 | INFO | train_inner | epoch 068:    874 / 1102 loss=7.248, nll_loss=3.471, ppl=11.09, wps=32280.5, ups=9.08, wpb=3556, bsz=132.6, num_updates=74700, lr=0.000115702, gnorm=1.9, loss_scale=4, train_wall=11, gb_free=19.6, wall=2688
2022-12-09 15:43:06 | INFO | train_inner | epoch 068:    974 / 1102 loss=7.217, nll_loss=3.439, ppl=10.85, wps=32900.9, ups=9.25, wpb=3558.2, bsz=142.2, num_updates=74800, lr=0.000115624, gnorm=1.909, loss_scale=4, train_wall=11, gb_free=19.4, wall=2699
2022-12-09 15:43:17 | INFO | train_inner | epoch 068:   1074 / 1102 loss=7.239, nll_loss=3.475, ppl=11.12, wps=32876.1, ups=9.12, wpb=3604.2, bsz=150.1, num_updates=74900, lr=0.000115547, gnorm=1.914, loss_scale=4, train_wall=11, gb_free=19.4, wall=2710
2022-12-09 15:43:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:44:03 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.699 | nll_loss 2.157 | ppl 4.46 | bleu 36.55 | wps 4203.8 | wpb 2835.3 | bsz 115.6 | num_updates 74928 | best_bleu 36.74
2022-12-09 15:44:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 74928 updates
2022-12-09 15:44:04 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint68.pt
2022-12-09 15:44:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint68.pt (epoch 68 @ 74928 updates, score 36.55) (writing took 1.381588171236217 seconds)
2022-12-09 15:44:05 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-12-09 15:44:05 | INFO | train | epoch 068 | loss 7.209 | nll_loss 3.441 | ppl 10.86 | wps 23929.1 | ups 6.68 | wpb 3583.6 | bsz 145.4 | num_updates 74928 | lr 0.000115526 | gnorm 1.888 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 2757
2022-12-09 15:44:05 | INFO | fairseq.trainer | begin training epoch 69
2022-12-09 15:44:13 | INFO | train_inner | epoch 069:     72 / 1102 loss=7.172, nll_loss=3.405, ppl=10.59, wps=6618.2, ups=1.8, wpb=3681, bsz=141.2, num_updates=75000, lr=0.00011547, gnorm=1.87, loss_scale=4, train_wall=11, gb_free=19.5, wall=2765
2022-12-09 15:44:24 | INFO | train_inner | epoch 069:    172 / 1102 loss=7.236, nll_loss=3.447, ppl=10.9, wps=32717.7, ups=9.25, wpb=3535.8, bsz=127, num_updates=75100, lr=0.000115393, gnorm=1.943, loss_scale=4, train_wall=11, gb_free=19.7, wall=2776
2022-12-09 15:44:35 | INFO | train_inner | epoch 069:    272 / 1102 loss=7.208, nll_loss=3.439, ppl=10.84, wps=33428, ups=9.19, wpb=3637.9, bsz=143.4, num_updates=75200, lr=0.000115316, gnorm=1.886, loss_scale=4, train_wall=11, gb_free=19.3, wall=2787
2022-12-09 15:44:46 | INFO | train_inner | epoch 069:    372 / 1102 loss=7.198, nll_loss=3.421, ppl=10.71, wps=32774.3, ups=9.14, wpb=3586.9, bsz=139.6, num_updates=75300, lr=0.00011524, gnorm=1.901, loss_scale=4, train_wall=11, gb_free=19.4, wall=2798
2022-12-09 15:44:57 | INFO | train_inner | epoch 069:    472 / 1102 loss=7.192, nll_loss=3.444, ppl=10.88, wps=32839.9, ups=9.01, wpb=3645.2, bsz=158.3, num_updates=75400, lr=0.000115163, gnorm=1.881, loss_scale=4, train_wall=11, gb_free=19.7, wall=2809
2022-12-09 15:45:08 | INFO | train_inner | epoch 069:    572 / 1102 loss=7.137, nll_loss=3.367, ppl=10.32, wps=32917.5, ups=9.22, wpb=3569.3, bsz=153, num_updates=75500, lr=0.000115087, gnorm=1.897, loss_scale=4, train_wall=11, gb_free=19.5, wall=2820
2022-12-09 15:45:19 | INFO | train_inner | epoch 069:    672 / 1102 loss=7.212, nll_loss=3.46, ppl=11, wps=32497.6, ups=9.18, wpb=3541.3, bsz=160.7, num_updates=75600, lr=0.000115011, gnorm=1.952, loss_scale=4, train_wall=11, gb_free=19.8, wall=2831
2022-12-09 15:45:29 | INFO | train_inner | epoch 069:    772 / 1102 loss=7.188, nll_loss=3.424, ppl=10.73, wps=32697.9, ups=9.15, wpb=3572.3, bsz=153.6, num_updates=75700, lr=0.000114935, gnorm=1.853, loss_scale=4, train_wall=11, gb_free=19.4, wall=2842
2022-12-09 15:45:40 | INFO | train_inner | epoch 069:    872 / 1102 loss=7.18, nll_loss=3.413, ppl=10.65, wps=32635.1, ups=9.17, wpb=3560.4, bsz=153.6, num_updates=75800, lr=0.000114859, gnorm=1.894, loss_scale=4, train_wall=11, gb_free=19.3, wall=2853
2022-12-09 15:45:51 | INFO | train_inner | epoch 069:    972 / 1102 loss=7.232, nll_loss=3.463, ppl=11.03, wps=32925, ups=9.13, wpb=3607.5, bsz=140, num_updates=75900, lr=0.000114783, gnorm=1.976, loss_scale=4, train_wall=11, gb_free=19.4, wall=2864
2022-12-09 15:46:02 | INFO | train_inner | epoch 069:   1072 / 1102 loss=7.261, nll_loss=3.471, ppl=11.09, wps=32330.6, ups=9.18, wpb=3521.9, bsz=130.7, num_updates=76000, lr=0.000114708, gnorm=2.024, loss_scale=4, train_wall=11, gb_free=19.7, wall=2875
2022-12-09 15:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:46:47 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.691 | nll_loss 2.148 | ppl 4.43 | bleu 36.84 | wps 4405.2 | wpb 2835.3 | bsz 115.6 | num_updates 76030 | best_bleu 36.84
2022-12-09 15:46:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 76030 updates
2022-12-09 15:46:47 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint69.pt
2022-12-09 15:46:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint69.pt (epoch 69 @ 76030 updates, score 36.84) (writing took 1.606301219202578 seconds)
2022-12-09 15:46:48 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-12-09 15:46:48 | INFO | train | epoch 069 | loss 7.202 | nll_loss 3.432 | ppl 10.79 | wps 24185.4 | ups 6.75 | wpb 3583.6 | bsz 145.4 | num_updates 76030 | lr 0.000114685 | gnorm 1.913 | loss_scale 4 | train_wall 118 | gb_free 19.2 | wall 2920
2022-12-09 15:46:48 | INFO | fairseq.trainer | begin training epoch 70
2022-12-09 15:46:56 | INFO | train_inner | epoch 070:     70 / 1102 loss=7.165, nll_loss=3.394, ppl=10.51, wps=6702, ups=1.86, wpb=3608.1, bsz=145.7, num_updates=76100, lr=0.000114632, gnorm=1.867, loss_scale=4, train_wall=11, gb_free=19.5, wall=2928
2022-12-09 15:47:07 | INFO | train_inner | epoch 070:    170 / 1102 loss=7.166, nll_loss=3.401, ppl=10.56, wps=33170.4, ups=9.18, wpb=3615.1, bsz=152.7, num_updates=76200, lr=0.000114557, gnorm=1.925, loss_scale=4, train_wall=11, gb_free=19.3, wall=2939
2022-12-09 15:47:18 | INFO | train_inner | epoch 070:    270 / 1102 loss=7.128, nll_loss=3.353, ppl=10.22, wps=32860.2, ups=9.15, wpb=3590.5, bsz=147.7, num_updates=76300, lr=0.000114482, gnorm=1.851, loss_scale=4, train_wall=11, gb_free=19.4, wall=2950
2022-12-09 15:47:29 | INFO | train_inner | epoch 070:    370 / 1102 loss=7.177, nll_loss=3.419, ppl=10.69, wps=33136.6, ups=9.27, wpb=3573, bsz=155.4, num_updates=76400, lr=0.000114407, gnorm=1.851, loss_scale=4, train_wall=11, gb_free=19.4, wall=2961
2022-12-09 15:47:40 | INFO | train_inner | epoch 070:    470 / 1102 loss=7.124, nll_loss=3.356, ppl=10.24, wps=32955.6, ups=9.15, wpb=3602.7, bsz=146.5, num_updates=76500, lr=0.000114332, gnorm=1.852, loss_scale=4, train_wall=11, gb_free=19.6, wall=2972
2022-12-09 15:47:50 | INFO | train_inner | epoch 070:    570 / 1102 loss=7.147, nll_loss=3.385, ppl=10.45, wps=32987.8, ups=9.17, wpb=3598.7, bsz=159.8, num_updates=76600, lr=0.000114258, gnorm=1.852, loss_scale=4, train_wall=11, gb_free=19.5, wall=2983
2022-12-09 15:48:01 | INFO | train_inner | epoch 070:    670 / 1102 loss=7.314, nll_loss=3.523, ppl=11.49, wps=32182.2, ups=9.19, wpb=3503.3, bsz=124.2, num_updates=76700, lr=0.000114183, gnorm=1.982, loss_scale=4, train_wall=11, gb_free=19.3, wall=2994
2022-12-09 15:48:12 | INFO | train_inner | epoch 070:    770 / 1102 loss=7.323, nll_loss=3.529, ppl=11.54, wps=32997.2, ups=9.22, wpb=3579.8, bsz=124.8, num_updates=76800, lr=0.000114109, gnorm=1.962, loss_scale=4, train_wall=11, gb_free=19.4, wall=3005
2022-12-09 15:48:23 | INFO | train_inner | epoch 070:    870 / 1102 loss=7.219, nll_loss=3.458, ppl=10.99, wps=33216.6, ups=9.21, wpb=3607.4, bsz=147.4, num_updates=76900, lr=0.000114035, gnorm=1.897, loss_scale=4, train_wall=11, gb_free=19.6, wall=3015
2022-12-09 15:48:34 | INFO | train_inner | epoch 070:    970 / 1102 loss=7.268, nll_loss=3.499, ppl=11.31, wps=33544.7, ups=9.23, wpb=3632.5, bsz=134.1, num_updates=77000, lr=0.000113961, gnorm=1.91, loss_scale=4, train_wall=11, gb_free=19.4, wall=3026
2022-12-09 15:48:45 | INFO | train_inner | epoch 070:   1070 / 1102 loss=7.129, nll_loss=3.359, ppl=10.26, wps=32399.7, ups=9.09, wpb=3565.4, bsz=154.2, num_updates=77100, lr=0.000113887, gnorm=1.936, loss_scale=4, train_wall=11, gb_free=19.8, wall=3037
2022-12-09 15:48:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:49:27 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.695 | nll_loss 2.151 | ppl 4.44 | bleu 36.77 | wps 4626.9 | wpb 2835.3 | bsz 115.6 | num_updates 77132 | best_bleu 36.84
2022-12-09 15:49:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 77132 updates
2022-12-09 15:49:28 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint70.pt
2022-12-09 15:49:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint70.pt (epoch 70 @ 77132 updates, score 36.77) (writing took 1.1556793004274368 seconds)
2022-12-09 15:49:29 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-12-09 15:49:29 | INFO | train | epoch 070 | loss 7.193 | nll_loss 3.422 | ppl 10.72 | wps 24598.2 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 77132 | lr 0.000113863 | gnorm 1.9 | loss_scale 4 | train_wall 117 | gb_free 19.5 | wall 3081
2022-12-09 15:49:29 | INFO | fairseq.trainer | begin training epoch 71
2022-12-09 15:49:36 | INFO | train_inner | epoch 071:     68 / 1102 loss=7.167, nll_loss=3.397, ppl=10.54, wps=6871.2, ups=1.95, wpb=3531.1, bsz=149.8, num_updates=77200, lr=0.000113813, gnorm=1.943, loss_scale=4, train_wall=11, gb_free=19.9, wall=3089
2022-12-09 15:49:47 | INFO | train_inner | epoch 071:    168 / 1102 loss=7.183, nll_loss=3.396, ppl=10.52, wps=32392.8, ups=9.18, wpb=3528.5, bsz=142.8, num_updates=77300, lr=0.000113739, gnorm=2.013, loss_scale=4, train_wall=11, gb_free=19.4, wall=3100
2022-12-09 15:49:58 | INFO | train_inner | epoch 071:    268 / 1102 loss=7.124, nll_loss=3.343, ppl=10.15, wps=33058.4, ups=9.27, wpb=3565.6, bsz=137.6, num_updates=77400, lr=0.000113666, gnorm=1.821, loss_scale=4, train_wall=11, gb_free=19.8, wall=3110
2022-12-09 15:50:09 | INFO | train_inner | epoch 071:    368 / 1102 loss=7.053, nll_loss=3.29, ppl=9.78, wps=32946.2, ups=9.11, wpb=3618.1, bsz=164.5, num_updates=77500, lr=0.000113592, gnorm=1.833, loss_scale=4, train_wall=11, gb_free=19.4, wall=3121
2022-12-09 15:50:20 | INFO | train_inner | epoch 071:    468 / 1102 loss=7.17, nll_loss=3.395, ppl=10.52, wps=32775.8, ups=9.08, wpb=3608.5, bsz=146.2, num_updates=77600, lr=0.000113519, gnorm=1.865, loss_scale=4, train_wall=11, gb_free=20, wall=3132
2022-12-09 15:50:31 | INFO | train_inner | epoch 071:    568 / 1102 loss=7.219, nll_loss=3.435, ppl=10.82, wps=32274.9, ups=9.03, wpb=3572.9, bsz=138.2, num_updates=77700, lr=0.000113446, gnorm=2.012, loss_scale=4, train_wall=11, gb_free=19.6, wall=3143
2022-12-09 15:50:42 | INFO | train_inner | epoch 071:    668 / 1102 loss=7.283, nll_loss=3.509, ppl=11.39, wps=32941.2, ups=9.18, wpb=3587.8, bsz=132.8, num_updates=77800, lr=0.000113373, gnorm=1.932, loss_scale=4, train_wall=11, gb_free=19.7, wall=3154
2022-12-09 15:50:53 | INFO | train_inner | epoch 071:    768 / 1102 loss=7.22, nll_loss=3.45, ppl=10.93, wps=32272.2, ups=9.05, wpb=3565.2, bsz=151.6, num_updates=77900, lr=0.0001133, gnorm=1.925, loss_scale=4, train_wall=11, gb_free=19.5, wall=3165
2022-12-09 15:51:04 | INFO | train_inner | epoch 071:    868 / 1102 loss=7.175, nll_loss=3.4, ppl=10.55, wps=32935.5, ups=9.08, wpb=3627, bsz=142.3, num_updates=78000, lr=0.000113228, gnorm=1.934, loss_scale=4, train_wall=11, gb_free=19.5, wall=3176
2022-12-09 15:51:15 | INFO | train_inner | epoch 071:    968 / 1102 loss=7.187, nll_loss=3.434, ppl=10.81, wps=33058, ups=9.1, wpb=3631, bsz=159.7, num_updates=78100, lr=0.000113155, gnorm=1.856, loss_scale=4, train_wall=11, gb_free=19.8, wall=3187
2022-12-09 15:51:26 | INFO | train_inner | epoch 071:   1068 / 1102 loss=7.273, nll_loss=3.502, ppl=11.33, wps=32697.8, ups=9.16, wpb=3569.1, bsz=136.1, num_updates=78200, lr=0.000113083, gnorm=1.899, loss_scale=4, train_wall=11, gb_free=19.4, wall=3198
2022-12-09 15:51:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:52:10 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.688 | nll_loss 2.147 | ppl 4.43 | bleu 36.73 | wps 4454 | wpb 2835.3 | bsz 115.6 | num_updates 78234 | best_bleu 36.84
2022-12-09 15:52:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 78234 updates
2022-12-09 15:52:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint71.pt
2022-12-09 15:52:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint71.pt (epoch 71 @ 78234 updates, score 36.73) (writing took 1.0641890252009034 seconds)
2022-12-09 15:52:11 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-12-09 15:52:11 | INFO | train | epoch 071 | loss 7.186 | nll_loss 3.413 | ppl 10.65 | wps 24285.5 | ups 6.78 | wpb 3583.6 | bsz 145.4 | num_updates 78234 | lr 0.000113058 | gnorm 1.922 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 3244
2022-12-09 15:52:11 | INFO | fairseq.trainer | begin training epoch 72
2022-12-09 15:52:19 | INFO | train_inner | epoch 072:     66 / 1102 loss=7.09, nll_loss=3.319, ppl=9.98, wps=6724, ups=1.89, wpb=3557.5, bsz=146.8, num_updates=78300, lr=0.000113011, gnorm=1.937, loss_scale=4, train_wall=11, gb_free=19.3, wall=3251
2022-12-09 15:52:30 | INFO | train_inner | epoch 072:    166 / 1102 loss=7.116, nll_loss=3.342, ppl=10.14, wps=32713.4, ups=9.11, wpb=3592.9, bsz=147.4, num_updates=78400, lr=0.000112938, gnorm=1.97, loss_scale=4, train_wall=11, gb_free=19.3, wall=3262
2022-12-09 15:52:41 | INFO | train_inner | epoch 072:    266 / 1102 loss=7.148, nll_loss=3.38, ppl=10.41, wps=33097.3, ups=9.14, wpb=3620.1, bsz=150.9, num_updates=78500, lr=0.000112867, gnorm=1.91, loss_scale=4, train_wall=11, gb_free=19.6, wall=3273
2022-12-09 15:52:52 | INFO | train_inner | epoch 072:    366 / 1102 loss=7.081, nll_loss=3.3, ppl=9.85, wps=32750.5, ups=9.25, wpb=3540.2, bsz=152, num_updates=78600, lr=0.000112795, gnorm=1.883, loss_scale=4, train_wall=11, gb_free=19.4, wall=3284
2022-12-09 15:53:02 | INFO | train_inner | epoch 072:    466 / 1102 loss=7.185, nll_loss=3.403, ppl=10.58, wps=32952.9, ups=9.2, wpb=3581.6, bsz=142.5, num_updates=78700, lr=0.000112723, gnorm=1.919, loss_scale=4, train_wall=11, gb_free=19.7, wall=3295
2022-12-09 15:53:13 | INFO | train_inner | epoch 072:    566 / 1102 loss=7.213, nll_loss=3.449, ppl=10.92, wps=33251.9, ups=9.06, wpb=3669.5, bsz=142.6, num_updates=78800, lr=0.000112651, gnorm=1.828, loss_scale=4, train_wall=11, gb_free=19.4, wall=3306
2022-12-09 15:53:24 | INFO | train_inner | epoch 072:    666 / 1102 loss=7.139, nll_loss=3.38, ppl=10.41, wps=32766.7, ups=9.09, wpb=3605.5, bsz=157.1, num_updates=78900, lr=0.00011258, gnorm=1.878, loss_scale=4, train_wall=11, gb_free=19.4, wall=3317
2022-12-09 15:53:36 | INFO | train_inner | epoch 072:    766 / 1102 loss=7.237, nll_loss=3.453, ppl=10.95, wps=31905.9, ups=9.03, wpb=3533.9, bsz=134.3, num_updates=79000, lr=0.000112509, gnorm=1.936, loss_scale=4, train_wall=11, gb_free=19.4, wall=3328
2022-12-09 15:53:47 | INFO | train_inner | epoch 072:    866 / 1102 loss=7.204, nll_loss=3.442, ppl=10.86, wps=32599.6, ups=9, wpb=3622.4, bsz=158.4, num_updates=79100, lr=0.000112438, gnorm=1.914, loss_scale=4, train_wall=11, gb_free=19.2, wall=3339
2022-12-09 15:53:58 | INFO | train_inner | epoch 072:    966 / 1102 loss=7.17, nll_loss=3.388, ppl=10.47, wps=32325.7, ups=9.09, wpb=3556.8, bsz=139.4, num_updates=79200, lr=0.000112367, gnorm=1.933, loss_scale=4, train_wall=11, gb_free=19.5, wall=3350
2022-12-09 15:54:09 | INFO | train_inner | epoch 072:   1066 / 1102 loss=7.346, nll_loss=3.562, ppl=11.81, wps=32139.1, ups=9.17, wpb=3503.2, bsz=127.9, num_updates=79300, lr=0.000112296, gnorm=2.145, loss_scale=4, train_wall=11, gb_free=19.8, wall=3361
2022-12-09 15:54:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:54:55 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 3.689 | nll_loss 2.147 | ppl 4.43 | bleu 36.73 | wps 4281.1 | wpb 2835.3 | bsz 115.6 | num_updates 79336 | best_bleu 36.84
2022-12-09 15:54:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 79336 updates
2022-12-09 15:54:56 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint72.pt
2022-12-09 15:54:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint72.pt (epoch 72 @ 79336 updates, score 36.73) (writing took 1.472274805419147 seconds)
2022-12-09 15:54:56 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-12-09 15:54:56 | INFO | train | epoch 072 | loss 7.178 | nll_loss 3.404 | ppl 10.59 | wps 23949.4 | ups 6.68 | wpb 3583.6 | bsz 145.4 | num_updates 79336 | lr 0.00011227 | gnorm 1.927 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 3409
2022-12-09 15:54:56 | INFO | fairseq.trainer | begin training epoch 73
2022-12-09 15:55:03 | INFO | train_inner | epoch 073:     64 / 1102 loss=7.217, nll_loss=3.448, ppl=10.91, wps=6581.1, ups=1.82, wpb=3616.2, bsz=144.1, num_updates=79400, lr=0.000112225, gnorm=1.937, loss_scale=4, train_wall=11, gb_free=19.4, wall=3416
2022-12-09 15:55:14 | INFO | train_inner | epoch 073:    164 / 1102 loss=7.063, nll_loss=3.278, ppl=9.7, wps=32769.2, ups=9.23, wpb=3549, bsz=143.4, num_updates=79500, lr=0.000112154, gnorm=1.937, loss_scale=4, train_wall=11, gb_free=19.3, wall=3427
2022-12-09 15:55:25 | INFO | train_inner | epoch 073:    264 / 1102 loss=7.108, nll_loss=3.324, ppl=10.01, wps=32762.4, ups=9.14, wpb=3585.2, bsz=143.4, num_updates=79600, lr=0.000112084, gnorm=2.02, loss_scale=4, train_wall=11, gb_free=19.3, wall=3438
2022-12-09 15:55:36 | INFO | train_inner | epoch 073:    364 / 1102 loss=7.15, nll_loss=3.37, ppl=10.34, wps=33079.9, ups=9.2, wpb=3595, bsz=147, num_updates=79700, lr=0.000112014, gnorm=1.915, loss_scale=4, train_wall=11, gb_free=19.6, wall=3449
2022-12-09 15:55:47 | INFO | train_inner | epoch 073:    464 / 1102 loss=7.137, nll_loss=3.358, ppl=10.25, wps=32762.9, ups=9.13, wpb=3589.1, bsz=143.8, num_updates=79800, lr=0.000111943, gnorm=1.883, loss_scale=4, train_wall=11, gb_free=19.4, wall=3459
2022-12-09 15:55:58 | INFO | train_inner | epoch 073:    564 / 1102 loss=7.185, nll_loss=3.404, ppl=10.59, wps=32739.5, ups=9.26, wpb=3537.4, bsz=146.2, num_updates=79900, lr=0.000111873, gnorm=2.052, loss_scale=4, train_wall=11, gb_free=19.3, wall=3470
2022-12-09 15:56:09 | INFO | train_inner | epoch 073:    664 / 1102 loss=7.143, nll_loss=3.381, ppl=10.42, wps=33220, ups=9.11, wpb=3647.3, bsz=152.2, num_updates=80000, lr=0.000111803, gnorm=1.83, loss_scale=4, train_wall=11, gb_free=19.9, wall=3481
2022-12-09 15:56:20 | INFO | train_inner | epoch 073:    764 / 1102 loss=7.148, nll_loss=3.387, ppl=10.46, wps=32760.6, ups=9.14, wpb=3583.7, bsz=159, num_updates=80100, lr=0.000111734, gnorm=1.866, loss_scale=4, train_wall=11, gb_free=19.5, wall=3492
2022-12-09 15:56:31 | INFO | train_inner | epoch 073:    864 / 1102 loss=7.235, nll_loss=3.454, ppl=10.96, wps=32673.4, ups=9.15, wpb=3569.1, bsz=138.6, num_updates=80200, lr=0.000111664, gnorm=2.011, loss_scale=4, train_wall=11, gb_free=19.5, wall=3503
2022-12-09 15:56:42 | INFO | train_inner | epoch 073:    964 / 1102 loss=7.283, nll_loss=3.499, ppl=11.3, wps=32238.6, ups=9.2, wpb=3505.8, bsz=135.5, num_updates=80300, lr=0.000111594, gnorm=2.028, loss_scale=4, train_wall=11, gb_free=19.6, wall=3514
2022-12-09 15:56:53 | INFO | train_inner | epoch 073:   1064 / 1102 loss=7.155, nll_loss=3.404, ppl=10.58, wps=33128.7, ups=9.1, wpb=3642.4, bsz=159.3, num_updates=80400, lr=0.000111525, gnorm=1.822, loss_scale=4, train_wall=11, gb_free=19.5, wall=3525
2022-12-09 15:56:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 15:57:40 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.685 | nll_loss 2.146 | ppl 4.42 | bleu 36.92 | wps 4179.6 | wpb 2835.3 | bsz 115.6 | num_updates 80438 | best_bleu 36.92
2022-12-09 15:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 80438 updates
2022-12-09 15:57:41 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint73.pt
2022-12-09 15:57:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint73.pt (epoch 73 @ 80438 updates, score 36.92) (writing took 1.7756228940561414 seconds)
2022-12-09 15:57:42 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-12-09 15:57:42 | INFO | train | epoch 073 | loss 7.17 | nll_loss 3.395 | ppl 10.52 | wps 23830.7 | ups 6.65 | wpb 3583.6 | bsz 145.4 | num_updates 80438 | lr 0.000111499 | gnorm 1.932 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 3574
2022-12-09 15:57:42 | INFO | fairseq.trainer | begin training epoch 74
2022-12-09 15:57:49 | INFO | train_inner | epoch 074:     62 / 1102 loss=7.215, nll_loss=3.43, ppl=10.78, wps=6359.1, ups=1.78, wpb=3581.1, bsz=126.9, num_updates=80500, lr=0.000111456, gnorm=1.933, loss_scale=4, train_wall=11, gb_free=19.5, wall=3581
2022-12-09 15:58:00 | INFO | train_inner | epoch 074:    162 / 1102 loss=7.173, nll_loss=3.381, ppl=10.42, wps=32378.5, ups=9.19, wpb=3523.8, bsz=141.7, num_updates=80600, lr=0.000111386, gnorm=1.933, loss_scale=4, train_wall=11, gb_free=19.6, wall=3592
2022-12-09 15:58:11 | INFO | train_inner | epoch 074:    262 / 1102 loss=7.086, nll_loss=3.326, ppl=10.03, wps=33267, ups=9.15, wpb=3635.7, bsz=155, num_updates=80700, lr=0.000111317, gnorm=1.839, loss_scale=4, train_wall=11, gb_free=19.8, wall=3603
2022-12-09 15:58:22 | INFO | train_inner | epoch 074:    362 / 1102 loss=7.201, nll_loss=3.408, ppl=10.61, wps=32696.8, ups=9.14, wpb=3576.7, bsz=136.2, num_updates=80800, lr=0.000111249, gnorm=1.993, loss_scale=4, train_wall=11, gb_free=19.6, wall=3614
2022-12-09 15:58:33 | INFO | train_inner | epoch 074:    462 / 1102 loss=7.158, nll_loss=3.369, ppl=10.33, wps=32700.2, ups=9.2, wpb=3554.4, bsz=143.7, num_updates=80900, lr=0.00011118, gnorm=1.987, loss_scale=4, train_wall=11, gb_free=19.7, wall=3625
2022-12-09 15:58:44 | INFO | train_inner | epoch 074:    562 / 1102 loss=7.067, nll_loss=3.305, ppl=9.88, wps=32463.2, ups=9.09, wpb=3571.9, bsz=172.2, num_updates=81000, lr=0.000111111, gnorm=1.938, loss_scale=4, train_wall=11, gb_free=19.3, wall=3636
2022-12-09 15:58:55 | INFO | train_inner | epoch 074:    662 / 1102 loss=7.13, nll_loss=3.357, ppl=10.25, wps=32925.1, ups=9.12, wpb=3611.3, bsz=154.2, num_updates=81100, lr=0.000111043, gnorm=1.898, loss_scale=4, train_wall=11, gb_free=19.7, wall=3647
2022-12-09 15:59:05 | INFO | train_inner | epoch 074:    762 / 1102 loss=7.196, nll_loss=3.404, ppl=10.59, wps=32741.3, ups=9.18, wpb=3566.1, bsz=132.2, num_updates=81200, lr=0.000110974, gnorm=2.024, loss_scale=4, train_wall=11, gb_free=19.6, wall=3658
2022-12-09 15:59:16 | INFO | train_inner | epoch 074:    862 / 1102 loss=7.156, nll_loss=3.377, ppl=10.39, wps=31903.7, ups=9.19, wpb=3472.8, bsz=149, num_updates=81300, lr=0.000110906, gnorm=1.946, loss_scale=4, train_wall=11, gb_free=19.3, wall=3669
2022-12-09 15:59:27 | INFO | train_inner | epoch 074:    962 / 1102 loss=7.192, nll_loss=3.413, ppl=10.65, wps=33174.1, ups=9.13, wpb=3634, bsz=135.6, num_updates=81400, lr=0.000110838, gnorm=1.864, loss_scale=4, train_wall=11, gb_free=19.4, wall=3680
2022-12-09 15:59:38 | INFO | train_inner | epoch 074:   1062 / 1102 loss=7.223, nll_loss=3.466, ppl=11.05, wps=33115.7, ups=9.09, wpb=3643.7, bsz=147.9, num_updates=81500, lr=0.00011077, gnorm=1.934, loss_scale=4, train_wall=11, gb_free=19.7, wall=3691
2022-12-09 15:59:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:00:25 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.687 | nll_loss 2.145 | ppl 4.42 | bleu 36.88 | wps 4312.5 | wpb 2835.3 | bsz 115.6 | num_updates 81540 | best_bleu 36.92
2022-12-09 16:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 81540 updates
2022-12-09 16:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint74.pt
2022-12-09 16:00:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint74.pt (epoch 74 @ 81540 updates, score 36.88) (writing took 1.1940598655492067 seconds)
2022-12-09 16:00:26 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-12-09 16:00:26 | INFO | train | epoch 074 | loss 7.164 | nll_loss 3.386 | ppl 10.46 | wps 24077.5 | ups 6.72 | wpb 3583.6 | bsz 145.4 | num_updates 81540 | lr 0.000110743 | gnorm 1.935 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 3738
2022-12-09 16:00:26 | INFO | fairseq.trainer | begin training epoch 75
2022-12-09 16:00:33 | INFO | train_inner | epoch 075:     60 / 1102 loss=7.206, nll_loss=3.436, ppl=10.83, wps=6741.3, ups=1.83, wpb=3681.9, bsz=140.2, num_updates=81600, lr=0.000110702, gnorm=1.878, loss_scale=4, train_wall=11, gb_free=19.3, wall=3745
2022-12-09 16:00:44 | INFO | train_inner | epoch 075:    160 / 1102 loss=7.129, nll_loss=3.343, ppl=10.15, wps=32195.9, ups=9.18, wpb=3505.8, bsz=137.7, num_updates=81700, lr=0.000110634, gnorm=1.973, loss_scale=4, train_wall=11, gb_free=19.3, wall=3756
2022-12-09 16:00:55 | INFO | train_inner | epoch 075:    260 / 1102 loss=7.094, nll_loss=3.322, ppl=10, wps=32671.3, ups=8.99, wpb=3632.9, bsz=154.1, num_updates=81800, lr=0.000110566, gnorm=1.896, loss_scale=4, train_wall=11, gb_free=19.7, wall=3767
2022-12-09 16:01:06 | INFO | train_inner | epoch 075:    360 / 1102 loss=7.165, nll_loss=3.379, ppl=10.4, wps=32182.6, ups=9.23, wpb=3488.5, bsz=146.6, num_updates=81900, lr=0.000110499, gnorm=1.998, loss_scale=4, train_wall=11, gb_free=19.7, wall=3778
2022-12-09 16:01:17 | INFO | train_inner | epoch 075:    460 / 1102 loss=7.17, nll_loss=3.389, ppl=10.47, wps=32963, ups=9.13, wpb=3608.8, bsz=142.5, num_updates=82000, lr=0.000110432, gnorm=1.92, loss_scale=4, train_wall=11, gb_free=19.6, wall=3789
2022-12-09 16:01:28 | INFO | train_inner | epoch 075:    560 / 1102 loss=7.125, nll_loss=3.353, ppl=10.22, wps=33102.6, ups=9.13, wpb=3624.2, bsz=147.4, num_updates=82100, lr=0.000110364, gnorm=1.864, loss_scale=4, train_wall=11, gb_free=19.3, wall=3800
2022-12-09 16:01:39 | INFO | train_inner | epoch 075:    660 / 1102 loss=7.164, nll_loss=3.387, ppl=10.46, wps=32633.6, ups=9.07, wpb=3598.6, bsz=141.3, num_updates=82200, lr=0.000110297, gnorm=1.854, loss_scale=4, train_wall=11, gb_free=19.4, wall=3811
2022-12-09 16:01:50 | INFO | train_inner | epoch 075:    760 / 1102 loss=7.166, nll_loss=3.399, ppl=10.55, wps=32387.5, ups=9.08, wpb=3565, bsz=153.7, num_updates=82300, lr=0.00011023, gnorm=1.955, loss_scale=4, train_wall=11, gb_free=19.4, wall=3822
2022-12-09 16:02:01 | INFO | train_inner | epoch 075:    860 / 1102 loss=7.198, nll_loss=3.414, ppl=10.66, wps=32411.9, ups=9.05, wpb=3580.3, bsz=148.6, num_updates=82400, lr=0.000110163, gnorm=1.945, loss_scale=4, train_wall=11, gb_free=19.7, wall=3833
2022-12-09 16:02:12 | INFO | train_inner | epoch 075:    960 / 1102 loss=7.234, nll_loss=3.451, ppl=10.94, wps=32902.9, ups=9.22, wpb=3567.6, bsz=133.9, num_updates=82500, lr=0.000110096, gnorm=1.926, loss_scale=4, train_wall=11, gb_free=19.4, wall=3844
2022-12-09 16:02:23 | INFO | train_inner | epoch 075:   1060 / 1102 loss=7.123, nll_loss=3.342, ppl=10.14, wps=32495.6, ups=9.1, wpb=3571.3, bsz=144.6, num_updates=82600, lr=0.00011003, gnorm=2.045, loss_scale=4, train_wall=11, gb_free=19.4, wall=3855
2022-12-09 16:02:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:03:10 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 3.685 | nll_loss 2.146 | ppl 4.43 | bleu 36.89 | wps 4247.3 | wpb 2835.3 | bsz 115.6 | num_updates 82642 | best_bleu 36.92
2022-12-09 16:03:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 82642 updates
2022-12-09 16:03:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint75.pt
2022-12-09 16:03:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint75.pt (epoch 75 @ 82642 updates, score 36.89) (writing took 1.1313125379383564 seconds)
2022-12-09 16:03:11 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-12-09 16:03:11 | INFO | train | epoch 075 | loss 7.157 | nll_loss 3.379 | ppl 10.4 | wps 23926.4 | ups 6.68 | wpb 3583.6 | bsz 145.4 | num_updates 82642 | lr 0.000110002 | gnorm 1.931 | loss_scale 4 | train_wall 118 | gb_free 19.9 | wall 3903
2022-12-09 16:03:11 | INFO | fairseq.trainer | begin training epoch 76
2022-12-09 16:03:18 | INFO | train_inner | epoch 076:     58 / 1102 loss=7.13, nll_loss=3.361, ppl=10.27, wps=6582.3, ups=1.81, wpb=3629, bsz=152.5, num_updates=82700, lr=0.000109963, gnorm=1.867, loss_scale=4, train_wall=11, gb_free=19.5, wall=3910
2022-12-09 16:03:29 | INFO | train_inner | epoch 076:    158 / 1102 loss=7.057, nll_loss=3.28, ppl=9.71, wps=32893.3, ups=9.11, wpb=3612.6, bsz=151.1, num_updates=82800, lr=0.000109897, gnorm=1.871, loss_scale=4, train_wall=11, gb_free=19.7, wall=3921
2022-12-09 16:03:40 | INFO | train_inner | epoch 076:    258 / 1102 loss=7.155, nll_loss=3.354, ppl=10.22, wps=33076.4, ups=9.2, wpb=3594.6, bsz=131.7, num_updates=82900, lr=0.00010983, gnorm=1.944, loss_scale=4, train_wall=11, gb_free=19.6, wall=3932
2022-12-09 16:03:51 | INFO | train_inner | epoch 076:    358 / 1102 loss=7.051, nll_loss=3.278, ppl=9.7, wps=32861.9, ups=9.1, wpb=3612.4, bsz=160.7, num_updates=83000, lr=0.000109764, gnorm=1.865, loss_scale=4, train_wall=11, gb_free=19.7, wall=3943
2022-12-09 16:04:01 | INFO | train_inner | epoch 076:    458 / 1102 loss=7.16, nll_loss=3.382, ppl=10.43, wps=32939.4, ups=9.15, wpb=3600.4, bsz=148.6, num_updates=83100, lr=0.000109698, gnorm=1.865, loss_scale=4, train_wall=11, gb_free=19.5, wall=3954
2022-12-09 16:04:12 | INFO | train_inner | epoch 076:    558 / 1102 loss=7.184, nll_loss=3.412, ppl=10.65, wps=32708.2, ups=9.07, wpb=3606.9, bsz=143.6, num_updates=83200, lr=0.000109632, gnorm=1.94, loss_scale=4, train_wall=11, gb_free=19.5, wall=3965
2022-12-09 16:04:23 | INFO | train_inner | epoch 076:    658 / 1102 loss=7.209, nll_loss=3.413, ppl=10.65, wps=32382, ups=9.12, wpb=3550, bsz=128.8, num_updates=83300, lr=0.000109566, gnorm=1.976, loss_scale=4, train_wall=11, gb_free=19.5, wall=3976
2022-12-09 16:04:35 | INFO | train_inner | epoch 076:    758 / 1102 loss=7.171, nll_loss=3.396, ppl=10.53, wps=31559.3, ups=8.98, wpb=3514.1, bsz=152.3, num_updates=83400, lr=0.000109501, gnorm=1.947, loss_scale=4, train_wall=11, gb_free=19.3, wall=3987
2022-12-09 16:04:45 | INFO | train_inner | epoch 076:    858 / 1102 loss=7.165, nll_loss=3.389, ppl=10.47, wps=32774.4, ups=9.15, wpb=3580.5, bsz=149.3, num_updates=83500, lr=0.000109435, gnorm=1.957, loss_scale=4, train_wall=11, gb_free=19.8, wall=3998
2022-12-09 16:04:57 | INFO | train_inner | epoch 076:    958 / 1102 loss=7.224, nll_loss=3.452, ppl=10.94, wps=32594.3, ups=9.03, wpb=3610, bsz=137.4, num_updates=83600, lr=0.00010937, gnorm=1.955, loss_scale=4, train_wall=11, gb_free=19.3, wall=4009
2022-12-09 16:05:08 | INFO | train_inner | epoch 076:   1058 / 1102 loss=7.172, nll_loss=3.392, ppl=10.5, wps=31770.9, ups=8.96, wpb=3546.6, bsz=145, num_updates=83700, lr=0.000109304, gnorm=1.942, loss_scale=4, train_wall=11, gb_free=19.6, wall=4020
2022-12-09 16:05:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:05:55 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.686 | nll_loss 2.145 | ppl 4.42 | bleu 36.89 | wps 4219.1 | wpb 2835.3 | bsz 115.6 | num_updates 83744 | best_bleu 36.92
2022-12-09 16:05:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 83744 updates
2022-12-09 16:05:56 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint76.pt
2022-12-09 16:05:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint76.pt (epoch 76 @ 83744 updates, score 36.89) (writing took 1.3978614583611488 seconds)
2022-12-09 16:05:57 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-12-09 16:05:57 | INFO | train | epoch 076 | loss 7.149 | nll_loss 3.37 | ppl 10.34 | wps 23803.5 | ups 6.64 | wpb 3583.6 | bsz 145.4 | num_updates 83744 | lr 0.000109276 | gnorm 1.921 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 4069
2022-12-09 16:05:57 | INFO | fairseq.trainer | begin training epoch 77
2022-12-09 16:06:03 | INFO | train_inner | epoch 077:     56 / 1102 loss=7.156, nll_loss=3.392, ppl=10.5, wps=6467.1, ups=1.8, wpb=3592.7, bsz=143.4, num_updates=83800, lr=0.000109239, gnorm=1.918, loss_scale=4, train_wall=11, gb_free=19.6, wall=4076
2022-12-09 16:06:14 | INFO | train_inner | epoch 077:    156 / 1102 loss=7.051, nll_loss=3.262, ppl=9.59, wps=32175.1, ups=9.11, wpb=3530.9, bsz=158, num_updates=83900, lr=0.000109174, gnorm=1.956, loss_scale=4, train_wall=11, gb_free=19.5, wall=4087
2022-12-09 16:06:25 | INFO | train_inner | epoch 077:    256 / 1102 loss=7.164, nll_loss=3.378, ppl=10.4, wps=32349.3, ups=9.03, wpb=3582, bsz=138.3, num_updates=84000, lr=0.000109109, gnorm=1.952, loss_scale=4, train_wall=11, gb_free=19.3, wall=4098
2022-12-09 16:06:36 | INFO | train_inner | epoch 077:    356 / 1102 loss=7.085, nll_loss=3.304, ppl=9.88, wps=31809.3, ups=9.04, wpb=3519.7, bsz=154.6, num_updates=84100, lr=0.000109044, gnorm=1.948, loss_scale=4, train_wall=11, gb_free=19.7, wall=4109
2022-12-09 16:06:47 | INFO | train_inner | epoch 077:    456 / 1102 loss=7.134, nll_loss=3.342, ppl=10.14, wps=32547.1, ups=9.1, wpb=3576.2, bsz=139.5, num_updates=84200, lr=0.000108979, gnorm=1.917, loss_scale=4, train_wall=11, gb_free=19.5, wall=4120
2022-12-09 16:06:58 | INFO | train_inner | epoch 077:    556 / 1102 loss=7.183, nll_loss=3.4, ppl=10.56, wps=32220.6, ups=9.03, wpb=3568.3, bsz=145, num_updates=84300, lr=0.000108915, gnorm=1.933, loss_scale=4, train_wall=11, gb_free=19.4, wall=4131
2022-12-09 16:07:10 | INFO | train_inner | epoch 077:    656 / 1102 loss=7.132, nll_loss=3.356, ppl=10.24, wps=32881, ups=9.03, wpb=3641.2, bsz=142.2, num_updates=84400, lr=0.00010885, gnorm=1.847, loss_scale=4, train_wall=11, gb_free=19.4, wall=4142
2022-12-09 16:07:21 | INFO | train_inner | epoch 077:    756 / 1102 loss=7.126, nll_loss=3.35, ppl=10.2, wps=32846.9, ups=9.04, wpb=3633.3, bsz=147.7, num_updates=84500, lr=0.000108786, gnorm=1.901, loss_scale=4, train_wall=11, gb_free=19.7, wall=4153
2022-12-09 16:07:32 | INFO | train_inner | epoch 077:    856 / 1102 loss=7.201, nll_loss=3.412, ppl=10.65, wps=32527.4, ups=9.04, wpb=3598.7, bsz=134.1, num_updates=84600, lr=0.000108721, gnorm=1.982, loss_scale=4, train_wall=11, gb_free=19.4, wall=4164
2022-12-09 16:07:43 | INFO | train_inner | epoch 077:    956 / 1102 loss=7.184, nll_loss=3.418, ppl=10.69, wps=32847.4, ups=9.07, wpb=3622.1, bsz=150.3, num_updates=84700, lr=0.000108657, gnorm=1.877, loss_scale=4, train_wall=11, gb_free=19.3, wall=4175
2022-12-09 16:07:54 | INFO | train_inner | epoch 077:   1056 / 1102 loss=7.182, nll_loss=3.389, ppl=10.47, wps=32601.6, ups=9.15, wpb=3561.4, bsz=132.9, num_updates=84800, lr=0.000108593, gnorm=2.012, loss_scale=4, train_wall=11, gb_free=19.5, wall=4186
2022-12-09 16:07:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:08:41 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 3.687 | nll_loss 2.144 | ppl 4.42 | bleu 36.77 | wps 4262.4 | wpb 2835.3 | bsz 115.6 | num_updates 84846 | best_bleu 36.92
2022-12-09 16:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 84846 updates
2022-12-09 16:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint77.pt
2022-12-09 16:08:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint77.pt (epoch 77 @ 84846 updates, score 36.77) (writing took 1.4053415609523654 seconds)
2022-12-09 16:08:43 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-12-09 16:08:43 | INFO | train | epoch 077 | loss 7.142 | nll_loss 3.361 | ppl 10.27 | wps 23835.7 | ups 6.65 | wpb 3583.6 | bsz 145.4 | num_updates 84846 | lr 0.000108564 | gnorm 1.93 | loss_scale 4 | train_wall 119 | gb_free 19.4 | wall 4235
2022-12-09 16:08:43 | INFO | fairseq.trainer | begin training epoch 78
2022-12-09 16:08:49 | INFO | train_inner | epoch 078:     54 / 1102 loss=7.07, nll_loss=3.304, ppl=9.88, wps=6511.8, ups=1.81, wpb=3591.5, bsz=162.6, num_updates=84900, lr=0.000108529, gnorm=1.963, loss_scale=4, train_wall=11, gb_free=19.7, wall=4241
2022-12-09 16:09:00 | INFO | train_inner | epoch 078:    154 / 1102 loss=7.047, nll_loss=3.24, ppl=9.45, wps=32439.1, ups=9.16, wpb=3542.2, bsz=138.2, num_updates=85000, lr=0.000108465, gnorm=1.975, loss_scale=4, train_wall=11, gb_free=19.6, wall=4252
2022-12-09 16:09:11 | INFO | train_inner | epoch 078:    254 / 1102 loss=7.091, nll_loss=3.309, ppl=9.91, wps=33015.2, ups=9.1, wpb=3626.4, bsz=147.4, num_updates=85100, lr=0.000108401, gnorm=1.893, loss_scale=4, train_wall=11, gb_free=19.3, wall=4263
2022-12-09 16:09:22 | INFO | train_inner | epoch 078:    354 / 1102 loss=7.111, nll_loss=3.321, ppl=9.99, wps=32681.1, ups=9.15, wpb=3573.4, bsz=145.2, num_updates=85200, lr=0.000108338, gnorm=1.95, loss_scale=4, train_wall=11, gb_free=19.5, wall=4274
2022-12-09 16:09:33 | INFO | train_inner | epoch 078:    454 / 1102 loss=7.176, nll_loss=3.392, ppl=10.49, wps=32768.2, ups=9.14, wpb=3583.5, bsz=139.3, num_updates=85300, lr=0.000108274, gnorm=2.043, loss_scale=4, train_wall=11, gb_free=19.6, wall=4285
2022-12-09 16:09:43 | INFO | train_inner | epoch 078:    554 / 1102 loss=7.11, nll_loss=3.336, ppl=10.1, wps=32855.6, ups=9.2, wpb=3570.4, bsz=156.7, num_updates=85400, lr=0.000108211, gnorm=1.907, loss_scale=4, train_wall=11, gb_free=19.4, wall=4296
2022-12-09 16:09:54 | INFO | train_inner | epoch 078:    654 / 1102 loss=7.167, nll_loss=3.4, ppl=10.56, wps=33213.1, ups=9.13, wpb=3635.9, bsz=150.5, num_updates=85500, lr=0.000108148, gnorm=1.865, loss_scale=4, train_wall=11, gb_free=19.3, wall=4307
2022-12-09 16:10:05 | INFO | train_inner | epoch 078:    754 / 1102 loss=7.196, nll_loss=3.411, ppl=10.64, wps=32663, ups=9.17, wpb=3561.9, bsz=142.9, num_updates=85600, lr=0.000108084, gnorm=2.029, loss_scale=4, train_wall=11, gb_free=19.2, wall=4318
2022-12-09 16:10:16 | INFO | train_inner | epoch 078:    854 / 1102 loss=7.217, nll_loss=3.429, ppl=10.77, wps=32104.6, ups=9.15, wpb=3509.9, bsz=135.1, num_updates=85700, lr=0.000108021, gnorm=1.932, loss_scale=4, train_wall=11, gb_free=19.6, wall=4329
2022-12-09 16:10:27 | INFO | train_inner | epoch 078:    954 / 1102 loss=7.202, nll_loss=3.434, ppl=10.81, wps=32898.4, ups=9.06, wpb=3630.7, bsz=146.7, num_updates=85800, lr=0.000107958, gnorm=1.887, loss_scale=4, train_wall=11, gb_free=19.4, wall=4340
2022-12-09 16:10:38 | INFO | train_inner | epoch 078:   1054 / 1102 loss=7.091, nll_loss=3.318, ppl=9.97, wps=33056, ups=9.17, wpb=3603.1, bsz=154.3, num_updates=85900, lr=0.000107896, gnorm=1.901, loss_scale=4, train_wall=11, gb_free=19.5, wall=4351
2022-12-09 16:10:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:11:21 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 3.693 | nll_loss 2.146 | ppl 4.43 | bleu 36.92 | wps 4819.7 | wpb 2835.3 | bsz 115.6 | num_updates 85948 | best_bleu 36.92
2022-12-09 16:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 85948 updates
2022-12-09 16:11:22 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint78.pt
2022-12-09 16:11:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint78.pt (epoch 78 @ 85948 updates, score 36.92) (writing took 1.6938603576272726 seconds)
2022-12-09 16:11:23 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-12-09 16:11:23 | INFO | train | epoch 078 | loss 7.136 | nll_loss 3.354 | ppl 10.22 | wps 24675.9 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 85948 | lr 0.000107865 | gnorm 1.942 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 4395
2022-12-09 16:11:23 | INFO | fairseq.trainer | begin training epoch 79
2022-12-09 16:11:28 | INFO | train_inner | epoch 079:     52 / 1102 loss=7.046, nll_loss=3.261, ppl=9.58, wps=7092.7, ups=1.99, wpb=3570.7, bsz=146.3, num_updates=86000, lr=0.000107833, gnorm=1.885, loss_scale=4, train_wall=11, gb_free=19.5, wall=4401
2022-12-09 16:11:39 | INFO | train_inner | epoch 079:    152 / 1102 loss=7.1, nll_loss=3.304, ppl=9.87, wps=32854.8, ups=9.13, wpb=3596.8, bsz=140.5, num_updates=86100, lr=0.00010777, gnorm=1.947, loss_scale=4, train_wall=11, gb_free=19.3, wall=4412
2022-12-09 16:11:50 | INFO | train_inner | epoch 079:    252 / 1102 loss=7.11, nll_loss=3.329, ppl=10.05, wps=33106.9, ups=9.15, wpb=3616.5, bsz=149.4, num_updates=86200, lr=0.000107708, gnorm=1.933, loss_scale=4, train_wall=11, gb_free=19.3, wall=4423
2022-12-09 16:12:01 | INFO | train_inner | epoch 079:    352 / 1102 loss=7.12, nll_loss=3.34, ppl=10.13, wps=32736, ups=9.05, wpb=3616.1, bsz=145.6, num_updates=86300, lr=0.000107645, gnorm=1.954, loss_scale=4, train_wall=11, gb_free=19.3, wall=4434
2022-12-09 16:12:12 | INFO | train_inner | epoch 079:    452 / 1102 loss=7.213, nll_loss=3.421, ppl=10.71, wps=32080.5, ups=9.07, wpb=3536, bsz=133.2, num_updates=86400, lr=0.000107583, gnorm=1.984, loss_scale=4, train_wall=11, gb_free=19.4, wall=4445
2022-12-09 16:12:23 | INFO | train_inner | epoch 079:    552 / 1102 loss=7.113, nll_loss=3.334, ppl=10.08, wps=32708.9, ups=9.03, wpb=3621.3, bsz=150.9, num_updates=86500, lr=0.000107521, gnorm=1.978, loss_scale=8, train_wall=11, gb_free=19.5, wall=4456
2022-12-09 16:12:34 | INFO | train_inner | epoch 079:    652 / 1102 loss=7.094, nll_loss=3.309, ppl=9.91, wps=32518.9, ups=9.09, wpb=3577.2, bsz=145.8, num_updates=86600, lr=0.000107459, gnorm=2.014, loss_scale=8, train_wall=11, gb_free=19.6, wall=4467
2022-12-09 16:12:46 | INFO | train_inner | epoch 079:    752 / 1102 loss=7.164, nll_loss=3.375, ppl=10.37, wps=32782.6, ups=9.03, wpb=3631.4, bsz=135.2, num_updates=86700, lr=0.000107397, gnorm=1.891, loss_scale=8, train_wall=11, gb_free=19.3, wall=4478
2022-12-09 16:12:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-09 16:12:57 | INFO | train_inner | epoch 079:    853 / 1102 loss=7.088, nll_loss=3.314, ppl=9.95, wps=31614.5, ups=8.98, wpb=3520.8, bsz=163.4, num_updates=86800, lr=0.000107335, gnorm=1.995, loss_scale=4, train_wall=11, gb_free=19.5, wall=4489
2022-12-09 16:13:08 | INFO | train_inner | epoch 079:    953 / 1102 loss=7.177, nll_loss=3.386, ppl=10.45, wps=32234.5, ups=9.08, wpb=3551.3, bsz=140.6, num_updates=86900, lr=0.000107273, gnorm=1.975, loss_scale=4, train_wall=11, gb_free=19.6, wall=4500
2022-12-09 16:13:19 | INFO | train_inner | epoch 079:   1053 / 1102 loss=7.163, nll_loss=3.383, ppl=10.44, wps=32302.7, ups=9.09, wpb=3555.4, bsz=142.6, num_updates=87000, lr=0.000107211, gnorm=1.956, loss_scale=4, train_wall=11, gb_free=19.5, wall=4511
2022-12-09 16:13:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:14:04 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 3.678 | nll_loss 2.137 | ppl 4.4 | bleu 36.98 | wps 4502.1 | wpb 2835.3 | bsz 115.6 | num_updates 87049 | best_bleu 36.98
2022-12-09 16:14:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 87049 updates
2022-12-09 16:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint79.pt
2022-12-09 16:14:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint79.pt (epoch 79 @ 87049 updates, score 36.98) (writing took 1.9205549592152238 seconds)
2022-12-09 16:14:06 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-12-09 16:14:06 | INFO | train | epoch 079 | loss 7.129 | nll_loss 3.346 | ppl 10.17 | wps 24092.1 | ups 6.72 | wpb 3583.4 | bsz 145.5 | num_updates 87049 | lr 0.000107181 | gnorm 1.957 | loss_scale 4 | train_wall 118 | gb_free 19.6 | wall 4559
2022-12-09 16:14:06 | INFO | fairseq.trainer | begin training epoch 80
2022-12-09 16:14:12 | INFO | train_inner | epoch 080:     51 / 1102 loss=7.128, nll_loss=3.342, ppl=10.14, wps=6727.8, ups=1.87, wpb=3594.2, bsz=139.5, num_updates=87100, lr=0.00010715, gnorm=1.902, loss_scale=4, train_wall=11, gb_free=19.5, wall=4565
2022-12-09 16:14:23 | INFO | train_inner | epoch 080:    151 / 1102 loss=7.12, nll_loss=3.326, ppl=10.03, wps=34001.9, ups=9.49, wpb=3583.4, bsz=134.2, num_updates=87200, lr=0.000107088, gnorm=1.946, loss_scale=4, train_wall=10, gb_free=19.5, wall=4575
2022-12-09 16:14:33 | INFO | train_inner | epoch 080:    251 / 1102 loss=7.117, nll_loss=3.333, ppl=10.08, wps=33930.2, ups=9.36, wpb=3625.2, bsz=144.3, num_updates=87300, lr=0.000107027, gnorm=1.907, loss_scale=4, train_wall=10, gb_free=19.3, wall=4586
2022-12-09 16:14:44 | INFO | train_inner | epoch 080:    351 / 1102 loss=7.132, nll_loss=3.339, ppl=10.12, wps=33329.3, ups=9.23, wpb=3609.7, bsz=139.4, num_updates=87400, lr=0.000106966, gnorm=1.95, loss_scale=4, train_wall=11, gb_free=19.3, wall=4597
2022-12-09 16:14:55 | INFO | train_inner | epoch 080:    451 / 1102 loss=7.136, nll_loss=3.337, ppl=10.11, wps=32720.9, ups=9.28, wpb=3524.8, bsz=135.9, num_updates=87500, lr=0.000106904, gnorm=1.982, loss_scale=4, train_wall=11, gb_free=19.4, wall=4607
2022-12-09 16:15:06 | INFO | train_inner | epoch 080:    551 / 1102 loss=7.106, nll_loss=3.326, ppl=10.03, wps=32641.2, ups=9.12, wpb=3577.4, bsz=158.6, num_updates=87600, lr=0.000106843, gnorm=1.974, loss_scale=4, train_wall=11, gb_free=19.3, wall=4618
2022-12-09 16:15:17 | INFO | train_inner | epoch 080:    651 / 1102 loss=7.115, nll_loss=3.352, ppl=10.21, wps=32978.2, ups=9.07, wpb=3636, bsz=157.6, num_updates=87700, lr=0.000106783, gnorm=1.97, loss_scale=4, train_wall=11, gb_free=19.4, wall=4629
2022-12-09 16:15:28 | INFO | train_inner | epoch 080:    751 / 1102 loss=7.141, nll_loss=3.362, ppl=10.28, wps=33375.3, ups=9.22, wpb=3618.8, bsz=141.8, num_updates=87800, lr=0.000106722, gnorm=1.882, loss_scale=4, train_wall=11, gb_free=19.4, wall=4640
2022-12-09 16:15:39 | INFO | train_inner | epoch 080:    851 / 1102 loss=7.104, nll_loss=3.324, ppl=10.01, wps=32842.2, ups=9.11, wpb=3603.3, bsz=152.6, num_updates=87900, lr=0.000106661, gnorm=2.015, loss_scale=4, train_wall=11, gb_free=19.3, wall=4651
2022-12-09 16:15:50 | INFO | train_inner | epoch 080:    951 / 1102 loss=7.153, nll_loss=3.364, ppl=10.29, wps=32768.4, ups=9.2, wpb=3560.2, bsz=138.1, num_updates=88000, lr=0.0001066, gnorm=1.947, loss_scale=4, train_wall=11, gb_free=19.7, wall=4662
2022-12-09 16:16:01 | INFO | train_inner | epoch 080:   1051 / 1102 loss=7.088, nll_loss=3.313, ppl=9.94, wps=32422.7, ups=9.19, wpb=3526.8, bsz=160.1, num_updates=88100, lr=0.00010654, gnorm=1.937, loss_scale=4, train_wall=11, gb_free=19.6, wall=4673
2022-12-09 16:16:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:16:46 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 3.689 | nll_loss 2.145 | ppl 4.42 | bleu 36.83 | wps 4578.2 | wpb 2835.3 | bsz 115.6 | num_updates 88151 | best_bleu 36.98
2022-12-09 16:16:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 88151 updates
2022-12-09 16:16:46 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint80.pt
2022-12-09 16:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint80.pt (epoch 80 @ 88151 updates, score 36.83) (writing took 1.2232501935213804 seconds)
2022-12-09 16:16:47 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-12-09 16:16:47 | INFO | train | epoch 080 | loss 7.122 | nll_loss 3.337 | ppl 10.1 | wps 24620.8 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 88151 | lr 0.000106509 | gnorm 1.951 | loss_scale 4 | train_wall 117 | gb_free 19.7 | wall 4719
2022-12-09 16:16:47 | INFO | fairseq.trainer | begin training epoch 81
2022-12-09 16:16:52 | INFO | train_inner | epoch 081:     49 / 1102 loss=7.093, nll_loss=3.311, ppl=9.93, wps=6875.3, ups=1.93, wpb=3569.8, bsz=153.2, num_updates=88200, lr=0.000106479, gnorm=1.998, loss_scale=4, train_wall=11, gb_free=19.2, wall=4725
2022-12-09 16:17:03 | INFO | train_inner | epoch 081:    149 / 1102 loss=6.996, nll_loss=3.212, ppl=9.27, wps=32776.5, ups=9.16, wpb=3579.7, bsz=155.9, num_updates=88300, lr=0.000106419, gnorm=1.901, loss_scale=4, train_wall=11, gb_free=19.4, wall=4736
2022-12-09 16:17:14 | INFO | train_inner | epoch 081:    249 / 1102 loss=7.086, nll_loss=3.294, ppl=9.81, wps=32528.1, ups=9.17, wpb=3548.2, bsz=143.1, num_updates=88400, lr=0.000106359, gnorm=1.914, loss_scale=4, train_wall=11, gb_free=19.6, wall=4747
2022-12-09 16:17:25 | INFO | train_inner | epoch 081:    349 / 1102 loss=7.119, nll_loss=3.33, ppl=10.06, wps=32498.6, ups=9.08, wpb=3580.8, bsz=145.2, num_updates=88500, lr=0.000106299, gnorm=1.985, loss_scale=4, train_wall=11, gb_free=19.7, wall=4758
2022-12-09 16:17:36 | INFO | train_inner | epoch 081:    449 / 1102 loss=7.128, nll_loss=3.349, ppl=10.19, wps=32916.7, ups=9.18, wpb=3583.9, bsz=146.2, num_updates=88600, lr=0.000106239, gnorm=1.941, loss_scale=4, train_wall=11, gb_free=19.5, wall=4769
2022-12-09 16:17:47 | INFO | train_inner | epoch 081:    549 / 1102 loss=7.158, nll_loss=3.371, ppl=10.35, wps=32899.2, ups=9.09, wpb=3620.8, bsz=140.6, num_updates=88700, lr=0.000106179, gnorm=1.912, loss_scale=4, train_wall=11, gb_free=19.4, wall=4780
2022-12-09 16:17:58 | INFO | train_inner | epoch 081:    649 / 1102 loss=7.133, nll_loss=3.356, ppl=10.24, wps=32831.5, ups=9.12, wpb=3601.3, bsz=156.9, num_updates=88800, lr=0.000106119, gnorm=1.947, loss_scale=4, train_wall=11, gb_free=19.3, wall=4791
2022-12-09 16:18:09 | INFO | train_inner | epoch 081:    749 / 1102 loss=7.095, nll_loss=3.31, ppl=9.92, wps=32671.4, ups=9.1, wpb=3590.4, bsz=140.6, num_updates=88900, lr=0.000106059, gnorm=1.933, loss_scale=4, train_wall=11, gb_free=19.5, wall=4802
2022-12-09 16:18:20 | INFO | train_inner | epoch 081:    849 / 1102 loss=7.148, nll_loss=3.352, ppl=10.21, wps=32666.3, ups=9.12, wpb=3581.8, bsz=136.8, num_updates=89000, lr=0.000106, gnorm=2.003, loss_scale=4, train_wall=11, gb_free=19.9, wall=4813
2022-12-09 16:18:31 | INFO | train_inner | epoch 081:    949 / 1102 loss=7.181, nll_loss=3.38, ppl=10.41, wps=32310, ups=9.17, wpb=3525.3, bsz=129.4, num_updates=89100, lr=0.00010594, gnorm=2.015, loss_scale=4, train_wall=11, gb_free=19.6, wall=4823
2022-12-09 16:18:42 | INFO | train_inner | epoch 081:   1049 / 1102 loss=7.141, nll_loss=3.36, ppl=10.27, wps=32758, ups=9.1, wpb=3600.7, bsz=148.8, num_updates=89200, lr=0.000105881, gnorm=1.921, loss_scale=4, train_wall=11, gb_free=19.2, wall=4834
2022-12-09 16:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:19:27 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 3.68 | nll_loss 2.138 | ppl 4.4 | bleu 37 | wps 4550.6 | wpb 2835.3 | bsz 115.6 | num_updates 89253 | best_bleu 37
2022-12-09 16:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 89253 updates
2022-12-09 16:19:28 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint81.pt
2022-12-09 16:19:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint81.pt (epoch 81 @ 89253 updates, score 37.0) (writing took 1.688539200462401 seconds)
2022-12-09 16:19:29 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-12-09 16:19:29 | INFO | train | epoch 081 | loss 7.115 | nll_loss 3.329 | ppl 10.05 | wps 24313.3 | ups 6.78 | wpb 3583.6 | bsz 145.4 | num_updates 89253 | lr 0.000105849 | gnorm 1.945 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 4882
2022-12-09 16:19:29 | INFO | fairseq.trainer | begin training epoch 82
2022-12-09 16:19:35 | INFO | train_inner | epoch 082:     47 / 1102 loss=7.137, nll_loss=3.354, ppl=10.23, wps=6879.1, ups=1.9, wpb=3614.9, bsz=144.6, num_updates=89300, lr=0.000105822, gnorm=1.927, loss_scale=4, train_wall=11, gb_free=19.3, wall=4887
2022-12-09 16:19:45 | INFO | train_inner | epoch 082:    147 / 1102 loss=7.072, nll_loss=3.284, ppl=9.74, wps=33145.5, ups=9.18, wpb=3612.3, bsz=139.1, num_updates=89400, lr=0.000105762, gnorm=1.987, loss_scale=4, train_wall=11, gb_free=19.8, wall=4898
2022-12-09 16:19:56 | INFO | train_inner | epoch 082:    247 / 1102 loss=7.053, nll_loss=3.269, ppl=9.64, wps=33045.9, ups=9.25, wpb=3572.2, bsz=147.6, num_updates=89500, lr=0.000105703, gnorm=1.891, loss_scale=4, train_wall=11, gb_free=19.4, wall=4909
2022-12-09 16:20:07 | INFO | train_inner | epoch 082:    347 / 1102 loss=7.009, nll_loss=3.228, ppl=9.37, wps=33230.7, ups=9.14, wpb=3636.7, bsz=160, num_updates=89600, lr=0.000105644, gnorm=1.837, loss_scale=4, train_wall=11, gb_free=19.7, wall=4920
2022-12-09 16:20:18 | INFO | train_inner | epoch 082:    447 / 1102 loss=7.035, nll_loss=3.239, ppl=9.44, wps=32854.7, ups=9.29, wpb=3537.3, bsz=149, num_updates=89700, lr=0.000105585, gnorm=1.99, loss_scale=4, train_wall=11, gb_free=19.4, wall=4930
2022-12-09 16:20:29 | INFO | train_inner | epoch 082:    547 / 1102 loss=7.135, nll_loss=3.344, ppl=10.16, wps=32848.3, ups=9.14, wpb=3594.6, bsz=141.5, num_updates=89800, lr=0.000105527, gnorm=1.983, loss_scale=4, train_wall=11, gb_free=19.5, wall=4941
2022-12-09 16:20:40 | INFO | train_inner | epoch 082:    647 / 1102 loss=7.073, nll_loss=3.304, ppl=9.87, wps=33420.8, ups=9.25, wpb=3612.8, bsz=159.7, num_updates=89900, lr=0.000105468, gnorm=1.886, loss_scale=4, train_wall=11, gb_free=19.5, wall=4952
2022-12-09 16:20:50 | INFO | train_inner | epoch 082:    747 / 1102 loss=7.201, nll_loss=3.413, ppl=10.65, wps=33301.4, ups=9.35, wpb=3560.4, bsz=142.2, num_updates=90000, lr=0.000105409, gnorm=2.029, loss_scale=4, train_wall=10, gb_free=19.7, wall=4963
2022-12-09 16:21:01 | INFO | train_inner | epoch 082:    847 / 1102 loss=7.178, nll_loss=3.394, ppl=10.52, wps=33812.3, ups=9.33, wpb=3624.7, bsz=139.9, num_updates=90100, lr=0.000105351, gnorm=1.918, loss_scale=4, train_wall=10, gb_free=19.4, wall=4974
2022-12-09 16:21:12 | INFO | train_inner | epoch 082:    947 / 1102 loss=7.153, nll_loss=3.36, ppl=10.27, wps=32683.9, ups=9.24, wpb=3537.4, bsz=135.6, num_updates=90200, lr=0.000105292, gnorm=1.926, loss_scale=4, train_wall=11, gb_free=19.6, wall=4984
2022-12-09 16:21:23 | INFO | train_inner | epoch 082:   1047 / 1102 loss=7.121, nll_loss=3.337, ppl=10.11, wps=32943.2, ups=9.15, wpb=3599.7, bsz=148.8, num_updates=90300, lr=0.000105234, gnorm=1.934, loss_scale=4, train_wall=11, gb_free=19.6, wall=4995
2022-12-09 16:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:22:07 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 3.681 | nll_loss 2.137 | ppl 4.4 | bleu 36.92 | wps 4743.2 | wpb 2835.3 | bsz 115.6 | num_updates 90355 | best_bleu 37
2022-12-09 16:22:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 90355 updates
2022-12-09 16:22:08 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint82.pt
2022-12-09 16:22:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint82.pt (epoch 82 @ 90355 updates, score 36.92) (writing took 1.186128905043006 seconds)
2022-12-09 16:22:08 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-12-09 16:22:08 | INFO | train | epoch 082 | loss 7.108 | nll_loss 3.321 | ppl 10 | wps 24842.4 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 90355 | lr 0.000105202 | gnorm 1.939 | loss_scale 4 | train_wall 117 | gb_free 19.8 | wall 5041
2022-12-09 16:22:08 | INFO | fairseq.trainer | begin training epoch 83
2022-12-09 16:22:13 | INFO | train_inner | epoch 083:     45 / 1102 loss=7.119, nll_loss=3.337, ppl=10.11, wps=7046.7, ups=1.98, wpb=3550.6, bsz=140.2, num_updates=90400, lr=0.000105176, gnorm=1.923, loss_scale=4, train_wall=10, gb_free=19.5, wall=5046
2022-12-09 16:22:24 | INFO | train_inner | epoch 083:    145 / 1102 loss=7.023, nll_loss=3.236, ppl=9.42, wps=33105.7, ups=9.28, wpb=3569.3, bsz=153.7, num_updates=90500, lr=0.000105118, gnorm=1.921, loss_scale=4, train_wall=11, gb_free=19.6, wall=5056
2022-12-09 16:22:35 | INFO | train_inner | epoch 083:    245 / 1102 loss=7.083, nll_loss=3.281, ppl=9.72, wps=32937.1, ups=9.25, wpb=3559.4, bsz=136.6, num_updates=90600, lr=0.00010506, gnorm=1.995, loss_scale=4, train_wall=11, gb_free=19.8, wall=5067
2022-12-09 16:22:46 | INFO | train_inner | epoch 083:    345 / 1102 loss=7.098, nll_loss=3.294, ppl=9.81, wps=32835.7, ups=9.21, wpb=3564.1, bsz=136.2, num_updates=90700, lr=0.000105002, gnorm=2.006, loss_scale=4, train_wall=11, gb_free=19.8, wall=5078
2022-12-09 16:22:57 | INFO | train_inner | epoch 083:    445 / 1102 loss=7.131, nll_loss=3.345, ppl=10.16, wps=33505.4, ups=9.17, wpb=3655, bsz=145.9, num_updates=90800, lr=0.000104944, gnorm=1.905, loss_scale=4, train_wall=11, gb_free=19.6, wall=5089
2022-12-09 16:23:08 | INFO | train_inner | epoch 083:    545 / 1102 loss=7.11, nll_loss=3.321, ppl=9.99, wps=32848.4, ups=9.2, wpb=3569.7, bsz=145.8, num_updates=90900, lr=0.000104886, gnorm=1.915, loss_scale=4, train_wall=11, gb_free=19.5, wall=5100
2022-12-09 16:23:18 | INFO | train_inner | epoch 083:    645 / 1102 loss=7.136, nll_loss=3.338, ppl=10.11, wps=32771, ups=9.32, wpb=3517.6, bsz=136.5, num_updates=91000, lr=0.000104828, gnorm=1.957, loss_scale=4, train_wall=10, gb_free=19.7, wall=5111
2022-12-09 16:23:29 | INFO | train_inner | epoch 083:    745 / 1102 loss=7.028, nll_loss=3.249, ppl=9.51, wps=32809.3, ups=9.14, wpb=3589.1, bsz=160.6, num_updates=91100, lr=0.000104771, gnorm=1.871, loss_scale=4, train_wall=11, gb_free=19.3, wall=5122
2022-12-09 16:23:40 | INFO | train_inner | epoch 083:    845 / 1102 loss=7.19, nll_loss=3.398, ppl=10.54, wps=33045.6, ups=9.23, wpb=3581.2, bsz=141.3, num_updates=91200, lr=0.000104713, gnorm=2.006, loss_scale=4, train_wall=11, gb_free=19.3, wall=5132
2022-12-09 16:23:51 | INFO | train_inner | epoch 083:    945 / 1102 loss=7.112, nll_loss=3.326, ppl=10.03, wps=32461.6, ups=9.17, wpb=3541.3, bsz=143.6, num_updates=91300, lr=0.000104656, gnorm=1.979, loss_scale=4, train_wall=11, gb_free=19.2, wall=5143
2022-12-09 16:24:02 | INFO | train_inner | epoch 083:   1045 / 1102 loss=7.093, nll_loss=3.324, ppl=10.01, wps=32710.8, ups=9.01, wpb=3630.3, bsz=157.1, num_updates=91400, lr=0.000104599, gnorm=1.892, loss_scale=4, train_wall=11, gb_free=19.4, wall=5154
2022-12-09 16:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:24:48 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 3.681 | nll_loss 2.139 | ppl 4.4 | bleu 36.9 | wps 4537.4 | wpb 2835.3 | bsz 115.6 | num_updates 91457 | best_bleu 37
2022-12-09 16:24:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 91457 updates
2022-12-09 16:24:49 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint83.pt
2022-12-09 16:24:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint83.pt (epoch 83 @ 91457 updates, score 36.9) (writing took 1.1141134034842253 seconds)
2022-12-09 16:24:49 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-12-09 16:24:49 | INFO | train | epoch 083 | loss 7.103 | nll_loss 3.314 | ppl 9.95 | wps 24503.2 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 91457 | lr 0.000104566 | gnorm 1.944 | loss_scale 4 | train_wall 117 | gb_free 19.5 | wall 5202
2022-12-09 16:24:49 | INFO | fairseq.trainer | begin training epoch 84
2022-12-09 16:24:54 | INFO | train_inner | epoch 084:     43 / 1102 loss=7.137, nll_loss=3.343, ppl=10.15, wps=6966.6, ups=1.92, wpb=3636.6, bsz=134.8, num_updates=91500, lr=0.000104542, gnorm=1.974, loss_scale=4, train_wall=11, gb_free=19.3, wall=5207
2022-12-09 16:25:05 | INFO | train_inner | epoch 084:    143 / 1102 loss=7.06, nll_loss=3.268, ppl=9.63, wps=33059.2, ups=9.32, wpb=3548.2, bsz=146.1, num_updates=91600, lr=0.000104485, gnorm=1.947, loss_scale=4, train_wall=10, gb_free=19.7, wall=5217
2022-12-09 16:25:16 | INFO | train_inner | epoch 084:    243 / 1102 loss=7.067, nll_loss=3.271, ppl=9.65, wps=32928.9, ups=9.36, wpb=3517.7, bsz=141.2, num_updates=91700, lr=0.000104428, gnorm=1.941, loss_scale=4, train_wall=10, gb_free=19.5, wall=5228
2022-12-09 16:25:26 | INFO | train_inner | epoch 084:    343 / 1102 loss=7.004, nll_loss=3.218, ppl=9.3, wps=33336.6, ups=9.31, wpb=3578.9, bsz=164, num_updates=91800, lr=0.000104371, gnorm=1.971, loss_scale=4, train_wall=10, gb_free=19.4, wall=5239
2022-12-09 16:25:37 | INFO | train_inner | epoch 084:    443 / 1102 loss=7.079, nll_loss=3.279, ppl=9.71, wps=32556.7, ups=9.2, wpb=3539.8, bsz=140.6, num_updates=91900, lr=0.000104314, gnorm=1.99, loss_scale=4, train_wall=11, gb_free=19.6, wall=5250
2022-12-09 16:25:49 | INFO | train_inner | epoch 084:    543 / 1102 loss=7.143, nll_loss=3.358, ppl=10.25, wps=32304.8, ups=8.86, wpb=3644.9, bsz=145.8, num_updates=92000, lr=0.000104257, gnorm=2.033, loss_scale=4, train_wall=11, gb_free=19.4, wall=5261
2022-12-09 16:26:00 | INFO | train_inner | epoch 084:    643 / 1102 loss=7.148, nll_loss=3.349, ppl=10.19, wps=32281.6, ups=9.06, wpb=3564.2, bsz=132.9, num_updates=92100, lr=0.000104201, gnorm=2.035, loss_scale=4, train_wall=11, gb_free=19.4, wall=5272
2022-12-09 16:26:11 | INFO | train_inner | epoch 084:    743 / 1102 loss=7.12, nll_loss=3.329, ppl=10.05, wps=32370.8, ups=9.03, wpb=3584.1, bsz=142.3, num_updates=92200, lr=0.000104144, gnorm=2.012, loss_scale=4, train_wall=11, gb_free=19.5, wall=5283
2022-12-09 16:26:21 | INFO | train_inner | epoch 084:    843 / 1102 loss=7.17, nll_loss=3.385, ppl=10.45, wps=33436.7, ups=9.27, wpb=3605.8, bsz=138.6, num_updates=92300, lr=0.000104088, gnorm=1.943, loss_scale=4, train_wall=11, gb_free=19.5, wall=5294
2022-12-09 16:26:32 | INFO | train_inner | epoch 084:    943 / 1102 loss=7.096, nll_loss=3.304, ppl=9.88, wps=33168.2, ups=9.29, wpb=3569.6, bsz=140.3, num_updates=92400, lr=0.000104031, gnorm=1.95, loss_scale=4, train_wall=11, gb_free=19.7, wall=5305
2022-12-09 16:26:43 | INFO | train_inner | epoch 084:   1043 / 1102 loss=7.115, nll_loss=3.331, ppl=10.07, wps=33683.9, ups=9.25, wpb=3639.9, bsz=152.6, num_updates=92500, lr=0.000103975, gnorm=1.955, loss_scale=4, train_wall=11, gb_free=19.6, wall=5315
2022-12-09 16:26:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:27:27 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 3.682 | nll_loss 2.139 | ppl 4.4 | bleu 36.85 | wps 4865 | wpb 2835.3 | bsz 115.6 | num_updates 92559 | best_bleu 37
2022-12-09 16:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 92559 updates
2022-12-09 16:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint84.pt
2022-12-09 16:27:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint84.pt (epoch 84 @ 92559 updates, score 36.85) (writing took 1.1453972402960062 seconds)
2022-12-09 16:27:28 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-12-09 16:27:28 | INFO | train | epoch 084 | loss 7.096 | nll_loss 3.307 | ppl 9.9 | wps 24917.9 | ups 6.95 | wpb 3583.6 | bsz 145.4 | num_updates 92559 | lr 0.000103942 | gnorm 1.981 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 5360
2022-12-09 16:27:28 | INFO | fairseq.trainer | begin training epoch 85
2022-12-09 16:27:33 | INFO | train_inner | epoch 085:     41 / 1102 loss=6.968, nll_loss=3.209, ppl=9.25, wps=7351, ups=2.02, wpb=3640.8, bsz=177, num_updates=92600, lr=0.000103919, gnorm=1.944, loss_scale=4, train_wall=11, gb_free=19.5, wall=5365
2022-12-09 16:27:43 | INFO | train_inner | epoch 085:    141 / 1102 loss=7.002, nll_loss=3.195, ppl=9.16, wps=32729.6, ups=9.41, wpb=3476.4, bsz=145.8, num_updates=92700, lr=0.000103863, gnorm=1.984, loss_scale=4, train_wall=10, gb_free=19.7, wall=5376
2022-12-09 16:27:54 | INFO | train_inner | epoch 085:    241 / 1102 loss=6.99, nll_loss=3.195, ppl=9.16, wps=33293.1, ups=9.27, wpb=3592.1, bsz=150.3, num_updates=92800, lr=0.000103807, gnorm=1.974, loss_scale=4, train_wall=11, gb_free=19.5, wall=5386
2022-12-09 16:28:05 | INFO | train_inner | epoch 085:    341 / 1102 loss=7.083, nll_loss=3.282, ppl=9.73, wps=33499.9, ups=9.34, wpb=3588.6, bsz=138.7, num_updates=92900, lr=0.000103751, gnorm=1.94, loss_scale=4, train_wall=10, gb_free=19.6, wall=5397
2022-12-09 16:28:16 | INFO | train_inner | epoch 085:    441 / 1102 loss=7.163, nll_loss=3.362, ppl=10.28, wps=32800.1, ups=9.21, wpb=3560.7, bsz=131.5, num_updates=93000, lr=0.000103695, gnorm=1.99, loss_scale=4, train_wall=11, gb_free=19.5, wall=5408
2022-12-09 16:28:26 | INFO | train_inner | epoch 085:    541 / 1102 loss=7.242, nll_loss=3.424, ppl=10.73, wps=32595.4, ups=9.31, wpb=3502.1, bsz=123.2, num_updates=93100, lr=0.000103639, gnorm=2.107, loss_scale=4, train_wall=11, gb_free=19.4, wall=5419
2022-12-09 16:28:37 | INFO | train_inner | epoch 085:    641 / 1102 loss=7.042, nll_loss=3.27, ppl=9.65, wps=33105.7, ups=9.14, wpb=3622.4, bsz=170.3, num_updates=93200, lr=0.000103584, gnorm=1.91, loss_scale=4, train_wall=11, gb_free=19.2, wall=5430
2022-12-09 16:28:48 | INFO | train_inner | epoch 085:    741 / 1102 loss=7.062, nll_loss=3.281, ppl=9.72, wps=32985.8, ups=9.14, wpb=3607.3, bsz=148.1, num_updates=93300, lr=0.000103528, gnorm=1.932, loss_scale=4, train_wall=11, gb_free=19.3, wall=5441
2022-12-09 16:28:59 | INFO | train_inner | epoch 085:    841 / 1102 loss=7.081, nll_loss=3.304, ppl=9.88, wps=33156.8, ups=9.13, wpb=3632.1, bsz=158.9, num_updates=93400, lr=0.000103473, gnorm=1.927, loss_scale=4, train_wall=11, gb_free=19.8, wall=5451
2022-12-09 16:29:10 | INFO | train_inner | epoch 085:    941 / 1102 loss=7.163, nll_loss=3.374, ppl=10.37, wps=32362.3, ups=9.09, wpb=3560.2, bsz=141.1, num_updates=93500, lr=0.000103418, gnorm=1.937, loss_scale=4, train_wall=11, gb_free=19.3, wall=5462
2022-12-09 16:29:21 | INFO | train_inner | epoch 085:   1041 / 1102 loss=7.128, nll_loss=3.341, ppl=10.13, wps=33065.8, ups=9.04, wpb=3657.7, bsz=142.2, num_updates=93600, lr=0.000103362, gnorm=1.901, loss_scale=4, train_wall=11, gb_free=19.5, wall=5474
2022-12-09 16:29:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:30:07 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 3.679 | nll_loss 2.134 | ppl 4.39 | bleu 36.82 | wps 4607 | wpb 2835.3 | bsz 115.6 | num_updates 93661 | best_bleu 37
2022-12-09 16:30:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 93661 updates
2022-12-09 16:30:08 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint85.pt
2022-12-09 16:30:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint85.pt (epoch 85 @ 93661 updates, score 36.82) (writing took 1.1871876027435064 seconds)
2022-12-09 16:30:08 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-12-09 16:30:08 | INFO | train | epoch 085 | loss 7.092 | nll_loss 3.3 | ppl 9.85 | wps 24624.3 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 93661 | lr 0.000103329 | gnorm 1.959 | loss_scale 4 | train_wall 117 | gb_free 19.3 | wall 5521
2022-12-09 16:30:08 | INFO | fairseq.trainer | begin training epoch 86
2022-12-09 16:30:13 | INFO | train_inner | epoch 086:     39 / 1102 loss=7.064, nll_loss=3.282, ppl=9.73, wps=6964.1, ups=1.94, wpb=3591.8, bsz=151.8, num_updates=93700, lr=0.000103307, gnorm=1.964, loss_scale=4, train_wall=11, gb_free=19.5, wall=5525
2022-12-09 16:30:24 | INFO | train_inner | epoch 086:    139 / 1102 loss=7.075, nll_loss=3.269, ppl=9.64, wps=33588.6, ups=9.26, wpb=3627.7, bsz=131.1, num_updates=93800, lr=0.000103252, gnorm=1.913, loss_scale=4, train_wall=11, gb_free=19.4, wall=5536
2022-12-09 16:30:34 | INFO | train_inner | epoch 086:    239 / 1102 loss=7.073, nll_loss=3.274, ppl=9.67, wps=32572.4, ups=9.2, wpb=3542.3, bsz=140.8, num_updates=93900, lr=0.000103197, gnorm=1.98, loss_scale=4, train_wall=11, gb_free=19.5, wall=5547
2022-12-09 16:30:45 | INFO | train_inner | epoch 086:    339 / 1102 loss=7.02, nll_loss=3.231, ppl=9.39, wps=32443.3, ups=9.13, wpb=3552.4, bsz=151.1, num_updates=94000, lr=0.000103142, gnorm=1.912, loss_scale=4, train_wall=11, gb_free=19.4, wall=5558
2022-12-09 16:30:56 | INFO | train_inner | epoch 086:    439 / 1102 loss=7.067, nll_loss=3.257, ppl=9.56, wps=32837.1, ups=9.15, wpb=3590.6, bsz=137.4, num_updates=94100, lr=0.000103087, gnorm=2.176, loss_scale=4, train_wall=11, gb_free=19.3, wall=5569
2022-12-09 16:31:07 | INFO | train_inner | epoch 086:    539 / 1102 loss=7.135, nll_loss=3.333, ppl=10.08, wps=32035.1, ups=9.29, wpb=3448.1, bsz=137.7, num_updates=94200, lr=0.000103033, gnorm=2.112, loss_scale=4, train_wall=11, gb_free=19.8, wall=5579
2022-12-09 16:31:18 | INFO | train_inner | epoch 086:    639 / 1102 loss=7.087, nll_loss=3.308, ppl=9.9, wps=32960.3, ups=9.12, wpb=3614.5, bsz=156.8, num_updates=94300, lr=0.000102978, gnorm=1.955, loss_scale=4, train_wall=11, gb_free=19.2, wall=5590
2022-12-09 16:31:29 | INFO | train_inner | epoch 086:    739 / 1102 loss=7.067, nll_loss=3.271, ppl=9.65, wps=32159, ups=8.97, wpb=3585.2, bsz=147.5, num_updates=94400, lr=0.000102923, gnorm=1.952, loss_scale=4, train_wall=11, gb_free=19.4, wall=5602
2022-12-09 16:31:40 | INFO | train_inner | epoch 086:    839 / 1102 loss=7.161, nll_loss=3.355, ppl=10.23, wps=33126.8, ups=9.26, wpb=3576.8, bsz=136.5, num_updates=94500, lr=0.000102869, gnorm=2.092, loss_scale=4, train_wall=11, gb_free=19.6, wall=5612
2022-12-09 16:31:51 | INFO | train_inner | epoch 086:    939 / 1102 loss=7.139, nll_loss=3.349, ppl=10.19, wps=32930.9, ups=9.15, wpb=3599.1, bsz=141.4, num_updates=94600, lr=0.000102815, gnorm=1.954, loss_scale=4, train_wall=11, gb_free=19.4, wall=5623
2022-12-09 16:32:02 | INFO | train_inner | epoch 086:   1039 / 1102 loss=7.121, nll_loss=3.345, ppl=10.16, wps=33523.2, ups=9.11, wpb=3679, bsz=144.3, num_updates=94700, lr=0.00010276, gnorm=1.903, loss_scale=4, train_wall=11, gb_free=19.7, wall=5634
2022-12-09 16:32:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:32:46 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 3.678 | nll_loss 2.136 | ppl 4.4 | bleu 37.1 | wps 4782.5 | wpb 2835.3 | bsz 115.6 | num_updates 94763 | best_bleu 37.1
2022-12-09 16:32:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 94763 updates
2022-12-09 16:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint86.pt
2022-12-09 16:32:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint86.pt (epoch 86 @ 94763 updates, score 37.1) (writing took 1.5520656062290072 seconds)
2022-12-09 16:32:48 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-12-09 16:32:48 | INFO | train | epoch 086 | loss 7.085 | nll_loss 3.293 | ppl 9.8 | wps 24707.6 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 94763 | lr 0.000102726 | gnorm 1.983 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 5680
2022-12-09 16:32:48 | INFO | fairseq.trainer | begin training epoch 87
2022-12-09 16:32:52 | INFO | train_inner | epoch 087:     37 / 1102 loss=7.003, nll_loss=3.238, ppl=9.44, wps=7138.9, ups=1.98, wpb=3605.7, bsz=162.7, num_updates=94800, lr=0.000102706, gnorm=1.84, loss_scale=4, train_wall=11, gb_free=19.5, wall=5685
2022-12-09 16:33:03 | INFO | train_inner | epoch 087:    137 / 1102 loss=7.012, nll_loss=3.214, ppl=9.28, wps=33107.3, ups=9.35, wpb=3540.6, bsz=150.1, num_updates=94900, lr=0.000102652, gnorm=1.926, loss_scale=4, train_wall=10, gb_free=19.5, wall=5695
2022-12-09 16:33:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-12-09 16:33:14 | INFO | train_inner | epoch 087:    238 / 1102 loss=6.995, nll_loss=3.211, ppl=9.26, wps=33281.1, ups=9.3, wpb=3577.7, bsz=159.4, num_updates=95000, lr=0.000102598, gnorm=1.916, loss_scale=2, train_wall=11, gb_free=20, wall=5706
2022-12-09 16:33:25 | INFO | train_inner | epoch 087:    338 / 1102 loss=7.017, nll_loss=3.215, ppl=9.28, wps=33025.7, ups=9.26, wpb=3566.6, bsz=141.2, num_updates=95100, lr=0.000102544, gnorm=2.052, loss_scale=2, train_wall=11, gb_free=19.3, wall=5717
2022-12-09 16:33:35 | INFO | train_inner | epoch 087:    438 / 1102 loss=7.089, nll_loss=3.281, ppl=9.72, wps=33420.1, ups=9.4, wpb=3554.3, bsz=141, num_updates=95200, lr=0.00010249, gnorm=1.978, loss_scale=2, train_wall=10, gb_free=19.9, wall=5728
2022-12-09 16:33:46 | INFO | train_inner | epoch 087:    538 / 1102 loss=7.069, nll_loss=3.269, ppl=9.64, wps=33191.1, ups=9.34, wpb=3554.5, bsz=141.7, num_updates=95300, lr=0.000102436, gnorm=2.044, loss_scale=2, train_wall=10, gb_free=19.5, wall=5738
2022-12-09 16:33:57 | INFO | train_inner | epoch 087:    638 / 1102 loss=7.072, nll_loss=3.28, ppl=9.71, wps=33464.4, ups=9.35, wpb=3579.1, bsz=150.7, num_updates=95400, lr=0.000102383, gnorm=1.922, loss_scale=2, train_wall=10, gb_free=19.8, wall=5749
2022-12-09 16:34:07 | INFO | train_inner | epoch 087:    738 / 1102 loss=7.025, nll_loss=3.246, ppl=9.49, wps=33743.9, ups=9.31, wpb=3624.4, bsz=157, num_updates=95500, lr=0.000102329, gnorm=1.87, loss_scale=2, train_wall=11, gb_free=19.6, wall=5760
2022-12-09 16:34:18 | INFO | train_inner | epoch 087:    838 / 1102 loss=7.159, nll_loss=3.372, ppl=10.35, wps=33178.4, ups=9.31, wpb=3562.2, bsz=141.3, num_updates=95600, lr=0.000102275, gnorm=1.985, loss_scale=2, train_wall=11, gb_free=19.3, wall=5771
2022-12-09 16:34:29 | INFO | train_inner | epoch 087:    938 / 1102 loss=7.188, nll_loss=3.382, ppl=10.43, wps=33226.1, ups=9.27, wpb=3584.6, bsz=130.5, num_updates=95700, lr=0.000102222, gnorm=2.109, loss_scale=2, train_wall=11, gb_free=19.5, wall=5781
2022-12-09 16:34:40 | INFO | train_inner | epoch 087:   1038 / 1102 loss=7.148, nll_loss=3.355, ppl=10.23, wps=33349.7, ups=9.28, wpb=3592.7, bsz=144.1, num_updates=95800, lr=0.000102169, gnorm=1.985, loss_scale=2, train_wall=11, gb_free=19.8, wall=5792
2022-12-09 16:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:35:25 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 3.679 | nll_loss 2.137 | ppl 4.4 | bleu 37.01 | wps 4696 | wpb 2835.3 | bsz 115.6 | num_updates 95864 | best_bleu 37.1
2022-12-09 16:35:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 95864 updates
2022-12-09 16:35:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint87.pt
2022-12-09 16:35:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint87.pt (epoch 87 @ 95864 updates, score 37.01) (writing took 1.186847485601902 seconds)
2022-12-09 16:35:26 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-12-09 16:35:26 | INFO | train | epoch 087 | loss 7.079 | nll_loss 3.286 | ppl 9.75 | wps 24907.1 | ups 6.95 | wpb 3583.6 | bsz 145.5 | num_updates 95864 | lr 0.000102134 | gnorm 1.971 | loss_scale 2 | train_wall 116 | gb_free 19.3 | wall 5839
2022-12-09 16:35:27 | INFO | fairseq.trainer | begin training epoch 88
2022-12-09 16:35:31 | INFO | train_inner | epoch 088:     36 / 1102 loss=7.145, nll_loss=3.353, ppl=10.22, wps=7198.5, ups=1.97, wpb=3663.2, bsz=137.3, num_updates=95900, lr=0.000102115, gnorm=1.963, loss_scale=2, train_wall=11, gb_free=19.3, wall=5843
2022-12-09 16:35:41 | INFO | train_inner | epoch 088:    136 / 1102 loss=7.061, nll_loss=3.253, ppl=9.53, wps=32984.2, ups=9.25, wpb=3567.7, bsz=127.8, num_updates=96000, lr=0.000102062, gnorm=1.941, loss_scale=2, train_wall=11, gb_free=19.8, wall=5854
2022-12-09 16:35:52 | INFO | train_inner | epoch 088:    236 / 1102 loss=7.071, nll_loss=3.267, ppl=9.62, wps=32910.5, ups=9.28, wpb=3546.9, bsz=144.3, num_updates=96100, lr=0.000102009, gnorm=2.049, loss_scale=2, train_wall=11, gb_free=19.9, wall=5865
2022-12-09 16:36:03 | INFO | train_inner | epoch 088:    336 / 1102 loss=7.071, nll_loss=3.281, ppl=9.72, wps=33383.5, ups=9.18, wpb=3637.8, bsz=143.7, num_updates=96200, lr=0.000101956, gnorm=1.87, loss_scale=2, train_wall=11, gb_free=20.1, wall=5875
2022-12-09 16:36:14 | INFO | train_inner | epoch 088:    436 / 1102 loss=7.031, nll_loss=3.243, ppl=9.47, wps=32846, ups=9.25, wpb=3552.1, bsz=157.4, num_updates=96300, lr=0.000101903, gnorm=1.957, loss_scale=2, train_wall=11, gb_free=19.3, wall=5886
2022-12-09 16:36:25 | INFO | train_inner | epoch 088:    536 / 1102 loss=7.001, nll_loss=3.212, ppl=9.27, wps=32765.2, ups=9.16, wpb=3576.1, bsz=159.2, num_updates=96400, lr=0.00010185, gnorm=1.979, loss_scale=2, train_wall=11, gb_free=19.5, wall=5897
2022-12-09 16:36:36 | INFO | train_inner | epoch 088:    636 / 1102 loss=7.133, nll_loss=3.331, ppl=10.07, wps=32958.9, ups=9.13, wpb=3610, bsz=138.4, num_updates=96500, lr=0.000101797, gnorm=1.983, loss_scale=2, train_wall=11, gb_free=19.7, wall=5908
2022-12-09 16:36:47 | INFO | train_inner | epoch 088:    736 / 1102 loss=7.075, nll_loss=3.278, ppl=9.7, wps=32949.1, ups=9.17, wpb=3594.8, bsz=141, num_updates=96600, lr=0.000101745, gnorm=1.939, loss_scale=2, train_wall=11, gb_free=20, wall=5919
2022-12-09 16:36:57 | INFO | train_inner | epoch 088:    836 / 1102 loss=7.093, nll_loss=3.291, ppl=9.79, wps=32558.6, ups=9.28, wpb=3510.2, bsz=137.4, num_updates=96700, lr=0.000101692, gnorm=1.992, loss_scale=2, train_wall=11, gb_free=19.4, wall=5930
2022-12-09 16:37:08 | INFO | train_inner | epoch 088:    936 / 1102 loss=7.106, nll_loss=3.309, ppl=9.91, wps=33036.7, ups=9.21, wpb=3587.9, bsz=140.6, num_updates=96800, lr=0.000101639, gnorm=1.989, loss_scale=2, train_wall=11, gb_free=19.7, wall=5941
2022-12-09 16:37:19 | INFO | train_inner | epoch 088:   1036 / 1102 loss=7.114, nll_loss=3.332, ppl=10.07, wps=33406.2, ups=9.17, wpb=3641.2, bsz=151.5, num_updates=96900, lr=0.000101587, gnorm=1.908, loss_scale=2, train_wall=11, gb_free=19.7, wall=5952
2022-12-09 16:37:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:38:05 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 3.681 | nll_loss 2.138 | ppl 4.4 | bleu 36.9 | wps 4663.9 | wpb 2835.3 | bsz 115.6 | num_updates 96966 | best_bleu 37.1
2022-12-09 16:38:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 96966 updates
2022-12-09 16:38:06 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint88.pt
2022-12-09 16:38:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint88.pt (epoch 88 @ 96966 updates, score 36.9) (writing took 1.1872932640835643 seconds)
2022-12-09 16:38:06 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-12-09 16:38:06 | INFO | train | epoch 088 | loss 7.071 | nll_loss 3.277 | ppl 9.7 | wps 24684.9 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 96966 | lr 0.000101552 | gnorm 1.957 | loss_scale 2 | train_wall 117 | gb_free 19.7 | wall 5999
2022-12-09 16:38:06 | INFO | fairseq.trainer | begin training epoch 89
2022-12-09 16:38:10 | INFO | train_inner | epoch 089:     34 / 1102 loss=6.997, nll_loss=3.216, ppl=9.3, wps=7007.6, ups=1.96, wpb=3581.2, bsz=157.4, num_updates=97000, lr=0.000101535, gnorm=1.96, loss_scale=2, train_wall=11, gb_free=19.6, wall=6003
2022-12-09 16:38:21 | INFO | train_inner | epoch 089:    134 / 1102 loss=6.991, nll_loss=3.208, ppl=9.24, wps=33083, ups=9.13, wpb=3624.6, bsz=155.9, num_updates=97100, lr=0.000101482, gnorm=1.964, loss_scale=2, train_wall=11, gb_free=19.8, wall=6014
2022-12-09 16:38:32 | INFO | train_inner | epoch 089:    234 / 1102 loss=7.004, nll_loss=3.204, ppl=9.21, wps=32959.2, ups=9.12, wpb=3615.5, bsz=149.3, num_updates=97200, lr=0.00010143, gnorm=1.896, loss_scale=2, train_wall=11, gb_free=19.2, wall=6025
2022-12-09 16:38:43 | INFO | train_inner | epoch 089:    334 / 1102 loss=7.074, nll_loss=3.254, ppl=9.54, wps=32773.8, ups=9.34, wpb=3509.3, bsz=124.7, num_updates=97300, lr=0.000101378, gnorm=2.033, loss_scale=2, train_wall=10, gb_free=19.3, wall=6035
2022-12-09 16:38:54 | INFO | train_inner | epoch 089:    434 / 1102 loss=7.052, nll_loss=3.259, ppl=9.57, wps=33524.6, ups=9.22, wpb=3635.3, bsz=147, num_updates=97400, lr=0.000101326, gnorm=1.967, loss_scale=2, train_wall=11, gb_free=20.1, wall=6046
2022-12-09 16:39:05 | INFO | train_inner | epoch 089:    534 / 1102 loss=7.044, nll_loss=3.266, ppl=9.62, wps=33403.4, ups=9.16, wpb=3646.2, bsz=152.6, num_updates=97500, lr=0.000101274, gnorm=1.935, loss_scale=2, train_wall=11, gb_free=19.7, wall=6057
2022-12-09 16:39:16 | INFO | train_inner | epoch 089:    634 / 1102 loss=7.148, nll_loss=3.323, ppl=10.01, wps=31743.1, ups=9.04, wpb=3510.4, bsz=126.7, num_updates=97600, lr=0.000101222, gnorm=2.102, loss_scale=2, train_wall=11, gb_free=19.4, wall=6068
2022-12-09 16:39:27 | INFO | train_inner | epoch 089:    734 / 1102 loss=7.025, nll_loss=3.252, ppl=9.53, wps=32835.4, ups=9.02, wpb=3641.6, bsz=161.1, num_updates=97700, lr=0.00010117, gnorm=1.88, loss_scale=2, train_wall=11, gb_free=19.5, wall=6079
2022-12-09 16:39:38 | INFO | train_inner | epoch 089:    834 / 1102 loss=7.158, nll_loss=3.373, ppl=10.36, wps=32649, ups=9.05, wpb=3607.4, bsz=144.6, num_updates=97800, lr=0.000101118, gnorm=1.996, loss_scale=2, train_wall=11, gb_free=19.3, wall=6090
2022-12-09 16:39:49 | INFO | train_inner | epoch 089:    934 / 1102 loss=7.127, nll_loss=3.318, ppl=9.97, wps=31946.4, ups=9.16, wpb=3488.7, bsz=134.5, num_updates=97900, lr=0.000101067, gnorm=2.133, loss_scale=2, train_wall=11, gb_free=19.5, wall=6101
2022-12-09 16:40:00 | INFO | train_inner | epoch 089:   1034 / 1102 loss=7.095, nll_loss=3.31, ppl=9.92, wps=32440.6, ups=9.16, wpb=3541.2, bsz=155.5, num_updates=98000, lr=0.000101015, gnorm=1.955, loss_scale=2, train_wall=11, gb_free=19.3, wall=6112
2022-12-09 16:40:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:40:48 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 3.677 | nll_loss 2.135 | ppl 4.39 | bleu 36.92 | wps 4456.2 | wpb 2835.3 | bsz 115.6 | num_updates 98068 | best_bleu 37.1
2022-12-09 16:40:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 98068 updates
2022-12-09 16:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint89.pt
2022-12-09 16:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint89.pt (epoch 89 @ 98068 updates, score 36.92) (writing took 1.3655280685052276 seconds)
2022-12-09 16:40:49 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-12-09 16:40:49 | INFO | train | epoch 089 | loss 7.068 | nll_loss 3.272 | ppl 9.66 | wps 24275.3 | ups 6.77 | wpb 3583.6 | bsz 145.4 | num_updates 98068 | lr 0.00010098 | gnorm 1.986 | loss_scale 2 | train_wall 118 | gb_free 19.8 | wall 6161
2022-12-09 16:40:49 | INFO | fairseq.trainer | begin training epoch 90
2022-12-09 16:40:53 | INFO | train_inner | epoch 090:     32 / 1102 loss=7.089, nll_loss=3.3, ppl=9.85, wps=6782.5, ups=1.89, wpb=3597.3, bsz=149, num_updates=98100, lr=0.000100964, gnorm=1.979, loss_scale=2, train_wall=11, gb_free=21.3, wall=6165
2022-12-09 16:41:03 | INFO | train_inner | epoch 090:    132 / 1102 loss=7.064, nll_loss=3.259, ppl=9.57, wps=33578.9, ups=9.38, wpb=3578.9, bsz=137.9, num_updates=98200, lr=0.000100912, gnorm=1.979, loss_scale=2, train_wall=10, gb_free=19.7, wall=6176
2022-12-09 16:41:14 | INFO | train_inner | epoch 090:    232 / 1102 loss=6.957, nll_loss=3.172, ppl=9.01, wps=33723.6, ups=9.41, wpb=3585.6, bsz=163.4, num_updates=98300, lr=0.000100861, gnorm=1.895, loss_scale=2, train_wall=10, gb_free=19.5, wall=6186
2022-12-09 16:41:25 | INFO | train_inner | epoch 090:    332 / 1102 loss=7.019, nll_loss=3.215, ppl=9.29, wps=32855, ups=9.29, wpb=3537.2, bsz=144.1, num_updates=98400, lr=0.00010081, gnorm=1.968, loss_scale=2, train_wall=11, gb_free=19.3, wall=6197
2022-12-09 16:41:36 | INFO | train_inner | epoch 090:    432 / 1102 loss=7.093, nll_loss=3.274, ppl=9.67, wps=33101.5, ups=9.29, wpb=3564.6, bsz=132.6, num_updates=98500, lr=0.000100759, gnorm=2.078, loss_scale=2, train_wall=11, gb_free=19.7, wall=6208
2022-12-09 16:41:46 | INFO | train_inner | epoch 090:    532 / 1102 loss=7.056, nll_loss=3.25, ppl=9.52, wps=32664.9, ups=9.26, wpb=3526.1, bsz=140.5, num_updates=98600, lr=0.000100707, gnorm=1.989, loss_scale=2, train_wall=11, gb_free=19.3, wall=6219
2022-12-09 16:41:57 | INFO | train_inner | epoch 090:    632 / 1102 loss=7.062, nll_loss=3.26, ppl=9.58, wps=32801.3, ups=9.09, wpb=3609.7, bsz=141.2, num_updates=98700, lr=0.000100656, gnorm=1.997, loss_scale=2, train_wall=11, gb_free=19.3, wall=6230
2022-12-09 16:42:08 | INFO | train_inner | epoch 090:    732 / 1102 loss=7.07, nll_loss=3.268, ppl=9.63, wps=32990.5, ups=9.17, wpb=3597.1, bsz=140.6, num_updates=98800, lr=0.000100605, gnorm=2.109, loss_scale=2, train_wall=11, gb_free=19.4, wall=6241
2022-12-09 16:42:19 | INFO | train_inner | epoch 090:    832 / 1102 loss=7.101, nll_loss=3.292, ppl=9.8, wps=32421.7, ups=9.17, wpb=3536.7, bsz=142.1, num_updates=98900, lr=0.000100555, gnorm=2.048, loss_scale=2, train_wall=11, gb_free=19.8, wall=6252
2022-12-09 16:42:30 | INFO | train_inner | epoch 090:    932 / 1102 loss=7.048, nll_loss=3.283, ppl=9.73, wps=33307.7, ups=9.07, wpb=3672.3, bsz=160.6, num_updates=99000, lr=0.000100504, gnorm=1.916, loss_scale=2, train_wall=11, gb_free=19.5, wall=6263
2022-12-09 16:42:41 | INFO | train_inner | epoch 090:   1032 / 1102 loss=7.101, nll_loss=3.324, ppl=10.02, wps=32957.1, ups=9.02, wpb=3654, bsz=148.8, num_updates=99100, lr=0.000100453, gnorm=1.957, loss_scale=2, train_wall=11, gb_free=19.4, wall=6274
2022-12-09 16:42:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:43:29 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 3.678 | nll_loss 2.135 | ppl 4.39 | bleu 37.01 | wps 4507.9 | wpb 2835.3 | bsz 115.6 | num_updates 99170 | best_bleu 37.1
2022-12-09 16:43:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 99170 updates
2022-12-09 16:43:30 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint90.pt
2022-12-09 16:43:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint90.pt (epoch 90 @ 99170 updates, score 37.01) (writing took 1.1919468007981777 seconds)
2022-12-09 16:43:30 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-12-09 16:43:30 | INFO | train | epoch 090 | loss 7.062 | nll_loss 3.265 | ppl 9.62 | wps 24483.3 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 99170 | lr 0.000100418 | gnorm 1.997 | loss_scale 2 | train_wall 117 | gb_free 19.7 | wall 6323
2022-12-09 16:43:30 | INFO | fairseq.trainer | begin training epoch 91
2022-12-09 16:43:34 | INFO | train_inner | epoch 091:     30 / 1102 loss=7.075, nll_loss=3.285, ppl=9.75, wps=6835.2, ups=1.9, wpb=3593.7, bsz=147.4, num_updates=99200, lr=0.000100402, gnorm=1.991, loss_scale=2, train_wall=11, gb_free=19.4, wall=6326
2022-12-09 16:43:45 | INFO | train_inner | epoch 091:    130 / 1102 loss=6.997, nll_loss=3.196, ppl=9.17, wps=32083.7, ups=9.29, wpb=3452.1, bsz=146.1, num_updates=99300, lr=0.000100352, gnorm=1.945, loss_scale=2, train_wall=11, gb_free=19.6, wall=6337
2022-12-09 16:43:56 | INFO | train_inner | epoch 091:    230 / 1102 loss=6.984, nll_loss=3.198, ppl=9.18, wps=33102.3, ups=9.22, wpb=3589.9, bsz=166.9, num_updates=99400, lr=0.000100301, gnorm=1.953, loss_scale=2, train_wall=11, gb_free=19.5, wall=6348
2022-12-09 16:44:06 | INFO | train_inner | epoch 091:    330 / 1102 loss=7.012, nll_loss=3.202, ppl=9.2, wps=33319.1, ups=9.28, wpb=3589.9, bsz=136.8, num_updates=99500, lr=0.000100251, gnorm=1.988, loss_scale=2, train_wall=11, gb_free=19.3, wall=6359
2022-12-09 16:44:17 | INFO | train_inner | epoch 091:    430 / 1102 loss=7.028, nll_loss=3.234, ppl=9.41, wps=33423.9, ups=9.33, wpb=3582.2, bsz=152.2, num_updates=99600, lr=0.000100201, gnorm=1.942, loss_scale=2, train_wall=10, gb_free=19.4, wall=6369
2022-12-09 16:44:28 | INFO | train_inner | epoch 091:    530 / 1102 loss=7.132, nll_loss=3.326, ppl=10.03, wps=33733.8, ups=9.23, wpb=3654.7, bsz=132.2, num_updates=99700, lr=0.00010015, gnorm=1.966, loss_scale=2, train_wall=11, gb_free=19.5, wall=6380
2022-12-09 16:44:39 | INFO | train_inner | epoch 091:    630 / 1102 loss=7.052, nll_loss=3.257, ppl=9.56, wps=33111, ups=9.21, wpb=3593.4, bsz=149.5, num_updates=99800, lr=0.0001001, gnorm=1.919, loss_scale=2, train_wall=11, gb_free=19.5, wall=6391
2022-12-09 16:44:50 | INFO | train_inner | epoch 091:    730 / 1102 loss=7.084, nll_loss=3.284, ppl=9.74, wps=33126.1, ups=9.23, wpb=3587.2, bsz=134.5, num_updates=99900, lr=0.00010005, gnorm=1.943, loss_scale=2, train_wall=11, gb_free=19.6, wall=6402
2022-12-09 16:45:00 | INFO | train_inner | epoch 091:    830 / 1102 loss=7.103, nll_loss=3.311, ppl=9.93, wps=33470.7, ups=9.23, wpb=3626.4, bsz=145, num_updates=100000, lr=0.0001, gnorm=2.004, loss_scale=2, train_wall=11, gb_free=19.2, wall=6413
2022-12-09 16:45:11 | INFO | train_inner | epoch 091:    930 / 1102 loss=7.075, nll_loss=3.265, ppl=9.62, wps=32412.4, ups=9.2, wpb=3524.5, bsz=137.6, num_updates=100100, lr=9.995e-05, gnorm=2.078, loss_scale=2, train_wall=11, gb_free=19.3, wall=6424
2022-12-09 16:45:22 | INFO | train_inner | epoch 091:   1030 / 1102 loss=7.082, nll_loss=3.304, ppl=9.87, wps=33242.3, ups=9.08, wpb=3662.9, bsz=154.1, num_updates=100200, lr=9.99001e-05, gnorm=1.96, loss_scale=2, train_wall=11, gb_free=19.3, wall=6435
2022-12-09 16:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:46:10 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 3.676 | nll_loss 2.134 | ppl 4.39 | bleu 37.1 | wps 4593.9 | wpb 2835.3 | bsz 115.6 | num_updates 100272 | best_bleu 37.1
2022-12-09 16:46:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 100272 updates
2022-12-09 16:46:10 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint91.pt
2022-12-09 16:46:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint91.pt (epoch 91 @ 100272 updates, score 37.1) (writing took 1.6650877203792334 seconds)
2022-12-09 16:46:11 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-12-09 16:46:11 | INFO | train | epoch 091 | loss 7.057 | nll_loss 3.259 | ppl 9.58 | wps 24555 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 100272 | lr 9.98643e-05 | gnorm 1.975 | loss_scale 2 | train_wall 117 | gb_free 19.1 | wall 6484
2022-12-09 16:46:11 | INFO | fairseq.trainer | begin training epoch 92
2022-12-09 16:46:15 | INFO | train_inner | epoch 092:     28 / 1102 loss=7.073, nll_loss=3.274, ppl=9.68, wps=6736.4, ups=1.91, wpb=3519.1, bsz=147.2, num_updates=100300, lr=9.98503e-05, gnorm=2.045, loss_scale=2, train_wall=11, gb_free=19.6, wall=6487
2022-12-09 16:46:25 | INFO | train_inner | epoch 092:    128 / 1102 loss=7.094, nll_loss=3.276, ppl=9.68, wps=32946.8, ups=9.28, wpb=3550.9, bsz=128.9, num_updates=100400, lr=9.98006e-05, gnorm=1.98, loss_scale=2, train_wall=11, gb_free=19.6, wall=6498
2022-12-09 16:46:36 | INFO | train_inner | epoch 092:    228 / 1102 loss=7.057, nll_loss=3.249, ppl=9.51, wps=33424, ups=9.3, wpb=3593.8, bsz=142.4, num_updates=100500, lr=9.97509e-05, gnorm=2.064, loss_scale=2, train_wall=11, gb_free=19.5, wall=6508
2022-12-09 16:46:47 | INFO | train_inner | epoch 092:    328 / 1102 loss=6.974, nll_loss=3.179, ppl=9.06, wps=33021.7, ups=9.25, wpb=3570.2, bsz=159.6, num_updates=100600, lr=9.97013e-05, gnorm=1.952, loss_scale=2, train_wall=11, gb_free=19.7, wall=6519
2022-12-09 16:46:58 | INFO | train_inner | epoch 092:    428 / 1102 loss=7.035, nll_loss=3.229, ppl=9.38, wps=33088.7, ups=9.27, wpb=3570.8, bsz=140.3, num_updates=100700, lr=9.96518e-05, gnorm=1.986, loss_scale=2, train_wall=11, gb_free=19.9, wall=6530
2022-12-09 16:47:09 | INFO | train_inner | epoch 092:    528 / 1102 loss=7.074, nll_loss=3.268, ppl=9.63, wps=33085.4, ups=9.14, wpb=3618.4, bsz=135.6, num_updates=100800, lr=9.96024e-05, gnorm=2.046, loss_scale=2, train_wall=11, gb_free=19.4, wall=6541
2022-12-09 16:47:20 | INFO | train_inner | epoch 092:    628 / 1102 loss=7.043, nll_loss=3.251, ppl=9.52, wps=32987.5, ups=9.16, wpb=3599.7, bsz=153.9, num_updates=100900, lr=9.9553e-05, gnorm=1.93, loss_scale=2, train_wall=11, gb_free=19.5, wall=6552
2022-12-09 16:47:30 | INFO | train_inner | epoch 092:    728 / 1102 loss=7.128, nll_loss=3.327, ppl=10.03, wps=33324, ups=9.18, wpb=3629.8, bsz=134.4, num_updates=101000, lr=9.95037e-05, gnorm=1.998, loss_scale=2, train_wall=11, gb_free=19.8, wall=6563
2022-12-09 16:47:41 | INFO | train_inner | epoch 092:    828 / 1102 loss=7.065, nll_loss=3.283, ppl=9.73, wps=33035, ups=9.22, wpb=3582.8, bsz=155.9, num_updates=101100, lr=9.94545e-05, gnorm=1.958, loss_scale=2, train_wall=11, gb_free=19.9, wall=6574
2022-12-09 16:47:52 | INFO | train_inner | epoch 092:    928 / 1102 loss=6.963, nll_loss=3.183, ppl=9.08, wps=32992, ups=9.13, wpb=3614.6, bsz=165.1, num_updates=101200, lr=9.94053e-05, gnorm=1.949, loss_scale=2, train_wall=11, gb_free=19.6, wall=6585
2022-12-09 16:48:03 | INFO | train_inner | epoch 092:   1028 / 1102 loss=7.127, nll_loss=3.313, ppl=9.94, wps=32163.3, ups=9.25, wpb=3478.9, bsz=131.6, num_updates=101300, lr=9.93563e-05, gnorm=2.091, loss_scale=2, train_wall=11, gb_free=20, wall=6595
2022-12-09 16:48:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:48:50 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 3.676 | nll_loss 2.134 | ppl 4.39 | bleu 36.93 | wps 4690.3 | wpb 2835.3 | bsz 115.6 | num_updates 101374 | best_bleu 37.1
2022-12-09 16:48:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 101374 updates
2022-12-09 16:48:51 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint92.pt
2022-12-09 16:48:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint92.pt (epoch 92 @ 101374 updates, score 36.93) (writing took 1.2410179218277335 seconds)
2022-12-09 16:48:51 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-12-09 16:48:51 | INFO | train | epoch 092 | loss 7.052 | nll_loss 3.253 | ppl 9.53 | wps 24717.2 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 101374 | lr 9.932e-05 | gnorm 1.993 | loss_scale 2 | train_wall 117 | gb_free 19.8 | wall 6643
2022-12-09 16:48:51 | INFO | fairseq.trainer | begin training epoch 93
2022-12-09 16:48:54 | INFO | train_inner | epoch 093:     26 / 1102 loss=7.008, nll_loss=3.218, ppl=9.31, wps=7076.9, ups=1.96, wpb=3606, bsz=149.7, num_updates=101400, lr=9.93073e-05, gnorm=1.988, loss_scale=2, train_wall=11, gb_free=19.4, wall=6646
2022-12-09 16:49:05 | INFO | train_inner | epoch 093:    126 / 1102 loss=6.956, nll_loss=3.172, ppl=9.02, wps=33748.7, ups=9.43, wpb=3579.9, bsz=153.9, num_updates=101500, lr=9.92583e-05, gnorm=1.913, loss_scale=2, train_wall=10, gb_free=19.4, wall=6657
2022-12-09 16:49:15 | INFO | train_inner | epoch 093:    226 / 1102 loss=6.981, nll_loss=3.181, ppl=9.07, wps=33568.9, ups=9.35, wpb=3590.1, bsz=148, num_updates=101600, lr=9.92095e-05, gnorm=1.947, loss_scale=2, train_wall=10, gb_free=19.4, wall=6668
2022-12-09 16:49:26 | INFO | train_inner | epoch 093:    326 / 1102 loss=6.992, nll_loss=3.192, ppl=9.14, wps=33095.8, ups=9.24, wpb=3581.2, bsz=150.2, num_updates=101700, lr=9.91607e-05, gnorm=1.998, loss_scale=2, train_wall=11, gb_free=19.3, wall=6678
2022-12-09 16:49:37 | INFO | train_inner | epoch 093:    426 / 1102 loss=7.001, nll_loss=3.207, ppl=9.24, wps=33774.8, ups=9.34, wpb=3616.9, bsz=147.3, num_updates=101800, lr=9.9112e-05, gnorm=1.922, loss_scale=2, train_wall=10, gb_free=19.5, wall=6689
2022-12-09 16:49:48 | INFO | train_inner | epoch 093:    526 / 1102 loss=7.029, nll_loss=3.225, ppl=9.35, wps=32364.2, ups=9.21, wpb=3515, bsz=148.2, num_updates=101900, lr=9.90633e-05, gnorm=2.09, loss_scale=2, train_wall=11, gb_free=19.5, wall=6700
2022-12-09 16:49:58 | INFO | train_inner | epoch 093:    626 / 1102 loss=7.169, nll_loss=3.35, ppl=10.2, wps=32903.8, ups=9.28, wpb=3547.6, bsz=123, num_updates=102000, lr=9.90148e-05, gnorm=2.062, loss_scale=2, train_wall=11, gb_free=19.6, wall=6711
2022-12-09 16:50:09 | INFO | train_inner | epoch 093:    726 / 1102 loss=7.027, nll_loss=3.221, ppl=9.33, wps=32933.6, ups=9.12, wpb=3612.9, bsz=152.4, num_updates=102100, lr=9.89663e-05, gnorm=2.022, loss_scale=2, train_wall=11, gb_free=19.5, wall=6722
2022-12-09 16:50:20 | INFO | train_inner | epoch 093:    826 / 1102 loss=7.103, nll_loss=3.297, ppl=9.83, wps=33258.8, ups=9.27, wpb=3586.6, bsz=131.2, num_updates=102200, lr=9.89178e-05, gnorm=2.011, loss_scale=2, train_wall=11, gb_free=19.8, wall=6733
2022-12-09 16:50:31 | INFO | train_inner | epoch 093:    926 / 1102 loss=7.028, nll_loss=3.239, ppl=9.44, wps=33137.7, ups=9.02, wpb=3672.5, bsz=152.9, num_updates=102300, lr=9.88695e-05, gnorm=1.875, loss_scale=2, train_wall=11, gb_free=19.4, wall=6744
2022-12-09 16:50:42 | INFO | train_inner | epoch 093:   1026 / 1102 loss=7.103, nll_loss=3.316, ppl=9.96, wps=32982.7, ups=9.14, wpb=3610.5, bsz=150.6, num_updates=102400, lr=9.88212e-05, gnorm=1.969, loss_scale=2, train_wall=11, gb_free=19.4, wall=6755
2022-12-09 16:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:51:31 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 3.673 | nll_loss 2.132 | ppl 4.38 | bleu 36.92 | wps 4472.4 | wpb 2835.3 | bsz 115.6 | num_updates 102476 | best_bleu 37.1
2022-12-09 16:51:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 102476 updates
2022-12-09 16:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint93.pt
2022-12-09 16:51:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint93.pt (epoch 93 @ 102476 updates, score 36.92) (writing took 1.1464523496106267 seconds)
2022-12-09 16:51:32 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-12-09 16:51:32 | INFO | train | epoch 093 | loss 7.045 | nll_loss 3.245 | ppl 9.48 | wps 24509.2 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 102476 | lr 9.87845e-05 | gnorm 1.998 | loss_scale 2 | train_wall 117 | gb_free 19.5 | wall 6804
2022-12-09 16:51:32 | INFO | fairseq.trainer | begin training epoch 94
2022-12-09 16:51:35 | INFO | train_inner | epoch 094:     24 / 1102 loss=7.095, nll_loss=3.29, ppl=9.78, wps=6672.9, ups=1.9, wpb=3520.4, bsz=144.3, num_updates=102500, lr=9.8773e-05, gnorm=2.135, loss_scale=2, train_wall=11, gb_free=19.4, wall=6807
2022-12-09 16:51:46 | INFO | train_inner | epoch 094:    124 / 1102 loss=6.969, nll_loss=3.184, ppl=9.09, wps=32923.2, ups=9.1, wpb=3618.4, bsz=160.6, num_updates=102600, lr=9.87248e-05, gnorm=1.92, loss_scale=2, train_wall=11, gb_free=19.5, wall=6818
2022-12-09 16:51:57 | INFO | train_inner | epoch 094:    224 / 1102 loss=7.017, nll_loss=3.206, ppl=9.22, wps=32770.9, ups=9.05, wpb=3622.4, bsz=140.8, num_updates=102700, lr=9.86767e-05, gnorm=1.913, loss_scale=2, train_wall=11, gb_free=19.6, wall=6829
2022-12-09 16:52:08 | INFO | train_inner | epoch 094:    324 / 1102 loss=7.04, nll_loss=3.247, ppl=9.49, wps=33520.5, ups=9.05, wpb=3705, bsz=148.1, num_updates=102800, lr=9.86287e-05, gnorm=1.883, loss_scale=2, train_wall=11, gb_free=19.5, wall=6840
2022-12-09 16:52:19 | INFO | train_inner | epoch 094:    424 / 1102 loss=7.006, nll_loss=3.203, ppl=9.21, wps=32996.3, ups=9.17, wpb=3600, bsz=149, num_updates=102900, lr=9.85808e-05, gnorm=2.014, loss_scale=2, train_wall=11, gb_free=19.6, wall=6851
2022-12-09 16:52:30 | INFO | train_inner | epoch 094:    524 / 1102 loss=7.009, nll_loss=3.221, ppl=9.33, wps=32818.7, ups=9.1, wpb=3606.3, bsz=160.6, num_updates=103000, lr=9.85329e-05, gnorm=1.912, loss_scale=2, train_wall=11, gb_free=19.4, wall=6862
2022-12-09 16:52:41 | INFO | train_inner | epoch 094:    624 / 1102 loss=7.062, nll_loss=3.253, ppl=9.53, wps=32849.3, ups=9.17, wpb=3581.5, bsz=135.5, num_updates=103100, lr=9.84851e-05, gnorm=2.051, loss_scale=2, train_wall=11, gb_free=19.5, wall=6873
2022-12-09 16:52:52 | INFO | train_inner | epoch 094:    724 / 1102 loss=7.109, nll_loss=3.292, ppl=9.79, wps=32439.8, ups=9.17, wpb=3539.4, bsz=129, num_updates=103200, lr=9.84374e-05, gnorm=2.037, loss_scale=2, train_wall=11, gb_free=19.8, wall=6884
2022-12-09 16:53:03 | INFO | train_inner | epoch 094:    824 / 1102 loss=7.104, nll_loss=3.308, ppl=9.91, wps=32220, ups=9.05, wpb=3560, bsz=149.4, num_updates=103300, lr=9.83897e-05, gnorm=1.996, loss_scale=2, train_wall=11, gb_free=19.5, wall=6895
2022-12-09 16:53:14 | INFO | train_inner | epoch 094:    924 / 1102 loss=7.103, nll_loss=3.294, ppl=9.81, wps=32788.1, ups=9.25, wpb=3545.3, bsz=135, num_updates=103400, lr=9.83422e-05, gnorm=2.029, loss_scale=2, train_wall=11, gb_free=19.5, wall=6906
2022-12-09 16:53:25 | INFO | train_inner | epoch 094:   1024 / 1102 loss=7.033, nll_loss=3.227, ppl=9.36, wps=32206.2, ups=9.11, wpb=3536, bsz=143.4, num_updates=103500, lr=9.82946e-05, gnorm=2.055, loss_scale=2, train_wall=11, gb_free=19.5, wall=6917
2022-12-09 16:53:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:54:11 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 3.679 | nll_loss 2.134 | ppl 4.39 | bleu 36.96 | wps 4827.8 | wpb 2835.3 | bsz 115.6 | num_updates 103578 | best_bleu 37.1
2022-12-09 16:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 103578 updates
2022-12-09 16:54:12 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint94.pt
2022-12-09 16:54:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint94.pt (epoch 94 @ 103578 updates, score 36.96) (writing took 1.2894133208319545 seconds)
2022-12-09 16:54:12 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-12-09 16:54:12 | INFO | train | epoch 094 | loss 7.04 | nll_loss 3.239 | ppl 9.44 | wps 24701.8 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 103578 | lr 9.82576e-05 | gnorm 1.983 | loss_scale 2 | train_wall 118 | gb_free 19.5 | wall 6964
2022-12-09 16:54:12 | INFO | fairseq.trainer | begin training epoch 95
2022-12-09 16:54:15 | INFO | train_inner | epoch 095:     22 / 1102 loss=7.02, nll_loss=3.215, ppl=9.29, wps=7034.4, ups=2, wpb=3517.7, bsz=144.1, num_updates=103600, lr=9.82472e-05, gnorm=2.025, loss_scale=2, train_wall=11, gb_free=19.4, wall=6967
2022-12-09 16:54:26 | INFO | train_inner | epoch 095:    122 / 1102 loss=6.963, nll_loss=3.161, ppl=8.94, wps=32531, ups=9.09, wpb=3578.8, bsz=145.9, num_updates=103700, lr=9.81998e-05, gnorm=1.905, loss_scale=2, train_wall=11, gb_free=19.4, wall=6978
2022-12-09 16:54:37 | INFO | train_inner | epoch 095:    222 / 1102 loss=7.003, nll_loss=3.189, ppl=9.12, wps=32539.5, ups=9.22, wpb=3529.4, bsz=137.8, num_updates=103800, lr=9.81525e-05, gnorm=2.032, loss_scale=2, train_wall=11, gb_free=19.4, wall=6989
2022-12-09 16:54:47 | INFO | train_inner | epoch 095:    322 / 1102 loss=7.111, nll_loss=3.291, ppl=9.79, wps=32648, ups=9.16, wpb=3565.2, bsz=129.9, num_updates=103900, lr=9.81052e-05, gnorm=2.044, loss_scale=2, train_wall=11, gb_free=19.6, wall=7000
2022-12-09 16:54:58 | INFO | train_inner | epoch 095:    422 / 1102 loss=7.037, nll_loss=3.236, ppl=9.42, wps=32584.4, ups=9.04, wpb=3603.5, bsz=143.8, num_updates=104000, lr=9.80581e-05, gnorm=1.988, loss_scale=2, train_wall=11, gb_free=19.4, wall=7011
2022-12-09 16:55:09 | INFO | train_inner | epoch 095:    522 / 1102 loss=6.995, nll_loss=3.193, ppl=9.14, wps=32695.3, ups=9.18, wpb=3563.1, bsz=151.2, num_updates=104100, lr=9.8011e-05, gnorm=2.045, loss_scale=2, train_wall=11, gb_free=19.5, wall=7022
2022-12-09 16:55:20 | INFO | train_inner | epoch 095:    622 / 1102 loss=7.07, nll_loss=3.275, ppl=9.68, wps=33048.4, ups=9.11, wpb=3629.6, bsz=149.1, num_updates=104200, lr=9.79639e-05, gnorm=1.984, loss_scale=2, train_wall=11, gb_free=19.8, wall=7033
2022-12-09 16:55:31 | INFO | train_inner | epoch 095:    722 / 1102 loss=7, nll_loss=3.196, ppl=9.17, wps=32361.5, ups=9.03, wpb=3585.7, bsz=152.4, num_updates=104300, lr=9.79169e-05, gnorm=2.114, loss_scale=2, train_wall=11, gb_free=19.5, wall=7044
2022-12-09 16:55:42 | INFO | train_inner | epoch 095:    822 / 1102 loss=7.087, nll_loss=3.279, ppl=9.71, wps=32896.3, ups=9.15, wpb=3596.8, bsz=141.4, num_updates=104400, lr=9.787e-05, gnorm=2.046, loss_scale=2, train_wall=11, gb_free=19.2, wall=7055
2022-12-09 16:55:53 | INFO | train_inner | epoch 095:    922 / 1102 loss=7.037, nll_loss=3.248, ppl=9.5, wps=32204.5, ups=9.04, wpb=3561.1, bsz=150.2, num_updates=104500, lr=9.78232e-05, gnorm=1.924, loss_scale=2, train_wall=11, gb_free=19.6, wall=7066
2022-12-09 16:56:04 | INFO | train_inner | epoch 095:   1022 / 1102 loss=7.049, nll_loss=3.239, ppl=9.44, wps=32202.3, ups=9.1, wpb=3537.2, bsz=145.8, num_updates=104600, lr=9.77764e-05, gnorm=1.998, loss_scale=2, train_wall=11, gb_free=19.3, wall=7077
2022-12-09 16:56:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:56:51 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 3.679 | nll_loss 2.134 | ppl 4.39 | bleu 36.94 | wps 4805.2 | wpb 2835.3 | bsz 115.6 | num_updates 104680 | best_bleu 37.1
2022-12-09 16:56:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 104680 updates
2022-12-09 16:56:52 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint95.pt
2022-12-09 16:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint95.pt (epoch 95 @ 104680 updates, score 36.94) (writing took 1.2017245758324862 seconds)
2022-12-09 16:56:52 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-12-09 16:56:52 | INFO | train | epoch 095 | loss 7.036 | nll_loss 3.233 | ppl 9.4 | wps 24652.3 | ups 6.88 | wpb 3583.6 | bsz 145.4 | num_updates 104680 | lr 9.77391e-05 | gnorm 2.004 | loss_scale 2 | train_wall 118 | gb_free 19.8 | wall 7125
2022-12-09 16:56:52 | INFO | fairseq.trainer | begin training epoch 96
2022-12-09 16:56:55 | INFO | train_inner | epoch 096:     20 / 1102 loss=7.036, nll_loss=3.249, ppl=9.51, wps=7339.6, ups=1.99, wpb=3682.1, bsz=155, num_updates=104700, lr=9.77297e-05, gnorm=1.969, loss_scale=2, train_wall=11, gb_free=19.4, wall=7127
2022-12-09 16:57:05 | INFO | train_inner | epoch 096:    120 / 1102 loss=7.003, nll_loss=3.188, ppl=9.11, wps=32838.8, ups=9.27, wpb=3542.8, bsz=135.8, num_updates=104800, lr=9.76831e-05, gnorm=2.043, loss_scale=2, train_wall=11, gb_free=19.5, wall=7138
2022-12-09 16:57:16 | INFO | train_inner | epoch 096:    220 / 1102 loss=6.992, nll_loss=3.185, ppl=9.1, wps=33269.6, ups=9.16, wpb=3633.8, bsz=141.7, num_updates=104900, lr=9.76365e-05, gnorm=1.923, loss_scale=2, train_wall=11, gb_free=19.4, wall=7149
2022-12-09 16:57:27 | INFO | train_inner | epoch 096:    320 / 1102 loss=6.979, nll_loss=3.162, ppl=8.95, wps=32591, ups=9.28, wpb=3510.9, bsz=141.8, num_updates=105000, lr=9.759e-05, gnorm=2.215, loss_scale=2, train_wall=11, gb_free=19.4, wall=7159
2022-12-09 16:57:38 | INFO | train_inner | epoch 096:    420 / 1102 loss=7.013, nll_loss=3.191, ppl=9.13, wps=32132.9, ups=9.11, wpb=3527.3, bsz=136.6, num_updates=105100, lr=9.75436e-05, gnorm=2.068, loss_scale=2, train_wall=11, gb_free=19.7, wall=7170
2022-12-09 16:57:49 | INFO | train_inner | epoch 096:    520 / 1102 loss=7.043, nll_loss=3.237, ppl=9.43, wps=33306.3, ups=9.21, wpb=3616.5, bsz=139.5, num_updates=105200, lr=9.74972e-05, gnorm=1.956, loss_scale=2, train_wall=11, gb_free=19.5, wall=7181
2022-12-09 16:58:00 | INFO | train_inner | epoch 096:    620 / 1102 loss=7.053, nll_loss=3.259, ppl=9.57, wps=33053.4, ups=9.05, wpb=3654, bsz=146.2, num_updates=105300, lr=9.74509e-05, gnorm=1.988, loss_scale=2, train_wall=11, gb_free=19.2, wall=7192
2022-12-09 16:58:11 | INFO | train_inner | epoch 096:    720 / 1102 loss=7.038, nll_loss=3.239, ppl=9.44, wps=32909.1, ups=9.19, wpb=3580.4, bsz=145.2, num_updates=105400, lr=9.74047e-05, gnorm=2.087, loss_scale=2, train_wall=11, gb_free=19.3, wall=7203
2022-12-09 16:58:22 | INFO | train_inner | epoch 096:    820 / 1102 loss=6.985, nll_loss=3.203, ppl=9.21, wps=32871.1, ups=9.08, wpb=3618.6, bsz=166.2, num_updates=105500, lr=9.73585e-05, gnorm=1.954, loss_scale=2, train_wall=11, gb_free=19.5, wall=7214
2022-12-09 16:58:33 | INFO | train_inner | epoch 096:    920 / 1102 loss=7.05, nll_loss=3.261, ppl=9.59, wps=33126.6, ups=9.13, wpb=3628.8, bsz=155.8, num_updates=105600, lr=9.73124e-05, gnorm=2.008, loss_scale=2, train_wall=11, gb_free=19.9, wall=7225
2022-12-09 16:58:44 | INFO | train_inner | epoch 096:   1020 / 1102 loss=7.119, nll_loss=3.309, ppl=9.91, wps=32648.1, ups=9.23, wpb=3538.2, bsz=141.2, num_updates=105700, lr=9.72663e-05, gnorm=2.088, loss_scale=2, train_wall=11, gb_free=19.4, wall=7236
2022-12-09 16:58:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 16:59:31 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 3.677 | nll_loss 2.135 | ppl 4.39 | bleu 36.82 | wps 4721.1 | wpb 2835.3 | bsz 115.6 | num_updates 105782 | best_bleu 37.1
2022-12-09 16:59:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 105782 updates
2022-12-09 16:59:32 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint96.pt
2022-12-09 16:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint96.pt (epoch 96 @ 105782 updates, score 36.82) (writing took 1.0922018280252814 seconds)
2022-12-09 16:59:32 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-12-09 16:59:32 | INFO | train | epoch 096 | loss 7.031 | nll_loss 3.228 | ppl 9.37 | wps 24694.1 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 105782 | lr 9.72286e-05 | gnorm 2.037 | loss_scale 2 | train_wall 118 | gb_free 19.7 | wall 7284
2022-12-09 16:59:32 | INFO | fairseq.trainer | begin training epoch 97
2022-12-09 16:59:34 | INFO | train_inner | epoch 097:     18 / 1102 loss=7.094, nll_loss=3.297, ppl=9.83, wps=7009.9, ups=1.97, wpb=3552.3, bsz=145.1, num_updates=105800, lr=9.72203e-05, gnorm=2.081, loss_scale=2, train_wall=11, gb_free=19.5, wall=7287
2022-12-09 16:59:45 | INFO | train_inner | epoch 097:    118 / 1102 loss=6.92, nll_loss=3.114, ppl=8.66, wps=32374.5, ups=9.12, wpb=3548.8, bsz=154.2, num_updates=105900, lr=9.71744e-05, gnorm=1.955, loss_scale=2, train_wall=11, gb_free=19.4, wall=7298
2022-12-09 16:59:56 | INFO | train_inner | epoch 097:    218 / 1102 loss=6.975, nll_loss=3.164, ppl=8.96, wps=32621.2, ups=9.11, wpb=3579.6, bsz=145.5, num_updates=106000, lr=9.71286e-05, gnorm=1.991, loss_scale=2, train_wall=11, gb_free=19.4, wall=7309
2022-12-09 17:00:07 | INFO | train_inner | epoch 097:    318 / 1102 loss=6.991, nll_loss=3.172, ppl=9.01, wps=32531.4, ups=9.19, wpb=3538.5, bsz=140.4, num_updates=106100, lr=9.70828e-05, gnorm=2.044, loss_scale=2, train_wall=11, gb_free=19.5, wall=7320
2022-12-09 17:00:18 | INFO | train_inner | epoch 097:    418 / 1102 loss=6.916, nll_loss=3.125, ppl=8.73, wps=32717.6, ups=9.06, wpb=3609.4, bsz=159.3, num_updates=106200, lr=9.70371e-05, gnorm=1.922, loss_scale=2, train_wall=11, gb_free=19.8, wall=7331
2022-12-09 17:00:29 | INFO | train_inner | epoch 097:    518 / 1102 loss=6.963, nll_loss=3.165, ppl=8.97, wps=32690.7, ups=9.19, wpb=3557.7, bsz=159.5, num_updates=106300, lr=9.69914e-05, gnorm=2.028, loss_scale=2, train_wall=11, gb_free=19.7, wall=7341
2022-12-09 17:00:40 | INFO | train_inner | epoch 097:    618 / 1102 loss=7.175, nll_loss=3.351, ppl=10.2, wps=33020.1, ups=8.99, wpb=3674.4, bsz=120.9, num_updates=106400, lr=9.69458e-05, gnorm=2.125, loss_scale=2, train_wall=11, gb_free=19.3, wall=7353
2022-12-09 17:00:51 | INFO | train_inner | epoch 097:    718 / 1102 loss=7.072, nll_loss=3.262, ppl=9.59, wps=32304.8, ups=9.19, wpb=3513.5, bsz=138.2, num_updates=106500, lr=9.69003e-05, gnorm=2.068, loss_scale=2, train_wall=11, gb_free=19.4, wall=7363
2022-12-09 17:01:02 | INFO | train_inner | epoch 097:    818 / 1102 loss=7.108, nll_loss=3.298, ppl=9.84, wps=33222.7, ups=9.23, wpb=3599.7, bsz=132.2, num_updates=106600, lr=9.68549e-05, gnorm=2.066, loss_scale=2, train_wall=11, gb_free=20, wall=7374
2022-12-09 17:01:13 | INFO | train_inner | epoch 097:    918 / 1102 loss=7.123, nll_loss=3.315, ppl=9.95, wps=32540.3, ups=9.17, wpb=3549.5, bsz=133.9, num_updates=106700, lr=9.68095e-05, gnorm=2.055, loss_scale=2, train_wall=11, gb_free=19.7, wall=7385
2022-12-09 17:01:24 | INFO | train_inner | epoch 097:   1018 / 1102 loss=6.999, nll_loss=3.205, ppl=9.22, wps=32493.3, ups=9.06, wpb=3587.1, bsz=153.4, num_updates=106800, lr=9.67641e-05, gnorm=1.952, loss_scale=2, train_wall=11, gb_free=19.3, wall=7396
2022-12-09 17:01:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:02:11 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 3.677 | nll_loss 2.133 | ppl 4.39 | bleu 36.9 | wps 4780 | wpb 2835.3 | bsz 115.6 | num_updates 106884 | best_bleu 37.1
2022-12-09 17:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 106884 updates
2022-12-09 17:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint97.pt
2022-12-09 17:02:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint97.pt (epoch 97 @ 106884 updates, score 36.9) (writing took 1.2280179131776094 seconds)
2022-12-09 17:02:12 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-12-09 17:02:12 | INFO | train | epoch 097 | loss 7.026 | nll_loss 3.221 | ppl 9.33 | wps 24674.9 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 106884 | lr 9.67261e-05 | gnorm 2.024 | loss_scale 2 | train_wall 118 | gb_free 19.8 | wall 7445
2022-12-09 17:02:12 | INFO | fairseq.trainer | begin training epoch 98
2022-12-09 17:02:14 | INFO | train_inner | epoch 098:     16 / 1102 loss=7.014, nll_loss=3.233, ppl=9.41, wps=7217.9, ups=1.98, wpb=3637, bsz=161, num_updates=106900, lr=9.67189e-05, gnorm=2.049, loss_scale=2, train_wall=11, gb_free=19.3, wall=7447
2022-12-09 17:02:25 | INFO | train_inner | epoch 098:    116 / 1102 loss=6.98, nll_loss=3.183, ppl=9.08, wps=32717.1, ups=9.12, wpb=3589.2, bsz=146.2, num_updates=107000, lr=9.66736e-05, gnorm=1.914, loss_scale=2, train_wall=11, gb_free=19.6, wall=7458
2022-12-09 17:02:36 | INFO | train_inner | epoch 098:    216 / 1102 loss=7.006, nll_loss=3.189, ppl=9.12, wps=32670.9, ups=9.17, wpb=3563.9, bsz=138, num_updates=107100, lr=9.66285e-05, gnorm=1.924, loss_scale=2, train_wall=11, gb_free=19.8, wall=7469
2022-12-09 17:02:47 | INFO | train_inner | epoch 098:    316 / 1102 loss=6.949, nll_loss=3.138, ppl=8.81, wps=32551.2, ups=9.16, wpb=3552.1, bsz=151, num_updates=107200, lr=9.65834e-05, gnorm=2.041, loss_scale=2, train_wall=11, gb_free=19.4, wall=7479
2022-12-09 17:02:58 | INFO | train_inner | epoch 098:    416 / 1102 loss=7.054, nll_loss=3.243, ppl=9.46, wps=33789.2, ups=9.31, wpb=3630.7, bsz=134.6, num_updates=107300, lr=9.65384e-05, gnorm=2.017, loss_scale=2, train_wall=11, gb_free=19.4, wall=7490
2022-12-09 17:03:09 | INFO | train_inner | epoch 098:    516 / 1102 loss=7.041, nll_loss=3.231, ppl=9.39, wps=33115.3, ups=9.29, wpb=3564.5, bsz=145.8, num_updates=107400, lr=9.64935e-05, gnorm=2.028, loss_scale=2, train_wall=11, gb_free=19.7, wall=7501
2022-12-09 17:03:20 | INFO | train_inner | epoch 098:    616 / 1102 loss=6.982, nll_loss=3.192, ppl=9.14, wps=32228.3, ups=9.06, wpb=3558.4, bsz=159.1, num_updates=107500, lr=9.64486e-05, gnorm=2, loss_scale=2, train_wall=11, gb_free=19.3, wall=7512
2022-12-09 17:03:31 | INFO | train_inner | epoch 098:    716 / 1102 loss=7.017, nll_loss=3.211, ppl=9.26, wps=32875, ups=9.12, wpb=3604.6, bsz=148.5, num_updates=107600, lr=9.64037e-05, gnorm=1.979, loss_scale=2, train_wall=11, gb_free=19.8, wall=7523
2022-12-09 17:03:42 | INFO | train_inner | epoch 098:    816 / 1102 loss=7.047, nll_loss=3.245, ppl=9.48, wps=32882.8, ups=9.11, wpb=3609.3, bsz=147.1, num_updates=107700, lr=9.6359e-05, gnorm=2.017, loss_scale=2, train_wall=11, gb_free=19.8, wall=7534
2022-12-09 17:03:52 | INFO | train_inner | epoch 098:    916 / 1102 loss=7.053, nll_loss=3.252, ppl=9.53, wps=32936.3, ups=9.19, wpb=3582.9, bsz=146.9, num_updates=107800, lr=9.63143e-05, gnorm=2.007, loss_scale=2, train_wall=11, gb_free=19.3, wall=7545
2022-12-09 17:04:03 | INFO | train_inner | epoch 098:   1016 / 1102 loss=7.029, nll_loss=3.232, ppl=9.39, wps=32806.2, ups=9.14, wpb=3589.8, bsz=143.4, num_updates=107900, lr=9.62696e-05, gnorm=2.02, loss_scale=2, train_wall=11, gb_free=19.6, wall=7556
2022-12-09 17:04:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:04:51 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 3.675 | nll_loss 2.133 | ppl 4.39 | bleu 37.24 | wps 4726.8 | wpb 2835.3 | bsz 115.6 | num_updates 107986 | best_bleu 37.24
2022-12-09 17:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 107986 updates
2022-12-09 17:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint98.pt
2022-12-09 17:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint98.pt (epoch 98 @ 107986 updates, score 37.24) (writing took 1.5042739994823933 seconds)
2022-12-09 17:04:52 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-12-09 17:04:52 | INFO | train | epoch 098 | loss 7.02 | nll_loss 3.215 | ppl 9.29 | wps 24630.8 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 107986 | lr 9.62313e-05 | gnorm 1.997 | loss_scale 2 | train_wall 117 | gb_free 19.5 | wall 7605
2022-12-09 17:04:53 | INFO | fairseq.trainer | begin training epoch 99
2022-12-09 17:04:54 | INFO | train_inner | epoch 099:     14 / 1102 loss=7.071, nll_loss=3.265, ppl=9.61, wps=7042.7, ups=1.96, wpb=3587.8, bsz=142.2, num_updates=108000, lr=9.6225e-05, gnorm=2.004, loss_scale=2, train_wall=11, gb_free=19.6, wall=7607
2022-12-09 17:05:05 | INFO | train_inner | epoch 099:    114 / 1102 loss=6.961, nll_loss=3.16, ppl=8.94, wps=32563.5, ups=9.25, wpb=3521.4, bsz=158.6, num_updates=108100, lr=9.61805e-05, gnorm=2.005, loss_scale=2, train_wall=11, gb_free=19.4, wall=7617
2022-12-09 17:05:16 | INFO | train_inner | epoch 099:    214 / 1102 loss=6.982, nll_loss=3.178, ppl=9.05, wps=33509.8, ups=9.26, wpb=3618.4, bsz=143.5, num_updates=108200, lr=9.61361e-05, gnorm=2.033, loss_scale=2, train_wall=11, gb_free=19.5, wall=7628
2022-12-09 17:05:27 | INFO | train_inner | epoch 099:    314 / 1102 loss=6.979, nll_loss=3.178, ppl=9.05, wps=33543.8, ups=9.27, wpb=3617.8, bsz=149.4, num_updates=108300, lr=9.60917e-05, gnorm=1.99, loss_scale=2, train_wall=11, gb_free=19.8, wall=7639
2022-12-09 17:05:37 | INFO | train_inner | epoch 099:    414 / 1102 loss=6.934, nll_loss=3.115, ppl=8.66, wps=32368.9, ups=9.32, wpb=3471.2, bsz=148.8, num_updates=108400, lr=9.60473e-05, gnorm=2.094, loss_scale=2, train_wall=10, gb_free=19.4, wall=7650
2022-12-09 17:05:48 | INFO | train_inner | epoch 099:    514 / 1102 loss=7.009, nll_loss=3.21, ppl=9.25, wps=33131.4, ups=9.35, wpb=3544, bsz=148.6, num_updates=108500, lr=9.60031e-05, gnorm=2.021, loss_scale=2, train_wall=10, gb_free=19.7, wall=7661
2022-12-09 17:05:59 | INFO | train_inner | epoch 099:    614 / 1102 loss=6.996, nll_loss=3.198, ppl=9.18, wps=33706.1, ups=9.21, wpb=3660.4, bsz=151.5, num_updates=108600, lr=9.59589e-05, gnorm=1.924, loss_scale=2, train_wall=11, gb_free=19.9, wall=7671
2022-12-09 17:06:10 | INFO | train_inner | epoch 099:    714 / 1102 loss=7.085, nll_loss=3.257, ppl=9.56, wps=33304.4, ups=9.31, wpb=3578.9, bsz=135.8, num_updates=108700, lr=9.59147e-05, gnorm=2.184, loss_scale=2, train_wall=10, gb_free=19.5, wall=7682
2022-12-09 17:06:21 | INFO | train_inner | epoch 099:    814 / 1102 loss=7.059, nll_loss=3.251, ppl=9.52, wps=33352.4, ups=9.27, wpb=3596.5, bsz=140, num_updates=108800, lr=9.58706e-05, gnorm=2.071, loss_scale=2, train_wall=11, gb_free=19.5, wall=7693
2022-12-09 17:06:31 | INFO | train_inner | epoch 099:    914 / 1102 loss=7.073, nll_loss=3.267, ppl=9.62, wps=33161.5, ups=9.23, wpb=3591, bsz=143, num_updates=108900, lr=9.58266e-05, gnorm=2.074, loss_scale=2, train_wall=11, gb_free=19.4, wall=7704
2022-12-09 17:06:42 | INFO | train_inner | epoch 099:   1014 / 1102 loss=7.011, nll_loss=3.218, ppl=9.3, wps=33815.4, ups=9.21, wpb=3672.6, bsz=148.2, num_updates=109000, lr=9.57826e-05, gnorm=1.961, loss_scale=2, train_wall=11, gb_free=19.5, wall=7715
2022-12-09 17:06:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:07:30 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 3.674 | nll_loss 2.129 | ppl 4.37 | bleu 37.04 | wps 4649.9 | wpb 2835.3 | bsz 115.6 | num_updates 109088 | best_bleu 37.24
2022-12-09 17:07:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 109088 updates
2022-12-09 17:07:31 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint99.pt
2022-12-09 17:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint99.pt (epoch 99 @ 109088 updates, score 37.04) (writing took 1.3224394330754876 seconds)
2022-12-09 17:07:32 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-12-09 17:07:32 | INFO | train | epoch 099 | loss 7.016 | nll_loss 3.209 | ppl 9.25 | wps 24789.4 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 109088 | lr 9.5744e-05 | gnorm 2.033 | loss_scale 2 | train_wall 116 | gb_free 19.7 | wall 7764
2022-12-09 17:07:32 | INFO | fairseq.trainer | begin training epoch 100
2022-12-09 17:07:33 | INFO | train_inner | epoch 100:     12 / 1102 loss=7.068, nll_loss=3.257, ppl=9.56, wps=6978.4, ups=1.95, wpb=3572.7, bsz=134.6, num_updates=109100, lr=9.57387e-05, gnorm=2.004, loss_scale=2, train_wall=11, gb_free=19.4, wall=7766
2022-12-09 17:07:44 | INFO | train_inner | epoch 100:    112 / 1102 loss=6.942, nll_loss=3.144, ppl=8.84, wps=33043.4, ups=9.17, wpb=3604.9, bsz=149.4, num_updates=109200, lr=9.56949e-05, gnorm=1.951, loss_scale=2, train_wall=11, gb_free=19.6, wall=7777
2022-12-09 17:07:55 | INFO | train_inner | epoch 100:    212 / 1102 loss=6.948, nll_loss=3.136, ppl=8.79, wps=32532, ups=9.14, wpb=3560.2, bsz=144.1, num_updates=109300, lr=9.56511e-05, gnorm=2.012, loss_scale=2, train_wall=11, gb_free=19.4, wall=7788
2022-12-09 17:08:06 | INFO | train_inner | epoch 100:    312 / 1102 loss=7, nll_loss=3.2, ppl=9.19, wps=32435.6, ups=8.93, wpb=3632.4, bsz=153.3, num_updates=109400, lr=9.56074e-05, gnorm=1.979, loss_scale=2, train_wall=11, gb_free=19.5, wall=7799
2022-12-09 17:08:18 | INFO | train_inner | epoch 100:    412 / 1102 loss=6.98, nll_loss=3.165, ppl=8.97, wps=31840, ups=9, wpb=3539.6, bsz=141.5, num_updates=109500, lr=9.55637e-05, gnorm=2.04, loss_scale=2, train_wall=11, gb_free=19.4, wall=7810
2022-12-09 17:08:29 | INFO | train_inner | epoch 100:    512 / 1102 loss=7.001, nll_loss=3.194, ppl=9.15, wps=32669, ups=9.02, wpb=3622.5, bsz=145.5, num_updates=109600, lr=9.55201e-05, gnorm=1.994, loss_scale=2, train_wall=11, gb_free=19.6, wall=7821
2022-12-09 17:08:40 | INFO | train_inner | epoch 100:    612 / 1102 loss=7.006, nll_loss=3.202, ppl=9.2, wps=33089.2, ups=9.15, wpb=3614.9, bsz=151, num_updates=109700, lr=9.54765e-05, gnorm=1.97, loss_scale=2, train_wall=11, gb_free=19.3, wall=7832
2022-12-09 17:08:50 | INFO | train_inner | epoch 100:    712 / 1102 loss=7.018, nll_loss=3.212, ppl=9.26, wps=32554.9, ups=9.18, wpb=3546.7, bsz=147.7, num_updates=109800, lr=9.54331e-05, gnorm=2.062, loss_scale=2, train_wall=11, gb_free=19.4, wall=7843
2022-12-09 17:09:01 | INFO | train_inner | epoch 100:    812 / 1102 loss=7.07, nll_loss=3.254, ppl=9.54, wps=33069.6, ups=9.35, wpb=3538.3, bsz=137, num_updates=109900, lr=9.53896e-05, gnorm=2.033, loss_scale=2, train_wall=10, gb_free=19.4, wall=7854
2022-12-09 17:09:12 | INFO | train_inner | epoch 100:    912 / 1102 loss=7.083, nll_loss=3.271, ppl=9.65, wps=33136.5, ups=9.39, wpb=3528.6, bsz=143.6, num_updates=110000, lr=9.53463e-05, gnorm=2.036, loss_scale=2, train_wall=10, gb_free=19.4, wall=7864
2022-12-09 17:09:23 | INFO | train_inner | epoch 100:   1012 / 1102 loss=7.075, nll_loss=3.261, ppl=9.59, wps=33108.2, ups=9.27, wpb=3570.7, bsz=134.5, num_updates=110100, lr=9.53029e-05, gnorm=2.075, loss_scale=2, train_wall=11, gb_free=19.4, wall=7875
2022-12-09 17:09:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:10:09 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 3.672 | nll_loss 2.13 | ppl 4.38 | bleu 37.11 | wps 4940.1 | wpb 2835.3 | bsz 115.6 | num_updates 110190 | best_bleu 37.24
2022-12-09 17:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 110190 updates
2022-12-09 17:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint100.pt
2022-12-09 17:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint100.pt (epoch 100 @ 110190 updates, score 37.11) (writing took 1.1347057363018394 seconds)
2022-12-09 17:10:10 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-12-09 17:10:10 | INFO | train | epoch 100 | loss 7.011 | nll_loss 3.203 | ppl 9.21 | wps 24954.1 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 110190 | lr 9.5264e-05 | gnorm 2.014 | loss_scale 2 | train_wall 117 | gb_free 19.5 | wall 7922
2022-12-09 17:10:10 | INFO | fairseq.trainer | begin training epoch 101
2022-12-09 17:10:11 | INFO | train_inner | epoch 101:     10 / 1102 loss=7.012, nll_loss=3.214, ppl=9.28, wps=7437.2, ups=2.05, wpb=3629.9, bsz=146.1, num_updates=110200, lr=9.52597e-05, gnorm=2.033, loss_scale=2, train_wall=11, gb_free=19.6, wall=7924
2022-12-09 17:10:22 | INFO | train_inner | epoch 101:    110 / 1102 loss=6.954, nll_loss=3.143, ppl=8.83, wps=32772.9, ups=9.15, wpb=3581.5, bsz=143.2, num_updates=110300, lr=9.52165e-05, gnorm=2.001, loss_scale=2, train_wall=11, gb_free=19.5, wall=7935
2022-12-09 17:10:33 | INFO | train_inner | epoch 101:    210 / 1102 loss=6.908, nll_loss=3.109, ppl=8.63, wps=33124.1, ups=9.14, wpb=3625.9, bsz=154.3, num_updates=110400, lr=9.51734e-05, gnorm=2.011, loss_scale=2, train_wall=11, gb_free=19.3, wall=7946
2022-12-09 17:10:44 | INFO | train_inner | epoch 101:    310 / 1102 loss=7.011, nll_loss=3.195, ppl=9.16, wps=33548, ups=9.37, wpb=3580.4, bsz=137, num_updates=110500, lr=9.51303e-05, gnorm=1.975, loss_scale=2, train_wall=10, gb_free=19.5, wall=7956
2022-12-09 17:10:54 | INFO | train_inner | epoch 101:    410 / 1102 loss=6.927, nll_loss=3.108, ppl=8.62, wps=33940.3, ups=9.58, wpb=3542.7, bsz=152.6, num_updates=110600, lr=9.50873e-05, gnorm=2.096, loss_scale=2, train_wall=10, gb_free=19.3, wall=7967
2022-12-09 17:11:05 | INFO | train_inner | epoch 101:    510 / 1102 loss=7.002, nll_loss=3.174, ppl=9.02, wps=33178.5, ups=9.58, wpb=3462.6, bsz=131.8, num_updates=110700, lr=9.50443e-05, gnorm=2.145, loss_scale=2, train_wall=10, gb_free=19.6, wall=7977
2022-12-09 17:11:15 | INFO | train_inner | epoch 101:    610 / 1102 loss=7.023, nll_loss=3.216, ppl=9.29, wps=33896.8, ups=9.39, wpb=3611.7, bsz=147.5, num_updates=110800, lr=9.50014e-05, gnorm=2.068, loss_scale=2, train_wall=10, gb_free=19.3, wall=7988
2022-12-09 17:11:26 | INFO | train_inner | epoch 101:    710 / 1102 loss=7.07, nll_loss=3.262, ppl=9.59, wps=34039.1, ups=9.41, wpb=3618.3, bsz=139, num_updates=110900, lr=9.49586e-05, gnorm=1.998, loss_scale=2, train_wall=10, gb_free=19.4, wall=7999
2022-12-09 17:11:37 | INFO | train_inner | epoch 101:    810 / 1102 loss=7.159, nll_loss=3.352, ppl=10.21, wps=33683.8, ups=9.35, wpb=3603.2, bsz=129.7, num_updates=111000, lr=9.49158e-05, gnorm=2.125, loss_scale=2, train_wall=10, gb_free=19.7, wall=8009
2022-12-09 17:11:48 | INFO | train_inner | epoch 101:    910 / 1102 loss=6.964, nll_loss=3.182, ppl=9.08, wps=34008.5, ups=9.32, wpb=3650.9, bsz=167.1, num_updates=111100, lr=9.48731e-05, gnorm=1.924, loss_scale=2, train_wall=11, gb_free=19.4, wall=8020
2022-12-09 17:11:58 | INFO | train_inner | epoch 101:   1010 / 1102 loss=6.99, nll_loss=3.176, ppl=9.04, wps=32811.8, ups=9.27, wpb=3539.4, bsz=148.7, num_updates=111200, lr=9.48304e-05, gnorm=2.093, loss_scale=2, train_wall=11, gb_free=19.3, wall=8031
2022-12-09 17:12:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:12:48 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 3.669 | nll_loss 2.127 | ppl 4.37 | bleu 37.1 | wps 4553.7 | wpb 2835.3 | bsz 115.6 | num_updates 111292 | best_bleu 37.24
2022-12-09 17:12:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 111292 updates
2022-12-09 17:12:49 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint101.pt
2022-12-09 17:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint101.pt (epoch 101 @ 111292 updates, score 37.1) (writing took 1.1828006291761994 seconds)
2022-12-09 17:12:49 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-12-09 17:12:49 | INFO | train | epoch 101 | loss 7.006 | nll_loss 3.198 | ppl 9.17 | wps 24818.9 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 111292 | lr 9.47912e-05 | gnorm 2.041 | loss_scale 2 | train_wall 115 | gb_free 19.8 | wall 8082
2022-12-09 17:12:49 | INFO | fairseq.trainer | begin training epoch 102
2022-12-09 17:12:50 | INFO | train_inner | epoch 102:      8 / 1102 loss=7.045, nll_loss=3.245, ppl=9.48, wps=6977, ups=1.92, wpb=3625.1, bsz=150.7, num_updates=111300, lr=9.47878e-05, gnorm=2.002, loss_scale=2, train_wall=11, gb_free=19.4, wall=8083
2022-12-09 17:13:01 | INFO | train_inner | epoch 102:    108 / 1102 loss=6.985, nll_loss=3.159, ppl=8.93, wps=33058.3, ups=9.39, wpb=3519.8, bsz=136.7, num_updates=111400, lr=9.47452e-05, gnorm=1.984, loss_scale=4, train_wall=10, gb_free=19.8, wall=8093
2022-12-09 17:13:12 | INFO | train_inner | epoch 102:    208 / 1102 loss=6.978, nll_loss=3.153, ppl=8.9, wps=33509.8, ups=9.45, wpb=3544.3, bsz=131.2, num_updates=111500, lr=9.47027e-05, gnorm=2.045, loss_scale=4, train_wall=10, gb_free=20.4, wall=8104
2022-12-09 17:13:22 | INFO | train_inner | epoch 102:    308 / 1102 loss=6.961, nll_loss=3.168, ppl=8.99, wps=33626.8, ups=9.27, wpb=3628.8, bsz=161.7, num_updates=111600, lr=9.46603e-05, gnorm=1.975, loss_scale=4, train_wall=11, gb_free=19.5, wall=8115
2022-12-09 17:13:33 | INFO | train_inner | epoch 102:    408 / 1102 loss=6.943, nll_loss=3.152, ppl=8.89, wps=34093.3, ups=9.2, wpb=3704.9, bsz=162.5, num_updates=111700, lr=9.46179e-05, gnorm=1.983, loss_scale=4, train_wall=11, gb_free=19.3, wall=8126
2022-12-09 17:13:44 | INFO | train_inner | epoch 102:    508 / 1102 loss=7.081, nll_loss=3.251, ppl=9.52, wps=33179, ups=9.38, wpb=3538.3, bsz=125.6, num_updates=111800, lr=9.45756e-05, gnorm=2.086, loss_scale=4, train_wall=10, gb_free=19.3, wall=8136
2022-12-09 17:13:55 | INFO | train_inner | epoch 102:    608 / 1102 loss=7.045, nll_loss=3.227, ppl=9.36, wps=33491.1, ups=9.29, wpb=3603.6, bsz=136.4, num_updates=111900, lr=9.45333e-05, gnorm=2.076, loss_scale=4, train_wall=11, gb_free=19.6, wall=8147
2022-12-09 17:14:05 | INFO | train_inner | epoch 102:    708 / 1102 loss=6.929, nll_loss=3.145, ppl=8.85, wps=33300.9, ups=9.27, wpb=3593.3, bsz=168.8, num_updates=112000, lr=9.44911e-05, gnorm=1.932, loss_scale=4, train_wall=11, gb_free=19.8, wall=8158
2022-12-09 17:14:16 | INFO | train_inner | epoch 102:    808 / 1102 loss=7.012, nll_loss=3.198, ppl=9.18, wps=32879.8, ups=9.24, wpb=3559, bsz=144.6, num_updates=112100, lr=9.4449e-05, gnorm=2.014, loss_scale=4, train_wall=11, gb_free=19.6, wall=8169
2022-12-09 17:14:27 | INFO | train_inner | epoch 102:    908 / 1102 loss=7.001, nll_loss=3.207, ppl=9.23, wps=32983.3, ups=9.18, wpb=3591, bsz=152.9, num_updates=112200, lr=9.44069e-05, gnorm=2.036, loss_scale=4, train_wall=11, gb_free=19.8, wall=8180
2022-12-09 17:14:38 | INFO | train_inner | epoch 102:   1008 / 1102 loss=7.035, nll_loss=3.231, ppl=9.39, wps=33497.8, ups=9.32, wpb=3594.9, bsz=144.7, num_updates=112300, lr=9.43648e-05, gnorm=2.064, loss_scale=4, train_wall=11, gb_free=19.9, wall=8190
2022-12-09 17:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:15:26 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 3.673 | nll_loss 2.132 | ppl 4.38 | bleu 37.11 | wps 4718.9 | wpb 2835.3 | bsz 115.6 | num_updates 112394 | best_bleu 37.24
2022-12-09 17:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 112394 updates
2022-12-09 17:15:27 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint102.pt
2022-12-09 17:15:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint102.pt (epoch 102 @ 112394 updates, score 37.11) (writing took 1.1476437924429774 seconds)
2022-12-09 17:15:27 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-12-09 17:15:27 | INFO | train | epoch 102 | loss 7.001 | nll_loss 3.192 | ppl 9.14 | wps 24941.3 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 112394 | lr 9.43254e-05 | gnorm 2.025 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 8240
2022-12-09 17:15:28 | INFO | fairseq.trainer | begin training epoch 103
2022-12-09 17:15:28 | INFO | train_inner | epoch 103:      6 / 1102 loss=7.045, nll_loss=3.217, ppl=9.3, wps=6968.7, ups=1.97, wpb=3529.7, bsz=133, num_updates=112400, lr=9.43228e-05, gnorm=2.087, loss_scale=4, train_wall=11, gb_free=19.3, wall=8241
2022-12-09 17:15:39 | INFO | train_inner | epoch 103:    106 / 1102 loss=6.892, nll_loss=3.081, ppl=8.46, wps=32731.5, ups=9.3, wpb=3518.3, bsz=152, num_updates=112500, lr=9.42809e-05, gnorm=1.941, loss_scale=4, train_wall=11, gb_free=19.4, wall=8252
2022-12-09 17:15:50 | INFO | train_inner | epoch 103:    206 / 1102 loss=6.946, nll_loss=3.136, ppl=8.79, wps=33381.7, ups=9.37, wpb=3563.5, bsz=150.3, num_updates=112600, lr=9.4239e-05, gnorm=2.132, loss_scale=4, train_wall=10, gb_free=19.5, wall=8262
2022-12-09 17:16:01 | INFO | train_inner | epoch 103:    306 / 1102 loss=7.006, nll_loss=3.189, ppl=9.12, wps=33391.9, ups=9.3, wpb=3591.6, bsz=140.9, num_updates=112700, lr=9.41972e-05, gnorm=2.07, loss_scale=4, train_wall=11, gb_free=19.3, wall=8273
2022-12-09 17:16:11 | INFO | train_inner | epoch 103:    406 / 1102 loss=7.007, nll_loss=3.196, ppl=9.16, wps=33345.7, ups=9.35, wpb=3567.8, bsz=149.3, num_updates=112800, lr=9.41554e-05, gnorm=2.074, loss_scale=4, train_wall=10, gb_free=19.6, wall=8284
2022-12-09 17:16:22 | INFO | train_inner | epoch 103:    506 / 1102 loss=6.98, nll_loss=3.169, ppl=8.99, wps=33763.6, ups=9.31, wpb=3627.5, bsz=143.2, num_updates=112900, lr=9.41137e-05, gnorm=1.995, loss_scale=4, train_wall=11, gb_free=19.4, wall=8295
2022-12-09 17:16:33 | INFO | train_inner | epoch 103:    606 / 1102 loss=7.016, nll_loss=3.185, ppl=9.1, wps=32774.1, ups=9.29, wpb=3529.4, bsz=131.5, num_updates=113000, lr=9.40721e-05, gnorm=2.116, loss_scale=4, train_wall=11, gb_free=19.3, wall=8305
2022-12-09 17:16:44 | INFO | train_inner | epoch 103:    706 / 1102 loss=7.05, nll_loss=3.245, ppl=9.48, wps=33525.2, ups=9.23, wpb=3634, bsz=141.8, num_updates=113100, lr=9.40305e-05, gnorm=1.985, loss_scale=4, train_wall=11, gb_free=19.3, wall=8316
2022-12-09 17:16:55 | INFO | train_inner | epoch 103:    806 / 1102 loss=7.013, nll_loss=3.207, ppl=9.24, wps=33244.8, ups=9.27, wpb=3587.6, bsz=146.8, num_updates=113200, lr=9.39889e-05, gnorm=1.974, loss_scale=4, train_wall=11, gb_free=19.6, wall=8327
2022-12-09 17:17:05 | INFO | train_inner | epoch 103:    906 / 1102 loss=7.027, nll_loss=3.217, ppl=9.3, wps=32774.9, ups=9.28, wpb=3532, bsz=150, num_updates=113300, lr=9.39475e-05, gnorm=2.053, loss_scale=4, train_wall=11, gb_free=19.8, wall=8338
2022-12-09 17:17:16 | INFO | train_inner | epoch 103:   1006 / 1102 loss=7.003, nll_loss=3.205, ppl=9.22, wps=33478.5, ups=9.15, wpb=3660.4, bsz=142.7, num_updates=113400, lr=9.3906e-05, gnorm=1.952, loss_scale=4, train_wall=11, gb_free=19.3, wall=8349
2022-12-09 17:17:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:18:06 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 3.664 | nll_loss 2.124 | ppl 4.36 | bleu 37.14 | wps 4588.8 | wpb 2835.3 | bsz 115.6 | num_updates 113496 | best_bleu 37.24
2022-12-09 17:18:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 113496 updates
2022-12-09 17:18:07 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint103.pt
2022-12-09 17:18:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint103.pt (epoch 103 @ 113496 updates, score 37.14) (writing took 1.1881883693858981 seconds)
2022-12-09 17:18:07 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-12-09 17:18:07 | INFO | train | epoch 103 | loss 6.997 | nll_loss 3.187 | ppl 9.11 | wps 24699.9 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 113496 | lr 9.38663e-05 | gnorm 2.027 | loss_scale 4 | train_wall 116 | gb_free 19.3 | wall 8400
2022-12-09 17:18:07 | INFO | fairseq.trainer | begin training epoch 104
2022-12-09 17:18:08 | INFO | train_inner | epoch 104:      4 / 1102 loss=7.035, nll_loss=3.242, ppl=9.46, wps=6951.3, ups=1.93, wpb=3604.8, bsz=151.9, num_updates=113500, lr=9.38647e-05, gnorm=2.007, loss_scale=4, train_wall=11, gb_free=19.2, wall=8400
2022-12-09 17:18:19 | INFO | train_inner | epoch 104:    104 / 1102 loss=6.947, nll_loss=3.132, ppl=8.77, wps=33293.6, ups=9.4, wpb=3541.7, bsz=139.9, num_updates=113600, lr=9.38233e-05, gnorm=1.989, loss_scale=4, train_wall=10, gb_free=19.3, wall=8411
2022-12-09 17:18:29 | INFO | train_inner | epoch 104:    204 / 1102 loss=6.925, nll_loss=3.117, ppl=8.67, wps=33531.7, ups=9.31, wpb=3600.8, bsz=149.6, num_updates=113700, lr=9.37821e-05, gnorm=1.98, loss_scale=4, train_wall=11, gb_free=19.2, wall=8422
2022-12-09 17:18:40 | INFO | train_inner | epoch 104:    304 / 1102 loss=6.97, nll_loss=3.148, ppl=8.86, wps=32574.9, ups=9.24, wpb=3523.6, bsz=141.8, num_updates=113800, lr=9.37408e-05, gnorm=2.047, loss_scale=4, train_wall=11, gb_free=19.7, wall=8433
2022-12-09 17:18:51 | INFO | train_inner | epoch 104:    404 / 1102 loss=6.974, nll_loss=3.153, ppl=8.9, wps=32356.4, ups=9.1, wpb=3557.1, bsz=142.6, num_updates=113900, lr=9.36997e-05, gnorm=2.073, loss_scale=4, train_wall=11, gb_free=19.4, wall=8444
2022-12-09 17:19:02 | INFO | train_inner | epoch 104:    504 / 1102 loss=7.032, nll_loss=3.208, ppl=9.24, wps=32792.7, ups=9.09, wpb=3608.2, bsz=133.5, num_updates=114000, lr=9.36586e-05, gnorm=2.048, loss_scale=4, train_wall=11, gb_free=19.4, wall=8455
2022-12-09 17:19:13 | INFO | train_inner | epoch 104:    604 / 1102 loss=6.975, nll_loss=3.166, ppl=8.98, wps=32776.8, ups=9.13, wpb=3588.4, bsz=155.5, num_updates=114100, lr=9.36175e-05, gnorm=1.996, loss_scale=4, train_wall=11, gb_free=19.7, wall=8466
2022-12-09 17:19:24 | INFO | train_inner | epoch 104:    704 / 1102 loss=7.015, nll_loss=3.193, ppl=9.15, wps=32118.9, ups=9.24, wpb=3476.6, bsz=137.4, num_updates=114200, lr=9.35765e-05, gnorm=2.194, loss_scale=4, train_wall=11, gb_free=19.4, wall=8476
2022-12-09 17:19:35 | INFO | train_inner | epoch 104:    804 / 1102 loss=7.016, nll_loss=3.207, ppl=9.24, wps=32705.4, ups=9.03, wpb=3623.4, bsz=148.9, num_updates=114300, lr=9.35356e-05, gnorm=2.003, loss_scale=4, train_wall=11, gb_free=19.4, wall=8488
2022-12-09 17:19:46 | INFO | train_inner | epoch 104:    904 / 1102 loss=7.014, nll_loss=3.226, ppl=9.36, wps=32949.9, ups=8.97, wpb=3672.7, bsz=160.6, num_updates=114400, lr=9.34947e-05, gnorm=2.035, loss_scale=4, train_wall=11, gb_free=19.3, wall=8499
2022-12-09 17:19:57 | INFO | train_inner | epoch 104:   1004 / 1102 loss=7.063, nll_loss=3.253, ppl=9.53, wps=32690.2, ups=9.05, wpb=3611, bsz=138.6, num_updates=114500, lr=9.34539e-05, gnorm=2.044, loss_scale=4, train_wall=11, gb_free=19.9, wall=8510
2022-12-09 17:20:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:20:46 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 3.673 | nll_loss 2.128 | ppl 4.37 | bleu 36.92 | wps 4786.5 | wpb 2835.3 | bsz 115.6 | num_updates 114598 | best_bleu 37.24
2022-12-09 17:20:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 114598 updates
2022-12-09 17:20:47 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint104.pt
2022-12-09 17:20:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint104.pt (epoch 104 @ 114598 updates, score 36.92) (writing took 1.201613194309175 seconds)
2022-12-09 17:20:47 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-12-09 17:20:47 | INFO | train | epoch 104 | loss 6.994 | nll_loss 3.182 | ppl 9.07 | wps 24714.7 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 114598 | lr 9.34139e-05 | gnorm 2.042 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 8560
2022-12-09 17:20:47 | INFO | fairseq.trainer | begin training epoch 105
2022-12-09 17:20:48 | INFO | train_inner | epoch 105:      2 / 1102 loss=6.983, nll_loss=3.181, ppl=9.07, wps=7184.3, ups=1.99, wpb=3616.1, bsz=153.4, num_updates=114600, lr=9.34131e-05, gnorm=2.056, loss_scale=4, train_wall=11, gb_free=20, wall=8560
2022-12-09 17:20:59 | INFO | train_inner | epoch 105:    102 / 1102 loss=6.909, nll_loss=3.09, ppl=8.52, wps=32650.4, ups=9.11, wpb=3585.8, bsz=141.9, num_updates=114700, lr=9.33724e-05, gnorm=1.989, loss_scale=4, train_wall=11, gb_free=19.5, wall=8571
2022-12-09 17:21:10 | INFO | train_inner | epoch 105:    202 / 1102 loss=6.948, nll_loss=3.125, ppl=8.72, wps=32791.9, ups=9.21, wpb=3562.2, bsz=138.3, num_updates=114800, lr=9.33317e-05, gnorm=2.073, loss_scale=4, train_wall=11, gb_free=19.5, wall=8582
2022-12-09 17:21:20 | INFO | train_inner | epoch 105:    302 / 1102 loss=7.055, nll_loss=3.237, ppl=9.43, wps=33397.1, ups=9.21, wpb=3626.4, bsz=131.8, num_updates=114900, lr=9.32911e-05, gnorm=2.051, loss_scale=4, train_wall=11, gb_free=19.4, wall=8593
2022-12-09 17:21:31 | INFO | train_inner | epoch 105:    402 / 1102 loss=6.949, nll_loss=3.137, ppl=8.8, wps=32397.8, ups=9.08, wpb=3568, bsz=151.6, num_updates=115000, lr=9.32505e-05, gnorm=2.083, loss_scale=4, train_wall=11, gb_free=19.5, wall=8604
2022-12-09 17:21:42 | INFO | train_inner | epoch 105:    502 / 1102 loss=6.951, nll_loss=3.14, ppl=8.82, wps=32707, ups=9.16, wpb=3569.8, bsz=153.8, num_updates=115100, lr=9.321e-05, gnorm=2.019, loss_scale=4, train_wall=11, gb_free=19.3, wall=8615
2022-12-09 17:21:53 | INFO | train_inner | epoch 105:    602 / 1102 loss=7.012, nll_loss=3.208, ppl=9.24, wps=32727.7, ups=9.1, wpb=3598.2, bsz=148.2, num_updates=115200, lr=9.31695e-05, gnorm=1.981, loss_scale=4, train_wall=11, gb_free=19.3, wall=8626
2022-12-09 17:22:04 | INFO | train_inner | epoch 105:    702 / 1102 loss=6.932, nll_loss=3.136, ppl=8.79, wps=33095.5, ups=9.15, wpb=3617.6, bsz=154.7, num_updates=115300, lr=9.31291e-05, gnorm=1.958, loss_scale=4, train_wall=11, gb_free=19.5, wall=8637
2022-12-09 17:22:15 | INFO | train_inner | epoch 105:    802 / 1102 loss=6.98, nll_loss=3.175, ppl=9.03, wps=32430.4, ups=9.02, wpb=3595, bsz=144.8, num_updates=115400, lr=9.30887e-05, gnorm=2.068, loss_scale=4, train_wall=11, gb_free=19.5, wall=8648
2022-12-09 17:22:26 | INFO | train_inner | epoch 105:    902 / 1102 loss=7.026, nll_loss=3.215, ppl=9.29, wps=31869.8, ups=8.96, wpb=3555.1, bsz=143.8, num_updates=115500, lr=9.30484e-05, gnorm=2.098, loss_scale=4, train_wall=11, gb_free=19.3, wall=8659
2022-12-09 17:22:38 | INFO | train_inner | epoch 105:   1002 / 1102 loss=7.085, nll_loss=3.256, ppl=9.56, wps=31715.3, ups=8.9, wpb=3563, bsz=133.2, num_updates=115600, lr=9.30082e-05, gnorm=2.092, loss_scale=4, train_wall=11, gb_free=19.3, wall=8670
2022-12-09 17:22:49 | INFO | train_inner | epoch 105:   1102 / 1102 loss=7.02, nll_loss=3.216, ppl=9.29, wps=32138.3, ups=8.98, wpb=3580.4, bsz=156.4, num_updates=115700, lr=9.2968e-05, gnorm=2.066, loss_scale=4, train_wall=11, gb_free=19.4, wall=8681
2022-12-09 17:22:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:23:30 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 3.667 | nll_loss 2.126 | ppl 4.36 | bleu 37.06 | wps 4401.5 | wpb 2835.3 | bsz 115.6 | num_updates 115700 | best_bleu 37.24
2022-12-09 17:23:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 115700 updates
2022-12-09 17:23:31 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint105.pt
2022-12-09 17:23:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint105.pt (epoch 105 @ 115700 updates, score 37.06) (writing took 1.329710696823895 seconds)
2022-12-09 17:23:31 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-12-09 17:23:31 | INFO | train | epoch 105 | loss 6.988 | nll_loss 3.176 | ppl 9.04 | wps 24044.9 | ups 6.71 | wpb 3583.6 | bsz 145.4 | num_updates 115700 | lr 9.2968e-05 | gnorm 2.044 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 8724
2022-12-09 17:23:31 | INFO | fairseq.trainer | begin training epoch 106
2022-12-09 17:23:42 | INFO | train_inner | epoch 106:    100 / 1102 loss=6.992, nll_loss=3.158, ppl=8.93, wps=6677.6, ups=1.87, wpb=3578.8, bsz=122.7, num_updates=115800, lr=9.29278e-05, gnorm=2.1, loss_scale=4, train_wall=10, gb_free=19.3, wall=8735
2022-12-09 17:23:53 | INFO | train_inner | epoch 106:    200 / 1102 loss=7.041, nll_loss=3.196, ppl=9.16, wps=32689.3, ups=9.26, wpb=3530.3, bsz=119.6, num_updates=115900, lr=9.28877e-05, gnorm=2.11, loss_scale=4, train_wall=11, gb_free=19.4, wall=8746
2022-12-09 17:24:04 | INFO | train_inner | epoch 106:    300 / 1102 loss=7.006, nll_loss=3.186, ppl=9.1, wps=32609.6, ups=9.21, wpb=3541.7, bsz=137.4, num_updates=116000, lr=9.28477e-05, gnorm=2.09, loss_scale=4, train_wall=11, gb_free=19.8, wall=8756
2022-12-09 17:24:15 | INFO | train_inner | epoch 106:    400 / 1102 loss=6.928, nll_loss=3.129, ppl=8.75, wps=33625.3, ups=9.27, wpb=3627.1, bsz=153.1, num_updates=116100, lr=9.28077e-05, gnorm=2.117, loss_scale=4, train_wall=11, gb_free=19.8, wall=8767
2022-12-09 17:24:26 | INFO | train_inner | epoch 106:    500 / 1102 loss=6.939, nll_loss=3.134, ppl=8.78, wps=33253.7, ups=9.26, wpb=3591.7, bsz=163.1, num_updates=116200, lr=9.27677e-05, gnorm=2.014, loss_scale=4, train_wall=11, gb_free=19.8, wall=8778
2022-12-09 17:24:37 | INFO | train_inner | epoch 106:    600 / 1102 loss=7.007, nll_loss=3.191, ppl=9.13, wps=33296.6, ups=9.2, wpb=3618, bsz=144.2, num_updates=116300, lr=9.27278e-05, gnorm=2.126, loss_scale=4, train_wall=11, gb_free=19.3, wall=8789
2022-12-09 17:24:47 | INFO | train_inner | epoch 106:    700 / 1102 loss=6.947, nll_loss=3.136, ppl=8.79, wps=33075.8, ups=9.2, wpb=3594.3, bsz=152.6, num_updates=116400, lr=9.2688e-05, gnorm=2.068, loss_scale=4, train_wall=11, gb_free=19.4, wall=8800
2022-12-09 17:24:58 | INFO | train_inner | epoch 106:    800 / 1102 loss=7.01, nll_loss=3.206, ppl=9.23, wps=33096.2, ups=9.29, wpb=3562.4, bsz=156.4, num_updates=116500, lr=9.26482e-05, gnorm=2.058, loss_scale=4, train_wall=11, gb_free=19.3, wall=8811
2022-12-09 17:25:09 | INFO | train_inner | epoch 106:    900 / 1102 loss=6.996, nll_loss=3.187, ppl=9.11, wps=32905.4, ups=9.27, wpb=3548.1, bsz=151.7, num_updates=116600, lr=9.26085e-05, gnorm=2.078, loss_scale=4, train_wall=11, gb_free=19.5, wall=8821
2022-12-09 17:25:20 | INFO | train_inner | epoch 106:   1000 / 1102 loss=6.978, nll_loss=3.166, ppl=8.98, wps=33328.6, ups=9.2, wpb=3620.8, bsz=144.3, num_updates=116700, lr=9.25688e-05, gnorm=2.022, loss_scale=4, train_wall=11, gb_free=19.4, wall=8832
2022-12-09 17:25:31 | INFO | train_inner | epoch 106:   1100 / 1102 loss=6.981, nll_loss=3.187, ppl=9.11, wps=33144.5, ups=9.15, wpb=3624.1, bsz=152.7, num_updates=116800, lr=9.25292e-05, gnorm=2.027, loss_scale=4, train_wall=11, gb_free=19.8, wall=8843
2022-12-09 17:25:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:26:09 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 3.673 | nll_loss 2.127 | ppl 4.37 | bleu 37 | wps 4725.4 | wpb 2835.3 | bsz 115.6 | num_updates 116802 | best_bleu 37.24
2022-12-09 17:26:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 116802 updates
2022-12-09 17:26:10 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint106.pt
2022-12-09 17:26:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint106.pt (epoch 106 @ 116802 updates, score 37.0) (writing took 1.1862351214513183 seconds)
2022-12-09 17:26:10 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-12-09 17:26:10 | INFO | train | epoch 106 | loss 6.983 | nll_loss 3.17 | ppl 9 | wps 24834.3 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 116802 | lr 9.25284e-05 | gnorm 2.074 | loss_scale 4 | train_wall 117 | gb_free 19.7 | wall 8883
2022-12-09 17:26:10 | INFO | fairseq.trainer | begin training epoch 107
2022-12-09 17:26:21 | INFO | train_inner | epoch 107:     98 / 1102 loss=6.971, nll_loss=3.143, ppl=8.83, wps=7086.9, ups=1.98, wpb=3574.6, bsz=137.5, num_updates=116900, lr=9.24896e-05, gnorm=2.097, loss_scale=4, train_wall=10, gb_free=19.7, wall=8894
2022-12-09 17:26:32 | INFO | train_inner | epoch 107:    198 / 1102 loss=6.91, nll_loss=3.094, ppl=8.54, wps=32945.8, ups=9.23, wpb=3569.4, bsz=143.3, num_updates=117000, lr=9.245e-05, gnorm=1.982, loss_scale=4, train_wall=11, gb_free=19.5, wall=8904
2022-12-09 17:26:43 | INFO | train_inner | epoch 107:    298 / 1102 loss=6.98, nll_loss=3.176, ppl=9.04, wps=33963.8, ups=9.27, wpb=3664.3, bsz=145.7, num_updates=117100, lr=9.24105e-05, gnorm=1.987, loss_scale=4, train_wall=11, gb_free=19.4, wall=8915
2022-12-09 17:26:54 | INFO | train_inner | epoch 107:    398 / 1102 loss=7.023, nll_loss=3.194, ppl=9.15, wps=33668.5, ups=9.34, wpb=3605.4, bsz=131.8, num_updates=117200, lr=9.23711e-05, gnorm=2.082, loss_scale=4, train_wall=10, gb_free=19.3, wall=8926
2022-12-09 17:27:04 | INFO | train_inner | epoch 107:    498 / 1102 loss=6.907, nll_loss=3.101, ppl=8.58, wps=33363, ups=9.28, wpb=3595.1, bsz=153.3, num_updates=117300, lr=9.23317e-05, gnorm=2.062, loss_scale=4, train_wall=11, gb_free=19.4, wall=8937
2022-12-09 17:27:15 | INFO | train_inner | epoch 107:    598 / 1102 loss=6.948, nll_loss=3.133, ppl=8.77, wps=33329.9, ups=9.25, wpb=3602.3, bsz=154.6, num_updates=117400, lr=9.22924e-05, gnorm=2.051, loss_scale=4, train_wall=11, gb_free=19.5, wall=8948
2022-12-09 17:27:26 | INFO | train_inner | epoch 107:    698 / 1102 loss=6.996, nll_loss=3.176, ppl=9.04, wps=33186.3, ups=9.4, wpb=3530.4, bsz=142.5, num_updates=117500, lr=9.22531e-05, gnorm=2.092, loss_scale=4, train_wall=10, gb_free=19.5, wall=8958
2022-12-09 17:27:36 | INFO | train_inner | epoch 107:    798 / 1102 loss=7.096, nll_loss=3.271, ppl=9.66, wps=33095.8, ups=9.37, wpb=3532.1, bsz=126.7, num_updates=117600, lr=9.22139e-05, gnorm=2.089, loss_scale=4, train_wall=10, gb_free=19.6, wall=8969
2022-12-09 17:27:47 | INFO | train_inner | epoch 107:    898 / 1102 loss=6.976, nll_loss=3.177, ppl=9.05, wps=32971.2, ups=9.24, wpb=3568.3, bsz=157.3, num_updates=117700, lr=9.21747e-05, gnorm=2.016, loss_scale=4, train_wall=11, gb_free=19.2, wall=8980
2022-12-09 17:27:58 | INFO | train_inner | epoch 107:    998 / 1102 loss=6.939, nll_loss=3.128, ppl=8.74, wps=33024.3, ups=9.3, wpb=3552.6, bsz=151, num_updates=117800, lr=9.21356e-05, gnorm=2.048, loss_scale=4, train_wall=11, gb_free=19.3, wall=8990
2022-12-09 17:28:09 | INFO | train_inner | epoch 107:   1098 / 1102 loss=7.01, nll_loss=3.213, ppl=9.27, wps=33377.6, ups=9.22, wpb=3618.5, bsz=161.3, num_updates=117900, lr=9.20965e-05, gnorm=2.032, loss_scale=4, train_wall=11, gb_free=19.4, wall=9001
2022-12-09 17:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:28:47 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 3.674 | nll_loss 2.131 | ppl 4.38 | bleu 37.08 | wps 4807.8 | wpb 2835.3 | bsz 115.6 | num_updates 117904 | best_bleu 37.24
2022-12-09 17:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 117904 updates
2022-12-09 17:28:48 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint107.pt
2022-12-09 17:28:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint107.pt (epoch 107 @ 117904 updates, score 37.08) (writing took 1.396172753535211 seconds)
2022-12-09 17:28:48 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-12-09 17:28:48 | INFO | train | epoch 107 | loss 6.98 | nll_loss 3.166 | ppl 8.97 | wps 25014.6 | ups 6.98 | wpb 3583.6 | bsz 145.4 | num_updates 117904 | lr 9.20949e-05 | gnorm 2.052 | loss_scale 4 | train_wall 116 | gb_free 19.7 | wall 9041
2022-12-09 17:28:48 | INFO | fairseq.trainer | begin training epoch 108
2022-12-09 17:28:59 | INFO | train_inner | epoch 108:     96 / 1102 loss=6.972, nll_loss=3.159, ppl=8.93, wps=7116.7, ups=1.99, wpb=3582.1, bsz=140.6, num_updates=118000, lr=9.20575e-05, gnorm=2.046, loss_scale=4, train_wall=11, gb_free=19.4, wall=9052
2022-12-09 17:29:10 | INFO | train_inner | epoch 108:    196 / 1102 loss=6.97, nll_loss=3.142, ppl=8.83, wps=32669, ups=9.26, wpb=3527.6, bsz=136.5, num_updates=118100, lr=9.20185e-05, gnorm=2.11, loss_scale=4, train_wall=11, gb_free=19.4, wall=9062
2022-12-09 17:29:21 | INFO | train_inner | epoch 108:    296 / 1102 loss=6.979, nll_loss=3.169, ppl=8.99, wps=33330.7, ups=9.1, wpb=3662.2, bsz=144.2, num_updates=118200, lr=9.19795e-05, gnorm=1.985, loss_scale=4, train_wall=11, gb_free=19.8, wall=9073
2022-12-09 17:29:32 | INFO | train_inner | epoch 108:    396 / 1102 loss=6.953, nll_loss=3.136, ppl=8.79, wps=32797.6, ups=9.1, wpb=3604.2, bsz=151, num_updates=118300, lr=9.19407e-05, gnorm=2.051, loss_scale=4, train_wall=11, gb_free=19.6, wall=9084
2022-12-09 17:29:43 | INFO | train_inner | epoch 108:    496 / 1102 loss=6.953, nll_loss=3.125, ppl=8.72, wps=32661.2, ups=9.22, wpb=3543.7, bsz=139.8, num_updates=118400, lr=9.19018e-05, gnorm=2.066, loss_scale=4, train_wall=11, gb_free=19.2, wall=9095
2022-12-09 17:29:54 | INFO | train_inner | epoch 108:    596 / 1102 loss=6.993, nll_loss=3.183, ppl=9.08, wps=33149, ups=9.26, wpb=3579.7, bsz=147.1, num_updates=118500, lr=9.1863e-05, gnorm=2.047, loss_scale=4, train_wall=11, gb_free=19.9, wall=9106
2022-12-09 17:30:04 | INFO | train_inner | epoch 108:    696 / 1102 loss=7.004, nll_loss=3.17, ppl=9, wps=32660.7, ups=9.37, wpb=3485.2, bsz=140.6, num_updates=118600, lr=9.18243e-05, gnorm=2.172, loss_scale=4, train_wall=10, gb_free=19.5, wall=9117
2022-12-09 17:30:15 | INFO | train_inner | epoch 108:    796 / 1102 loss=6.903, nll_loss=3.106, ppl=8.61, wps=33192.1, ups=9.13, wpb=3634.3, bsz=160.8, num_updates=118700, lr=9.17856e-05, gnorm=1.94, loss_scale=4, train_wall=11, gb_free=19.3, wall=9128
2022-12-09 17:30:26 | INFO | train_inner | epoch 108:    896 / 1102 loss=7.033, nll_loss=3.212, ppl=9.27, wps=33183.7, ups=9.23, wpb=3596.7, bsz=134.3, num_updates=118800, lr=9.1747e-05, gnorm=2.096, loss_scale=4, train_wall=11, gb_free=19.3, wall=9138
2022-12-09 17:30:37 | INFO | train_inner | epoch 108:    996 / 1102 loss=7.033, nll_loss=3.229, ppl=9.38, wps=33265.6, ups=9.23, wpb=3605.3, bsz=148.2, num_updates=118900, lr=9.17084e-05, gnorm=2.132, loss_scale=4, train_wall=11, gb_free=19.4, wall=9149
2022-12-09 17:30:48 | INFO | train_inner | epoch 108:   1096 / 1102 loss=6.945, nll_loss=3.137, ppl=8.79, wps=33126.9, ups=9.27, wpb=3574, bsz=153.6, num_updates=119000, lr=9.16698e-05, gnorm=2.113, loss_scale=4, train_wall=11, gb_free=19.4, wall=9160
2022-12-09 17:30:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:31:27 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 3.672 | nll_loss 2.129 | ppl 4.37 | bleu 37.12 | wps 4703.2 | wpb 2835.3 | bsz 115.6 | num_updates 119006 | best_bleu 37.24
2022-12-09 17:31:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 119006 updates
2022-12-09 17:31:28 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint108.pt
2022-12-09 17:31:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint108.pt (epoch 108 @ 119006 updates, score 37.12) (writing took 1.4267301792278886 seconds)
2022-12-09 17:31:28 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-12-09 17:31:28 | INFO | train | epoch 108 | loss 6.975 | nll_loss 3.16 | ppl 8.94 | wps 24683 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 119006 | lr 9.16675e-05 | gnorm 2.065 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 9201
2022-12-09 17:31:28 | INFO | fairseq.trainer | begin training epoch 109
2022-12-09 17:31:39 | INFO | train_inner | epoch 109:     94 / 1102 loss=6.99, nll_loss=3.167, ppl=8.98, wps=7032.1, ups=1.96, wpb=3581.3, bsz=132.5, num_updates=119100, lr=9.16314e-05, gnorm=2.1, loss_scale=4, train_wall=10, gb_free=19.3, wall=9211
2022-12-09 17:31:50 | INFO | train_inner | epoch 109:    194 / 1102 loss=6.911, nll_loss=3.117, ppl=8.68, wps=33719.8, ups=9.17, wpb=3676.4, bsz=160.3, num_updates=119200, lr=9.15929e-05, gnorm=1.948, loss_scale=4, train_wall=11, gb_free=19.8, wall=9222
2022-12-09 17:32:00 | INFO | train_inner | epoch 109:    294 / 1102 loss=6.983, nll_loss=3.17, ppl=9, wps=33360.2, ups=9.25, wpb=3607, bsz=147.6, num_updates=119300, lr=9.15545e-05, gnorm=2.087, loss_scale=4, train_wall=11, gb_free=19.3, wall=9233
2022-12-09 17:32:11 | INFO | train_inner | epoch 109:    394 / 1102 loss=6.924, nll_loss=3.098, ppl=8.56, wps=33114.7, ups=9.29, wpb=3564.9, bsz=145, num_updates=119400, lr=9.15162e-05, gnorm=2.086, loss_scale=4, train_wall=11, gb_free=19.8, wall=9244
2022-12-09 17:32:22 | INFO | train_inner | epoch 109:    494 / 1102 loss=6.981, nll_loss=3.171, ppl=9.01, wps=33521.7, ups=9.18, wpb=3653.3, bsz=142.2, num_updates=119500, lr=9.14779e-05, gnorm=1.977, loss_scale=4, train_wall=11, gb_free=19.7, wall=9254
2022-12-09 17:32:33 | INFO | train_inner | epoch 109:    594 / 1102 loss=7.042, nll_loss=3.201, ppl=9.19, wps=32359, ups=9.28, wpb=3488, bsz=125.4, num_updates=119600, lr=9.14396e-05, gnorm=2.137, loss_scale=4, train_wall=11, gb_free=19.6, wall=9265
2022-12-09 17:32:44 | INFO | train_inner | epoch 109:    694 / 1102 loss=6.953, nll_loss=3.145, ppl=8.85, wps=32982.7, ups=9.22, wpb=3576.8, bsz=162.8, num_updates=119700, lr=9.14014e-05, gnorm=2.15, loss_scale=4, train_wall=11, gb_free=19.3, wall=9276
2022-12-09 17:32:54 | INFO | train_inner | epoch 109:    794 / 1102 loss=6.957, nll_loss=3.135, ppl=8.78, wps=33035.3, ups=9.33, wpb=3540.4, bsz=139.2, num_updates=119800, lr=9.13633e-05, gnorm=2.072, loss_scale=4, train_wall=10, gb_free=19.7, wall=9287
2022-12-09 17:33:05 | INFO | train_inner | epoch 109:    894 / 1102 loss=6.968, nll_loss=3.154, ppl=8.9, wps=32989.5, ups=9.29, wpb=3552.5, bsz=150.6, num_updates=119900, lr=9.13252e-05, gnorm=2.091, loss_scale=4, train_wall=11, gb_free=19.7, wall=9298
2022-12-09 17:33:16 | INFO | train_inner | epoch 109:    994 / 1102 loss=6.995, nll_loss=3.176, ppl=9.04, wps=33353.9, ups=9.41, wpb=3543.9, bsz=145.8, num_updates=120000, lr=9.12871e-05, gnorm=2.075, loss_scale=4, train_wall=10, gb_free=19.3, wall=9308
2022-12-09 17:33:27 | INFO | train_inner | epoch 109:   1094 / 1102 loss=6.974, nll_loss=3.166, ppl=8.97, wps=33504.7, ups=9.21, wpb=3637.1, bsz=149.5, num_updates=120100, lr=9.12491e-05, gnorm=2.131, loss_scale=4, train_wall=11, gb_free=19.4, wall=9319
2022-12-09 17:33:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:34:06 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 3.674 | nll_loss 2.13 | ppl 4.38 | bleu 37.22 | wps 4714.6 | wpb 2835.3 | bsz 115.6 | num_updates 120108 | best_bleu 37.24
2022-12-09 17:34:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 120108 updates
2022-12-09 17:34:07 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint109.pt
2022-12-09 17:34:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint109.pt (epoch 109 @ 120108 updates, score 37.22) (writing took 1.4612403390929103 seconds)
2022-12-09 17:34:07 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-12-09 17:34:07 | INFO | train | epoch 109 | loss 6.971 | nll_loss 3.154 | ppl 8.9 | wps 24840.1 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 120108 | lr 9.1246e-05 | gnorm 2.077 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 9360
2022-12-09 17:34:07 | INFO | fairseq.trainer | begin training epoch 110
2022-12-09 17:34:17 | INFO | train_inner | epoch 110:     92 / 1102 loss=6.969, nll_loss=3.139, ppl=8.81, wps=6968.1, ups=1.97, wpb=3532.3, bsz=129.4, num_updates=120200, lr=9.12111e-05, gnorm=2.044, loss_scale=4, train_wall=10, gb_free=19.8, wall=9370
2022-12-09 17:34:28 | INFO | train_inner | epoch 110:    192 / 1102 loss=6.889, nll_loss=3.084, ppl=8.48, wps=33252.3, ups=9.2, wpb=3615, bsz=150, num_updates=120300, lr=9.11732e-05, gnorm=2.019, loss_scale=4, train_wall=11, gb_free=19.7, wall=9381
2022-12-09 17:34:39 | INFO | train_inner | epoch 110:    292 / 1102 loss=6.948, nll_loss=3.132, ppl=8.77, wps=32966.9, ups=9.18, wpb=3589.6, bsz=145.4, num_updates=120400, lr=9.11353e-05, gnorm=2.049, loss_scale=4, train_wall=11, gb_free=19.5, wall=9391
2022-12-09 17:34:50 | INFO | train_inner | epoch 110:    392 / 1102 loss=6.902, nll_loss=3.086, ppl=8.49, wps=32860.6, ups=9.2, wpb=3572.9, bsz=156.6, num_updates=120500, lr=9.10975e-05, gnorm=2.109, loss_scale=4, train_wall=11, gb_free=19.7, wall=9402
2022-12-09 17:35:01 | INFO | train_inner | epoch 110:    492 / 1102 loss=6.97, nll_loss=3.149, ppl=8.87, wps=32612.4, ups=9.24, wpb=3528.4, bsz=143.1, num_updates=120600, lr=9.10597e-05, gnorm=2.095, loss_scale=4, train_wall=11, gb_free=20, wall=9413
2022-12-09 17:35:12 | INFO | train_inner | epoch 110:    592 / 1102 loss=6.983, nll_loss=3.167, ppl=8.98, wps=33001.7, ups=9.2, wpb=3585.9, bsz=147, num_updates=120700, lr=9.1022e-05, gnorm=2.049, loss_scale=4, train_wall=11, gb_free=19.3, wall=9424
2022-12-09 17:35:22 | INFO | train_inner | epoch 110:    692 / 1102 loss=6.991, nll_loss=3.172, ppl=9.01, wps=33004.6, ups=9.23, wpb=3573.9, bsz=140.7, num_updates=120800, lr=9.09843e-05, gnorm=2.069, loss_scale=4, train_wall=11, gb_free=19.5, wall=9435
2022-12-09 17:35:33 | INFO | train_inner | epoch 110:    792 / 1102 loss=6.931, nll_loss=3.125, ppl=8.72, wps=32780.3, ups=9.21, wpb=3560.4, bsz=159.1, num_updates=120900, lr=9.09467e-05, gnorm=2.033, loss_scale=4, train_wall=11, gb_free=19.3, wall=9446
2022-12-09 17:35:44 | INFO | train_inner | epoch 110:    892 / 1102 loss=7.066, nll_loss=3.243, ppl=9.47, wps=32631.6, ups=8.98, wpb=3634.4, bsz=135.7, num_updates=121000, lr=9.09091e-05, gnorm=2.076, loss_scale=4, train_wall=11, gb_free=19.8, wall=9457
2022-12-09 17:35:55 | INFO | train_inner | epoch 110:    992 / 1102 loss=6.994, nll_loss=3.174, ppl=9.02, wps=32551.9, ups=9.05, wpb=3595, bsz=139.4, num_updates=121100, lr=9.08715e-05, gnorm=2.029, loss_scale=4, train_wall=11, gb_free=19.3, wall=9468
2022-12-09 17:36:06 | INFO | train_inner | epoch 110:   1092 / 1102 loss=6.97, nll_loss=3.167, ppl=8.98, wps=33893.9, ups=9.33, wpb=3634.3, bsz=153.5, num_updates=121200, lr=9.08341e-05, gnorm=2.047, loss_scale=4, train_wall=10, gb_free=19.4, wall=9479
2022-12-09 17:36:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:36:44 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 3.671 | nll_loss 2.128 | ppl 4.37 | bleu 37.25 | wps 4984.3 | wpb 2835.3 | bsz 115.6 | num_updates 121210 | best_bleu 37.25
2022-12-09 17:36:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 121210 updates
2022-12-09 17:36:44 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint110.pt
2022-12-09 17:36:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint110.pt (epoch 110 @ 121210 updates, score 37.25) (writing took 1.5657315412536263 seconds)
2022-12-09 17:36:45 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-12-09 17:36:45 | INFO | train | epoch 110 | loss 6.967 | nll_loss 3.15 | ppl 8.88 | wps 25021.2 | ups 6.98 | wpb 3583.6 | bsz 145.4 | num_updates 121210 | lr 9.08303e-05 | gnorm 2.058 | loss_scale 4 | train_wall 117 | gb_free 19.3 | wall 9517
2022-12-09 17:36:45 | INFO | fairseq.trainer | begin training epoch 111
2022-12-09 17:36:55 | INFO | train_inner | epoch 111:     90 / 1102 loss=6.886, nll_loss=3.078, ppl=8.44, wps=7243.3, ups=2.04, wpb=3551.7, bsz=160.1, num_updates=121300, lr=9.07966e-05, gnorm=1.989, loss_scale=4, train_wall=11, gb_free=19.3, wall=9528
2022-12-09 17:37:06 | INFO | train_inner | epoch 111:    190 / 1102 loss=6.911, nll_loss=3.09, ppl=8.52, wps=31916.4, ups=8.97, wpb=3558.6, bsz=146.5, num_updates=121400, lr=9.07592e-05, gnorm=2.014, loss_scale=4, train_wall=11, gb_free=19.6, wall=9539
2022-12-09 17:37:17 | INFO | train_inner | epoch 111:    290 / 1102 loss=6.862, nll_loss=3.063, ppl=8.36, wps=33315.3, ups=9.18, wpb=3630.4, bsz=168.6, num_updates=121500, lr=9.07218e-05, gnorm=2.024, loss_scale=4, train_wall=11, gb_free=20, wall=9550
2022-12-09 17:37:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-12-09 17:37:28 | INFO | train_inner | epoch 111:    391 / 1102 loss=6.958, nll_loss=3.138, ppl=8.8, wps=33931.7, ups=9.38, wpb=3618, bsz=141.7, num_updates=121600, lr=9.06845e-05, gnorm=2.062, loss_scale=2, train_wall=10, gb_free=19.8, wall=9560
2022-12-09 17:37:39 | INFO | train_inner | epoch 111:    491 / 1102 loss=6.961, nll_loss=3.144, ppl=8.84, wps=33695, ups=9.27, wpb=3632.9, bsz=144.2, num_updates=121700, lr=9.06473e-05, gnorm=2.024, loss_scale=2, train_wall=11, gb_free=19.3, wall=9571
2022-12-09 17:37:50 | INFO | train_inner | epoch 111:    591 / 1102 loss=6.96, nll_loss=3.145, ppl=8.85, wps=33421.8, ups=9.26, wpb=3607.6, bsz=139.9, num_updates=121800, lr=9.061e-05, gnorm=2.059, loss_scale=2, train_wall=11, gb_free=19.7, wall=9582
2022-12-09 17:38:00 | INFO | train_inner | epoch 111:    691 / 1102 loss=6.97, nll_loss=3.14, ppl=8.82, wps=32919.2, ups=9.37, wpb=3513.8, bsz=147.3, num_updates=121900, lr=9.05729e-05, gnorm=2.164, loss_scale=2, train_wall=10, gb_free=19.5, wall=9593
2022-12-09 17:38:11 | INFO | train_inner | epoch 111:    791 / 1102 loss=6.995, nll_loss=3.186, ppl=9.1, wps=32931.2, ups=9.16, wpb=3594.8, bsz=141.9, num_updates=122000, lr=9.05357e-05, gnorm=2.063, loss_scale=2, train_wall=11, gb_free=19.4, wall=9604
2022-12-09 17:38:22 | INFO | train_inner | epoch 111:    891 / 1102 loss=7.014, nll_loss=3.203, ppl=9.21, wps=33773.2, ups=9.34, wpb=3617.4, bsz=139.9, num_updates=122100, lr=9.04987e-05, gnorm=2.036, loss_scale=2, train_wall=10, gb_free=19.8, wall=9614
2022-12-09 17:38:32 | INFO | train_inner | epoch 111:    991 / 1102 loss=7.022, nll_loss=3.204, ppl=9.22, wps=33423.1, ups=9.41, wpb=3551.6, bsz=140.5, num_updates=122200, lr=9.04616e-05, gnorm=2.138, loss_scale=2, train_wall=10, gb_free=19.4, wall=9625
2022-12-09 17:38:43 | INFO | train_inner | epoch 111:   1091 / 1102 loss=7.071, nll_loss=3.237, ppl=9.43, wps=33332.6, ups=9.37, wpb=3556.6, bsz=132.4, num_updates=122300, lr=9.04246e-05, gnorm=2.136, loss_scale=2, train_wall=10, gb_free=19.5, wall=9636
2022-12-09 17:38:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:39:23 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 3.668 | nll_loss 2.127 | ppl 4.37 | bleu 37.16 | wps 4728.2 | wpb 2835.3 | bsz 115.6 | num_updates 122311 | best_bleu 37.25
2022-12-09 17:39:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 122311 updates
2022-12-09 17:39:23 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint111.pt
2022-12-09 17:39:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint111.pt (epoch 111 @ 122311 updates, score 37.16) (writing took 1.0375330625101924 seconds)
2022-12-09 17:39:24 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-12-09 17:39:24 | INFO | train | epoch 111 | loss 6.963 | nll_loss 3.146 | ppl 8.85 | wps 24895.9 | ups 6.95 | wpb 3583.4 | bsz 145.4 | num_updates 122311 | lr 9.04206e-05 | gnorm 2.065 | loss_scale 2 | train_wall 116 | gb_free 19.3 | wall 9676
2022-12-09 17:39:24 | INFO | fairseq.trainer | begin training epoch 112
2022-12-09 17:39:33 | INFO | train_inner | epoch 112:     89 / 1102 loss=6.864, nll_loss=3.044, ppl=8.25, wps=7053.9, ups=1.99, wpb=3548.1, bsz=139.9, num_updates=122400, lr=9.03877e-05, gnorm=2.008, loss_scale=2, train_wall=11, gb_free=19.5, wall=9686
2022-12-09 17:39:44 | INFO | train_inner | epoch 112:    189 / 1102 loss=6.892, nll_loss=3.074, ppl=8.42, wps=34215.2, ups=9.52, wpb=3595.9, bsz=153.4, num_updates=122500, lr=9.03508e-05, gnorm=2.009, loss_scale=2, train_wall=10, gb_free=19.4, wall=9696
2022-12-09 17:39:54 | INFO | train_inner | epoch 112:    289 / 1102 loss=6.942, nll_loss=3.119, ppl=8.69, wps=33785.6, ups=9.49, wpb=3561, bsz=147.3, num_updates=122600, lr=9.03139e-05, gnorm=2.077, loss_scale=2, train_wall=10, gb_free=19.8, wall=9707
2022-12-09 17:40:05 | INFO | train_inner | epoch 112:    389 / 1102 loss=6.981, nll_loss=3.174, ppl=9.02, wps=33867.7, ups=9.42, wpb=3593.8, bsz=147.2, num_updates=122700, lr=9.02771e-05, gnorm=2.052, loss_scale=2, train_wall=10, gb_free=19.8, wall=9717
2022-12-09 17:40:16 | INFO | train_inner | epoch 112:    489 / 1102 loss=6.916, nll_loss=3.104, ppl=8.6, wps=34077.5, ups=9.36, wpb=3639.6, bsz=149.8, num_updates=122800, lr=9.02404e-05, gnorm=2.16, loss_scale=2, train_wall=10, gb_free=19.5, wall=9728
2022-12-09 17:40:27 | INFO | train_inner | epoch 112:    589 / 1102 loss=7.018, nll_loss=3.202, ppl=9.2, wps=34342.8, ups=9.31, wpb=3687.4, bsz=138.8, num_updates=122900, lr=9.02036e-05, gnorm=1.987, loss_scale=2, train_wall=11, gb_free=19.4, wall=9739
2022-12-09 17:40:37 | INFO | train_inner | epoch 112:    689 / 1102 loss=7.029, nll_loss=3.193, ppl=9.15, wps=33633.5, ups=9.43, wpb=3567.4, bsz=132, num_updates=123000, lr=9.0167e-05, gnorm=2.106, loss_scale=2, train_wall=10, gb_free=19.4, wall=9750
2022-12-09 17:40:48 | INFO | train_inner | epoch 112:    789 / 1102 loss=6.927, nll_loss=3.114, ppl=8.66, wps=33088.6, ups=9.41, wpb=3515.6, bsz=153.5, num_updates=123100, lr=9.01303e-05, gnorm=2.151, loss_scale=2, train_wall=10, gb_free=19.5, wall=9760
2022-12-09 17:40:58 | INFO | train_inner | epoch 112:    889 / 1102 loss=6.96, nll_loss=3.136, ppl=8.79, wps=33126.3, ups=9.33, wpb=3550.4, bsz=140.1, num_updates=123200, lr=9.00937e-05, gnorm=2.145, loss_scale=2, train_wall=10, gb_free=19.7, wall=9771
2022-12-09 17:41:09 | INFO | train_inner | epoch 112:    989 / 1102 loss=7.019, nll_loss=3.2, ppl=9.19, wps=33297.4, ups=9.41, wpb=3538.3, bsz=143.5, num_updates=123300, lr=9.00572e-05, gnorm=2.11, loss_scale=2, train_wall=10, gb_free=19.4, wall=9781
2022-12-09 17:41:20 | INFO | train_inner | epoch 112:   1089 / 1102 loss=6.98, nll_loss=3.173, ppl=9.02, wps=33512.9, ups=9.32, wpb=3594.1, bsz=150.4, num_updates=123400, lr=9.00207e-05, gnorm=2.053, loss_scale=2, train_wall=10, gb_free=19.8, wall=9792
2022-12-09 17:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:41:58 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 3.674 | nll_loss 2.128 | ppl 4.37 | bleu 37.16 | wps 4898.5 | wpb 2835.3 | bsz 115.6 | num_updates 123413 | best_bleu 37.25
2022-12-09 17:41:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 123413 updates
2022-12-09 17:41:59 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint112.pt
2022-12-09 17:41:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint112.pt (epoch 112 @ 123413 updates, score 37.16) (writing took 1.0054424637928605 seconds)
2022-12-09 17:41:59 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-12-09 17:41:59 | INFO | train | epoch 112 | loss 6.957 | nll_loss 3.14 | ppl 8.81 | wps 25386.5 | ups 7.08 | wpb 3583.6 | bsz 145.4 | num_updates 123413 | lr 9.0016e-05 | gnorm 2.077 | loss_scale 2 | train_wall 115 | gb_free 19.4 | wall 9831
2022-12-09 17:41:59 | INFO | fairseq.trainer | begin training epoch 113
2022-12-09 17:42:09 | INFO | train_inner | epoch 113:     87 / 1102 loss=6.885, nll_loss=3.077, ppl=8.44, wps=7413.4, ups=2.04, wpb=3635.7, bsz=151.1, num_updates=123500, lr=8.99843e-05, gnorm=2.013, loss_scale=2, train_wall=11, gb_free=19.4, wall=9841
2022-12-09 17:42:20 | INFO | train_inner | epoch 113:    187 / 1102 loss=6.95, nll_loss=3.121, ppl=8.7, wps=33239.4, ups=9.21, wpb=3607.2, bsz=136.9, num_updates=123600, lr=8.99478e-05, gnorm=2.048, loss_scale=2, train_wall=11, gb_free=19.5, wall=9852
2022-12-09 17:42:30 | INFO | train_inner | epoch 113:    287 / 1102 loss=6.945, nll_loss=3.122, ppl=8.71, wps=33707.9, ups=9.34, wpb=3608, bsz=143, num_updates=123700, lr=8.99115e-05, gnorm=2.097, loss_scale=2, train_wall=10, gb_free=20, wall=9863
2022-12-09 17:42:41 | INFO | train_inner | epoch 113:    387 / 1102 loss=6.927, nll_loss=3.116, ppl=8.67, wps=33520.2, ups=9.36, wpb=3581.6, bsz=150.4, num_updates=123800, lr=8.98752e-05, gnorm=2.068, loss_scale=2, train_wall=10, gb_free=19.4, wall=9873
2022-12-09 17:42:52 | INFO | train_inner | epoch 113:    487 / 1102 loss=6.911, nll_loss=3.099, ppl=8.57, wps=32442.1, ups=9.11, wpb=3562, bsz=157.1, num_updates=123900, lr=8.98389e-05, gnorm=2.122, loss_scale=2, train_wall=11, gb_free=19.4, wall=9884
2022-12-09 17:43:03 | INFO | train_inner | epoch 113:    587 / 1102 loss=6.966, nll_loss=3.149, ppl=8.87, wps=32727.4, ups=9.13, wpb=3583.4, bsz=147.9, num_updates=124000, lr=8.98027e-05, gnorm=2.065, loss_scale=2, train_wall=11, gb_free=19.4, wall=9895
2022-12-09 17:43:14 | INFO | train_inner | epoch 113:    687 / 1102 loss=7.025, nll_loss=3.209, ppl=9.25, wps=33098.4, ups=9.28, wpb=3564.9, bsz=143.8, num_updates=124100, lr=8.97665e-05, gnorm=2.113, loss_scale=2, train_wall=11, gb_free=19.5, wall=9906
2022-12-09 17:43:25 | INFO | train_inner | epoch 113:    787 / 1102 loss=6.94, nll_loss=3.108, ppl=8.62, wps=32983.8, ups=9.32, wpb=3538.9, bsz=142.2, num_updates=124200, lr=8.97303e-05, gnorm=2.146, loss_scale=2, train_wall=11, gb_free=19.4, wall=9917
2022-12-09 17:43:35 | INFO | train_inner | epoch 113:    887 / 1102 loss=6.973, nll_loss=3.159, ppl=8.93, wps=33232.5, ups=9.19, wpb=3615.1, bsz=145, num_updates=124300, lr=8.96942e-05, gnorm=2.034, loss_scale=2, train_wall=11, gb_free=19.7, wall=9928
2022-12-09 17:43:46 | INFO | train_inner | epoch 113:    987 / 1102 loss=7.005, nll_loss=3.165, ppl=8.97, wps=33077.3, ups=9.37, wpb=3531, bsz=128.3, num_updates=124400, lr=8.96582e-05, gnorm=2.116, loss_scale=2, train_wall=10, gb_free=19.7, wall=9938
2022-12-09 17:43:57 | INFO | train_inner | epoch 113:   1087 / 1102 loss=6.97, nll_loss=3.161, ppl=8.94, wps=32554.5, ups=9.06, wpb=3593, bsz=148.2, num_updates=124500, lr=8.96221e-05, gnorm=2.098, loss_scale=2, train_wall=11, gb_free=19.7, wall=9950
2022-12-09 17:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:44:35 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 3.673 | nll_loss 2.13 | ppl 4.38 | bleu 37.04 | wps 4957 | wpb 2835.3 | bsz 115.6 | num_updates 124515 | best_bleu 37.25
2022-12-09 17:44:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 124515 updates
2022-12-09 17:44:36 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint113.pt
2022-12-09 17:44:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint113.pt (epoch 113 @ 124515 updates, score 37.04) (writing took 1.1191446213051677 seconds)
2022-12-09 17:44:36 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-12-09 17:44:36 | INFO | train | epoch 113 | loss 6.954 | nll_loss 3.136 | ppl 8.79 | wps 25110.4 | ups 7.01 | wpb 3583.6 | bsz 145.4 | num_updates 124515 | lr 8.96167e-05 | gnorm 2.086 | loss_scale 2 | train_wall 117 | gb_free 19.4 | wall 9989
2022-12-09 17:44:36 | INFO | fairseq.trainer | begin training epoch 114
2022-12-09 17:44:46 | INFO | train_inner | epoch 114:     85 / 1102 loss=6.904, nll_loss=3.109, ppl=8.63, wps=7537.6, ups=2.04, wpb=3689.7, bsz=158.8, num_updates=124600, lr=8.95862e-05, gnorm=1.927, loss_scale=2, train_wall=11, gb_free=19.3, wall=9998
2022-12-09 17:44:57 | INFO | train_inner | epoch 114:    185 / 1102 loss=6.968, nll_loss=3.136, ppl=8.79, wps=33590.8, ups=9.32, wpb=3602.8, bsz=131.2, num_updates=124700, lr=8.95502e-05, gnorm=2.028, loss_scale=2, train_wall=10, gb_free=19.3, wall=10009
2022-12-09 17:45:08 | INFO | train_inner | epoch 114:    285 / 1102 loss=6.91, nll_loss=3.083, ppl=8.47, wps=33153.8, ups=9.26, wpb=3581.8, bsz=141.8, num_updates=124800, lr=8.95144e-05, gnorm=2.031, loss_scale=2, train_wall=11, gb_free=19.4, wall=10020
2022-12-09 17:45:18 | INFO | train_inner | epoch 114:    385 / 1102 loss=6.872, nll_loss=3.054, ppl=8.3, wps=33167.7, ups=9.18, wpb=3612, bsz=150.4, num_updates=124900, lr=8.94785e-05, gnorm=2.035, loss_scale=2, train_wall=11, gb_free=19.4, wall=10031
2022-12-09 17:45:29 | INFO | train_inner | epoch 114:    485 / 1102 loss=6.895, nll_loss=3.095, ppl=8.54, wps=34218.8, ups=9.24, wpb=3705.2, bsz=154.9, num_updates=125000, lr=8.94427e-05, gnorm=1.961, loss_scale=2, train_wall=11, gb_free=19.5, wall=10042
2022-12-09 17:45:40 | INFO | train_inner | epoch 114:    585 / 1102 loss=6.977, nll_loss=3.155, ppl=8.91, wps=32153.5, ups=9.05, wpb=3552.4, bsz=138.8, num_updates=125100, lr=8.9407e-05, gnorm=2.075, loss_scale=2, train_wall=11, gb_free=19.4, wall=10053
2022-12-09 17:45:51 | INFO | train_inner | epoch 114:    685 / 1102 loss=6.959, nll_loss=3.129, ppl=8.75, wps=33497.3, ups=9.31, wpb=3596.5, bsz=134.3, num_updates=125200, lr=8.93713e-05, gnorm=2.024, loss_scale=2, train_wall=11, gb_free=19.3, wall=10063
2022-12-09 17:46:02 | INFO | train_inner | epoch 114:    785 / 1102 loss=6.98, nll_loss=3.157, ppl=8.92, wps=32604.2, ups=9.25, wpb=3524.6, bsz=149.4, num_updates=125300, lr=8.93356e-05, gnorm=2.176, loss_scale=2, train_wall=11, gb_free=19.4, wall=10074
2022-12-09 17:46:13 | INFO | train_inner | epoch 114:    885 / 1102 loss=7.029, nll_loss=3.201, ppl=9.2, wps=32686, ups=9.15, wpb=3573, bsz=132.6, num_updates=125400, lr=8.93e-05, gnorm=2.155, loss_scale=2, train_wall=11, gb_free=19.2, wall=10085
2022-12-09 17:46:24 | INFO | train_inner | epoch 114:    985 / 1102 loss=6.935, nll_loss=3.115, ppl=8.67, wps=32161.9, ups=9.35, wpb=3439.8, bsz=163.7, num_updates=125500, lr=8.92644e-05, gnorm=2.166, loss_scale=2, train_wall=10, gb_free=20.4, wall=10096
2022-12-09 17:46:34 | INFO | train_inner | epoch 114:   1085 / 1102 loss=7.037, nll_loss=3.219, ppl=9.31, wps=33338.3, ups=9.38, wpb=3553.3, bsz=139.8, num_updates=125600, lr=8.92288e-05, gnorm=2.079, loss_scale=2, train_wall=10, gb_free=19.8, wall=10107
2022-12-09 17:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:47:13 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 3.676 | nll_loss 2.128 | ppl 4.37 | bleu 37.13 | wps 4914.4 | wpb 2835.3 | bsz 115.6 | num_updates 125617 | best_bleu 37.25
2022-12-09 17:47:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 125617 updates
2022-12-09 17:47:14 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint114.pt
2022-12-09 17:47:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint114.pt (epoch 114 @ 125617 updates, score 37.13) (writing took 1.113039550371468 seconds)
2022-12-09 17:47:14 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-12-09 17:47:14 | INFO | train | epoch 114 | loss 6.949 | nll_loss 3.13 | ppl 8.75 | wps 25053.3 | ups 6.99 | wpb 3583.6 | bsz 145.4 | num_updates 125617 | lr 8.92228e-05 | gnorm 2.058 | loss_scale 2 | train_wall 117 | gb_free 19.5 | wall 10146
2022-12-09 17:47:14 | INFO | fairseq.trainer | begin training epoch 115
2022-12-09 17:47:23 | INFO | train_inner | epoch 115:     83 / 1102 loss=6.876, nll_loss=3.055, ppl=8.31, wps=7350.4, ups=2.03, wpb=3619, bsz=152.5, num_updates=125700, lr=8.91933e-05, gnorm=2.18, loss_scale=2, train_wall=11, gb_free=19.7, wall=10156
2022-12-09 17:47:35 | INFO | train_inner | epoch 115:    183 / 1102 loss=6.925, nll_loss=3.119, ppl=8.68, wps=33249.4, ups=9.03, wpb=3681.8, bsz=154.5, num_updates=125800, lr=8.91579e-05, gnorm=2.029, loss_scale=2, train_wall=11, gb_free=19.4, wall=10167
2022-12-09 17:47:45 | INFO | train_inner | epoch 115:    283 / 1102 loss=6.947, nll_loss=3.123, ppl=8.71, wps=33006.5, ups=9.2, wpb=3587.5, bsz=139.5, num_updates=125900, lr=8.91225e-05, gnorm=2.149, loss_scale=2, train_wall=11, gb_free=19.6, wall=10178
2022-12-09 17:47:56 | INFO | train_inner | epoch 115:    383 / 1102 loss=6.869, nll_loss=3.04, ppl=8.22, wps=32926.7, ups=9.2, wpb=3577.4, bsz=150.6, num_updates=126000, lr=8.90871e-05, gnorm=2.105, loss_scale=2, train_wall=11, gb_free=19.7, wall=10189
2022-12-09 17:48:07 | INFO | train_inner | epoch 115:    483 / 1102 loss=6.911, nll_loss=3.1, ppl=8.58, wps=33560.7, ups=9.25, wpb=3629.2, bsz=150.7, num_updates=126100, lr=8.90517e-05, gnorm=1.975, loss_scale=2, train_wall=11, gb_free=19.5, wall=10199
2022-12-09 17:48:18 | INFO | train_inner | epoch 115:    583 / 1102 loss=7.019, nll_loss=3.199, ppl=9.18, wps=33092, ups=9.21, wpb=3592.3, bsz=139.8, num_updates=126200, lr=8.90165e-05, gnorm=2.072, loss_scale=2, train_wall=11, gb_free=19.7, wall=10210
2022-12-09 17:48:29 | INFO | train_inner | epoch 115:    683 / 1102 loss=6.967, nll_loss=3.153, ppl=8.9, wps=33101.9, ups=9.34, wpb=3543.6, bsz=148.5, num_updates=126300, lr=8.89812e-05, gnorm=2.094, loss_scale=2, train_wall=10, gb_free=19.2, wall=10221
2022-12-09 17:48:40 | INFO | train_inner | epoch 115:    783 / 1102 loss=6.995, nll_loss=3.175, ppl=9.03, wps=32546.4, ups=9.16, wpb=3552, bsz=143, num_updates=126400, lr=8.8946e-05, gnorm=2.088, loss_scale=2, train_wall=11, gb_free=19.4, wall=10232
2022-12-09 17:48:50 | INFO | train_inner | epoch 115:    883 / 1102 loss=6.971, nll_loss=3.143, ppl=8.83, wps=32733.2, ups=9.31, wpb=3516.4, bsz=140.6, num_updates=126500, lr=8.89108e-05, gnorm=2.09, loss_scale=2, train_wall=11, gb_free=19.3, wall=10243
2022-12-09 17:49:01 | INFO | train_inner | epoch 115:    983 / 1102 loss=6.974, nll_loss=3.141, ppl=8.82, wps=33020.1, ups=9.27, wpb=3562, bsz=137.1, num_updates=126600, lr=8.88757e-05, gnorm=2.091, loss_scale=2, train_wall=11, gb_free=19.4, wall=10253
2022-12-09 17:49:12 | INFO | train_inner | epoch 115:   1083 / 1102 loss=6.963, nll_loss=3.136, ppl=8.79, wps=33885.9, ups=9.46, wpb=3583.2, bsz=141.8, num_updates=126700, lr=8.88406e-05, gnorm=2.104, loss_scale=2, train_wall=10, gb_free=19.2, wall=10264
2022-12-09 17:49:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:49:52 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 3.673 | nll_loss 2.132 | ppl 4.38 | bleu 36.96 | wps 4731.3 | wpb 2835.3 | bsz 115.6 | num_updates 126719 | best_bleu 37.25
2022-12-09 17:49:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 126719 updates
2022-12-09 17:49:53 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint115.pt
2022-12-09 17:49:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint115.pt (epoch 115 @ 126719 updates, score 36.96) (writing took 1.0999606745317578 seconds)
2022-12-09 17:49:53 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-12-09 17:49:53 | INFO | train | epoch 115 | loss 6.946 | nll_loss 3.125 | ppl 8.73 | wps 24838 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 126719 | lr 8.8834e-05 | gnorm 2.088 | loss_scale 2 | train_wall 117 | gb_free 19.3 | wall 10305
2022-12-09 17:49:53 | INFO | fairseq.trainer | begin training epoch 116
2022-12-09 17:50:02 | INFO | train_inner | epoch 116:     81 / 1102 loss=6.877, nll_loss=3.065, ppl=8.37, wps=7049.2, ups=1.98, wpb=3560.6, bsz=155.4, num_updates=126800, lr=8.88056e-05, gnorm=2.069, loss_scale=2, train_wall=11, gb_free=19.6, wall=10315
2022-12-09 17:50:13 | INFO | train_inner | epoch 116:    181 / 1102 loss=6.884, nll_loss=3.039, ppl=8.22, wps=33088.2, ups=9.46, wpb=3498.7, bsz=133.2, num_updates=126900, lr=8.87706e-05, gnorm=2.125, loss_scale=2, train_wall=10, gb_free=19.4, wall=10325
2022-12-09 17:50:24 | INFO | train_inner | epoch 116:    281 / 1102 loss=6.897, nll_loss=3.068, ppl=8.39, wps=32632.3, ups=9.18, wpb=3554.2, bsz=144.9, num_updates=127000, lr=8.87357e-05, gnorm=2.044, loss_scale=2, train_wall=11, gb_free=19.9, wall=10336
2022-12-09 17:50:34 | INFO | train_inner | epoch 116:    381 / 1102 loss=6.977, nll_loss=3.16, ppl=8.94, wps=33412.6, ups=9.23, wpb=3621.6, bsz=143.7, num_updates=127100, lr=8.87007e-05, gnorm=2.046, loss_scale=2, train_wall=11, gb_free=21.3, wall=10347
2022-12-09 17:50:46 | INFO | train_inner | epoch 116:    481 / 1102 loss=6.915, nll_loss=3.089, ppl=8.51, wps=32072.2, ups=8.99, wpb=3567.1, bsz=145.2, num_updates=127200, lr=8.86659e-05, gnorm=2.088, loss_scale=2, train_wall=11, gb_free=19.8, wall=10358
2022-12-09 17:50:56 | INFO | train_inner | epoch 116:    581 / 1102 loss=6.95, nll_loss=3.117, ppl=8.68, wps=32891.2, ups=9.27, wpb=3547.7, bsz=133.8, num_updates=127300, lr=8.8631e-05, gnorm=2.082, loss_scale=2, train_wall=11, gb_free=19.8, wall=10369
2022-12-09 17:51:07 | INFO | train_inner | epoch 116:    681 / 1102 loss=7.005, nll_loss=3.181, ppl=9.07, wps=32904.7, ups=9.1, wpb=3614.8, bsz=143.4, num_updates=127400, lr=8.85962e-05, gnorm=2.129, loss_scale=2, train_wall=11, gb_free=19.6, wall=10380
2022-12-09 17:51:18 | INFO | train_inner | epoch 116:    781 / 1102 loss=6.874, nll_loss=3.078, ppl=8.44, wps=33870, ups=9.18, wpb=3690.1, bsz=165.5, num_updates=127500, lr=8.85615e-05, gnorm=1.945, loss_scale=2, train_wall=11, gb_free=19.4, wall=10391
2022-12-09 17:51:29 | INFO | train_inner | epoch 116:    881 / 1102 loss=6.967, nll_loss=3.157, ppl=8.92, wps=32609.2, ups=9.2, wpb=3543.7, bsz=156.8, num_updates=127600, lr=8.85268e-05, gnorm=2.044, loss_scale=2, train_wall=11, gb_free=19.7, wall=10402
2022-12-09 17:51:40 | INFO | train_inner | epoch 116:    981 / 1102 loss=7.021, nll_loss=3.201, ppl=9.19, wps=33406.4, ups=9.26, wpb=3606.3, bsz=139.3, num_updates=127700, lr=8.84921e-05, gnorm=2.058, loss_scale=2, train_wall=11, gb_free=19.5, wall=10412
2022-12-09 17:51:51 | INFO | train_inner | epoch 116:   1081 / 1102 loss=6.962, nll_loss=3.149, ppl=8.87, wps=33042.6, ups=9.24, wpb=3577.7, bsz=147.6, num_updates=127800, lr=8.84575e-05, gnorm=2.047, loss_scale=2, train_wall=11, gb_free=20, wall=10423
2022-12-09 17:51:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:52:31 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 3.67 | nll_loss 2.126 | ppl 4.36 | bleu 37.18 | wps 4751.8 | wpb 2835.3 | bsz 115.6 | num_updates 127821 | best_bleu 37.25
2022-12-09 17:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 127821 updates
2022-12-09 17:52:32 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint116.pt
2022-12-09 17:52:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint116.pt (epoch 116 @ 127821 updates, score 37.18) (writing took 1.1677293572574854 seconds)
2022-12-09 17:52:32 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-12-09 17:52:32 | INFO | train | epoch 116 | loss 6.942 | nll_loss 3.12 | ppl 8.7 | wps 24807.9 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 127821 | lr 8.84502e-05 | gnorm 2.062 | loss_scale 2 | train_wall 117 | gb_free 19.6 | wall 10465
2022-12-09 17:52:32 | INFO | fairseq.trainer | begin training epoch 117
2022-12-09 17:52:41 | INFO | train_inner | epoch 117:     79 / 1102 loss=6.979, nll_loss=3.143, ppl=8.84, wps=7080.4, ups=1.98, wpb=3577.1, bsz=129.4, num_updates=127900, lr=8.84229e-05, gnorm=2.091, loss_scale=2, train_wall=11, gb_free=19.4, wall=10474
2022-12-09 17:52:52 | INFO | train_inner | epoch 117:    179 / 1102 loss=6.88, nll_loss=3.054, ppl=8.31, wps=32082.4, ups=9.14, wpb=3510.3, bsz=145.8, num_updates=128000, lr=8.83883e-05, gnorm=2.068, loss_scale=2, train_wall=11, gb_free=19.8, wall=10485
2022-12-09 17:53:03 | INFO | train_inner | epoch 117:    279 / 1102 loss=6.971, nll_loss=3.15, ppl=8.88, wps=32984, ups=9.06, wpb=3639.4, bsz=141.4, num_updates=128100, lr=8.83538e-05, gnorm=2.026, loss_scale=2, train_wall=11, gb_free=19.3, wall=10496
2022-12-09 17:53:14 | INFO | train_inner | epoch 117:    379 / 1102 loss=6.887, nll_loss=3.075, ppl=8.43, wps=32946.5, ups=9.16, wpb=3596.3, bsz=161.5, num_updates=128200, lr=8.83194e-05, gnorm=2.012, loss_scale=2, train_wall=11, gb_free=19.7, wall=10507
2022-12-09 17:53:25 | INFO | train_inner | epoch 117:    479 / 1102 loss=6.941, nll_loss=3.106, ppl=8.61, wps=32510.1, ups=9.13, wpb=3559.5, bsz=138.5, num_updates=128300, lr=8.82849e-05, gnorm=2.105, loss_scale=2, train_wall=11, gb_free=19.7, wall=10517
2022-12-09 17:53:36 | INFO | train_inner | epoch 117:    579 / 1102 loss=6.939, nll_loss=3.108, ppl=8.62, wps=33022.9, ups=9.26, wpb=3565.8, bsz=139.4, num_updates=128400, lr=8.82506e-05, gnorm=2.085, loss_scale=2, train_wall=11, gb_free=19.8, wall=10528
2022-12-09 17:53:47 | INFO | train_inner | epoch 117:    679 / 1102 loss=6.905, nll_loss=3.107, ppl=8.62, wps=33668.1, ups=9.17, wpb=3671.2, bsz=161.8, num_updates=128500, lr=8.82162e-05, gnorm=1.948, loss_scale=2, train_wall=11, gb_free=19.7, wall=10539
2022-12-09 17:53:58 | INFO | train_inner | epoch 117:    779 / 1102 loss=6.928, nll_loss=3.107, ppl=8.61, wps=32572.5, ups=9.13, wpb=3567.6, bsz=153.5, num_updates=128600, lr=8.81819e-05, gnorm=2.154, loss_scale=2, train_wall=11, gb_free=19.8, wall=10550
2022-12-09 17:54:09 | INFO | train_inner | epoch 117:    879 / 1102 loss=6.972, nll_loss=3.14, ppl=8.82, wps=33110.3, ups=9.16, wpb=3614.7, bsz=137.6, num_updates=128700, lr=8.81476e-05, gnorm=2.066, loss_scale=2, train_wall=11, gb_free=19.9, wall=10561
2022-12-09 17:54:19 | INFO | train_inner | epoch 117:    979 / 1102 loss=6.931, nll_loss=3.096, ppl=8.55, wps=32704.9, ups=9.34, wpb=3501.8, bsz=136.5, num_updates=128800, lr=8.81134e-05, gnorm=2.179, loss_scale=2, train_wall=10, gb_free=19.7, wall=10572
2022-12-09 17:54:30 | INFO | train_inner | epoch 117:   1079 / 1102 loss=6.997, nll_loss=3.192, ppl=9.14, wps=34368.7, ups=9.49, wpb=3621.9, bsz=153.1, num_updates=128900, lr=8.80792e-05, gnorm=2.054, loss_scale=2, train_wall=10, gb_free=19.9, wall=10582
2022-12-09 17:54:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:55:10 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 3.668 | nll_loss 2.126 | ppl 4.36 | bleu 37.29 | wps 4799 | wpb 2835.3 | bsz 115.6 | num_updates 128923 | best_bleu 37.29
2022-12-09 17:55:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 128923 updates
2022-12-09 17:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint117.pt
2022-12-09 17:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint117.pt (epoch 117 @ 128923 updates, score 37.29) (writing took 1.4912073081359267 seconds)
2022-12-09 17:55:12 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-12-09 17:55:12 | INFO | train | epoch 117 | loss 6.939 | nll_loss 3.117 | ppl 8.67 | wps 24781.8 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 128923 | lr 8.80714e-05 | gnorm 2.072 | loss_scale 2 | train_wall 117 | gb_free 19.4 | wall 10624
2022-12-09 17:55:12 | INFO | fairseq.trainer | begin training epoch 118
2022-12-09 17:55:20 | INFO | train_inner | epoch 118:     77 / 1102 loss=6.91, nll_loss=3.098, ppl=8.56, wps=7202.8, ups=1.99, wpb=3625.6, bsz=150.2, num_updates=129000, lr=8.80451e-05, gnorm=2.051, loss_scale=2, train_wall=11, gb_free=19.6, wall=10633
2022-12-09 17:55:31 | INFO | train_inner | epoch 118:    177 / 1102 loss=6.888, nll_loss=3.061, ppl=8.35, wps=33065.8, ups=9.29, wpb=3559.6, bsz=147.2, num_updates=129100, lr=8.8011e-05, gnorm=2.095, loss_scale=2, train_wall=11, gb_free=19.6, wall=10643
2022-12-09 17:55:42 | INFO | train_inner | epoch 118:    277 / 1102 loss=7.023, nll_loss=3.181, ppl=9.07, wps=32887.6, ups=9.22, wpb=3565.1, bsz=124.8, num_updates=129200, lr=8.79769e-05, gnorm=2.125, loss_scale=2, train_wall=11, gb_free=19.6, wall=10654
2022-12-09 17:55:53 | INFO | train_inner | epoch 118:    377 / 1102 loss=6.855, nll_loss=3.029, ppl=8.16, wps=32801.1, ups=9.22, wpb=3556.9, bsz=157.1, num_updates=129300, lr=8.79429e-05, gnorm=2.154, loss_scale=2, train_wall=11, gb_free=19.6, wall=10665
2022-12-09 17:56:04 | INFO | train_inner | epoch 118:    477 / 1102 loss=6.925, nll_loss=3.089, ppl=8.51, wps=32835.5, ups=9.15, wpb=3588.4, bsz=135.8, num_updates=129400, lr=8.79089e-05, gnorm=2.16, loss_scale=2, train_wall=11, gb_free=19.7, wall=10676
2022-12-09 17:56:14 | INFO | train_inner | epoch 118:    577 / 1102 loss=6.882, nll_loss=3.058, ppl=8.33, wps=33548, ups=9.41, wpb=3564.8, bsz=146.5, num_updates=129500, lr=8.7875e-05, gnorm=2.106, loss_scale=2, train_wall=10, gb_free=19.4, wall=10687
2022-12-09 17:56:25 | INFO | train_inner | epoch 118:    677 / 1102 loss=6.952, nll_loss=3.12, ppl=8.69, wps=33113.6, ups=9.3, wpb=3559.7, bsz=137.2, num_updates=129600, lr=8.7841e-05, gnorm=2.043, loss_scale=2, train_wall=11, gb_free=19.3, wall=10697
2022-12-09 17:56:36 | INFO | train_inner | epoch 118:    777 / 1102 loss=6.941, nll_loss=3.135, ppl=8.78, wps=33498.2, ups=9.19, wpb=3643.8, bsz=150.1, num_updates=129700, lr=8.78072e-05, gnorm=1.987, loss_scale=2, train_wall=11, gb_free=20, wall=10708
2022-12-09 17:56:47 | INFO | train_inner | epoch 118:    877 / 1102 loss=6.973, nll_loss=3.155, ppl=8.91, wps=31850.6, ups=8.97, wpb=3550.1, bsz=147.1, num_updates=129800, lr=8.77733e-05, gnorm=2.036, loss_scale=2, train_wall=11, gb_free=19.6, wall=10719
2022-12-09 17:56:58 | INFO | train_inner | epoch 118:    977 / 1102 loss=6.946, nll_loss=3.125, ppl=8.73, wps=33196.6, ups=9.28, wpb=3578.6, bsz=147.8, num_updates=129900, lr=8.77396e-05, gnorm=2.054, loss_scale=2, train_wall=11, gb_free=19.3, wall=10730
2022-12-09 17:57:08 | INFO | train_inner | epoch 118:   1077 / 1102 loss=6.973, nll_loss=3.163, ppl=8.96, wps=34405.2, ups=9.5, wpb=3620, bsz=155.8, num_updates=130000, lr=8.77058e-05, gnorm=2.123, loss_scale=2, train_wall=10, gb_free=19.4, wall=10741
2022-12-09 17:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 17:57:47 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 3.666 | nll_loss 2.127 | ppl 4.37 | bleu 37.06 | wps 5022.1 | wpb 2835.3 | bsz 115.6 | num_updates 130025 | best_bleu 37.29
2022-12-09 17:57:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 130025 updates
2022-12-09 17:57:48 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint118.pt
2022-12-09 17:57:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint118.pt (epoch 118 @ 130025 updates, score 37.06) (writing took 1.0883059678599238 seconds)
2022-12-09 17:57:48 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-12-09 17:57:48 | INFO | train | epoch 118 | loss 6.934 | nll_loss 3.111 | ppl 8.64 | wps 25232.1 | ups 7.04 | wpb 3583.6 | bsz 145.4 | num_updates 130025 | lr 8.76974e-05 | gnorm 2.085 | loss_scale 2 | train_wall 117 | gb_free 19.3 | wall 10780
2022-12-09 17:57:48 | INFO | fairseq.trainer | begin training epoch 119
2022-12-09 17:57:56 | INFO | train_inner | epoch 119:     75 / 1102 loss=6.927, nll_loss=3.105, ppl=8.61, wps=7379.7, ups=2.08, wpb=3552.4, bsz=143.8, num_updates=130100, lr=8.76721e-05, gnorm=2.061, loss_scale=2, train_wall=11, gb_free=19.4, wall=10789
2022-12-09 17:58:07 | INFO | train_inner | epoch 119:    175 / 1102 loss=6.927, nll_loss=3.103, ppl=8.59, wps=32772.1, ups=9.24, wpb=3546.5, bsz=136.1, num_updates=130200, lr=8.76384e-05, gnorm=2.031, loss_scale=2, train_wall=11, gb_free=19.8, wall=10800
2022-12-09 17:58:18 | INFO | train_inner | epoch 119:    275 / 1102 loss=6.88, nll_loss=3.05, ppl=8.28, wps=33362, ups=9.26, wpb=3603.6, bsz=149, num_updates=130300, lr=8.76048e-05, gnorm=2.09, loss_scale=2, train_wall=11, gb_free=19.8, wall=10810
2022-12-09 17:58:29 | INFO | train_inner | epoch 119:    375 / 1102 loss=6.94, nll_loss=3.106, ppl=8.61, wps=31806.5, ups=9.15, wpb=3475.8, bsz=146, num_updates=130400, lr=8.75712e-05, gnorm=2.133, loss_scale=2, train_wall=11, gb_free=19.6, wall=10821
2022-12-09 17:58:40 | INFO | train_inner | epoch 119:    475 / 1102 loss=6.867, nll_loss=3.05, ppl=8.28, wps=33645.9, ups=9.22, wpb=3650.5, bsz=152.4, num_updates=130500, lr=8.75376e-05, gnorm=1.976, loss_scale=2, train_wall=11, gb_free=19.3, wall=10832
2022-12-09 17:58:50 | INFO | train_inner | epoch 119:    575 / 1102 loss=6.898, nll_loss=3.072, ppl=8.41, wps=34186.9, ups=9.55, wpb=3579.6, bsz=147, num_updates=130600, lr=8.75041e-05, gnorm=2.078, loss_scale=2, train_wall=10, gb_free=19.5, wall=10843
2022-12-09 17:59:01 | INFO | train_inner | epoch 119:    675 / 1102 loss=6.915, nll_loss=3.095, ppl=8.55, wps=33545.1, ups=9.29, wpb=3612.3, bsz=153.4, num_updates=130700, lr=8.74706e-05, gnorm=2.044, loss_scale=2, train_wall=11, gb_free=19.8, wall=10854
2022-12-09 17:59:12 | INFO | train_inner | epoch 119:    775 / 1102 loss=6.983, nll_loss=3.166, ppl=8.98, wps=34094.8, ups=9.3, wpb=3665.1, bsz=140.2, num_updates=130800, lr=8.74372e-05, gnorm=2.092, loss_scale=2, train_wall=11, gb_free=19.8, wall=10864
2022-12-09 17:59:22 | INFO | train_inner | epoch 119:    875 / 1102 loss=6.987, nll_loss=3.145, ppl=8.85, wps=33339.7, ups=9.42, wpb=3538.7, bsz=134.5, num_updates=130900, lr=8.74038e-05, gnorm=2.399, loss_scale=2, train_wall=10, gb_free=19.3, wall=10875
2022-12-09 17:59:33 | INFO | train_inner | epoch 119:    975 / 1102 loss=6.973, nll_loss=3.162, ppl=8.95, wps=34316.9, ups=9.33, wpb=3677.9, bsz=146.6, num_updates=131000, lr=8.73704e-05, gnorm=2.004, loss_scale=2, train_wall=10, gb_free=19.3, wall=10886
2022-12-09 17:59:44 | INFO | train_inner | epoch 119:   1075 / 1102 loss=6.988, nll_loss=3.154, ppl=8.9, wps=33092, ups=9.43, wpb=3509.4, bsz=141.8, num_updates=131100, lr=8.73371e-05, gnorm=2.231, loss_scale=2, train_wall=10, gb_free=19.9, wall=10896
2022-12-09 17:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:00:25 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 3.673 | nll_loss 2.13 | ppl 4.38 | bleu 37.08 | wps 4762.2 | wpb 2835.3 | bsz 115.6 | num_updates 131127 | best_bleu 37.29
2022-12-09 18:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 131127 updates
2022-12-09 18:00:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint119.pt
2022-12-09 18:00:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint119.pt (epoch 119 @ 131127 updates, score 37.08) (writing took 1.0879003601148725 seconds)
2022-12-09 18:00:26 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-12-09 18:00:26 | INFO | train | epoch 119 | loss 6.931 | nll_loss 3.107 | ppl 8.61 | wps 25025.1 | ups 6.98 | wpb 3583.6 | bsz 145.4 | num_updates 131127 | lr 8.73281e-05 | gnorm 2.1 | loss_scale 2 | train_wall 116 | gb_free 19.6 | wall 10938
2022-12-09 18:00:26 | INFO | fairseq.trainer | begin training epoch 120
2022-12-09 18:00:34 | INFO | train_inner | epoch 120:     73 / 1102 loss=6.892, nll_loss=3.073, ppl=8.42, wps=7126.1, ups=2, wpb=3567.6, bsz=148.6, num_updates=131200, lr=8.73038e-05, gnorm=2.087, loss_scale=2, train_wall=10, gb_free=19.6, wall=10946
2022-12-09 18:00:45 | INFO | train_inner | epoch 120:    173 / 1102 loss=6.864, nll_loss=3.049, ppl=8.27, wps=33049.8, ups=9.32, wpb=3544.3, bsz=152.8, num_updates=131300, lr=8.72705e-05, gnorm=2.031, loss_scale=2, train_wall=10, gb_free=19.6, wall=10957
2022-12-09 18:00:55 | INFO | train_inner | epoch 120:    273 / 1102 loss=6.8, nll_loss=2.989, ppl=7.94, wps=33044.3, ups=9.26, wpb=3567, bsz=166.4, num_updates=131400, lr=8.72373e-05, gnorm=2.096, loss_scale=2, train_wall=11, gb_free=19.9, wall=10968
2022-12-09 18:01:06 | INFO | train_inner | epoch 120:    373 / 1102 loss=6.916, nll_loss=3.095, ppl=8.54, wps=33567.9, ups=9.32, wpb=3603.2, bsz=146.6, num_updates=131500, lr=8.72041e-05, gnorm=2.121, loss_scale=2, train_wall=11, gb_free=19.4, wall=10979
2022-12-09 18:01:17 | INFO | train_inner | epoch 120:    473 / 1102 loss=6.869, nll_loss=3.04, ppl=8.22, wps=32833.3, ups=9.25, wpb=3549.6, bsz=151, num_updates=131600, lr=8.7171e-05, gnorm=2.106, loss_scale=2, train_wall=11, gb_free=19.5, wall=10989
2022-12-09 18:01:28 | INFO | train_inner | epoch 120:    573 / 1102 loss=6.949, nll_loss=3.105, ppl=8.61, wps=33046.7, ups=9.23, wpb=3580.2, bsz=130.2, num_updates=131700, lr=8.71379e-05, gnorm=2.075, loss_scale=2, train_wall=11, gb_free=19.3, wall=11000
2022-12-09 18:01:39 | INFO | train_inner | epoch 120:    673 / 1102 loss=6.982, nll_loss=3.153, ppl=8.89, wps=33388.9, ups=9.3, wpb=3591.7, bsz=139.7, num_updates=131800, lr=8.71048e-05, gnorm=2.073, loss_scale=2, train_wall=11, gb_free=19.2, wall=11011
2022-12-09 18:01:49 | INFO | train_inner | epoch 120:    773 / 1102 loss=6.926, nll_loss=3.101, ppl=8.58, wps=34910.7, ups=9.52, wpb=3665.3, bsz=145.8, num_updates=131900, lr=8.70718e-05, gnorm=2.048, loss_scale=2, train_wall=10, gb_free=19.5, wall=11021
2022-12-09 18:02:00 | INFO | train_inner | epoch 120:    873 / 1102 loss=6.935, nll_loss=3.098, ppl=8.56, wps=33304.3, ups=9.39, wpb=3545.1, bsz=136.5, num_updates=132000, lr=8.70388e-05, gnorm=2.136, loss_scale=2, train_wall=10, gb_free=19.3, wall=11032
2022-12-09 18:02:11 | INFO | train_inner | epoch 120:    973 / 1102 loss=6.972, nll_loss=3.159, ppl=8.93, wps=32355.1, ups=9.05, wpb=3576.3, bsz=153.1, num_updates=132100, lr=8.70059e-05, gnorm=2.088, loss_scale=2, train_wall=11, gb_free=19.3, wall=11043
2022-12-09 18:02:22 | INFO | train_inner | epoch 120:   1073 / 1102 loss=7.059, nll_loss=3.242, ppl=9.46, wps=34093.7, ups=9.28, wpb=3675.3, bsz=134.6, num_updates=132200, lr=8.6973e-05, gnorm=2.028, loss_scale=2, train_wall=11, gb_free=19.4, wall=11054
2022-12-09 18:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:03:04 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 3.667 | nll_loss 2.122 | ppl 4.35 | bleu 37.17 | wps 4629.9 | wpb 2835.3 | bsz 115.6 | num_updates 132229 | best_bleu 37.29
2022-12-09 18:03:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 132229 updates
2022-12-09 18:03:04 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint120.pt
2022-12-09 18:03:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint120.pt (epoch 120 @ 132229 updates, score 37.17) (writing took 1.0689036175608635 seconds)
2022-12-09 18:03:05 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-12-09 18:03:05 | INFO | train | epoch 120 | loss 6.927 | nll_loss 3.102 | ppl 8.59 | wps 24844.8 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 132229 | lr 8.69634e-05 | gnorm 2.088 | loss_scale 2 | train_wall 116 | gb_free 19.8 | wall 11097
2022-12-09 18:03:05 | INFO | fairseq.trainer | begin training epoch 121
2022-12-09 18:03:13 | INFO | train_inner | epoch 121:     71 / 1102 loss=6.881, nll_loss=3.063, ppl=8.35, wps=6887.6, ups=1.94, wpb=3555.9, bsz=149.3, num_updates=132300, lr=8.69401e-05, gnorm=2.206, loss_scale=2, train_wall=11, gb_free=19.5, wall=11106
2022-12-09 18:03:24 | INFO | train_inner | epoch 121:    171 / 1102 loss=6.895, nll_loss=3.054, ppl=8.31, wps=31925.4, ups=9, wpb=3546.6, bsz=136.4, num_updates=132400, lr=8.69072e-05, gnorm=2.133, loss_scale=2, train_wall=11, gb_free=19.2, wall=11117
2022-12-09 18:03:35 | INFO | train_inner | epoch 121:    271 / 1102 loss=6.853, nll_loss=3.022, ppl=8.12, wps=32711.5, ups=9.19, wpb=3557.7, bsz=148.6, num_updates=132500, lr=8.68744e-05, gnorm=2.12, loss_scale=2, train_wall=11, gb_free=19.3, wall=11128
2022-12-09 18:03:46 | INFO | train_inner | epoch 121:    371 / 1102 loss=6.854, nll_loss=3.027, ppl=8.15, wps=32611.7, ups=9.18, wpb=3552.5, bsz=151.3, num_updates=132600, lr=8.68417e-05, gnorm=2.103, loss_scale=2, train_wall=11, gb_free=19.4, wall=11138
2022-12-09 18:03:57 | INFO | train_inner | epoch 121:    471 / 1102 loss=6.874, nll_loss=3.074, ppl=8.42, wps=33540.4, ups=9.24, wpb=3630.2, bsz=169.5, num_updates=132700, lr=8.6809e-05, gnorm=2.005, loss_scale=2, train_wall=11, gb_free=19.5, wall=11149
2022-12-09 18:04:08 | INFO | train_inner | epoch 121:    571 / 1102 loss=6.882, nll_loss=3.065, ppl=8.37, wps=32949.1, ups=9.06, wpb=3637.2, bsz=157.8, num_updates=132800, lr=8.67763e-05, gnorm=2.084, loss_scale=2, train_wall=11, gb_free=19.5, wall=11160
2022-12-09 18:04:19 | INFO | train_inner | epoch 121:    671 / 1102 loss=7.029, nll_loss=3.186, ppl=9.1, wps=32421.1, ups=9.23, wpb=3511.1, bsz=123.4, num_updates=132900, lr=8.67436e-05, gnorm=2.22, loss_scale=2, train_wall=11, gb_free=19.5, wall=11171
2022-12-09 18:04:30 | INFO | train_inner | epoch 121:    771 / 1102 loss=6.918, nll_loss=3.109, ppl=8.63, wps=33358.9, ups=9.18, wpb=3635.3, bsz=156.8, num_updates=133000, lr=8.6711e-05, gnorm=2.03, loss_scale=2, train_wall=11, gb_free=19.4, wall=11182
2022-12-09 18:04:41 | INFO | train_inner | epoch 121:    871 / 1102 loss=6.972, nll_loss=3.131, ppl=8.76, wps=32873.7, ups=9.17, wpb=3583.5, bsz=130.7, num_updates=133100, lr=8.66784e-05, gnorm=2.109, loss_scale=2, train_wall=11, gb_free=19.3, wall=11193
2022-12-09 18:04:52 | INFO | train_inner | epoch 121:    971 / 1102 loss=6.968, nll_loss=3.146, ppl=8.85, wps=32378.7, ups=9.01, wpb=3595.2, bsz=140.5, num_updates=133200, lr=8.66459e-05, gnorm=2.063, loss_scale=2, train_wall=11, gb_free=19.4, wall=11204
2022-12-09 18:05:03 | INFO | train_inner | epoch 121:   1071 / 1102 loss=7.02, nll_loss=3.201, ppl=9.19, wps=32594.5, ups=9.07, wpb=3592.6, bsz=137.2, num_updates=133300, lr=8.66134e-05, gnorm=2.067, loss_scale=2, train_wall=11, gb_free=19.6, wall=11215
2022-12-09 18:05:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:05:43 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.33 | wps 4935.8 | wpb 2835.3 | bsz 115.6 | num_updates 133331 | best_bleu 37.33
2022-12-09 18:05:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 133331 updates
2022-12-09 18:05:43 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint121.pt
2022-12-09 18:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint121.pt (epoch 121 @ 133331 updates, score 37.33) (writing took 1.6732504665851593 seconds)
2022-12-09 18:05:44 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-12-09 18:05:44 | INFO | train | epoch 121 | loss 6.923 | nll_loss 3.099 | ppl 8.57 | wps 24748.3 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 133331 | lr 8.66033e-05 | gnorm 2.098 | loss_scale 2 | train_wall 118 | gb_free 19.3 | wall 11257
2022-12-09 18:05:44 | INFO | fairseq.trainer | begin training epoch 122
2022-12-09 18:05:52 | INFO | train_inner | epoch 122:     69 / 1102 loss=6.932, nll_loss=3.118, ppl=8.68, wps=7273.1, ups=2.02, wpb=3605.9, bsz=152.9, num_updates=133400, lr=8.65809e-05, gnorm=2.061, loss_scale=2, train_wall=11, gb_free=19.3, wall=11265
2022-12-09 18:06:03 | INFO | train_inner | epoch 122:    169 / 1102 loss=6.893, nll_loss=3.075, ppl=8.42, wps=33723.3, ups=9.23, wpb=3654.3, bsz=146.7, num_updates=133500, lr=8.65485e-05, gnorm=2.047, loss_scale=2, train_wall=11, gb_free=19.8, wall=11275
2022-12-09 18:06:14 | INFO | train_inner | epoch 122:    269 / 1102 loss=6.903, nll_loss=3.063, ppl=8.36, wps=32887.3, ups=9.11, wpb=3609.4, bsz=129.6, num_updates=133600, lr=8.65161e-05, gnorm=2.071, loss_scale=2, train_wall=11, gb_free=19.8, wall=11286
2022-12-09 18:06:25 | INFO | train_inner | epoch 122:    369 / 1102 loss=6.903, nll_loss=3.078, ppl=8.44, wps=33232.3, ups=9.22, wpb=3603.8, bsz=143.1, num_updates=133700, lr=8.64837e-05, gnorm=2.074, loss_scale=2, train_wall=11, gb_free=19.4, wall=11297
2022-12-09 18:06:36 | INFO | train_inner | epoch 122:    469 / 1102 loss=6.857, nll_loss=3.056, ppl=8.32, wps=32915.8, ups=8.98, wpb=3664, bsz=169.4, num_updates=133800, lr=8.64514e-05, gnorm=2.017, loss_scale=2, train_wall=11, gb_free=19.3, wall=11308
2022-12-09 18:06:47 | INFO | train_inner | epoch 122:    569 / 1102 loss=6.918, nll_loss=3.079, ppl=8.45, wps=33161.6, ups=9.29, wpb=3570.2, bsz=132.4, num_updates=133900, lr=8.64191e-05, gnorm=2.083, loss_scale=2, train_wall=11, gb_free=19.4, wall=11319
2022-12-09 18:06:58 | INFO | train_inner | epoch 122:    669 / 1102 loss=6.891, nll_loss=3.07, ppl=8.4, wps=32601.2, ups=9.18, wpb=3549.8, bsz=149, num_updates=134000, lr=8.63868e-05, gnorm=2.136, loss_scale=2, train_wall=11, gb_free=19.3, wall=11330
2022-12-09 18:07:08 | INFO | train_inner | epoch 122:    769 / 1102 loss=6.887, nll_loss=3.059, ppl=8.33, wps=32913.5, ups=9.38, wpb=3510, bsz=152.4, num_updates=134100, lr=8.63546e-05, gnorm=2.13, loss_scale=2, train_wall=10, gb_free=19.3, wall=11341
2022-12-09 18:07:19 | INFO | train_inner | epoch 122:    869 / 1102 loss=6.918, nll_loss=3.093, ppl=8.53, wps=32748.3, ups=9.11, wpb=3596.5, bsz=143, num_updates=134200, lr=8.63224e-05, gnorm=2.083, loss_scale=2, train_wall=11, gb_free=19.6, wall=11352
2022-12-09 18:07:30 | INFO | train_inner | epoch 122:    969 / 1102 loss=6.95, nll_loss=3.132, ppl=8.76, wps=32937.6, ups=9.14, wpb=3603.4, bsz=144.6, num_updates=134300, lr=8.62903e-05, gnorm=2.089, loss_scale=2, train_wall=11, gb_free=19.5, wall=11363
2022-12-09 18:07:41 | INFO | train_inner | epoch 122:   1069 / 1102 loss=7.012, nll_loss=3.183, ppl=9.08, wps=31989.8, ups=9.2, wpb=3476.5, bsz=142.7, num_updates=134400, lr=8.62582e-05, gnorm=2.226, loss_scale=2, train_wall=11, gb_free=19.1, wall=11373
2022-12-09 18:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:08:22 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 3.665 | nll_loss 2.123 | ppl 4.36 | bleu 37.18 | wps 4865.7 | wpb 2835.3 | bsz 115.6 | num_updates 134433 | best_bleu 37.33
2022-12-09 18:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 134433 updates
2022-12-09 18:08:23 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint122.pt
2022-12-09 18:08:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint122.pt (epoch 122 @ 134433 updates, score 37.18) (writing took 1.4547730553895235 seconds)
2022-12-09 18:08:23 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-12-09 18:08:23 | INFO | train | epoch 122 | loss 6.918 | nll_loss 3.094 | ppl 8.54 | wps 24848.2 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 134433 | lr 8.62476e-05 | gnorm 2.096 | loss_scale 2 | train_wall 117 | gb_free 19.5 | wall 11416
2022-12-09 18:08:23 | INFO | fairseq.trainer | begin training epoch 123
2022-12-09 18:08:31 | INFO | train_inner | epoch 123:     67 / 1102 loss=6.95, nll_loss=3.115, ppl=8.66, wps=7160.7, ups=2.01, wpb=3562.4, bsz=135.7, num_updates=134500, lr=8.62261e-05, gnorm=2.042, loss_scale=2, train_wall=10, gb_free=19.2, wall=11423
2022-12-09 18:08:41 | INFO | train_inner | epoch 123:    167 / 1102 loss=6.885, nll_loss=3.048, ppl=8.27, wps=33639.9, ups=9.41, wpb=3575.6, bsz=139.1, num_updates=134600, lr=8.61941e-05, gnorm=2.097, loss_scale=2, train_wall=10, gb_free=19.5, wall=11434
2022-12-09 18:08:52 | INFO | train_inner | epoch 123:    267 / 1102 loss=6.877, nll_loss=3.059, ppl=8.33, wps=34117.2, ups=9.43, wpb=3618.8, bsz=148.3, num_updates=134700, lr=8.61621e-05, gnorm=2.048, loss_scale=2, train_wall=10, gb_free=19.5, wall=11444
2022-12-09 18:09:03 | INFO | train_inner | epoch 123:    367 / 1102 loss=6.955, nll_loss=3.108, ppl=8.62, wps=33136.3, ups=9.4, wpb=3526.1, bsz=131.8, num_updates=134800, lr=8.61301e-05, gnorm=2.176, loss_scale=2, train_wall=10, gb_free=19.3, wall=11455
2022-12-09 18:09:14 | INFO | train_inner | epoch 123:    467 / 1102 loss=6.905, nll_loss=3.081, ppl=8.46, wps=32914.9, ups=9.14, wpb=3602.3, bsz=150.4, num_updates=134900, lr=8.60982e-05, gnorm=2.147, loss_scale=2, train_wall=11, gb_free=19.5, wall=11466
2022-12-09 18:09:25 | INFO | train_inner | epoch 123:    567 / 1102 loss=6.915, nll_loss=3.095, ppl=8.54, wps=33387.7, ups=9.22, wpb=3622.3, bsz=152.1, num_updates=135000, lr=8.60663e-05, gnorm=2.056, loss_scale=2, train_wall=11, gb_free=19.9, wall=11477
2022-12-09 18:09:36 | INFO | train_inner | epoch 123:    667 / 1102 loss=6.857, nll_loss=3.035, ppl=8.2, wps=32051.2, ups=8.95, wpb=3581.9, bsz=162.8, num_updates=135100, lr=8.60344e-05, gnorm=2.073, loss_scale=2, train_wall=11, gb_free=19.7, wall=11488
2022-12-09 18:09:47 | INFO | train_inner | epoch 123:    767 / 1102 loss=6.906, nll_loss=3.083, ppl=8.47, wps=32047.3, ups=8.9, wpb=3602, bsz=144.6, num_updates=135200, lr=8.60026e-05, gnorm=2.096, loss_scale=2, train_wall=11, gb_free=19.4, wall=11499
2022-12-09 18:09:58 | INFO | train_inner | epoch 123:    867 / 1102 loss=6.928, nll_loss=3.108, ppl=8.62, wps=32053.4, ups=8.98, wpb=3568.3, bsz=147.3, num_updates=135300, lr=8.59708e-05, gnorm=2.059, loss_scale=2, train_wall=11, gb_free=19.6, wall=11510
2022-12-09 18:10:09 | INFO | train_inner | epoch 123:    967 / 1102 loss=7.007, nll_loss=3.168, ppl=8.99, wps=32234.1, ups=9.07, wpb=3555.7, bsz=130, num_updates=135400, lr=8.59391e-05, gnorm=2.148, loss_scale=2, train_wall=11, gb_free=19.6, wall=11522
2022-12-09 18:10:20 | INFO | train_inner | epoch 123:   1067 / 1102 loss=6.974, nll_loss=3.15, ppl=8.88, wps=32877, ups=9.11, wpb=3609.8, bsz=146.8, num_updates=135500, lr=8.59074e-05, gnorm=2.13, loss_scale=2, train_wall=11, gb_free=19.7, wall=11532
2022-12-09 18:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:11:03 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 3.671 | nll_loss 2.125 | ppl 4.36 | bleu 37.11 | wps 4690 | wpb 2835.3 | bsz 115.6 | num_updates 135535 | best_bleu 37.33
2022-12-09 18:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 135535 updates
2022-12-09 18:11:04 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint123.pt
2022-12-09 18:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint123.pt (epoch 123 @ 135535 updates, score 37.11) (writing took 1.234901805408299 seconds)
2022-12-09 18:11:04 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-12-09 18:11:04 | INFO | train | epoch 123 | loss 6.915 | nll_loss 3.088 | ppl 8.5 | wps 24594 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 135535 | lr 8.58963e-05 | gnorm 2.096 | loss_scale 2 | train_wall 118 | gb_free 19.3 | wall 11576
2022-12-09 18:11:04 | INFO | fairseq.trainer | begin training epoch 124
2022-12-09 18:11:11 | INFO | train_inner | epoch 124:     65 / 1102 loss=6.81, nll_loss=2.985, ppl=7.92, wps=6953.9, ups=1.95, wpb=3567.4, bsz=155.6, num_updates=135600, lr=8.58757e-05, gnorm=2.179, loss_scale=2, train_wall=11, gb_free=19.3, wall=11584
2022-12-09 18:11:22 | INFO | train_inner | epoch 124:    165 / 1102 loss=6.883, nll_loss=3.059, ppl=8.34, wps=33352.7, ups=9.22, wpb=3618.7, bsz=151, num_updates=135700, lr=8.5844e-05, gnorm=2.131, loss_scale=2, train_wall=11, gb_free=20.1, wall=11595
2022-12-09 18:11:33 | INFO | train_inner | epoch 124:    265 / 1102 loss=6.928, nll_loss=3.083, ppl=8.47, wps=32717.4, ups=9.33, wpb=3506.7, bsz=135.5, num_updates=135800, lr=8.58124e-05, gnorm=2.125, loss_scale=2, train_wall=10, gb_free=19.8, wall=11605
2022-12-09 18:11:44 | INFO | train_inner | epoch 124:    365 / 1102 loss=6.915, nll_loss=3.093, ppl=8.53, wps=33525.1, ups=9.28, wpb=3613, bsz=146.1, num_updates=135900, lr=8.57808e-05, gnorm=2.043, loss_scale=2, train_wall=11, gb_free=19.7, wall=11616
2022-12-09 18:11:55 | INFO | train_inner | epoch 124:    465 / 1102 loss=6.941, nll_loss=3.106, ppl=8.61, wps=33190.8, ups=9.17, wpb=3619.8, bsz=138.6, num_updates=136000, lr=8.57493e-05, gnorm=2.063, loss_scale=2, train_wall=11, gb_free=19.3, wall=11627
2022-12-09 18:12:06 | INFO | train_inner | epoch 124:    565 / 1102 loss=6.86, nll_loss=3.048, ppl=8.27, wps=32648.1, ups=9.03, wpb=3616.1, bsz=167, num_updates=136100, lr=8.57178e-05, gnorm=2.082, loss_scale=2, train_wall=11, gb_free=19.4, wall=11638
2022-12-09 18:12:17 | INFO | train_inner | epoch 124:    665 / 1102 loss=6.876, nll_loss=3.021, ppl=8.11, wps=31860.9, ups=9.15, wpb=3482.1, bsz=138.2, num_updates=136200, lr=8.56863e-05, gnorm=2.229, loss_scale=2, train_wall=11, gb_free=19.3, wall=11649
2022-12-09 18:12:28 | INFO | train_inner | epoch 124:    765 / 1102 loss=6.952, nll_loss=3.12, ppl=8.69, wps=32725.1, ups=9.12, wpb=3586.4, bsz=133.8, num_updates=136300, lr=8.56549e-05, gnorm=2.129, loss_scale=2, train_wall=11, gb_free=19.5, wall=11660
2022-12-09 18:12:39 | INFO | train_inner | epoch 124:    865 / 1102 loss=6.928, nll_loss=3.102, ppl=8.59, wps=32641.8, ups=9.13, wpb=3576.8, bsz=145.8, num_updates=136400, lr=8.56235e-05, gnorm=2.064, loss_scale=2, train_wall=11, gb_free=19.5, wall=11671
2022-12-09 18:12:49 | INFO | train_inner | epoch 124:    965 / 1102 loss=6.904, nll_loss=3.096, ppl=8.55, wps=33223.1, ups=9.24, wpb=3594.5, bsz=158.6, num_updates=136500, lr=8.55921e-05, gnorm=2.044, loss_scale=2, train_wall=11, gb_free=19.8, wall=11682
2022-12-09 18:13:00 | INFO | train_inner | epoch 124:   1065 / 1102 loss=7.001, nll_loss=3.178, ppl=9.05, wps=32595.4, ups=9.01, wpb=3619.4, bsz=133.4, num_updates=136600, lr=8.55608e-05, gnorm=2.062, loss_scale=2, train_wall=11, gb_free=19.4, wall=11693
2022-12-09 18:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:13:41 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 3.666 | nll_loss 2.124 | ppl 4.36 | bleu 37.2 | wps 5017.1 | wpb 2835.3 | bsz 115.6 | num_updates 136637 | best_bleu 37.33
2022-12-09 18:13:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 136637 updates
2022-12-09 18:13:41 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint124.pt
2022-12-09 18:13:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint124.pt (epoch 124 @ 136637 updates, score 37.2) (writing took 1.2090674405917525 seconds)
2022-12-09 18:13:42 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-12-09 18:13:42 | INFO | train | epoch 124 | loss 6.912 | nll_loss 3.085 | ppl 8.48 | wps 24999 | ups 6.98 | wpb 3583.6 | bsz 145.4 | num_updates 136637 | lr 8.55492e-05 | gnorm 2.106 | loss_scale 2 | train_wall 118 | gb_free 19.6 | wall 11734
2022-12-09 18:13:42 | INFO | fairseq.trainer | begin training epoch 125
2022-12-09 18:13:49 | INFO | train_inner | epoch 125:     63 / 1102 loss=6.875, nll_loss=3.051, ppl=8.29, wps=7426.8, ups=2.06, wpb=3601, bsz=147.6, num_updates=136700, lr=8.55295e-05, gnorm=2.12, loss_scale=2, train_wall=11, gb_free=19.3, wall=11741
2022-12-09 18:14:00 | INFO | train_inner | epoch 125:    163 / 1102 loss=6.89, nll_loss=3.048, ppl=8.27, wps=32751.9, ups=9.25, wpb=3539.9, bsz=140.6, num_updates=136800, lr=8.54982e-05, gnorm=2.397, loss_scale=2, train_wall=11, gb_free=19.4, wall=11752
2022-12-09 18:14:11 | INFO | train_inner | epoch 125:    263 / 1102 loss=6.896, nll_loss=3.049, ppl=8.28, wps=32041.4, ups=9.06, wpb=3537, bsz=137.8, num_updates=136900, lr=8.5467e-05, gnorm=2.125, loss_scale=2, train_wall=11, gb_free=19.3, wall=11763
2022-12-09 18:14:22 | INFO | train_inner | epoch 125:    363 / 1102 loss=6.875, nll_loss=3.047, ppl=8.27, wps=33399.4, ups=9.24, wpb=3615.7, bsz=150.2, num_updates=137000, lr=8.54358e-05, gnorm=2.027, loss_scale=2, train_wall=11, gb_free=19.6, wall=11774
2022-12-09 18:14:32 | INFO | train_inner | epoch 125:    463 / 1102 loss=6.938, nll_loss=3.102, ppl=8.59, wps=34049.9, ups=9.55, wpb=3563.9, bsz=138.5, num_updates=137100, lr=8.54046e-05, gnorm=2.159, loss_scale=2, train_wall=10, gb_free=19.4, wall=11785
2022-12-09 18:14:43 | INFO | train_inner | epoch 125:    563 / 1102 loss=6.964, nll_loss=3.122, ppl=8.7, wps=33510.7, ups=9.34, wpb=3589.1, bsz=130.2, num_updates=137200, lr=8.53735e-05, gnorm=2.129, loss_scale=2, train_wall=10, gb_free=19.3, wall=11795
2022-12-09 18:14:54 | INFO | train_inner | epoch 125:    663 / 1102 loss=6.891, nll_loss=3.086, ppl=8.49, wps=33584.5, ups=9.27, wpb=3624.4, bsz=166.5, num_updates=137300, lr=8.53424e-05, gnorm=2.027, loss_scale=2, train_wall=11, gb_free=19.5, wall=11806
2022-12-09 18:15:05 | INFO | train_inner | epoch 125:    763 / 1102 loss=6.916, nll_loss=3.116, ppl=8.67, wps=33149.4, ups=9.01, wpb=3679.4, bsz=158.6, num_updates=137400, lr=8.53113e-05, gnorm=1.989, loss_scale=2, train_wall=11, gb_free=19.3, wall=11817
2022-12-09 18:15:16 | INFO | train_inner | epoch 125:    863 / 1102 loss=6.846, nll_loss=3.026, ppl=8.14, wps=32559.8, ups=9.1, wpb=3576.2, bsz=154.4, num_updates=137500, lr=8.52803e-05, gnorm=2.117, loss_scale=2, train_wall=11, gb_free=19.3, wall=11828
2022-12-09 18:15:27 | INFO | train_inner | epoch 125:    963 / 1102 loss=6.959, nll_loss=3.11, ppl=8.63, wps=32187.9, ups=9.17, wpb=3509.6, bsz=128.3, num_updates=137600, lr=8.52493e-05, gnorm=2.251, loss_scale=2, train_wall=11, gb_free=19.3, wall=11839
2022-12-09 18:15:38 | INFO | train_inner | epoch 125:   1063 / 1102 loss=6.975, nll_loss=3.149, ppl=8.87, wps=32810.2, ups=9.16, wpb=3582.1, bsz=141, num_updates=137700, lr=8.52183e-05, gnorm=2.132, loss_scale=2, train_wall=11, gb_free=19.7, wall=11850
2022-12-09 18:15:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:16:25 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 3.669 | nll_loss 2.125 | ppl 4.36 | bleu 37.2 | wps 4169.9 | wpb 2835.3 | bsz 115.6 | num_updates 137739 | best_bleu 37.33
2022-12-09 18:16:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 137739 updates
2022-12-09 18:16:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint125.pt
2022-12-09 18:16:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint125.pt (epoch 125 @ 137739 updates, score 37.2) (writing took 1.4111969899386168 seconds)
2022-12-09 18:16:27 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-12-09 18:16:27 | INFO | train | epoch 125 | loss 6.909 | nll_loss 3.081 | ppl 8.46 | wps 23978.2 | ups 6.69 | wpb 3583.6 | bsz 145.4 | num_updates 137739 | lr 8.52063e-05 | gnorm 2.133 | loss_scale 2 | train_wall 117 | gb_free 20.1 | wall 11899
2022-12-09 18:16:27 | INFO | fairseq.trainer | begin training epoch 126
2022-12-09 18:16:33 | INFO | train_inner | epoch 126:     61 / 1102 loss=6.836, nll_loss=3.021, ppl=8.11, wps=6406.7, ups=1.79, wpb=3580.3, bsz=159, num_updates=137800, lr=8.51874e-05, gnorm=2.083, loss_scale=2, train_wall=11, gb_free=19.4, wall=11906
2022-12-09 18:16:44 | INFO | train_inner | epoch 126:    161 / 1102 loss=6.857, nll_loss=3.031, ppl=8.18, wps=32950.8, ups=9.12, wpb=3612, bsz=143.9, num_updates=137900, lr=8.51565e-05, gnorm=1.983, loss_scale=2, train_wall=11, gb_free=19.3, wall=11917
2022-12-09 18:16:55 | INFO | train_inner | epoch 126:    261 / 1102 loss=6.916, nll_loss=3.093, ppl=8.53, wps=32909.7, ups=9.07, wpb=3626.9, bsz=150.4, num_updates=138000, lr=8.51257e-05, gnorm=2.045, loss_scale=4, train_wall=11, gb_free=19.5, wall=11928
2022-12-09 18:17:06 | INFO | train_inner | epoch 126:    361 / 1102 loss=6.875, nll_loss=3.034, ppl=8.19, wps=31840.1, ups=9.09, wpb=3504.1, bsz=145.2, num_updates=138100, lr=8.50948e-05, gnorm=2.118, loss_scale=4, train_wall=11, gb_free=19.3, wall=11939
2022-12-09 18:17:17 | INFO | train_inner | epoch 126:    461 / 1102 loss=6.92, nll_loss=3.086, ppl=8.49, wps=32729.5, ups=9.07, wpb=3606.7, bsz=139.5, num_updates=138200, lr=8.5064e-05, gnorm=2.08, loss_scale=4, train_wall=11, gb_free=19.4, wall=11950
2022-12-09 18:17:28 | INFO | train_inner | epoch 126:    561 / 1102 loss=6.894, nll_loss=3.071, ppl=8.41, wps=32693, ups=9.11, wpb=3586.8, bsz=145.5, num_updates=138300, lr=8.50333e-05, gnorm=2.042, loss_scale=4, train_wall=11, gb_free=19.3, wall=11961
2022-12-09 18:17:39 | INFO | train_inner | epoch 126:    661 / 1102 loss=6.832, nll_loss=3.019, ppl=8.11, wps=32779.1, ups=9.15, wpb=3581.5, bsz=163.8, num_updates=138400, lr=8.50026e-05, gnorm=2.028, loss_scale=4, train_wall=11, gb_free=19.7, wall=11972
2022-12-09 18:17:50 | INFO | train_inner | epoch 126:    761 / 1102 loss=6.897, nll_loss=3.064, ppl=8.36, wps=32404, ups=9.15, wpb=3541.7, bsz=141.7, num_updates=138500, lr=8.49719e-05, gnorm=2.118, loss_scale=4, train_wall=11, gb_free=19.5, wall=11983
2022-12-09 18:18:01 | INFO | train_inner | epoch 126:    861 / 1102 loss=6.965, nll_loss=3.14, ppl=8.81, wps=32640.3, ups=9.1, wpb=3587.3, bsz=146.7, num_updates=138600, lr=8.49412e-05, gnorm=2.219, loss_scale=4, train_wall=11, gb_free=19.9, wall=11994
2022-12-09 18:18:12 | INFO | train_inner | epoch 126:    961 / 1102 loss=6.957, nll_loss=3.122, ppl=8.71, wps=33131.6, ups=9.09, wpb=3644.7, bsz=142.1, num_updates=138700, lr=8.49106e-05, gnorm=2.095, loss_scale=4, train_wall=11, gb_free=19.6, wall=12005
2022-12-09 18:18:23 | INFO | train_inner | epoch 126:   1061 / 1102 loss=6.966, nll_loss=3.129, ppl=8.75, wps=32076.5, ups=9.07, wpb=3536.1, bsz=135.1, num_updates=138800, lr=8.488e-05, gnorm=2.16, loss_scale=4, train_wall=11, gb_free=19.4, wall=12016
2022-12-09 18:18:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:19:07 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 3.672 | nll_loss 2.128 | ppl 4.37 | bleu 37.06 | wps 4568.4 | wpb 2835.3 | bsz 115.6 | num_updates 138841 | best_bleu 37.33
2022-12-09 18:19:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 138841 updates
2022-12-09 18:19:08 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint126.pt
2022-12-09 18:19:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint126.pt (epoch 126 @ 138841 updates, score 37.06) (writing took 1.5588751956820488 seconds)
2022-12-09 18:19:09 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-12-09 18:19:09 | INFO | train | epoch 126 | loss 6.904 | nll_loss 3.075 | ppl 8.43 | wps 24337.4 | ups 6.79 | wpb 3583.6 | bsz 145.4 | num_updates 138841 | lr 8.48674e-05 | gnorm 2.088 | loss_scale 4 | train_wall 118 | gb_free 19.6 | wall 12061
2022-12-09 18:19:09 | INFO | fairseq.trainer | begin training epoch 127
2022-12-09 18:19:16 | INFO | train_inner | epoch 127:     59 / 1102 loss=6.875, nll_loss=3.045, ppl=8.25, wps=6825.6, ups=1.91, wpb=3568.1, bsz=144.5, num_updates=138900, lr=8.48494e-05, gnorm=2.129, loss_scale=4, train_wall=11, gb_free=19.6, wall=12068
2022-12-09 18:19:26 | INFO | train_inner | epoch 127:    159 / 1102 loss=6.844, nll_loss=3.019, ppl=8.1, wps=32651.4, ups=9.13, wpb=3575.8, bsz=146.7, num_updates=139000, lr=8.48189e-05, gnorm=2.01, loss_scale=4, train_wall=11, gb_free=19.6, wall=12079
2022-12-09 18:19:38 | INFO | train_inner | epoch 127:    259 / 1102 loss=6.896, nll_loss=3.061, ppl=8.34, wps=32552.1, ups=9.07, wpb=3587.8, bsz=146.4, num_updates=139100, lr=8.47884e-05, gnorm=2.12, loss_scale=4, train_wall=11, gb_free=19.4, wall=12090
2022-12-09 18:19:48 | INFO | train_inner | epoch 127:    359 / 1102 loss=6.953, nll_loss=3.108, ppl=8.62, wps=33083.3, ups=9.15, wpb=3617, bsz=135.6, num_updates=139200, lr=8.47579e-05, gnorm=2.201, loss_scale=4, train_wall=11, gb_free=19.3, wall=12101
2022-12-09 18:20:00 | INFO | train_inner | epoch 127:    459 / 1102 loss=6.907, nll_loss=3.067, ppl=8.38, wps=31818.1, ups=9.04, wpb=3521.5, bsz=132.6, num_updates=139300, lr=8.47275e-05, gnorm=2.135, loss_scale=4, train_wall=11, gb_free=19.4, wall=12112
2022-12-09 18:20:11 | INFO | train_inner | epoch 127:    559 / 1102 loss=6.879, nll_loss=3.06, ppl=8.34, wps=33063.9, ups=9.08, wpb=3643.4, bsz=158.8, num_updates=139400, lr=8.46971e-05, gnorm=2.082, loss_scale=4, train_wall=11, gb_free=19.3, wall=12123
2022-12-09 18:20:22 | INFO | train_inner | epoch 127:    659 / 1102 loss=6.808, nll_loss=2.988, ppl=7.94, wps=32615.1, ups=9.04, wpb=3606.3, bsz=155, num_updates=139500, lr=8.46668e-05, gnorm=2.033, loss_scale=4, train_wall=11, gb_free=19.4, wall=12134
2022-12-09 18:20:33 | INFO | train_inner | epoch 127:    759 / 1102 loss=6.933, nll_loss=3.124, ppl=8.72, wps=32945.2, ups=9, wpb=3662.5, bsz=158.4, num_updates=139600, lr=8.46364e-05, gnorm=2.062, loss_scale=4, train_wall=11, gb_free=19.5, wall=12145
2022-12-09 18:20:44 | INFO | train_inner | epoch 127:    859 / 1102 loss=6.894, nll_loss=3.079, ppl=8.45, wps=32321.3, ups=9.04, wpb=3575.2, bsz=151.7, num_updates=139700, lr=8.46061e-05, gnorm=2.085, loss_scale=4, train_wall=11, gb_free=19.4, wall=12156
2022-12-09 18:20:55 | INFO | train_inner | epoch 127:    959 / 1102 loss=7.003, nll_loss=3.154, ppl=8.9, wps=32844, ups=9.22, wpb=3563.1, bsz=122.9, num_updates=139800, lr=8.45759e-05, gnorm=2.232, loss_scale=4, train_wall=11, gb_free=19.8, wall=12167
2022-12-09 18:21:06 | INFO | train_inner | epoch 127:   1059 / 1102 loss=6.914, nll_loss=3.084, ppl=8.48, wps=31874.4, ups=9.03, wpb=3529.5, bsz=147.1, num_updates=139900, lr=8.45456e-05, gnorm=2.152, loss_scale=4, train_wall=11, gb_free=19.5, wall=12178
2022-12-09 18:21:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:21:50 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 3.667 | nll_loss 2.125 | ppl 4.36 | bleu 37.34 | wps 4510.2 | wpb 2835.3 | bsz 115.6 | num_updates 139943 | best_bleu 37.34
2022-12-09 18:21:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 139943 updates
2022-12-09 18:21:51 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint127.pt
2022-12-09 18:21:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint127.pt (epoch 127 @ 139943 updates, score 37.34) (writing took 1.492455255240202 seconds)
2022-12-09 18:21:52 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-12-09 18:21:52 | INFO | train | epoch 127 | loss 6.901 | nll_loss 3.072 | ppl 8.41 | wps 24234.2 | ups 6.76 | wpb 3583.6 | bsz 145.4 | num_updates 139943 | lr 8.45326e-05 | gnorm 2.121 | loss_scale 4 | train_wall 118 | gb_free 19.6 | wall 12224
2022-12-09 18:21:52 | INFO | fairseq.trainer | begin training epoch 128
2022-12-09 18:21:58 | INFO | train_inner | epoch 128:     57 / 1102 loss=6.906, nll_loss=3.061, ppl=8.34, wps=6775.1, ups=1.9, wpb=3562.8, bsz=135, num_updates=140000, lr=8.45154e-05, gnorm=2.184, loss_scale=4, train_wall=11, gb_free=19.3, wall=12231
2022-12-09 18:22:09 | INFO | train_inner | epoch 128:    157 / 1102 loss=6.852, nll_loss=3.019, ppl=8.11, wps=32960.8, ups=9.16, wpb=3599.5, bsz=146.2, num_updates=140100, lr=8.44853e-05, gnorm=2.093, loss_scale=4, train_wall=11, gb_free=19.6, wall=12242
2022-12-09 18:22:20 | INFO | train_inner | epoch 128:    257 / 1102 loss=6.834, nll_loss=3.004, ppl=8.02, wps=32659.7, ups=9.12, wpb=3581.1, bsz=150.7, num_updates=140200, lr=8.44551e-05, gnorm=2.076, loss_scale=4, train_wall=11, gb_free=19.4, wall=12253
2022-12-09 18:22:31 | INFO | train_inner | epoch 128:    357 / 1102 loss=6.922, nll_loss=3.079, ppl=8.45, wps=32208.6, ups=9.17, wpb=3512.5, bsz=137.4, num_updates=140300, lr=8.4425e-05, gnorm=2.219, loss_scale=4, train_wall=11, gb_free=19.4, wall=12263
2022-12-09 18:22:42 | INFO | train_inner | epoch 128:    457 / 1102 loss=6.846, nll_loss=3.018, ppl=8.1, wps=32807.5, ups=9.08, wpb=3611.5, bsz=152.9, num_updates=140400, lr=8.43949e-05, gnorm=2.123, loss_scale=4, train_wall=11, gb_free=19.5, wall=12274
2022-12-09 18:22:53 | INFO | train_inner | epoch 128:    557 / 1102 loss=6.822, nll_loss=2.983, ppl=7.91, wps=32342, ups=9.13, wpb=3541.4, bsz=144.6, num_updates=140500, lr=8.43649e-05, gnorm=2.125, loss_scale=4, train_wall=11, gb_free=19.3, wall=12285
2022-12-09 18:23:04 | INFO | train_inner | epoch 128:    657 / 1102 loss=6.947, nll_loss=3.124, ppl=8.72, wps=33220.6, ups=9.07, wpb=3661.8, bsz=141.4, num_updates=140600, lr=8.43349e-05, gnorm=2.044, loss_scale=4, train_wall=11, gb_free=19.4, wall=12296
2022-12-09 18:23:15 | INFO | train_inner | epoch 128:    757 / 1102 loss=6.875, nll_loss=3.06, ppl=8.34, wps=32743.5, ups=9.15, wpb=3579.2, bsz=159, num_updates=140700, lr=8.43049e-05, gnorm=2.113, loss_scale=4, train_wall=11, gb_free=19.5, wall=12307
2022-12-09 18:23:26 | INFO | train_inner | epoch 128:    857 / 1102 loss=6.901, nll_loss=3.083, ppl=8.47, wps=32890.5, ups=8.99, wpb=3660.1, bsz=153.9, num_updates=140800, lr=8.4275e-05, gnorm=2.119, loss_scale=4, train_wall=11, gb_free=19.4, wall=12319
2022-12-09 18:23:37 | INFO | train_inner | epoch 128:    957 / 1102 loss=7.007, nll_loss=3.177, ppl=9.04, wps=32148.3, ups=9.05, wpb=3551.7, bsz=139, num_updates=140900, lr=8.42451e-05, gnorm=2.191, loss_scale=4, train_wall=11, gb_free=19.4, wall=12330
2022-12-09 18:23:48 | INFO | train_inner | epoch 128:   1057 / 1102 loss=6.967, nll_loss=3.132, ppl=8.77, wps=32726.5, ups=9.11, wpb=3592.7, bsz=139.9, num_updates=141000, lr=8.42152e-05, gnorm=2.12, loss_scale=4, train_wall=11, gb_free=19.6, wall=12341
2022-12-09 18:23:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:24:33 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 3.672 | nll_loss 2.126 | ppl 4.37 | bleu 37.09 | wps 4547 | wpb 2835.3 | bsz 115.6 | num_updates 141045 | best_bleu 37.34
2022-12-09 18:24:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 141045 updates
2022-12-09 18:24:34 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint128.pt
2022-12-09 18:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint128.pt (epoch 128 @ 141045 updates, score 37.09) (writing took 1.292487465776503 seconds)
2022-12-09 18:24:34 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-12-09 18:24:34 | INFO | train | epoch 128 | loss 6.898 | nll_loss 3.068 | ppl 8.39 | wps 24332.2 | ups 6.79 | wpb 3583.6 | bsz 145.4 | num_updates 141045 | lr 8.42018e-05 | gnorm 2.132 | loss_scale 4 | train_wall 118 | gb_free 19.7 | wall 12386
2022-12-09 18:24:34 | INFO | fairseq.trainer | begin training epoch 129
2022-12-09 18:24:40 | INFO | train_inner | epoch 129:     55 / 1102 loss=6.876, nll_loss=3.053, ppl=8.3, wps=6787.8, ups=1.92, wpb=3539.8, bsz=150.2, num_updates=141100, lr=8.41853e-05, gnorm=2.231, loss_scale=4, train_wall=11, gb_free=19.5, wall=12393
2022-12-09 18:24:51 | INFO | train_inner | epoch 129:    155 / 1102 loss=6.881, nll_loss=3.042, ppl=8.23, wps=32831.4, ups=9.25, wpb=3548.2, bsz=142.9, num_updates=141200, lr=8.41555e-05, gnorm=2.144, loss_scale=4, train_wall=11, gb_free=19.4, wall=12403
2022-12-09 18:25:02 | INFO | train_inner | epoch 129:    255 / 1102 loss=6.936, nll_loss=3.104, ppl=8.6, wps=33016, ups=9.09, wpb=3633.6, bsz=138.1, num_updates=141300, lr=8.41257e-05, gnorm=2.048, loss_scale=4, train_wall=11, gb_free=19.2, wall=12414
2022-12-09 18:25:13 | INFO | train_inner | epoch 129:    355 / 1102 loss=6.831, nll_loss=2.989, ppl=7.94, wps=32564, ups=9.08, wpb=3585, bsz=154.1, num_updates=141400, lr=8.4096e-05, gnorm=2.148, loss_scale=4, train_wall=11, gb_free=19.4, wall=12426
2022-12-09 18:25:24 | INFO | train_inner | epoch 129:    455 / 1102 loss=6.846, nll_loss=3.023, ppl=8.13, wps=32638.2, ups=9.2, wpb=3546.1, bsz=152.2, num_updates=141500, lr=8.40663e-05, gnorm=2.056, loss_scale=4, train_wall=11, gb_free=19.3, wall=12436
2022-12-09 18:25:35 | INFO | train_inner | epoch 129:    555 / 1102 loss=6.869, nll_loss=3.053, ppl=8.3, wps=32786.9, ups=8.97, wpb=3657, bsz=155.9, num_updates=141600, lr=8.40366e-05, gnorm=2.068, loss_scale=4, train_wall=11, gb_free=19.3, wall=12448
2022-12-09 18:25:46 | INFO | train_inner | epoch 129:    655 / 1102 loss=6.896, nll_loss=3.058, ppl=8.33, wps=32780.7, ups=9.11, wpb=3600.1, bsz=138.6, num_updates=141700, lr=8.40069e-05, gnorm=2.103, loss_scale=4, train_wall=11, gb_free=19.4, wall=12459
2022-12-09 18:25:57 | INFO | train_inner | epoch 129:    755 / 1102 loss=6.894, nll_loss=3.054, ppl=8.31, wps=32422.8, ups=9.17, wpb=3534.3, bsz=143, num_updates=141800, lr=8.39773e-05, gnorm=2.197, loss_scale=4, train_wall=11, gb_free=19.5, wall=12469
2022-12-09 18:26:08 | INFO | train_inner | epoch 129:    855 / 1102 loss=6.895, nll_loss=3.073, ppl=8.42, wps=32524.6, ups=9, wpb=3614.7, bsz=151.4, num_updates=141900, lr=8.39477e-05, gnorm=2.159, loss_scale=4, train_wall=11, gb_free=19.4, wall=12481
2022-12-09 18:26:19 | INFO | train_inner | epoch 129:    955 / 1102 loss=7.009, nll_loss=3.17, ppl=9, wps=32785.5, ups=9.08, wpb=3611.8, bsz=130.7, num_updates=142000, lr=8.39181e-05, gnorm=2.191, loss_scale=4, train_wall=11, gb_free=19.8, wall=12492
2022-12-09 18:26:30 | INFO | train_inner | epoch 129:   1055 / 1102 loss=6.912, nll_loss=3.075, ppl=8.43, wps=32334.3, ups=9.17, wpb=3527.4, bsz=141.8, num_updates=142100, lr=8.38886e-05, gnorm=2.225, loss_scale=4, train_wall=11, gb_free=19.6, wall=12502
2022-12-09 18:26:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:27:15 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 3.667 | nll_loss 2.121 | ppl 4.35 | bleu 37.18 | wps 4490.9 | wpb 2835.3 | bsz 115.6 | num_updates 142147 | best_bleu 37.34
2022-12-09 18:27:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 142147 updates
2022-12-09 18:27:16 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint129.pt
2022-12-09 18:27:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint129.pt (epoch 129 @ 142147 updates, score 37.18) (writing took 1.3457901198416948 seconds)
2022-12-09 18:27:17 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-12-09 18:27:17 | INFO | train | epoch 129 | loss 6.895 | nll_loss 3.063 | ppl 8.36 | wps 24276.7 | ups 6.77 | wpb 3583.6 | bsz 145.4 | num_updates 142147 | lr 8.38747e-05 | gnorm 2.133 | loss_scale 4 | train_wall 118 | gb_free 19.7 | wall 12549
2022-12-09 18:27:17 | INFO | fairseq.trainer | begin training epoch 130
2022-12-09 18:27:23 | INFO | train_inner | epoch 130:     53 / 1102 loss=6.899, nll_loss=3.072, ppl=8.41, wps=6721.7, ups=1.89, wpb=3553.9, bsz=141, num_updates=142200, lr=8.38591e-05, gnorm=2.089, loss_scale=4, train_wall=11, gb_free=19.7, wall=12555
2022-12-09 18:27:34 | INFO | train_inner | epoch 130:    153 / 1102 loss=6.869, nll_loss=3.037, ppl=8.21, wps=33743.8, ups=9.38, wpb=3598.8, bsz=147.2, num_updates=142300, lr=8.38296e-05, gnorm=2.078, loss_scale=4, train_wall=10, gb_free=19.4, wall=12566
2022-12-09 18:27:44 | INFO | train_inner | epoch 130:    253 / 1102 loss=6.853, nll_loss=3.019, ppl=8.11, wps=34012.5, ups=9.53, wpb=3570.4, bsz=152.1, num_updates=142400, lr=8.38002e-05, gnorm=2.162, loss_scale=4, train_wall=10, gb_free=19.5, wall=12576
2022-12-09 18:27:55 | INFO | train_inner | epoch 130:    353 / 1102 loss=6.889, nll_loss=3.045, ppl=8.25, wps=33299.3, ups=9.36, wpb=3556.3, bsz=136, num_updates=142500, lr=8.37708e-05, gnorm=2.083, loss_scale=4, train_wall=10, gb_free=19.4, wall=12587
2022-12-09 18:28:06 | INFO | train_inner | epoch 130:    453 / 1102 loss=6.87, nll_loss=3.026, ppl=8.14, wps=32745.6, ups=9.27, wpb=3532.5, bsz=139.6, num_updates=142600, lr=8.37414e-05, gnorm=2.184, loss_scale=4, train_wall=11, gb_free=19.4, wall=12598
2022-12-09 18:28:17 | INFO | train_inner | epoch 130:    553 / 1102 loss=6.844, nll_loss=3.002, ppl=8.01, wps=31982.3, ups=8.99, wpb=3557.5, bsz=143.6, num_updates=142700, lr=8.37121e-05, gnorm=2.133, loss_scale=4, train_wall=11, gb_free=19.4, wall=12609
2022-12-09 18:28:28 | INFO | train_inner | epoch 130:    653 / 1102 loss=6.913, nll_loss=3.107, ppl=8.61, wps=32818.2, ups=8.96, wpb=3663.8, bsz=157.4, num_updates=142800, lr=8.36827e-05, gnorm=2.089, loss_scale=4, train_wall=11, gb_free=19.6, wall=12620
2022-12-09 18:28:39 | INFO | train_inner | epoch 130:    753 / 1102 loss=6.911, nll_loss=3.083, ppl=8.48, wps=31693.6, ups=8.89, wpb=3563.6, bsz=146, num_updates=142900, lr=8.36535e-05, gnorm=2.13, loss_scale=4, train_wall=11, gb_free=19.5, wall=12631
2022-12-09 18:28:50 | INFO | train_inner | epoch 130:    853 / 1102 loss=6.9, nll_loss=3.077, ppl=8.44, wps=32141.2, ups=8.93, wpb=3599.1, bsz=146.9, num_updates=143000, lr=8.36242e-05, gnorm=2.091, loss_scale=4, train_wall=11, gb_free=19.6, wall=12643
2022-12-09 18:29:01 | INFO | train_inner | epoch 130:    953 / 1102 loss=6.888, nll_loss=3.054, ppl=8.31, wps=32381, ups=8.97, wpb=3609.8, bsz=142.9, num_updates=143100, lr=8.3595e-05, gnorm=2.101, loss_scale=4, train_wall=11, gb_free=19.3, wall=12654
2022-12-09 18:29:12 | INFO | train_inner | epoch 130:   1053 / 1102 loss=6.969, nll_loss=3.13, ppl=8.75, wps=32221.6, ups=9.06, wpb=3555.1, bsz=145.4, num_updates=143200, lr=8.35658e-05, gnorm=2.227, loss_scale=4, train_wall=11, gb_free=19.4, wall=12665
2022-12-09 18:29:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:29:59 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 3.67 | nll_loss 2.127 | ppl 4.37 | bleu 37.22 | wps 4438.6 | wpb 2835.3 | bsz 115.6 | num_updates 143249 | best_bleu 37.34
2022-12-09 18:29:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 143249 updates
2022-12-09 18:30:00 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint130.pt
2022-12-09 18:30:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint130.pt (epoch 130 @ 143249 updates, score 37.22) (writing took 1.468075324781239 seconds)
2022-12-09 18:30:00 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-12-09 18:30:00 | INFO | train | epoch 130 | loss 6.891 | nll_loss 3.059 | ppl 8.34 | wps 24154.9 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 143249 | lr 8.35515e-05 | gnorm 2.123 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 12713
2022-12-09 18:30:00 | INFO | fairseq.trainer | begin training epoch 131
2022-12-09 18:30:06 | INFO | train_inner | epoch 131:     51 / 1102 loss=6.875, nll_loss=3.045, ppl=8.25, wps=6721.5, ups=1.86, wpb=3605.5, bsz=140.7, num_updates=143300, lr=8.35366e-05, gnorm=2.174, loss_scale=4, train_wall=11, gb_free=19.5, wall=12719
2022-12-09 18:30:17 | INFO | train_inner | epoch 131:    151 / 1102 loss=6.858, nll_loss=3.019, ppl=8.11, wps=32053.2, ups=9.18, wpb=3492.6, bsz=146.2, num_updates=143400, lr=8.35075e-05, gnorm=2.164, loss_scale=4, train_wall=11, gb_free=19.4, wall=12729
2022-12-09 18:30:28 | INFO | train_inner | epoch 131:    251 / 1102 loss=6.866, nll_loss=3.019, ppl=8.11, wps=32260.4, ups=9.04, wpb=3570.6, bsz=136.2, num_updates=143500, lr=8.34784e-05, gnorm=2.125, loss_scale=4, train_wall=11, gb_free=19.8, wall=12740
2022-12-09 18:30:39 | INFO | train_inner | epoch 131:    351 / 1102 loss=6.865, nll_loss=3.034, ppl=8.19, wps=32807.4, ups=9.07, wpb=3616.8, bsz=147.2, num_updates=143600, lr=8.34493e-05, gnorm=2.088, loss_scale=4, train_wall=11, gb_free=19.6, wall=12751
2022-12-09 18:30:50 | INFO | train_inner | epoch 131:    451 / 1102 loss=6.86, nll_loss=3.033, ppl=8.19, wps=32223.4, ups=9.05, wpb=3561.7, bsz=152.2, num_updates=143700, lr=8.34203e-05, gnorm=2.123, loss_scale=4, train_wall=11, gb_free=19.5, wall=12763
2022-12-09 18:31:01 | INFO | train_inner | epoch 131:    551 / 1102 loss=6.893, nll_loss=3.059, ppl=8.33, wps=32048.2, ups=8.96, wpb=3575.1, bsz=149.3, num_updates=143800, lr=8.33913e-05, gnorm=2.154, loss_scale=4, train_wall=11, gb_free=19.5, wall=12774
2022-12-09 18:31:12 | INFO | train_inner | epoch 131:    651 / 1102 loss=7, nll_loss=3.148, ppl=8.87, wps=32568.4, ups=9.08, wpb=3585.3, bsz=121.2, num_updates=143900, lr=8.33623e-05, gnorm=2.167, loss_scale=4, train_wall=11, gb_free=19.4, wall=12785
2022-12-09 18:31:23 | INFO | train_inner | epoch 131:    751 / 1102 loss=6.878, nll_loss=3.056, ppl=8.32, wps=32378.1, ups=9.08, wpb=3566.8, bsz=152.7, num_updates=144000, lr=8.33333e-05, gnorm=2.154, loss_scale=4, train_wall=11, gb_free=19.4, wall=12796
2022-12-09 18:31:34 | INFO | train_inner | epoch 131:    851 / 1102 loss=6.878, nll_loss=3.053, ppl=8.3, wps=32886.7, ups=9.12, wpb=3607.7, bsz=152.4, num_updates=144100, lr=8.33044e-05, gnorm=2.155, loss_scale=4, train_wall=11, gb_free=19.6, wall=12807
2022-12-09 18:31:45 | INFO | train_inner | epoch 131:    951 / 1102 loss=6.873, nll_loss=3.046, ppl=8.26, wps=32394.6, ups=9.06, wpb=3577.2, bsz=153.5, num_updates=144200, lr=8.32755e-05, gnorm=2.157, loss_scale=4, train_wall=11, gb_free=19.5, wall=12818
2022-12-09 18:31:56 | INFO | train_inner | epoch 131:   1051 / 1102 loss=6.932, nll_loss=3.109, ppl=8.63, wps=33169.4, ups=9.13, wpb=3631.9, bsz=144.1, num_updates=144300, lr=8.32467e-05, gnorm=2.077, loss_scale=4, train_wall=11, gb_free=19.4, wall=12829
2022-12-09 18:32:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:32:43 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 3.665 | nll_loss 2.121 | ppl 4.35 | bleu 37.26 | wps 4445.9 | wpb 2835.3 | bsz 115.6 | num_updates 144351 | best_bleu 37.34
2022-12-09 18:32:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 144351 updates
2022-12-09 18:32:44 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint131.pt
2022-12-09 18:32:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint131.pt (epoch 131 @ 144351 updates, score 37.26) (writing took 1.622326459735632 seconds)
2022-12-09 18:32:44 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-12-09 18:32:44 | INFO | train | epoch 131 | loss 6.889 | nll_loss 3.057 | ppl 8.32 | wps 24075 | ups 6.72 | wpb 3583.6 | bsz 145.4 | num_updates 144351 | lr 8.3232e-05 | gnorm 2.134 | loss_scale 4 | train_wall 119 | gb_free 19.3 | wall 12877
2022-12-09 18:32:44 | INFO | fairseq.trainer | begin training epoch 132
2022-12-09 18:32:50 | INFO | train_inner | epoch 132:     49 / 1102 loss=6.875, nll_loss=3.051, ppl=8.29, wps=6782.1, ups=1.86, wpb=3640.9, bsz=145.2, num_updates=144400, lr=8.32178e-05, gnorm=2.034, loss_scale=4, train_wall=11, gb_free=19.3, wall=12882
2022-12-09 18:33:01 | INFO | train_inner | epoch 132:    149 / 1102 loss=6.844, nll_loss=3.027, ppl=8.15, wps=33059.3, ups=9.03, wpb=3659.5, bsz=150.3, num_updates=144500, lr=8.3189e-05, gnorm=2.065, loss_scale=4, train_wall=11, gb_free=19.5, wall=12893
2022-12-09 18:33:12 | INFO | train_inner | epoch 132:    249 / 1102 loss=6.894, nll_loss=3.034, ppl=8.19, wps=32092.3, ups=9.24, wpb=3474, bsz=129.6, num_updates=144600, lr=8.31603e-05, gnorm=2.394, loss_scale=4, train_wall=11, gb_free=19.3, wall=12904
2022-12-09 18:33:23 | INFO | train_inner | epoch 132:    349 / 1102 loss=6.863, nll_loss=3.035, ppl=8.2, wps=32626.1, ups=9.1, wpb=3584.8, bsz=151.8, num_updates=144700, lr=8.31315e-05, gnorm=2.099, loss_scale=4, train_wall=11, gb_free=19.4, wall=12915
2022-12-09 18:33:34 | INFO | train_inner | epoch 132:    449 / 1102 loss=6.863, nll_loss=3.03, ppl=8.17, wps=32491.1, ups=9.14, wpb=3555.7, bsz=148.5, num_updates=144800, lr=8.31028e-05, gnorm=2.082, loss_scale=4, train_wall=11, gb_free=19.4, wall=12926
2022-12-09 18:33:45 | INFO | train_inner | epoch 132:    549 / 1102 loss=6.874, nll_loss=3.03, ppl=8.17, wps=32707.6, ups=9.11, wpb=3589.4, bsz=136.2, num_updates=144900, lr=8.30741e-05, gnorm=2.226, loss_scale=4, train_wall=11, gb_free=19.3, wall=12937
2022-12-09 18:33:56 | INFO | train_inner | epoch 132:    649 / 1102 loss=6.891, nll_loss=3.071, ppl=8.41, wps=32988.1, ups=9.02, wpb=3657.2, bsz=152, num_updates=145000, lr=8.30455e-05, gnorm=2.023, loss_scale=4, train_wall=11, gb_free=19.3, wall=12948
2022-12-09 18:34:07 | INFO | train_inner | epoch 132:    749 / 1102 loss=6.902, nll_loss=3.067, ppl=8.38, wps=32583.9, ups=9.14, wpb=3564.7, bsz=144.2, num_updates=145100, lr=8.30169e-05, gnorm=2.208, loss_scale=4, train_wall=11, gb_free=19.5, wall=12959
2022-12-09 18:34:18 | INFO | train_inner | epoch 132:    849 / 1102 loss=6.93, nll_loss=3.086, ppl=8.49, wps=32421.4, ups=9.16, wpb=3537.8, bsz=136, num_updates=145200, lr=8.29883e-05, gnorm=2.23, loss_scale=4, train_wall=11, gb_free=20, wall=12970
2022-12-09 18:34:29 | INFO | train_inner | epoch 132:    949 / 1102 loss=6.907, nll_loss=3.066, ppl=8.37, wps=32459.5, ups=9.09, wpb=3571.8, bsz=149.4, num_updates=145300, lr=8.29597e-05, gnorm=2.263, loss_scale=4, train_wall=11, gb_free=19.4, wall=12981
2022-12-09 18:34:40 | INFO | train_inner | epoch 132:   1049 / 1102 loss=6.926, nll_loss=3.107, ppl=8.61, wps=32447.9, ups=9.06, wpb=3579.8, bsz=145.8, num_updates=145400, lr=8.29312e-05, gnorm=2.215, loss_scale=4, train_wall=11, gb_free=19.7, wall=12992
2022-12-09 18:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:35:25 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 3.67 | nll_loss 2.125 | ppl 4.36 | bleu 37.3 | wps 4609.1 | wpb 2835.3 | bsz 115.6 | num_updates 145453 | best_bleu 37.34
2022-12-09 18:35:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 145453 updates
2022-12-09 18:35:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint132.pt
2022-12-09 18:35:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint132.pt (epoch 132 @ 145453 updates, score 37.3) (writing took 1.3783317189663649 seconds)
2022-12-09 18:35:27 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-12-09 18:35:27 | INFO | train | epoch 132 | loss 6.884 | nll_loss 3.052 | ppl 8.29 | wps 24340.1 | ups 6.79 | wpb 3583.6 | bsz 145.4 | num_updates 145453 | lr 8.29161e-05 | gnorm 2.165 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 13039
2022-12-09 18:35:27 | INFO | fairseq.trainer | begin training epoch 133
2022-12-09 18:35:32 | INFO | train_inner | epoch 133:     47 / 1102 loss=6.811, nll_loss=3.002, ppl=8.01, wps=6942, ups=1.92, wpb=3618.8, bsz=160.1, num_updates=145500, lr=8.29027e-05, gnorm=2.04, loss_scale=4, train_wall=11, gb_free=19.4, wall=13044
2022-12-09 18:35:43 | INFO | train_inner | epoch 133:    147 / 1102 loss=6.802, nll_loss=2.975, ppl=7.86, wps=33243.4, ups=9.13, wpb=3642.9, bsz=155.3, num_updates=145600, lr=8.28742e-05, gnorm=2.081, loss_scale=4, train_wall=11, gb_free=19.4, wall=13055
2022-12-09 18:35:54 | INFO | train_inner | epoch 133:    247 / 1102 loss=6.848, nll_loss=2.994, ppl=7.97, wps=32642.8, ups=9.18, wpb=3554.8, bsz=141, num_updates=145700, lr=8.28457e-05, gnorm=2.118, loss_scale=4, train_wall=11, gb_free=19.5, wall=13066
2022-12-09 18:36:05 | INFO | train_inner | epoch 133:    347 / 1102 loss=6.932, nll_loss=3.098, ppl=8.56, wps=32886.8, ups=9.21, wpb=3570.4, bsz=141.4, num_updates=145800, lr=8.28173e-05, gnorm=2.113, loss_scale=4, train_wall=11, gb_free=19.6, wall=13077
2022-12-09 18:36:16 | INFO | train_inner | epoch 133:    447 / 1102 loss=6.912, nll_loss=3.093, ppl=8.53, wps=33257.3, ups=9.11, wpb=3648.9, bsz=146.2, num_updates=145900, lr=8.27889e-05, gnorm=2.023, loss_scale=4, train_wall=11, gb_free=19.4, wall=13088
2022-12-09 18:36:26 | INFO | train_inner | epoch 133:    547 / 1102 loss=6.881, nll_loss=3.048, ppl=8.27, wps=32693.3, ups=9.25, wpb=3534.1, bsz=144, num_updates=146000, lr=8.27606e-05, gnorm=2.208, loss_scale=4, train_wall=11, gb_free=19.6, wall=13099
2022-12-09 18:36:37 | INFO | train_inner | epoch 133:    647 / 1102 loss=6.88, nll_loss=3.029, ppl=8.16, wps=33153.3, ups=9.3, wpb=3565.7, bsz=139, num_updates=146100, lr=8.27323e-05, gnorm=2.186, loss_scale=4, train_wall=11, gb_free=19.4, wall=13110
2022-12-09 18:36:48 | INFO | train_inner | epoch 133:    747 / 1102 loss=6.858, nll_loss=3.036, ppl=8.2, wps=32883.6, ups=9.16, wpb=3588.1, bsz=157.3, num_updates=146200, lr=8.2704e-05, gnorm=2.077, loss_scale=4, train_wall=11, gb_free=19.5, wall=13120
2022-12-09 18:36:59 | INFO | train_inner | epoch 133:    847 / 1102 loss=6.929, nll_loss=3.08, ppl=8.45, wps=32868.6, ups=9.29, wpb=3538.6, bsz=132.8, num_updates=146300, lr=8.26757e-05, gnorm=2.217, loss_scale=4, train_wall=11, gb_free=19.7, wall=13131
2022-12-09 18:37:10 | INFO | train_inner | epoch 133:    947 / 1102 loss=6.881, nll_loss=3.051, ppl=8.29, wps=32935.4, ups=9.16, wpb=3596.1, bsz=154.4, num_updates=146400, lr=8.26475e-05, gnorm=2.127, loss_scale=4, train_wall=11, gb_free=19.9, wall=13142
2022-12-09 18:37:21 | INFO | train_inner | epoch 133:   1047 / 1102 loss=6.923, nll_loss=3.095, ppl=8.54, wps=32963.1, ups=9.14, wpb=3606.8, bsz=143.2, num_updates=146500, lr=8.26192e-05, gnorm=2.148, loss_scale=4, train_wall=11, gb_free=19.5, wall=13153
2022-12-09 18:37:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:38:07 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.24 | wps 4467.2 | wpb 2835.3 | bsz 115.6 | num_updates 146555 | best_bleu 37.34
2022-12-09 18:38:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 146555 updates
2022-12-09 18:38:08 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint133.pt
2022-12-09 18:38:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint133.pt (epoch 133 @ 146555 updates, score 37.24) (writing took 1.3530305745080113 seconds)
2022-12-09 18:38:08 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-12-09 18:38:08 | INFO | train | epoch 133 | loss 6.883 | nll_loss 3.048 | ppl 8.27 | wps 24385.4 | ups 6.8 | wpb 3583.6 | bsz 145.4 | num_updates 146555 | lr 8.26037e-05 | gnorm 2.132 | loss_scale 4 | train_wall 117 | gb_free 19.5 | wall 13201
2022-12-09 18:38:09 | INFO | fairseq.trainer | begin training epoch 134
2022-12-09 18:38:14 | INFO | train_inner | epoch 134:     45 / 1102 loss=6.864, nll_loss=3.024, ppl=8.13, wps=6780.1, ups=1.89, wpb=3596, bsz=137.4, num_updates=146600, lr=8.25911e-05, gnorm=2.151, loss_scale=4, train_wall=11, gb_free=19.7, wall=13206
2022-12-09 18:38:24 | INFO | train_inner | epoch 134:    145 / 1102 loss=6.791, nll_loss=2.968, ppl=7.82, wps=33079.9, ups=9.3, wpb=3558.2, bsz=159, num_updates=146700, lr=8.25629e-05, gnorm=2.157, loss_scale=4, train_wall=11, gb_free=19.3, wall=13217
2022-12-09 18:38:35 | INFO | train_inner | epoch 134:    245 / 1102 loss=6.836, nll_loss=3.001, ppl=8.01, wps=32933.3, ups=9.26, wpb=3557.4, bsz=142.5, num_updates=146800, lr=8.25348e-05, gnorm=2.2, loss_scale=4, train_wall=11, gb_free=19.6, wall=13228
2022-12-09 18:38:46 | INFO | train_inner | epoch 134:    345 / 1102 loss=6.826, nll_loss=2.996, ppl=7.98, wps=33126.7, ups=9.13, wpb=3630.1, bsz=155.8, num_updates=146900, lr=8.25067e-05, gnorm=2.069, loss_scale=4, train_wall=11, gb_free=19.4, wall=13239
2022-12-09 18:38:57 | INFO | train_inner | epoch 134:    445 / 1102 loss=6.875, nll_loss=3.029, ppl=8.16, wps=32550.5, ups=9.19, wpb=3540.7, bsz=142.2, num_updates=147000, lr=8.24786e-05, gnorm=2.32, loss_scale=4, train_wall=11, gb_free=19.1, wall=13250
2022-12-09 18:39:08 | INFO | train_inner | epoch 134:    545 / 1102 loss=6.853, nll_loss=3.019, ppl=8.11, wps=32729.9, ups=9.08, wpb=3603.6, bsz=145.8, num_updates=147100, lr=8.24506e-05, gnorm=2.149, loss_scale=4, train_wall=11, gb_free=19.2, wall=13261
2022-12-09 18:39:19 | INFO | train_inner | epoch 134:    645 / 1102 loss=6.934, nll_loss=3.086, ppl=8.49, wps=32992.1, ups=9.26, wpb=3564.6, bsz=138.9, num_updates=147200, lr=8.24226e-05, gnorm=2.147, loss_scale=4, train_wall=11, gb_free=19.5, wall=13271
2022-12-09 18:39:30 | INFO | train_inner | epoch 134:    745 / 1102 loss=6.907, nll_loss=3.065, ppl=8.37, wps=32406.4, ups=9.11, wpb=3557.2, bsz=138.5, num_updates=147300, lr=8.23946e-05, gnorm=2.204, loss_scale=4, train_wall=11, gb_free=20, wall=13282
2022-12-09 18:39:41 | INFO | train_inner | epoch 134:    845 / 1102 loss=6.957, nll_loss=3.115, ppl=8.66, wps=33306.9, ups=9.3, wpb=3579.9, bsz=134.9, num_updates=147400, lr=8.23666e-05, gnorm=2.141, loss_scale=4, train_wall=11, gb_free=19.3, wall=13293
2022-12-09 18:39:52 | INFO | train_inner | epoch 134:    945 / 1102 loss=6.925, nll_loss=3.099, ppl=8.57, wps=33154, ups=9.08, wpb=3649.7, bsz=149.5, num_updates=147500, lr=8.23387e-05, gnorm=2.112, loss_scale=4, train_wall=11, gb_free=19.3, wall=13304
2022-12-09 18:40:02 | INFO | train_inner | epoch 134:   1045 / 1102 loss=6.895, nll_loss=3.071, ppl=8.4, wps=33174.4, ups=9.31, wpb=3564.8, bsz=150.7, num_updates=147600, lr=8.23108e-05, gnorm=2.059, loss_scale=4, train_wall=11, gb_free=19.8, wall=13315
2022-12-09 18:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:40:47 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 3.665 | nll_loss 2.118 | ppl 4.34 | bleu 37.32 | wps 4703.3 | wpb 2835.3 | bsz 115.6 | num_updates 147657 | best_bleu 37.34
2022-12-09 18:40:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 147657 updates
2022-12-09 18:40:48 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint134.pt
2022-12-09 18:40:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint134.pt (epoch 134 @ 147657 updates, score 37.32) (writing took 1.3271792335435748 seconds)
2022-12-09 18:40:48 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-12-09 18:40:48 | INFO | train | epoch 134 | loss 6.88 | nll_loss 3.045 | ppl 8.25 | wps 24697 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 147657 | lr 8.22949e-05 | gnorm 2.155 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 13361
2022-12-09 18:40:48 | INFO | fairseq.trainer | begin training epoch 135
2022-12-09 18:40:53 | INFO | train_inner | epoch 135:     43 / 1102 loss=6.89, nll_loss=3.058, ppl=8.33, wps=7086.4, ups=1.97, wpb=3605.2, bsz=149.1, num_updates=147700, lr=8.22829e-05, gnorm=2.123, loss_scale=4, train_wall=11, gb_free=19.5, wall=13366
2022-12-09 18:41:04 | INFO | train_inner | epoch 135:    143 / 1102 loss=6.887, nll_loss=3.059, ppl=8.34, wps=33405.3, ups=9.24, wpb=3615.5, bsz=148.6, num_updates=147800, lr=8.22551e-05, gnorm=2.154, loss_scale=4, train_wall=11, gb_free=19.2, wall=13377
2022-12-09 18:41:15 | INFO | train_inner | epoch 135:    243 / 1102 loss=6.882, nll_loss=3.025, ppl=8.14, wps=33004.7, ups=9.27, wpb=3559, bsz=127.9, num_updates=147900, lr=8.22273e-05, gnorm=2.245, loss_scale=4, train_wall=11, gb_free=19.7, wall=13387
2022-12-09 18:41:26 | INFO | train_inner | epoch 135:    343 / 1102 loss=6.863, nll_loss=3.011, ppl=8.06, wps=32588.5, ups=9.3, wpb=3505.8, bsz=136.1, num_updates=148000, lr=8.21995e-05, gnorm=2.237, loss_scale=4, train_wall=11, gb_free=19.4, wall=13398
2022-12-09 18:41:36 | INFO | train_inner | epoch 135:    443 / 1102 loss=6.848, nll_loss=3.021, ppl=8.12, wps=33276.5, ups=9.27, wpb=3590.8, bsz=157.9, num_updates=148100, lr=8.21717e-05, gnorm=2.141, loss_scale=4, train_wall=11, gb_free=19.3, wall=13409
2022-12-09 18:41:47 | INFO | train_inner | epoch 135:    543 / 1102 loss=6.924, nll_loss=3.09, ppl=8.52, wps=33139.4, ups=9.23, wpb=3588.5, bsz=147.4, num_updates=148200, lr=8.2144e-05, gnorm=2.158, loss_scale=4, train_wall=11, gb_free=19.5, wall=13420
2022-12-09 18:41:58 | INFO | train_inner | epoch 135:    643 / 1102 loss=6.844, nll_loss=3.006, ppl=8.03, wps=33150.8, ups=9.23, wpb=3590.7, bsz=146.2, num_updates=148300, lr=8.21163e-05, gnorm=2.073, loss_scale=4, train_wall=11, gb_free=19.4, wall=13430
2022-12-09 18:42:09 | INFO | train_inner | epoch 135:    743 / 1102 loss=6.947, nll_loss=3.106, ppl=8.61, wps=32657.2, ups=9.07, wpb=3599.5, bsz=131.4, num_updates=148400, lr=8.20886e-05, gnorm=2.141, loss_scale=4, train_wall=11, gb_free=19.3, wall=13442
2022-12-09 18:42:20 | INFO | train_inner | epoch 135:    843 / 1102 loss=6.823, nll_loss=3.002, ppl=8.01, wps=32998.9, ups=9.18, wpb=3594.1, bsz=153.4, num_updates=148500, lr=8.2061e-05, gnorm=2.114, loss_scale=4, train_wall=11, gb_free=19.5, wall=13452
2022-12-09 18:42:31 | INFO | train_inner | epoch 135:    943 / 1102 loss=6.957, nll_loss=3.115, ppl=8.67, wps=32657.5, ups=9.19, wpb=3553.3, bsz=133, num_updates=148600, lr=8.20334e-05, gnorm=2.342, loss_scale=4, train_wall=11, gb_free=19.6, wall=13463
2022-12-09 18:42:42 | INFO | train_inner | epoch 135:   1043 / 1102 loss=6.869, nll_loss=3.037, ppl=8.21, wps=32597.9, ups=9.1, wpb=3582.3, bsz=151.7, num_updates=148700, lr=8.20058e-05, gnorm=2.108, loss_scale=4, train_wall=11, gb_free=19.3, wall=13474
2022-12-09 18:42:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:43:30 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 3.666 | nll_loss 2.127 | ppl 4.37 | bleu 37.31 | wps 4358.9 | wpb 2835.3 | bsz 115.6 | num_updates 148759 | best_bleu 37.34
2022-12-09 18:43:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 148759 updates
2022-12-09 18:43:31 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint135.pt
2022-12-09 18:43:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint135.pt (epoch 135 @ 148759 updates, score 37.31) (writing took 1.319843809120357 seconds)
2022-12-09 18:43:31 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-12-09 18:43:31 | INFO | train | epoch 135 | loss 6.875 | nll_loss 3.04 | ppl 8.22 | wps 24248 | ups 6.77 | wpb 3583.6 | bsz 145.4 | num_updates 148759 | lr 8.19895e-05 | gnorm 2.157 | loss_scale 4 | train_wall 117 | gb_free 19.3 | wall 13524
2022-12-09 18:43:31 | INFO | fairseq.trainer | begin training epoch 136
2022-12-09 18:43:36 | INFO | train_inner | epoch 136:     41 / 1102 loss=6.815, nll_loss=2.997, ppl=7.98, wps=6712.9, ups=1.85, wpb=3634.2, bsz=156.4, num_updates=148800, lr=8.19782e-05, gnorm=2.038, loss_scale=4, train_wall=11, gb_free=19.6, wall=13528
2022-12-09 18:43:47 | INFO | train_inner | epoch 136:    141 / 1102 loss=6.842, nll_loss=3.001, ppl=8, wps=32491.4, ups=9.2, wpb=3533.3, bsz=144.1, num_updates=148900, lr=8.19507e-05, gnorm=2.187, loss_scale=4, train_wall=11, gb_free=19.4, wall=13539
2022-12-09 18:43:58 | INFO | train_inner | epoch 136:    241 / 1102 loss=6.881, nll_loss=3.05, ppl=8.28, wps=32649.8, ups=9.09, wpb=3591.8, bsz=144.5, num_updates=149000, lr=8.19232e-05, gnorm=2.167, loss_scale=4, train_wall=11, gb_free=20.1, wall=13550
2022-12-09 18:44:09 | INFO | train_inner | epoch 136:    341 / 1102 loss=6.815, nll_loss=2.973, ppl=7.85, wps=32084.4, ups=9.02, wpb=3555.2, bsz=148.7, num_updates=149100, lr=8.18957e-05, gnorm=2.136, loss_scale=4, train_wall=11, gb_free=20.2, wall=13561
2022-12-09 18:44:20 | INFO | train_inner | epoch 136:    441 / 1102 loss=6.903, nll_loss=3.043, ppl=8.24, wps=32657.6, ups=9.17, wpb=3562.5, bsz=125.3, num_updates=149200, lr=8.18683e-05, gnorm=2.204, loss_scale=4, train_wall=11, gb_free=19.7, wall=13572
2022-12-09 18:44:31 | INFO | train_inner | epoch 136:    541 / 1102 loss=6.891, nll_loss=3.055, ppl=8.31, wps=32958.9, ups=9.11, wpb=3617, bsz=146.2, num_updates=149300, lr=8.18408e-05, gnorm=2.178, loss_scale=4, train_wall=11, gb_free=19.6, wall=13583
2022-12-09 18:44:42 | INFO | train_inner | epoch 136:    641 / 1102 loss=6.866, nll_loss=3.016, ppl=8.09, wps=32147.9, ups=9.08, wpb=3541.6, bsz=134.1, num_updates=149400, lr=8.18134e-05, gnorm=2.163, loss_scale=4, train_wall=11, gb_free=19.3, wall=13594
2022-12-09 18:44:53 | INFO | train_inner | epoch 136:    741 / 1102 loss=6.861, nll_loss=3.041, ppl=8.23, wps=32426.7, ups=9.04, wpb=3587.9, bsz=155.8, num_updates=149500, lr=8.17861e-05, gnorm=2.056, loss_scale=4, train_wall=11, gb_free=19.6, wall=13605
2022-12-09 18:45:04 | INFO | train_inner | epoch 136:    841 / 1102 loss=6.928, nll_loss=3.098, ppl=8.56, wps=32406.6, ups=9.06, wpb=3578.2, bsz=148.5, num_updates=149600, lr=8.17587e-05, gnorm=2.278, loss_scale=4, train_wall=11, gb_free=19.3, wall=13616
2022-12-09 18:45:15 | INFO | train_inner | epoch 136:    941 / 1102 loss=6.871, nll_loss=3.034, ppl=8.19, wps=32634.6, ups=9.05, wpb=3605.3, bsz=150, num_updates=149700, lr=8.17314e-05, gnorm=2.179, loss_scale=4, train_wall=11, gb_free=19.9, wall=13627
2022-12-09 18:45:26 | INFO | train_inner | epoch 136:   1041 / 1102 loss=6.881, nll_loss=3.058, ppl=8.33, wps=33293.9, ups=9.13, wpb=3648.4, bsz=152.8, num_updates=149800, lr=8.17041e-05, gnorm=2.129, loss_scale=4, train_wall=11, gb_free=19.4, wall=13638
2022-12-09 18:45:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:46:14 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 3.669 | nll_loss 2.121 | ppl 4.35 | bleu 37.12 | wps 4411.8 | wpb 2835.3 | bsz 115.6 | num_updates 149861 | best_bleu 37.34
2022-12-09 18:46:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 149861 updates
2022-12-09 18:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint136.pt
2022-12-09 18:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint136.pt (epoch 136 @ 149861 updates, score 37.12) (writing took 1.344676229171455 seconds)
2022-12-09 18:46:15 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-12-09 18:46:15 | INFO | train | epoch 136 | loss 6.872 | nll_loss 3.036 | ppl 8.2 | wps 24101.5 | ups 6.73 | wpb 3583.6 | bsz 145.4 | num_updates 149861 | lr 8.16875e-05 | gnorm 2.163 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 13687
2022-12-09 18:46:15 | INFO | fairseq.trainer | begin training epoch 137
2022-12-09 18:46:20 | INFO | train_inner | epoch 137:     39 / 1102 loss=6.833, nll_loss=2.998, ppl=7.99, wps=6573.8, ups=1.86, wpb=3530.1, bsz=148.3, num_updates=149900, lr=8.16769e-05, gnorm=2.137, loss_scale=4, train_wall=11, gb_free=19.3, wall=13692
2022-12-09 18:46:31 | INFO | train_inner | epoch 137:    139 / 1102 loss=6.826, nll_loss=2.988, ppl=7.93, wps=32856, ups=9.03, wpb=3636.8, bsz=150.2, num_updates=150000, lr=8.16497e-05, gnorm=2.094, loss_scale=4, train_wall=11, gb_free=19.4, wall=13703
2022-12-09 18:46:42 | INFO | train_inner | epoch 137:    239 / 1102 loss=6.807, nll_loss=2.983, ppl=7.91, wps=33103, ups=9.06, wpb=3653, bsz=153, num_updates=150100, lr=8.16225e-05, gnorm=2.042, loss_scale=4, train_wall=11, gb_free=19.5, wall=13714
2022-12-09 18:46:53 | INFO | train_inner | epoch 137:    339 / 1102 loss=6.838, nll_loss=3.009, ppl=8.05, wps=32637.4, ups=9.02, wpb=3618, bsz=153.4, num_updates=150200, lr=8.15953e-05, gnorm=2.112, loss_scale=4, train_wall=11, gb_free=19.8, wall=13725
2022-12-09 18:47:04 | INFO | train_inner | epoch 137:    439 / 1102 loss=6.902, nll_loss=3.06, ppl=8.34, wps=32592.7, ups=9.06, wpb=3596.1, bsz=136.6, num_updates=150300, lr=8.15681e-05, gnorm=2.109, loss_scale=4, train_wall=11, gb_free=19.3, wall=13736
2022-12-09 18:47:15 | INFO | train_inner | epoch 137:    539 / 1102 loss=6.847, nll_loss=3.013, ppl=8.07, wps=32056.7, ups=9.08, wpb=3531.4, bsz=151.6, num_updates=150400, lr=8.1541e-05, gnorm=2.136, loss_scale=4, train_wall=11, gb_free=19.4, wall=13747
2022-12-09 18:47:26 | INFO | train_inner | epoch 137:    639 / 1102 loss=6.862, nll_loss=3.024, ppl=8.13, wps=32713.1, ups=9.07, wpb=3605.3, bsz=142.9, num_updates=150500, lr=8.15139e-05, gnorm=2.259, loss_scale=4, train_wall=11, gb_free=19.6, wall=13758
2022-12-09 18:47:37 | INFO | train_inner | epoch 137:    739 / 1102 loss=6.89, nll_loss=3.049, ppl=8.28, wps=32807.6, ups=9.06, wpb=3619.9, bsz=140.7, num_updates=150600, lr=8.14868e-05, gnorm=2.137, loss_scale=4, train_wall=11, gb_free=19.6, wall=13769
2022-12-09 18:47:48 | INFO | train_inner | epoch 137:    839 / 1102 loss=6.948, nll_loss=3.104, ppl=8.6, wps=32899.8, ups=9.24, wpb=3560.3, bsz=129.9, num_updates=150700, lr=8.14598e-05, gnorm=2.192, loss_scale=4, train_wall=11, gb_free=19.7, wall=13780
2022-12-09 18:47:59 | INFO | train_inner | epoch 137:    939 / 1102 loss=6.88, nll_loss=3.054, ppl=8.31, wps=32762.6, ups=9.09, wpb=3605.1, bsz=147.7, num_updates=150800, lr=8.14328e-05, gnorm=2.13, loss_scale=4, train_wall=11, gb_free=20, wall=13791
2022-12-09 18:48:10 | INFO | train_inner | epoch 137:   1039 / 1102 loss=6.858, nll_loss=3.027, ppl=8.15, wps=31728.6, ups=9.07, wpb=3497.1, bsz=155.3, num_updates=150900, lr=8.14058e-05, gnorm=2.157, loss_scale=4, train_wall=11, gb_free=19.5, wall=13802
2022-12-09 18:48:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:48:57 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 3.666 | nll_loss 2.123 | ppl 4.36 | bleu 37.15 | wps 4474 | wpb 2835.3 | bsz 115.6 | num_updates 150963 | best_bleu 37.34
2022-12-09 18:48:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 150963 updates
2022-12-09 18:48:58 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint137.pt
2022-12-09 18:48:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint137.pt (epoch 137 @ 150963 updates, score 37.15) (writing took 1.3207817310467362 seconds)
2022-12-09 18:48:59 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-12-09 18:48:59 | INFO | train | epoch 137 | loss 6.869 | nll_loss 3.033 | ppl 8.19 | wps 24165.3 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 150963 | lr 8.13888e-05 | gnorm 2.14 | loss_scale 4 | train_wall 119 | gb_free 19.2 | wall 13851
2022-12-09 18:48:59 | INFO | fairseq.trainer | begin training epoch 138
2022-12-09 18:49:03 | INFO | train_inner | epoch 138:     37 / 1102 loss=6.918, nll_loss=3.08, ppl=8.45, wps=6683.9, ups=1.89, wpb=3537.1, bsz=142.7, num_updates=151000, lr=8.13788e-05, gnorm=2.157, loss_scale=4, train_wall=11, gb_free=19.5, wall=13855
2022-12-09 18:49:14 | INFO | train_inner | epoch 138:    137 / 1102 loss=6.72, nll_loss=2.881, ppl=7.37, wps=31899.6, ups=9.14, wpb=3491.9, bsz=161.8, num_updates=151100, lr=8.13519e-05, gnorm=2.202, loss_scale=4, train_wall=11, gb_free=19.7, wall=13866
2022-12-09 18:49:25 | INFO | train_inner | epoch 138:    237 / 1102 loss=6.772, nll_loss=2.928, ppl=7.61, wps=32702.4, ups=9.11, wpb=3589.3, bsz=156.3, num_updates=151200, lr=8.1325e-05, gnorm=2.242, loss_scale=4, train_wall=11, gb_free=19.6, wall=13877
2022-12-09 18:49:36 | INFO | train_inner | epoch 138:    337 / 1102 loss=6.939, nll_loss=3.088, ppl=8.5, wps=32692.3, ups=9.1, wpb=3593.3, bsz=130.2, num_updates=151300, lr=8.12981e-05, gnorm=2.141, loss_scale=4, train_wall=11, gb_free=19.9, wall=13888
2022-12-09 18:49:47 | INFO | train_inner | epoch 138:    437 / 1102 loss=6.883, nll_loss=3.038, ppl=8.21, wps=32729.7, ups=9.08, wpb=3603.1, bsz=132.9, num_updates=151400, lr=8.12713e-05, gnorm=2.132, loss_scale=4, train_wall=11, gb_free=19.4, wall=13899
2022-12-09 18:49:58 | INFO | train_inner | epoch 138:    537 / 1102 loss=6.884, nll_loss=3.04, ppl=8.22, wps=32560.2, ups=9.1, wpb=3576.7, bsz=142.8, num_updates=151500, lr=8.12444e-05, gnorm=2.158, loss_scale=4, train_wall=11, gb_free=19.4, wall=13910
2022-12-09 18:50:09 | INFO | train_inner | epoch 138:    637 / 1102 loss=6.834, nll_loss=3.011, ppl=8.06, wps=32856.8, ups=9.04, wpb=3636.6, bsz=151.6, num_updates=151600, lr=8.12176e-05, gnorm=2.075, loss_scale=4, train_wall=11, gb_free=19.4, wall=13921
2022-12-09 18:50:20 | INFO | train_inner | epoch 138:    737 / 1102 loss=6.953, nll_loss=3.106, ppl=8.61, wps=32390.2, ups=9.05, wpb=3578.9, bsz=130.3, num_updates=151700, lr=8.11909e-05, gnorm=2.194, loss_scale=4, train_wall=11, gb_free=19.5, wall=13932
2022-12-09 18:50:31 | INFO | train_inner | epoch 138:    837 / 1102 loss=6.886, nll_loss=3.048, ppl=8.27, wps=32795.6, ups=9.21, wpb=3559.2, bsz=146.6, num_updates=151800, lr=8.11641e-05, gnorm=2.133, loss_scale=4, train_wall=11, gb_free=19.3, wall=13943
2022-12-09 18:50:42 | INFO | train_inner | epoch 138:    937 / 1102 loss=6.868, nll_loss=3.039, ppl=8.22, wps=32791.3, ups=9.09, wpb=3608.2, bsz=149, num_updates=151900, lr=8.11374e-05, gnorm=2.146, loss_scale=4, train_wall=11, gb_free=19.6, wall=13954
2022-12-09 18:50:53 | INFO | train_inner | epoch 138:   1037 / 1102 loss=6.909, nll_loss=3.085, ppl=8.48, wps=32682.5, ups=9.05, wpb=3609.4, bsz=146, num_updates=152000, lr=8.11107e-05, gnorm=2.105, loss_scale=4, train_wall=11, gb_free=19.4, wall=13965
2022-12-09 18:51:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:51:38 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 3.677 | nll_loss 2.131 | ppl 4.38 | bleu 37.07 | wps 4733.3 | wpb 2835.3 | bsz 115.6 | num_updates 152065 | best_bleu 37.34
2022-12-09 18:51:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 152065 updates
2022-12-09 18:51:39 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint138.pt
2022-12-09 18:51:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint138.pt (epoch 138 @ 152065 updates, score 37.07) (writing took 1.4863938950002193 seconds)
2022-12-09 18:51:40 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-12-09 18:51:40 | INFO | train | epoch 138 | loss 6.866 | nll_loss 3.029 | ppl 8.16 | wps 24498.2 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 152065 | lr 8.10934e-05 | gnorm 2.145 | loss_scale 4 | train_wall 118 | gb_free 19.8 | wall 14012
2022-12-09 18:51:40 | INFO | fairseq.trainer | begin training epoch 139
2022-12-09 18:51:44 | INFO | train_inner | epoch 139:     35 / 1102 loss=6.872, nll_loss=3.048, ppl=8.27, wps=7105, ups=1.95, wpb=3637.4, bsz=146, num_updates=152100, lr=8.1084e-05, gnorm=2.089, loss_scale=4, train_wall=11, gb_free=19.5, wall=14016
2022-12-09 18:51:55 | INFO | train_inner | epoch 139:    135 / 1102 loss=6.822, nll_loss=2.982, ppl=7.9, wps=32571.2, ups=9.16, wpb=3557, bsz=149.3, num_updates=152200, lr=8.10574e-05, gnorm=2.19, loss_scale=4, train_wall=11, gb_free=19.6, wall=14027
2022-12-09 18:52:06 | INFO | train_inner | epoch 139:    235 / 1102 loss=6.891, nll_loss=3.044, ppl=8.25, wps=32478.2, ups=9.06, wpb=3585.1, bsz=132, num_updates=152300, lr=8.10308e-05, gnorm=2.108, loss_scale=4, train_wall=11, gb_free=19.6, wall=14038
2022-12-09 18:52:17 | INFO | train_inner | epoch 139:    335 / 1102 loss=6.832, nll_loss=2.992, ppl=7.95, wps=32658.8, ups=9.22, wpb=3543.6, bsz=143.2, num_updates=152400, lr=8.10042e-05, gnorm=2.119, loss_scale=4, train_wall=11, gb_free=19.8, wall=14049
2022-12-09 18:52:28 | INFO | train_inner | epoch 139:    435 / 1102 loss=6.85, nll_loss=3.015, ppl=8.08, wps=32662.6, ups=9.1, wpb=3590.4, bsz=144.9, num_updates=152500, lr=8.09776e-05, gnorm=2.078, loss_scale=4, train_wall=11, gb_free=19.5, wall=14060
2022-12-09 18:52:39 | INFO | train_inner | epoch 139:    535 / 1102 loss=6.838, nll_loss=2.997, ppl=7.98, wps=32534.4, ups=9.1, wpb=3573.3, bsz=148.1, num_updates=152600, lr=8.09511e-05, gnorm=2.137, loss_scale=4, train_wall=11, gb_free=19.9, wall=14071
2022-12-09 18:52:50 | INFO | train_inner | epoch 139:    635 / 1102 loss=6.882, nll_loss=3.042, ppl=8.24, wps=32851.1, ups=9.15, wpb=3591, bsz=141.3, num_updates=152700, lr=8.09246e-05, gnorm=2.227, loss_scale=4, train_wall=11, gb_free=19.7, wall=14082
2022-12-09 18:53:01 | INFO | train_inner | epoch 139:    735 / 1102 loss=6.845, nll_loss=2.999, ppl=7.99, wps=32424, ups=9.15, wpb=3542.4, bsz=148.2, num_updates=152800, lr=8.08981e-05, gnorm=2.167, loss_scale=4, train_wall=11, gb_free=19.7, wall=14093
2022-12-09 18:53:12 | INFO | train_inner | epoch 139:    835 / 1102 loss=6.911, nll_loss=3.081, ppl=8.46, wps=32870.9, ups=9.07, wpb=3624.4, bsz=138.2, num_updates=152900, lr=8.08716e-05, gnorm=2.138, loss_scale=4, train_wall=11, gb_free=19.5, wall=14104
2022-12-09 18:53:23 | INFO | train_inner | epoch 139:    935 / 1102 loss=6.782, nll_loss=2.969, ppl=7.83, wps=32633, ups=8.99, wpb=3630.9, bsz=173.8, num_updates=153000, lr=8.08452e-05, gnorm=2.07, loss_scale=4, train_wall=11, gb_free=19.3, wall=14115
2022-12-09 18:53:34 | INFO | train_inner | epoch 139:   1035 / 1102 loss=6.91, nll_loss=3.08, ppl=8.46, wps=32745.8, ups=9.1, wpb=3597.3, bsz=148.6, num_updates=153100, lr=8.08188e-05, gnorm=2.244, loss_scale=4, train_wall=11, gb_free=19.2, wall=14126
2022-12-09 18:53:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:54:21 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 3.666 | nll_loss 2.122 | ppl 4.35 | bleu 37.36 | wps 4490.9 | wpb 2835.3 | bsz 115.6 | num_updates 153167 | best_bleu 37.36
2022-12-09 18:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 153167 updates
2022-12-09 18:54:23 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint139.pt
2022-12-09 18:54:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint139.pt (epoch 139 @ 153167 updates, score 37.36) (writing took 2.084287641569972 seconds)
2022-12-09 18:54:23 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-12-09 18:54:23 | INFO | train | epoch 139 | loss 6.862 | nll_loss 3.025 | ppl 8.14 | wps 24121.7 | ups 6.73 | wpb 3583.6 | bsz 145.4 | num_updates 153167 | lr 8.08011e-05 | gnorm 2.155 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 14176
2022-12-09 18:54:24 | INFO | fairseq.trainer | begin training epoch 140
2022-12-09 18:54:27 | INFO | train_inner | epoch 140:     33 / 1102 loss=6.943, nll_loss=3.091, ppl=8.52, wps=6605.4, ups=1.86, wpb=3546.7, bsz=132, num_updates=153200, lr=8.07924e-05, gnorm=2.286, loss_scale=4, train_wall=11, gb_free=19.7, wall=14180
2022-12-09 18:54:38 | INFO | train_inner | epoch 140:    133 / 1102 loss=6.796, nll_loss=2.967, ppl=7.82, wps=33174, ups=9.04, wpb=3668.3, bsz=154.5, num_updates=153300, lr=8.07661e-05, gnorm=2.037, loss_scale=4, train_wall=11, gb_free=19.6, wall=14191
2022-12-09 18:54:49 | INFO | train_inner | epoch 140:    233 / 1102 loss=6.817, nll_loss=2.981, ppl=7.9, wps=32689.4, ups=9.1, wpb=3591, bsz=149.2, num_updates=153400, lr=8.07397e-05, gnorm=2.139, loss_scale=4, train_wall=11, gb_free=19.7, wall=14202
2022-12-09 18:55:00 | INFO | train_inner | epoch 140:    333 / 1102 loss=6.864, nll_loss=3.038, ppl=8.21, wps=32583.5, ups=9.1, wpb=3579.2, bsz=153.8, num_updates=153500, lr=8.07134e-05, gnorm=2.113, loss_scale=4, train_wall=11, gb_free=19.3, wall=14213
2022-12-09 18:55:11 | INFO | train_inner | epoch 140:    433 / 1102 loss=6.831, nll_loss=2.986, ppl=7.92, wps=32768.4, ups=9.16, wpb=3577, bsz=142.6, num_updates=153600, lr=8.06872e-05, gnorm=2.158, loss_scale=4, train_wall=11, gb_free=19.6, wall=14224
2022-12-09 18:55:22 | INFO | train_inner | epoch 140:    533 / 1102 loss=6.786, nll_loss=2.957, ppl=7.77, wps=32566, ups=9.16, wpb=3557.2, bsz=165.8, num_updates=153700, lr=8.06609e-05, gnorm=2.173, loss_scale=4, train_wall=11, gb_free=19.7, wall=14235
2022-12-09 18:55:33 | INFO | train_inner | epoch 140:    633 / 1102 loss=6.856, nll_loss=3, ppl=8, wps=31848.5, ups=9.17, wpb=3474.2, bsz=133.5, num_updates=153800, lr=8.06347e-05, gnorm=2.194, loss_scale=4, train_wall=11, gb_free=19.7, wall=14246
2022-12-09 18:55:44 | INFO | train_inner | epoch 140:    733 / 1102 loss=6.837, nll_loss=2.995, ppl=7.97, wps=32439.5, ups=9.04, wpb=3588, bsz=144.6, num_updates=153900, lr=8.06085e-05, gnorm=2.233, loss_scale=4, train_wall=11, gb_free=19.4, wall=14257
2022-12-09 18:55:55 | INFO | train_inner | epoch 140:    833 / 1102 loss=6.915, nll_loss=3.074, ppl=8.42, wps=33099.6, ups=9.12, wpb=3629.2, bsz=140.3, num_updates=154000, lr=8.05823e-05, gnorm=2.175, loss_scale=4, train_wall=11, gb_free=19.4, wall=14268
2022-12-09 18:56:06 | INFO | train_inner | epoch 140:    933 / 1102 loss=6.92, nll_loss=3.077, ppl=8.44, wps=32591, ups=9.09, wpb=3584.4, bsz=139.6, num_updates=154100, lr=8.05561e-05, gnorm=2.215, loss_scale=4, train_wall=11, gb_free=19.7, wall=14279
2022-12-09 18:56:17 | INFO | train_inner | epoch 140:   1033 / 1102 loss=6.88, nll_loss=3.042, ppl=8.24, wps=32928.7, ups=9.13, wpb=3605.1, bsz=144.3, num_updates=154200, lr=8.053e-05, gnorm=2.127, loss_scale=4, train_wall=11, gb_free=19.3, wall=14290
2022-12-09 18:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:57:06 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 3.66 | nll_loss 2.12 | ppl 4.35 | bleu 37.25 | wps 4412 | wpb 2835.3 | bsz 115.6 | num_updates 154269 | best_bleu 37.36
2022-12-09 18:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 154269 updates
2022-12-09 18:57:07 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint140.pt
2022-12-09 18:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint140.pt (epoch 140 @ 154269 updates, score 37.25) (writing took 1.3268876485526562 seconds)
2022-12-09 18:57:07 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-12-09 18:57:07 | INFO | train | epoch 140 | loss 6.86 | nll_loss 3.02 | ppl 8.11 | wps 24150 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 154269 | lr 8.0512e-05 | gnorm 2.165 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 14339
2022-12-09 18:57:07 | INFO | fairseq.trainer | begin training epoch 141
2022-12-09 18:57:11 | INFO | train_inner | epoch 141:     31 / 1102 loss=6.918, nll_loss=3.075, ppl=8.43, wps=6698.3, ups=1.87, wpb=3585.9, bsz=135.4, num_updates=154300, lr=8.05039e-05, gnorm=2.157, loss_scale=4, train_wall=11, gb_free=19.5, wall=14343
2022-12-09 18:57:22 | INFO | train_inner | epoch 141:    131 / 1102 loss=6.798, nll_loss=2.956, ppl=7.76, wps=32582.7, ups=9.13, wpb=3570.5, bsz=147.1, num_updates=154400, lr=8.04778e-05, gnorm=2.198, loss_scale=8, train_wall=11, gb_free=19.3, wall=14354
2022-12-09 18:57:33 | INFO | train_inner | epoch 141:    231 / 1102 loss=6.876, nll_loss=3.034, ppl=8.19, wps=32715.6, ups=9.17, wpb=3567.8, bsz=143.1, num_updates=154500, lr=8.04518e-05, gnorm=2.121, loss_scale=8, train_wall=11, gb_free=19.2, wall=14365
2022-12-09 18:57:43 | INFO | train_inner | epoch 141:    331 / 1102 loss=6.81, nll_loss=2.973, ppl=7.85, wps=32225.4, ups=9.14, wpb=3524.9, bsz=152.9, num_updates=154600, lr=8.04258e-05, gnorm=2.133, loss_scale=8, train_wall=11, gb_free=19.5, wall=14376
2022-12-09 18:57:54 | INFO | train_inner | epoch 141:    431 / 1102 loss=6.793, nll_loss=2.953, ppl=7.74, wps=32214.2, ups=9.06, wpb=3556, bsz=156.5, num_updates=154700, lr=8.03998e-05, gnorm=2.15, loss_scale=8, train_wall=11, gb_free=19.4, wall=14387
2022-12-09 18:58:06 | INFO | train_inner | epoch 141:    531 / 1102 loss=6.839, nll_loss=3.007, ppl=8.04, wps=32772.7, ups=9.03, wpb=3630.7, bsz=147.7, num_updates=154800, lr=8.03738e-05, gnorm=2.068, loss_scale=8, train_wall=11, gb_free=19.8, wall=14398
2022-12-09 18:58:17 | INFO | train_inner | epoch 141:    631 / 1102 loss=6.882, nll_loss=3.035, ppl=8.19, wps=32600.5, ups=9.12, wpb=3573.5, bsz=132.6, num_updates=154900, lr=8.03479e-05, gnorm=2.186, loss_scale=8, train_wall=11, gb_free=20.1, wall=14409
2022-12-09 18:58:28 | INFO | train_inner | epoch 141:    731 / 1102 loss=6.921, nll_loss=3.086, ppl=8.49, wps=32979.6, ups=9.11, wpb=3621, bsz=154.3, num_updates=155000, lr=8.03219e-05, gnorm=2.158, loss_scale=8, train_wall=11, gb_free=19.4, wall=14420
2022-12-09 18:58:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-09 18:58:38 | INFO | train_inner | epoch 141:    832 / 1102 loss=6.879, nll_loss=3.026, ppl=8.14, wps=32653.3, ups=9.11, wpb=3584.4, bsz=134.2, num_updates=155100, lr=8.0296e-05, gnorm=2.242, loss_scale=4, train_wall=11, gb_free=19.8, wall=14431
2022-12-09 18:58:50 | INFO | train_inner | epoch 141:    932 / 1102 loss=6.856, nll_loss=3.031, ppl=8.17, wps=32539.6, ups=9, wpb=3615.6, bsz=153.8, num_updates=155200, lr=8.02702e-05, gnorm=2.152, loss_scale=4, train_wall=11, gb_free=19.6, wall=14442
2022-12-09 18:59:01 | INFO | train_inner | epoch 141:   1032 / 1102 loss=6.895, nll_loss=3.058, ppl=8.33, wps=32541.2, ups=9.09, wpb=3581.6, bsz=141.8, num_updates=155300, lr=8.02443e-05, gnorm=2.116, loss_scale=4, train_wall=11, gb_free=19.5, wall=14453
2022-12-09 18:59:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 18:59:47 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 3.663 | nll_loss 2.12 | ppl 4.35 | bleu 37.32 | wps 4652 | wpb 2835.3 | bsz 115.6 | num_updates 155370 | best_bleu 37.36
2022-12-09 18:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 155370 updates
2022-12-09 18:59:48 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint141.pt
2022-12-09 18:59:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint141.pt (epoch 141 @ 155370 updates, score 37.32) (writing took 1.3171696867793798 seconds)
2022-12-09 18:59:48 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-12-09 18:59:48 | INFO | train | epoch 141 | loss 6.857 | nll_loss 3.017 | ppl 8.1 | wps 24435.2 | ups 6.82 | wpb 3583.9 | bsz 145.5 | num_updates 155370 | lr 8.02262e-05 | gnorm 2.149 | loss_scale 4 | train_wall 118 | gb_free 19.8 | wall 14501
2022-12-09 18:59:49 | INFO | fairseq.trainer | begin training epoch 142
2022-12-09 18:59:52 | INFO | train_inner | epoch 142:     30 / 1102 loss=6.859, nll_loss=3.014, ppl=8.08, wps=6944.6, ups=1.95, wpb=3569.2, bsz=142.2, num_updates=155400, lr=8.02185e-05, gnorm=2.139, loss_scale=4, train_wall=11, gb_free=19.5, wall=14504
2022-12-09 19:00:03 | INFO | train_inner | epoch 142:    130 / 1102 loss=6.876, nll_loss=3.033, ppl=8.19, wps=32806.9, ups=9.15, wpb=3586.9, bsz=139.2, num_updates=155500, lr=8.01927e-05, gnorm=2.205, loss_scale=4, train_wall=11, gb_free=19.4, wall=14515
2022-12-09 19:00:14 | INFO | train_inner | epoch 142:    230 / 1102 loss=6.82, nll_loss=2.984, ppl=7.91, wps=32769.7, ups=9.11, wpb=3595.5, bsz=148.5, num_updates=155600, lr=8.01669e-05, gnorm=2.137, loss_scale=4, train_wall=11, gb_free=19.5, wall=14526
2022-12-09 19:00:25 | INFO | train_inner | epoch 142:    330 / 1102 loss=6.9, nll_loss=3.043, ppl=8.24, wps=33219.9, ups=9.21, wpb=3608.5, bsz=127, num_updates=155700, lr=8.01412e-05, gnorm=2.142, loss_scale=4, train_wall=11, gb_free=19.2, wall=14537
2022-12-09 19:00:36 | INFO | train_inner | epoch 142:    430 / 1102 loss=6.817, nll_loss=2.984, ppl=7.91, wps=33036.5, ups=9.03, wpb=3658.3, bsz=151.8, num_updates=155800, lr=8.01154e-05, gnorm=2.12, loss_scale=4, train_wall=11, gb_free=19.5, wall=14548
2022-12-09 19:00:47 | INFO | train_inner | epoch 142:    530 / 1102 loss=6.836, nll_loss=2.995, ppl=7.97, wps=32511.7, ups=9.08, wpb=3581.3, bsz=148.2, num_updates=155900, lr=8.00898e-05, gnorm=2.203, loss_scale=4, train_wall=11, gb_free=19.2, wall=14559
2022-12-09 19:00:58 | INFO | train_inner | epoch 142:    630 / 1102 loss=6.82, nll_loss=2.997, ppl=7.98, wps=32899.8, ups=9.11, wpb=3610.1, bsz=159, num_updates=156000, lr=8.00641e-05, gnorm=2.07, loss_scale=4, train_wall=11, gb_free=19.7, wall=14570
2022-12-09 19:01:09 | INFO | train_inner | epoch 142:    730 / 1102 loss=6.868, nll_loss=3.04, ppl=8.23, wps=32919.1, ups=9.07, wpb=3629.6, bsz=155.1, num_updates=156100, lr=8.00384e-05, gnorm=2.178, loss_scale=4, train_wall=11, gb_free=19.4, wall=14581
2022-12-09 19:01:20 | INFO | train_inner | epoch 142:    830 / 1102 loss=6.858, nll_loss=3.018, ppl=8.1, wps=32181.5, ups=9.02, wpb=3568.9, bsz=143.8, num_updates=156200, lr=8.00128e-05, gnorm=2.211, loss_scale=4, train_wall=11, gb_free=19.6, wall=14592
2022-12-09 19:01:31 | INFO | train_inner | epoch 142:    930 / 1102 loss=6.866, nll_loss=3.017, ppl=8.1, wps=31690.8, ups=9.15, wpb=3465.2, bsz=143.4, num_updates=156300, lr=7.99872e-05, gnorm=2.188, loss_scale=4, train_wall=11, gb_free=19.5, wall=14603
2022-12-09 19:01:42 | INFO | train_inner | epoch 142:   1030 / 1102 loss=6.907, nll_loss=3.055, ppl=8.31, wps=32474.2, ups=9.09, wpb=3571, bsz=136.2, num_updates=156400, lr=7.99616e-05, gnorm=2.217, loss_scale=4, train_wall=11, gb_free=19.7, wall=14614
2022-12-09 19:01:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 19:02:29 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 3.671 | nll_loss 2.126 | ppl 4.36 | bleu 37.18 | wps 4655.6 | wpb 2835.3 | bsz 115.6 | num_updates 156472 | best_bleu 37.36
2022-12-09 19:02:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 156472 updates
2022-12-09 19:02:30 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint142.pt
2022-12-09 19:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint142.pt (epoch 142 @ 156472 updates, score 37.18) (writing took 1.275061652995646 seconds)
2022-12-09 19:02:30 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-12-09 19:02:30 | INFO | train | epoch 142 | loss 6.854 | nll_loss 3.015 | ppl 8.08 | wps 24459.2 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 156472 | lr 7.99432e-05 | gnorm 2.161 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 14662
2022-12-09 19:02:30 | INFO | fairseq.trainer | begin training epoch 143
2022-12-09 19:02:33 | INFO | train_inner | epoch 143:     28 / 1102 loss=6.872, nll_loss=3.033, ppl=8.18, wps=6944.6, ups=1.94, wpb=3574.3, bsz=140.2, num_updates=156500, lr=7.99361e-05, gnorm=2.14, loss_scale=4, train_wall=11, gb_free=19.8, wall=14666
2022-12-09 19:02:44 | INFO | train_inner | epoch 143:    128 / 1102 loss=6.771, nll_loss=2.935, ppl=7.65, wps=32378.4, ups=9.06, wpb=3573.7, bsz=153.3, num_updates=156600, lr=7.99106e-05, gnorm=2.12, loss_scale=4, train_wall=11, gb_free=19.3, wall=14677
2022-12-09 19:02:55 | INFO | train_inner | epoch 143:    228 / 1102 loss=6.878, nll_loss=3.036, ppl=8.2, wps=33161.6, ups=9.08, wpb=3650.5, bsz=134.3, num_updates=156700, lr=7.9885e-05, gnorm=2.083, loss_scale=4, train_wall=11, gb_free=19.3, wall=14688
2022-12-09 19:03:06 | INFO | train_inner | epoch 143:    328 / 1102 loss=6.789, nll_loss=2.954, ppl=7.75, wps=32520.7, ups=9.08, wpb=3582.2, bsz=159.6, num_updates=156800, lr=7.98596e-05, gnorm=2.189, loss_scale=4, train_wall=11, gb_free=19.5, wall=14699
2022-12-09 19:03:17 | INFO | train_inner | epoch 143:    428 / 1102 loss=6.828, nll_loss=2.981, ppl=7.9, wps=32617.9, ups=9.18, wpb=3554.7, bsz=144.9, num_updates=156900, lr=7.98341e-05, gnorm=2.248, loss_scale=4, train_wall=11, gb_free=19.4, wall=14710
2022-12-09 19:03:28 | INFO | train_inner | epoch 143:    528 / 1102 loss=6.777, nll_loss=2.954, ppl=7.75, wps=32678.7, ups=9.09, wpb=3594.7, bsz=166.9, num_updates=157000, lr=7.98087e-05, gnorm=2.117, loss_scale=4, train_wall=11, gb_free=19.3, wall=14721
2022-12-09 19:03:39 | INFO | train_inner | epoch 143:    628 / 1102 loss=6.87, nll_loss=3.011, ppl=8.06, wps=32171.2, ups=9.1, wpb=3533.7, bsz=132.1, num_updates=157100, lr=7.97833e-05, gnorm=2.222, loss_scale=4, train_wall=11, gb_free=19.4, wall=14732
2022-12-09 19:03:50 | INFO | train_inner | epoch 143:    728 / 1102 loss=6.806, nll_loss=2.957, ppl=7.76, wps=31857.9, ups=9.16, wpb=3478.8, bsz=148.7, num_updates=157200, lr=7.97579e-05, gnorm=2.232, loss_scale=4, train_wall=11, gb_free=19.3, wall=14743
2022-12-09 19:04:01 | INFO | train_inner | epoch 143:    828 / 1102 loss=6.971, nll_loss=3.125, ppl=8.72, wps=32901.1, ups=9.14, wpb=3600.7, bsz=133.1, num_updates=157300, lr=7.97325e-05, gnorm=2.178, loss_scale=4, train_wall=11, gb_free=19.5, wall=14754
2022-12-09 19:04:12 | INFO | train_inner | epoch 143:    928 / 1102 loss=6.849, nll_loss=3.022, ppl=8.12, wps=33338.8, ups=9.09, wpb=3666.6, bsz=149, num_updates=157400, lr=7.97072e-05, gnorm=2.124, loss_scale=4, train_wall=11, gb_free=19.3, wall=14765
2022-12-09 19:04:23 | INFO | train_inner | epoch 143:   1028 / 1102 loss=6.894, nll_loss=3.056, ppl=8.32, wps=32321.8, ups=9.06, wpb=3569.1, bsz=142.5, num_updates=157500, lr=7.96819e-05, gnorm=2.158, loss_scale=4, train_wall=11, gb_free=19.4, wall=14776
2022-12-09 19:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 19:05:10 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 3.666 | nll_loss 2.123 | ppl 4.35 | bleu 37.33 | wps 4670.2 | wpb 2835.3 | bsz 115.6 | num_updates 157574 | best_bleu 37.36
2022-12-09 19:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 157574 updates
2022-12-09 19:05:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint143.pt
2022-12-09 19:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint143.pt (epoch 143 @ 157574 updates, score 37.33) (writing took 1.2895183023065329 seconds)
2022-12-09 19:05:11 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-12-09 19:05:11 | INFO | train | epoch 143 | loss 6.85 | nll_loss 3.01 | ppl 8.06 | wps 24480.7 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 157574 | lr 7.96632e-05 | gnorm 2.164 | loss_scale 4 | train_wall 118 | gb_free 19.6 | wall 14824
2022-12-09 19:05:11 | INFO | fairseq.trainer | begin training epoch 144
2022-12-09 19:05:14 | INFO | train_inner | epoch 144:     26 / 1102 loss=6.876, nll_loss=3.04, ppl=8.23, wps=7089.6, ups=1.95, wpb=3629.1, bsz=140.2, num_updates=157600, lr=7.96566e-05, gnorm=2.078, loss_scale=4, train_wall=11, gb_free=19.5, wall=14827
2022-12-09 19:05:25 | INFO | train_inner | epoch 144:    126 / 1102 loss=6.815, nll_loss=2.954, ppl=7.75, wps=32354.6, ups=9.1, wpb=3554.9, bsz=128.5, num_updates=157700, lr=7.96314e-05, gnorm=2.254, loss_scale=4, train_wall=11, gb_free=19.8, wall=14838
2022-12-09 19:05:36 | INFO | train_inner | epoch 144:    226 / 1102 loss=6.777, nll_loss=2.938, ppl=7.66, wps=32651.5, ups=9.05, wpb=3608.5, bsz=153.8, num_updates=157800, lr=7.96061e-05, gnorm=2.061, loss_scale=4, train_wall=11, gb_free=20, wall=14849
2022-12-09 19:05:47 | INFO | train_inner | epoch 144:    326 / 1102 loss=6.749, nll_loss=2.916, ppl=7.55, wps=32465, ups=9.16, wpb=3545.7, bsz=163.3, num_updates=157900, lr=7.95809e-05, gnorm=2.145, loss_scale=4, train_wall=11, gb_free=19.2, wall=14860
2022-12-09 19:05:58 | INFO | train_inner | epoch 144:    426 / 1102 loss=6.875, nll_loss=3.027, ppl=8.15, wps=32592.5, ups=9.07, wpb=3594.7, bsz=138.7, num_updates=158000, lr=7.95557e-05, gnorm=2.154, loss_scale=4, train_wall=11, gb_free=19.5, wall=14871
2022-12-09 19:06:09 | INFO | train_inner | epoch 144:    526 / 1102 loss=6.856, nll_loss=3.023, ppl=8.13, wps=32967.7, ups=9.02, wpb=3653.6, bsz=144.8, num_updates=158100, lr=7.95306e-05, gnorm=2.107, loss_scale=4, train_wall=11, gb_free=19.3, wall=14882
2022-12-09 19:06:21 | INFO | train_inner | epoch 144:    626 / 1102 loss=6.855, nll_loss=3.011, ppl=8.06, wps=32249.7, ups=9.05, wpb=3564.1, bsz=148.2, num_updates=158200, lr=7.95054e-05, gnorm=2.203, loss_scale=4, train_wall=11, gb_free=19.4, wall=14893
2022-12-09 19:06:31 | INFO | train_inner | epoch 144:    726 / 1102 loss=6.918, nll_loss=3.057, ppl=8.32, wps=32962.9, ups=9.27, wpb=3557.3, bsz=126.6, num_updates=158300, lr=7.94803e-05, gnorm=2.178, loss_scale=4, train_wall=11, gb_free=19.7, wall=14904
2022-12-09 19:06:42 | INFO | train_inner | epoch 144:    826 / 1102 loss=6.867, nll_loss=3.042, ppl=8.24, wps=32951.9, ups=9.08, wpb=3628.6, bsz=152.6, num_updates=158400, lr=7.94552e-05, gnorm=2.111, loss_scale=4, train_wall=11, gb_free=19.5, wall=14915
2022-12-09 19:06:53 | INFO | train_inner | epoch 144:    926 / 1102 loss=6.868, nll_loss=3.034, ppl=8.19, wps=32508.5, ups=9.13, wpb=3562.1, bsz=151.7, num_updates=158500, lr=7.94301e-05, gnorm=2.354, loss_scale=4, train_wall=11, gb_free=19.5, wall=14926
2022-12-09 19:07:04 | INFO | train_inner | epoch 144:   1026 / 1102 loss=6.88, nll_loss=3.048, ppl=8.27, wps=32406.5, ups=9.07, wpb=3572.2, bsz=153, num_updates=158600, lr=7.94051e-05, gnorm=2.236, loss_scale=4, train_wall=11, gb_free=19.3, wall=14937
2022-12-09 19:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 19:07:51 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 3.663 | nll_loss 2.12 | ppl 4.35 | bleu 37.21 | wps 4696.4 | wpb 2835.3 | bsz 115.6 | num_updates 158676 | best_bleu 37.36
2022-12-09 19:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 158676 updates
2022-12-09 19:07:52 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint144.pt
2022-12-09 19:07:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint144.pt (epoch 144 @ 158676 updates, score 37.21) (writing took 1.3466139491647482 seconds)
2022-12-09 19:07:52 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-12-09 19:07:52 | INFO | train | epoch 144 | loss 6.847 | nll_loss 3.006 | ppl 8.03 | wps 24493.8 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 158676 | lr 7.93861e-05 | gnorm 2.179 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 14985
2022-12-09 19:07:53 | INFO | fairseq.trainer | begin training epoch 145
2022-12-09 19:07:55 | INFO | train_inner | epoch 145:     24 / 1102 loss=6.821, nll_loss=2.983, ppl=7.91, wps=6942.8, ups=1.96, wpb=3546.3, bsz=148.2, num_updates=158700, lr=7.93801e-05, gnorm=2.2, loss_scale=4, train_wall=11, gb_free=19.5, wall=14988
2022-12-09 19:08:06 | INFO | train_inner | epoch 145:    124 / 1102 loss=6.772, nll_loss=2.923, ppl=7.58, wps=32129.3, ups=9.08, wpb=3538, bsz=140.4, num_updates=158800, lr=7.93551e-05, gnorm=2.301, loss_scale=4, train_wall=11, gb_free=19.5, wall=14999
2022-12-09 19:08:17 | INFO | train_inner | epoch 145:    224 / 1102 loss=6.75, nll_loss=2.92, ppl=7.57, wps=32765.6, ups=9.08, wpb=3610.3, bsz=160.9, num_updates=158900, lr=7.93301e-05, gnorm=2.05, loss_scale=4, train_wall=11, gb_free=19.5, wall=15010
2022-12-09 19:08:29 | INFO | train_inner | epoch 145:    324 / 1102 loss=6.828, nll_loss=2.986, ppl=7.92, wps=32323.2, ups=8.96, wpb=3606.9, bsz=148.4, num_updates=159000, lr=7.93052e-05, gnorm=2.103, loss_scale=4, train_wall=11, gb_free=19.6, wall=15021
2022-12-09 19:08:40 | INFO | train_inner | epoch 145:    424 / 1102 loss=6.834, nll_loss=2.999, ppl=7.99, wps=32818.8, ups=9.11, wpb=3603.6, bsz=156.3, num_updates=159100, lr=7.92802e-05, gnorm=2.156, loss_scale=4, train_wall=11, gb_free=20, wall=15032
2022-12-09 19:08:51 | INFO | train_inner | epoch 145:    524 / 1102 loss=6.894, nll_loss=3.035, ppl=8.2, wps=32832.5, ups=9.11, wpb=3603.6, bsz=130.6, num_updates=159200, lr=7.92553e-05, gnorm=2.141, loss_scale=4, train_wall=11, gb_free=19.8, wall=15043
2022-12-09 19:09:02 | INFO | train_inner | epoch 145:    624 / 1102 loss=6.829, nll_loss=2.98, ppl=7.89, wps=32826.3, ups=9.1, wpb=3607.2, bsz=134.2, num_updates=159300, lr=7.92304e-05, gnorm=2.162, loss_scale=4, train_wall=11, gb_free=19.3, wall=15054
2022-12-09 19:09:12 | INFO | train_inner | epoch 145:    724 / 1102 loss=6.841, nll_loss=2.99, ppl=7.95, wps=32529.8, ups=9.22, wpb=3529.4, bsz=142.5, num_updates=159400, lr=7.92056e-05, gnorm=2.216, loss_scale=4, train_wall=11, gb_free=19.6, wall=15065
2022-12-09 19:09:23 | INFO | train_inner | epoch 145:    824 / 1102 loss=6.849, nll_loss=3.007, ppl=8.04, wps=32366.6, ups=9.02, wpb=3586.8, bsz=150.5, num_updates=159500, lr=7.91808e-05, gnorm=2.184, loss_scale=4, train_wall=11, gb_free=19.6, wall=15076
2022-12-09 19:09:35 | INFO | train_inner | epoch 145:    924 / 1102 loss=6.861, nll_loss=3.025, ppl=8.14, wps=31791.4, ups=8.96, wpb=3546.2, bsz=150.7, num_updates=159600, lr=7.91559e-05, gnorm=2.196, loss_scale=4, train_wall=11, gb_free=19.6, wall=15087
2022-12-09 19:09:46 | INFO | train_inner | epoch 145:   1024 / 1102 loss=6.959, nll_loss=3.115, ppl=8.67, wps=32770.7, ups=9.15, wpb=3581.3, bsz=134.8, num_updates=159700, lr=7.91312e-05, gnorm=2.168, loss_scale=4, train_wall=11, gb_free=19.5, wall=15098
2022-12-09 19:09:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 19:10:34 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 3.662 | nll_loss 2.121 | ppl 4.35 | bleu 37.43 | wps 4515.8 | wpb 2835.3 | bsz 115.6 | num_updates 159778 | best_bleu 37.43
2022-12-09 19:10:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 159778 updates
2022-12-09 19:10:35 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint145.pt
2022-12-09 19:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint145.pt (epoch 145 @ 159778 updates, score 37.43) (writing took 1.783203799277544 seconds)
2022-12-09 19:10:36 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-12-09 19:10:36 | INFO | train | epoch 145 | loss 6.845 | nll_loss 3.003 | ppl 8.02 | wps 24147.8 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 159778 | lr 7.91118e-05 | gnorm 2.163 | loss_scale 4 | train_wall 119 | gb_free 19.5 | wall 15148
2022-12-09 19:10:36 | INFO | fairseq.trainer | begin training epoch 146
2022-12-09 19:10:39 | INFO | train_inner | epoch 146:     22 / 1102 loss=6.938, nll_loss=3.1, ppl=8.57, wps=6773.6, ups=1.88, wpb=3596.2, bsz=136.6, num_updates=159800, lr=7.91064e-05, gnorm=2.115, loss_scale=4, train_wall=11, gb_free=19.5, wall=15151
2022-12-09 19:10:49 | INFO | train_inner | epoch 146:    122 / 1102 loss=6.823, nll_loss=2.981, ppl=7.9, wps=32957.9, ups=9.29, wpb=3545.8, bsz=145.1, num_updates=159900, lr=7.90817e-05, gnorm=2.146, loss_scale=4, train_wall=11, gb_free=20, wall=15162
2022-12-09 19:11:00 | INFO | train_inner | epoch 146:    222 / 1102 loss=6.818, nll_loss=2.976, ppl=7.87, wps=33316.7, ups=9.18, wpb=3629.8, bsz=147.1, num_updates=160000, lr=7.90569e-05, gnorm=2.154, loss_scale=4, train_wall=11, gb_free=19.2, wall=15173
2022-12-09 19:11:11 | INFO | train_inner | epoch 146:    322 / 1102 loss=6.808, nll_loss=2.964, ppl=7.81, wps=33087.2, ups=9.18, wpb=3604.8, bsz=142.2, num_updates=160100, lr=7.90322e-05, gnorm=2.133, loss_scale=4, train_wall=11, gb_free=19.5, wall=15184
2022-12-09 19:11:22 | INFO | train_inner | epoch 146:    422 / 1102 loss=6.784, nll_loss=2.947, ppl=7.71, wps=33097, ups=9.19, wpb=3601, bsz=149.5, num_updates=160200, lr=7.90076e-05, gnorm=2.102, loss_scale=4, train_wall=11, gb_free=19.4, wall=15194
2022-12-09 19:11:33 | INFO | train_inner | epoch 146:    522 / 1102 loss=6.875, nll_loss=3.028, ppl=8.16, wps=32552.4, ups=9.19, wpb=3540.5, bsz=141.3, num_updates=160300, lr=7.89829e-05, gnorm=2.246, loss_scale=4, train_wall=11, gb_free=19.4, wall=15205
2022-12-09 19:11:44 | INFO | train_inner | epoch 146:    622 / 1102 loss=6.847, nll_loss=3.006, ppl=8.03, wps=33227.7, ups=9.34, wpb=3558.1, bsz=148.8, num_updates=160400, lr=7.89583e-05, gnorm=2.38, loss_scale=4, train_wall=10, gb_free=19.8, wall=15216
2022-12-09 19:11:54 | INFO | train_inner | epoch 146:    722 / 1102 loss=6.876, nll_loss=3.032, ppl=8.18, wps=33462.5, ups=9.23, wpb=3626.2, bsz=143.6, num_updates=160500, lr=7.89337e-05, gnorm=2.212, loss_scale=4, train_wall=11, gb_free=19.4, wall=15227
2022-12-09 19:12:05 | INFO | train_inner | epoch 146:    822 / 1102 loss=6.889, nll_loss=3.048, ppl=8.27, wps=32711.1, ups=9.21, wpb=3552, bsz=139.9, num_updates=160600, lr=7.89091e-05, gnorm=2.149, loss_scale=4, train_wall=11, gb_free=19.3, wall=15238
2022-12-09 19:12:16 | INFO | train_inner | epoch 146:    922 / 1102 loss=6.859, nll_loss=3.017, ppl=8.09, wps=33011, ups=9.15, wpb=3607.4, bsz=147.8, num_updates=160700, lr=7.88846e-05, gnorm=2.201, loss_scale=4, train_wall=11, gb_free=19.7, wall=15249
2022-12-09 19:12:27 | INFO | train_inner | epoch 146:   1022 / 1102 loss=6.816, nll_loss=2.993, ppl=7.96, wps=33049.2, ups=9.21, wpb=3588.5, bsz=158.1, num_updates=160800, lr=7.886e-05, gnorm=2.138, loss_scale=4, train_wall=11, gb_free=19.5, wall=15260
2022-12-09 19:12:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 19:13:14 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 3.661 | nll_loss 2.118 | ppl 4.34 | bleu 37.37 | wps 4738.7 | wpb 2835.3 | bsz 115.6 | num_updates 160880 | best_bleu 37.43
2022-12-09 19:13:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 160880 updates
2022-12-09 19:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint146.pt
2022-12-09 19:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint146.pt (epoch 146 @ 160880 updates, score 37.37) (writing took 1.4328931234776974 seconds)
2022-12-09 19:13:15 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-12-09 19:13:15 | INFO | train | epoch 146 | loss 6.843 | nll_loss 3.001 | ppl 8 | wps 24786.6 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 160880 | lr 7.88404e-05 | gnorm 2.19 | loss_scale 4 | train_wall 117 | gb_free 19.8 | wall 15308
2022-12-09 19:13:15 | INFO | fairseq.trainer | begin training epoch 147
2022-12-09 19:13:18 | INFO | train_inner | epoch 147:     20 / 1102 loss=6.875, nll_loss=3.024, ppl=8.14, wps=7060.1, ups=1.97, wpb=3577.9, bsz=139.8, num_updates=160900, lr=7.88355e-05, gnorm=2.248, loss_scale=4, train_wall=11, gb_free=19.4, wall=15310
2022-12-09 19:13:29 | INFO | train_inner | epoch 147:    120 / 1102 loss=6.813, nll_loss=2.963, ppl=7.8, wps=33417.4, ups=9.24, wpb=3617.9, bsz=137.2, num_updates=161000, lr=7.8811e-05, gnorm=2.136, loss_scale=4, train_wall=11, gb_free=19.4, wall=15321
2022-12-09 19:13:39 | INFO | train_inner | epoch 147:    220 / 1102 loss=6.783, nll_loss=2.93, ppl=7.62, wps=32920.7, ups=9.22, wpb=3569.2, bsz=139.5, num_updates=161100, lr=7.87866e-05, gnorm=2.21, loss_scale=4, train_wall=11, gb_free=19.6, wall=15332
2022-12-09 19:13:50 | INFO | train_inner | epoch 147:    320 / 1102 loss=6.803, nll_loss=2.986, ppl=7.92, wps=32958.8, ups=9.18, wpb=3591.8, bsz=162.5, num_updates=161200, lr=7.87621e-05, gnorm=2.076, loss_scale=4, train_wall=11, gb_free=19.4, wall=15343
2022-12-09 19:14:01 | INFO | train_inner | epoch 147:    420 / 1102 loss=6.855, nll_loss=3.002, ppl=8.01, wps=32718.1, ups=9.18, wpb=3565.9, bsz=140.6, num_updates=161300, lr=7.87377e-05, gnorm=2.207, loss_scale=4, train_wall=11, gb_free=19.6, wall=15354
2022-12-09 19:14:12 | INFO | train_inner | epoch 147:    520 / 1102 loss=6.832, nll_loss=2.989, ppl=7.94, wps=32795.1, ups=9.18, wpb=3571.3, bsz=147.9, num_updates=161400, lr=7.87133e-05, gnorm=2.21, loss_scale=4, train_wall=11, gb_free=19.6, wall=15365
2022-12-09 19:14:23 | INFO | train_inner | epoch 147:    620 / 1102 loss=6.892, nll_loss=3.048, ppl=8.27, wps=33009.6, ups=9.18, wpb=3596.7, bsz=139.9, num_updates=161500, lr=7.86889e-05, gnorm=2.182, loss_scale=4, train_wall=11, gb_free=19.6, wall=15375
2022-12-09 19:14:34 | INFO | train_inner | epoch 147:    720 / 1102 loss=6.921, nll_loss=3.063, ppl=8.36, wps=32554.3, ups=9.09, wpb=3581.5, bsz=131.1, num_updates=161600, lr=7.86646e-05, gnorm=2.233, loss_scale=4, train_wall=11, gb_free=19.3, wall=15386
2022-12-09 19:14:45 | INFO | train_inner | epoch 147:    820 / 1102 loss=6.856, nll_loss=2.998, ppl=7.99, wps=32401.6, ups=9.05, wpb=3580.8, bsz=137.6, num_updates=161700, lr=7.86403e-05, gnorm=2.204, loss_scale=4, train_wall=11, gb_free=19.6, wall=15398
2022-12-09 19:14:56 | INFO | train_inner | epoch 147:    920 / 1102 loss=6.788, nll_loss=2.967, ppl=7.82, wps=33078.1, ups=9.15, wpb=3613.2, bsz=162.3, num_updates=161800, lr=7.8616e-05, gnorm=2.084, loss_scale=4, train_wall=11, gb_free=19.3, wall=15408
2022-12-09 19:15:07 | INFO | train_inner | epoch 147:   1020 / 1102 loss=6.82, nll_loss=2.98, ppl=7.89, wps=31471.5, ups=9.08, wpb=3467.6, bsz=160.2, num_updates=161900, lr=7.85917e-05, gnorm=2.3, loss_scale=4, train_wall=11, gb_free=19.7, wall=15419
2022-12-09 19:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 19:15:58 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.28 | wps 4350.9 | wpb 2835.3 | bsz 115.6 | num_updates 161982 | best_bleu 37.43
2022-12-09 19:15:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 161982 updates
2022-12-09 20:00:24 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 20:00:24 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 20:00:24 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 20:00:24 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 20:00:24 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 20:00:24 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 20:00:25 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 20:00:25 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 20:00:25 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 20:00:25 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 20:00:25 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 20:00:29 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 20:00:29 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 20:00:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 20:00:29 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 20:00:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 20:00:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 20:00:29 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 20:00:29 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 20:00:29 | INFO | fairseq.trainer | Loaded checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt (epoch 147 @ 160880 updates)
2022-12-09 20:00:29 | INFO | fairseq.trainer | loading train data for epoch 147
2022-12-09 20:00:29 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 20:00:29 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 20:00:29 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 20:00:30 | INFO | fairseq.trainer | begin training epoch 147
2022-12-09 20:00:32 | INFO | train_inner | epoch 147:     20 / 1102 loss=6.798, nll_loss=2.981, ppl=7.9, wps=33705.7, ups=9.46, wpb=3585.3, bsz=153.9, num_updates=160900, lr=7.88355e-05, gnorm=2.232, loss_scale=4, train_wall=3, gb_free=19.4, wall=4
2022-12-09 20:00:55 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 64, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': 'examples/translation_rdrop/translation_rdrop_src', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 300000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'iwslt14.rdrop.de-en-ckpt', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'task': Namespace(_name='rdrop_translation', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'criterion': Namespace(_name='reg_label_smoothed_cross_entropy', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='reg_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.rdrop.tokenized.de-en/', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=300000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=True, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=5, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='iwslt14.rdrop.de-en-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=64, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang='en', task='rdrop_translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir='examples/translation_rdrop/translation_rdrop_src', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001, zero_sharding='none'), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
2022-12-09 20:00:55 | INFO | fairseq.tasks.translation | [de] dictionary: 10152 types
2022-12-09 20:00:55 | INFO | fairseq.tasks.translation | [en] dictionary: 10152 types
2022-12-09 20:00:55 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.de
2022-12-09 20:00:55 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/valid.de-en.en
2022-12-09 20:00:55 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ valid de-en 7283 examples
2022-12-09 20:00:56 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10152, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=10152, bias=False)
  )
)
2022-12-09 20:00:56 | INFO | fairseq_cli.train | task: RDropTranslation
2022-12-09 20:00:56 | INFO | fairseq_cli.train | model: TransformerModel
2022-12-09 20:00:56 | INFO | fairseq_cli.train | criterion: RegLabelSmoothedCrossEntropyCriterion
2022-12-09 20:00:56 | INFO | fairseq_cli.train | num. model params: 36,741,120 (num. trained: 36,741,120)
2022-12-09 20:00:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-09 20:00:59 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-09 20:00:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 20:00:59 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2022-12-09 20:00:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-12-09 20:00:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-12-09 20:00:59 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2022-12-09 20:00:59 | INFO | fairseq.trainer | Preparing to load checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-09 20:01:00 | INFO | fairseq.trainer | Loaded checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt (epoch 147 @ 160880 updates)
2022-12-09 20:01:00 | INFO | fairseq.trainer | loading train data for epoch 147
2022-12-09 20:01:00 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.de
2022-12-09 20:01:00 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.rdrop.tokenized.de-en/train.de-en.en
2022-12-09 20:01:00 | INFO | fairseq.tasks.translation | data-bin/iwslt14.rdrop.tokenized.de-en/ train de-en 160239 examples
2022-12-09 20:01:00 | INFO | fairseq.trainer | begin training epoch 147
2022-12-09 20:01:03 | INFO | train_inner | epoch 147:     20 / 1102 loss=6.798, nll_loss=2.981, ppl=7.9, wps=33858.2, ups=9.5, wpb=3585.3, bsz=153.9, num_updates=160900, lr=7.88355e-05, gnorm=2.232, loss_scale=4, train_wall=3, gb_free=19.4, wall=4
2022-12-09 20:01:14 | INFO | train_inner | epoch 147:    120 / 1102 loss=6.813, nll_loss=2.963, ppl=7.8, wps=33725.9, ups=9.32, wpb=3617.9, bsz=137.2, num_updates=161000, lr=7.8811e-05, gnorm=2.136, loss_scale=4, train_wall=10, gb_free=19.4, wall=14
2022-12-09 20:01:25 | INFO | train_inner | epoch 147:    220 / 1102 loss=6.783, nll_loss=2.93, ppl=7.62, wps=33160.3, ups=9.29, wpb=3569.2, bsz=139.5, num_updates=161100, lr=7.87866e-05, gnorm=2.232, loss_scale=4, train_wall=11, gb_free=19.6, wall=25
2022-12-09 20:01:35 | INFO | train_inner | epoch 147:    320 / 1102 loss=6.803, nll_loss=2.986, ppl=7.92, wps=33292.4, ups=9.27, wpb=3591.8, bsz=162.5, num_updates=161200, lr=7.87621e-05, gnorm=2.07, loss_scale=4, train_wall=11, gb_free=19.4, wall=36
2022-12-09 20:01:46 | INFO | train_inner | epoch 147:    420 / 1102 loss=6.855, nll_loss=3.001, ppl=8.01, wps=33105.1, ups=9.28, wpb=3565.9, bsz=140.6, num_updates=161300, lr=7.87377e-05, gnorm=2.223, loss_scale=4, train_wall=11, gb_free=19.6, wall=47
2022-12-09 20:01:57 | INFO | train_inner | epoch 147:    520 / 1102 loss=6.832, nll_loss=2.989, ppl=7.94, wps=33281.5, ups=9.32, wpb=3571.3, bsz=147.9, num_updates=161400, lr=7.87133e-05, gnorm=2.175, loss_scale=4, train_wall=10, gb_free=19.6, wall=58
2022-12-09 20:02:08 | INFO | train_inner | epoch 147:    620 / 1102 loss=6.892, nll_loss=3.048, ppl=8.27, wps=33120.8, ups=9.21, wpb=3596.7, bsz=139.9, num_updates=161500, lr=7.86889e-05, gnorm=2.186, loss_scale=4, train_wall=11, gb_free=19.6, wall=68
2022-12-09 20:02:18 | INFO | train_inner | epoch 147:    720 / 1102 loss=6.921, nll_loss=3.063, ppl=8.36, wps=33529.8, ups=9.36, wpb=3581.5, bsz=131.1, num_updates=161600, lr=7.86646e-05, gnorm=2.244, loss_scale=4, train_wall=10, gb_free=19.3, wall=79
2022-12-09 20:02:29 | INFO | train_inner | epoch 147:    820 / 1102 loss=6.855, nll_loss=2.997, ppl=7.99, wps=33140.3, ups=9.25, wpb=3580.8, bsz=137.6, num_updates=161700, lr=7.86403e-05, gnorm=2.202, loss_scale=4, train_wall=11, gb_free=19.6, wall=90
2022-12-09 20:02:40 | INFO | train_inner | epoch 147:    920 / 1102 loss=6.788, nll_loss=2.967, ppl=7.82, wps=33451.4, ups=9.26, wpb=3613.2, bsz=162.3, num_updates=161800, lr=7.8616e-05, gnorm=2.136, loss_scale=4, train_wall=11, gb_free=19.3, wall=101
2022-12-09 20:02:51 | INFO | train_inner | epoch 147:   1020 / 1102 loss=6.82, nll_loss=2.98, ppl=7.89, wps=31966.1, ups=9.22, wpb=3467.6, bsz=160.2, num_updates=161900, lr=7.85917e-05, gnorm=2.265, loss_scale=4, train_wall=11, gb_free=19.7, wall=112
2022-12-09 20:03:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:03:39 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 3.664 | nll_loss 2.123 | ppl 4.35 | bleu 37.27 | wps 4587.3 | wpb 2835.3 | bsz 115.6 | num_updates 161982 | best_bleu 37.43
2022-12-09 20:03:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 161982 updates
2022-12-09 20:03:40 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint147.pt
2022-12-09 20:03:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint147.pt (epoch 147 @ 161982 updates, score 37.27) (writing took 1.318563848733902 seconds)
2022-12-09 20:03:41 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-12-09 20:03:41 | INFO | train | epoch 147 | loss 6.839 | nll_loss 2.996 | ppl 7.98 | wps 24754.9 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 161982 | lr 7.85718e-05 | gnorm 2.18 | loss_scale 4 | train_wall 117 | gb_free 19.9 | wall 161
2022-12-09 20:03:41 | INFO | fairseq.trainer | begin training epoch 148
2022-12-09 20:03:43 | INFO | train_inner | epoch 148:     18 / 1102 loss=6.895, nll_loss=3.047, ppl=8.27, wps=7066.3, ups=1.93, wpb=3660.7, bsz=134.3, num_updates=162000, lr=7.85674e-05, gnorm=2.13, loss_scale=4, train_wall=11, gb_free=19.2, wall=163
2022-12-09 20:03:53 | INFO | train_inner | epoch 148:    118 / 1102 loss=6.743, nll_loss=2.916, ppl=7.55, wps=33730.7, ups=9.33, wpb=3614.6, bsz=158.1, num_updates=162100, lr=7.85432e-05, gnorm=2.137, loss_scale=4, train_wall=10, gb_free=19.3, wall=174
2022-12-09 20:04:04 | INFO | train_inner | epoch 148:    218 / 1102 loss=6.805, nll_loss=2.958, ppl=7.77, wps=33349.7, ups=9.29, wpb=3590.1, bsz=142.1, num_updates=162200, lr=7.8519e-05, gnorm=2.13, loss_scale=4, train_wall=11, gb_free=19.8, wall=185
2022-12-09 20:04:15 | INFO | train_inner | epoch 148:    318 / 1102 loss=6.802, nll_loss=2.967, ppl=7.82, wps=33043.7, ups=9.2, wpb=3593.1, bsz=154.3, num_updates=162300, lr=7.84948e-05, gnorm=2.107, loss_scale=4, train_wall=11, gb_free=20.1, wall=196
2022-12-09 20:04:26 | INFO | train_inner | epoch 148:    418 / 1102 loss=6.887, nll_loss=3.024, ppl=8.13, wps=32890.4, ups=9.23, wpb=3563.4, bsz=131.4, num_updates=162400, lr=7.84706e-05, gnorm=2.239, loss_scale=4, train_wall=11, gb_free=19.6, wall=207
2022-12-09 20:04:37 | INFO | train_inner | epoch 148:    518 / 1102 loss=6.822, nll_loss=2.985, ppl=7.92, wps=33501.3, ups=9.3, wpb=3601.4, bsz=149.2, num_updates=162500, lr=7.84465e-05, gnorm=2.189, loss_scale=4, train_wall=11, gb_free=19.5, wall=217
2022-12-09 20:04:47 | INFO | train_inner | epoch 148:    618 / 1102 loss=6.885, nll_loss=3.033, ppl=8.18, wps=33687.2, ups=9.32, wpb=3613.7, bsz=132.2, num_updates=162600, lr=7.84223e-05, gnorm=2.233, loss_scale=4, train_wall=10, gb_free=19.6, wall=228
2022-12-09 20:04:58 | INFO | train_inner | epoch 148:    718 / 1102 loss=6.825, nll_loss=2.971, ppl=7.84, wps=33273.5, ups=9.36, wpb=3554.4, bsz=141.2, num_updates=162700, lr=7.83982e-05, gnorm=2.227, loss_scale=4, train_wall=10, gb_free=19.8, wall=239
2022-12-09 20:05:09 | INFO | train_inner | epoch 148:    818 / 1102 loss=6.877, nll_loss=3.029, ppl=8.16, wps=33084.2, ups=9.23, wpb=3585.1, bsz=143.4, num_updates=162800, lr=7.83741e-05, gnorm=2.31, loss_scale=4, train_wall=11, gb_free=19.4, wall=250
2022-12-09 20:05:20 | INFO | train_inner | epoch 148:    918 / 1102 loss=6.797, nll_loss=2.948, ppl=7.72, wps=32398.2, ups=9.24, wpb=3505, bsz=153, num_updates=162900, lr=7.83501e-05, gnorm=2.263, loss_scale=4, train_wall=11, gb_free=19.5, wall=260
2022-12-09 20:05:31 | INFO | train_inner | epoch 148:   1018 / 1102 loss=6.918, nll_loss=3.077, ppl=8.44, wps=32973.5, ups=9.23, wpb=3573.1, bsz=140.9, num_updates=163000, lr=7.8326e-05, gnorm=2.168, loss_scale=4, train_wall=11, gb_free=19.3, wall=271
2022-12-09 20:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:06:18 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 3.665 | nll_loss 2.123 | ppl 4.36 | bleu 37.31 | wps 4768.9 | wpb 2835.3 | bsz 115.6 | num_updates 163084 | best_bleu 37.43
2022-12-09 20:06:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 163084 updates
2022-12-09 20:06:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint148.pt
2022-12-09 20:06:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint148.pt (epoch 148 @ 163084 updates, score 37.31) (writing took 1.1937393937259912 seconds)
2022-12-09 20:06:19 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-12-09 20:06:19 | INFO | train | epoch 148 | loss 6.838 | nll_loss 2.994 | ppl 7.96 | wps 24945.8 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 163084 | lr 7.83059e-05 | gnorm 2.2 | loss_scale 4 | train_wall 116 | gb_free 19.3 | wall 319
2022-12-09 20:06:19 | INFO | fairseq.trainer | begin training epoch 149
2022-12-09 20:06:21 | INFO | train_inner | epoch 149:     16 / 1102 loss=6.839, nll_loss=3.012, ppl=8.07, wps=7252.9, ups=1.99, wpb=3646.5, bsz=155.1, num_updates=163100, lr=7.8302e-05, gnorm=2.149, loss_scale=4, train_wall=11, gb_free=19.7, wall=321
2022-12-09 20:06:32 | INFO | train_inner | epoch 149:    116 / 1102 loss=6.796, nll_loss=2.948, ppl=7.72, wps=32901.2, ups=9.32, wpb=3531.5, bsz=141.4, num_updates=163200, lr=7.8278e-05, gnorm=2.215, loss_scale=4, train_wall=10, gb_free=19.6, wall=332
2022-12-09 20:06:42 | INFO | train_inner | epoch 149:    216 / 1102 loss=6.779, nll_loss=2.931, ppl=7.62, wps=33441.8, ups=9.33, wpb=3583.1, bsz=147.9, num_updates=163300, lr=7.82541e-05, gnorm=2.225, loss_scale=4, train_wall=10, gb_free=19.5, wall=343
2022-12-09 20:06:53 | INFO | train_inner | epoch 149:    316 / 1102 loss=6.827, nll_loss=2.98, ppl=7.89, wps=33353.4, ups=9.24, wpb=3610.1, bsz=140.3, num_updates=163400, lr=7.82301e-05, gnorm=2.169, loss_scale=4, train_wall=11, gb_free=19.5, wall=354
2022-12-09 20:07:04 | INFO | train_inner | epoch 149:    416 / 1102 loss=6.827, nll_loss=2.976, ppl=7.87, wps=32690.6, ups=9.26, wpb=3529.8, bsz=141.4, num_updates=163500, lr=7.82062e-05, gnorm=2.277, loss_scale=4, train_wall=11, gb_free=19.3, wall=365
2022-12-09 20:07:15 | INFO | train_inner | epoch 149:    516 / 1102 loss=6.74, nll_loss=2.916, ppl=7.55, wps=33264.6, ups=9.14, wpb=3641, bsz=164.2, num_updates=163600, lr=7.81823e-05, gnorm=2.084, loss_scale=4, train_wall=11, gb_free=19.4, wall=375
2022-12-09 20:07:26 | INFO | train_inner | epoch 149:    616 / 1102 loss=6.856, nll_loss=3.017, ppl=8.09, wps=33236.3, ups=9.17, wpb=3623.3, bsz=148.8, num_updates=163700, lr=7.81584e-05, gnorm=2.195, loss_scale=4, train_wall=11, gb_free=19.6, wall=386
2022-12-09 20:07:37 | INFO | train_inner | epoch 149:    716 / 1102 loss=6.86, nll_loss=3.022, ppl=8.13, wps=32922.3, ups=9.25, wpb=3559.3, bsz=149.5, num_updates=163800, lr=7.81345e-05, gnorm=2.196, loss_scale=4, train_wall=11, gb_free=19.4, wall=397
2022-12-09 20:07:47 | INFO | train_inner | epoch 149:    816 / 1102 loss=6.849, nll_loss=2.988, ppl=7.94, wps=32660.7, ups=9.24, wpb=3534.6, bsz=139.5, num_updates=163900, lr=7.81107e-05, gnorm=2.284, loss_scale=4, train_wall=11, gb_free=19.5, wall=408
2022-12-09 20:07:58 | INFO | train_inner | epoch 149:    916 / 1102 loss=6.872, nll_loss=3.023, ppl=8.13, wps=33105, ups=9.23, wpb=3586.2, bsz=143.9, num_updates=164000, lr=7.80869e-05, gnorm=2.196, loss_scale=4, train_wall=11, gb_free=19.5, wall=419
2022-12-09 20:08:09 | INFO | train_inner | epoch 149:   1016 / 1102 loss=6.856, nll_loss=3.023, ppl=8.13, wps=33375.5, ups=9.21, wpb=3623.2, bsz=146.2, num_updates=164100, lr=7.80631e-05, gnorm=2.179, loss_scale=4, train_wall=11, gb_free=19.3, wall=430
2022-12-09 20:08:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:08:57 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 3.661 | nll_loss 2.119 | ppl 4.35 | bleu 37.42 | wps 4649.7 | wpb 2835.3 | bsz 115.6 | num_updates 164186 | best_bleu 37.43
2022-12-09 20:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 164186 updates
2022-12-09 20:08:58 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint149.pt
2022-12-09 20:08:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint149.pt (epoch 149 @ 164186 updates, score 37.42) (writing took 1.1736415531486273 seconds)
2022-12-09 20:08:58 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-12-09 20:08:58 | INFO | train | epoch 149 | loss 6.834 | nll_loss 2.99 | ppl 7.94 | wps 24754 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 164186 | lr 7.80426e-05 | gnorm 2.2 | loss_scale 4 | train_wall 117 | gb_free 19.8 | wall 479
2022-12-09 20:08:58 | INFO | fairseq.trainer | begin training epoch 150
2022-12-09 20:09:00 | INFO | train_inner | epoch 150:     14 / 1102 loss=6.917, nll_loss=3.07, ppl=8.4, wps=7008.4, ups=1.96, wpb=3578, bsz=135.1, num_updates=164200, lr=7.80393e-05, gnorm=2.185, loss_scale=4, train_wall=11, gb_free=19.6, wall=481
2022-12-09 20:09:11 | INFO | train_inner | epoch 150:    114 / 1102 loss=6.789, nll_loss=2.939, ppl=7.67, wps=32845.4, ups=9.36, wpb=3510.3, bsz=144, num_updates=164300, lr=7.80156e-05, gnorm=2.209, loss_scale=4, train_wall=10, gb_free=19.7, wall=491
2022-12-09 20:09:22 | INFO | train_inner | epoch 150:    214 / 1102 loss=6.776, nll_loss=2.941, ppl=7.68, wps=33883.7, ups=9.23, wpb=3669.2, bsz=153, num_updates=164400, lr=7.79918e-05, gnorm=2.261, loss_scale=4, train_wall=11, gb_free=19.5, wall=502
2022-12-09 20:09:32 | INFO | train_inner | epoch 150:    314 / 1102 loss=6.785, nll_loss=2.935, ppl=7.65, wps=33541, ups=9.28, wpb=3614.6, bsz=145, num_updates=164500, lr=7.79681e-05, gnorm=2.163, loss_scale=4, train_wall=11, gb_free=19.8, wall=513
2022-12-09 20:09:43 | INFO | train_inner | epoch 150:    414 / 1102 loss=6.825, nll_loss=2.981, ppl=7.89, wps=33100.2, ups=9.37, wpb=3531.6, bsz=146.7, num_updates=164600, lr=7.79444e-05, gnorm=2.213, loss_scale=4, train_wall=10, gb_free=19.4, wall=524
2022-12-09 20:09:54 | INFO | train_inner | epoch 150:    514 / 1102 loss=6.869, nll_loss=3.028, ppl=8.16, wps=33255.7, ups=9.15, wpb=3636.1, bsz=143.5, num_updates=164700, lr=7.79208e-05, gnorm=2.202, loss_scale=4, train_wall=11, gb_free=19.6, wall=535
2022-12-09 20:10:05 | INFO | train_inner | epoch 150:    614 / 1102 loss=6.818, nll_loss=2.97, ppl=7.84, wps=32576.5, ups=9.18, wpb=3549.2, bsz=144.7, num_updates=164800, lr=7.78971e-05, gnorm=2.293, loss_scale=4, train_wall=11, gb_free=19.6, wall=546
2022-12-09 20:10:16 | INFO | train_inner | epoch 150:    714 / 1102 loss=6.845, nll_loss=3.004, ppl=8.02, wps=32726.8, ups=9.13, wpb=3586.1, bsz=149, num_updates=164900, lr=7.78735e-05, gnorm=2.172, loss_scale=4, train_wall=11, gb_free=19.6, wall=557
2022-12-09 20:10:27 | INFO | train_inner | epoch 150:    814 / 1102 loss=6.878, nll_loss=3.037, ppl=8.21, wps=32823.9, ups=9.16, wpb=3582.7, bsz=152.2, num_updates=165000, lr=7.78499e-05, gnorm=2.173, loss_scale=4, train_wall=11, gb_free=19.6, wall=567
2022-12-09 20:10:38 | INFO | train_inner | epoch 150:    914 / 1102 loss=6.798, nll_loss=2.964, ppl=7.8, wps=33028.6, ups=9.06, wpb=3647, bsz=149.8, num_updates=165100, lr=7.78263e-05, gnorm=2.108, loss_scale=4, train_wall=11, gb_free=19.5, wall=578
2022-12-09 20:10:49 | INFO | train_inner | epoch 150:   1014 / 1102 loss=6.871, nll_loss=3.026, ppl=8.15, wps=32932.3, ups=9.19, wpb=3584.1, bsz=141.3, num_updates=165200, lr=7.78028e-05, gnorm=2.176, loss_scale=4, train_wall=11, gb_free=19.4, wall=589
2022-12-09 20:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:11:38 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 3.669 | nll_loss 2.125 | ppl 4.36 | bleu 37.23 | wps 4576.3 | wpb 2835.3 | bsz 115.6 | num_updates 165288 | best_bleu 37.43
2022-12-09 20:11:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 165288 updates
2022-12-09 20:11:38 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint150.pt
2022-12-09 20:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint150.pt (epoch 150 @ 165288 updates, score 37.23) (writing took 1.1945426845923066 seconds)
2022-12-09 20:11:39 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-12-09 20:11:39 | INFO | train | epoch 150 | loss 6.832 | nll_loss 2.987 | ppl 7.93 | wps 24605 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 165288 | lr 7.7782e-05 | gnorm 2.205 | loss_scale 4 | train_wall 117 | gb_free 19.7 | wall 639
2022-12-09 20:11:39 | INFO | fairseq.trainer | begin training epoch 151
2022-12-09 20:11:41 | INFO | train_inner | epoch 151:     12 / 1102 loss=6.897, nll_loss=3.03, ppl=8.17, wps=6800.2, ups=1.93, wpb=3518.4, bsz=131, num_updates=165300, lr=7.77792e-05, gnorm=2.282, loss_scale=4, train_wall=11, gb_free=19.8, wall=641
2022-12-09 20:11:51 | INFO | train_inner | epoch 151:    112 / 1102 loss=6.756, nll_loss=2.922, ppl=7.58, wps=32985.6, ups=9.15, wpb=3604.2, bsz=155.3, num_updates=165400, lr=7.77557e-05, gnorm=2.152, loss_scale=4, train_wall=11, gb_free=19.5, wall=652
2022-12-09 20:12:02 | INFO | train_inner | epoch 151:    212 / 1102 loss=6.772, nll_loss=2.917, ppl=7.55, wps=32579.9, ups=9.18, wpb=3550, bsz=135.8, num_updates=165500, lr=7.77322e-05, gnorm=2.198, loss_scale=4, train_wall=11, gb_free=19.5, wall=663
2022-12-09 20:12:13 | INFO | train_inner | epoch 151:    312 / 1102 loss=6.721, nll_loss=2.892, ppl=7.42, wps=32741.2, ups=9.15, wpb=3578.1, bsz=166.1, num_updates=165600, lr=7.77087e-05, gnorm=2.103, loss_scale=4, train_wall=11, gb_free=19.5, wall=674
2022-12-09 20:12:24 | INFO | train_inner | epoch 151:    412 / 1102 loss=6.821, nll_loss=2.972, ppl=7.85, wps=32742.3, ups=9.03, wpb=3626.5, bsz=144.1, num_updates=165700, lr=7.76853e-05, gnorm=2.173, loss_scale=4, train_wall=11, gb_free=19.3, wall=685
2022-12-09 20:12:35 | INFO | train_inner | epoch 151:    512 / 1102 loss=6.884, nll_loss=3.026, ppl=8.15, wps=32934.1, ups=9.1, wpb=3618.2, bsz=131, num_updates=165800, lr=7.76619e-05, gnorm=2.305, loss_scale=4, train_wall=11, gb_free=19.3, wall=696
2022-12-09 20:12:46 | INFO | train_inner | epoch 151:    612 / 1102 loss=6.863, nll_loss=3.002, ppl=8.01, wps=32403.2, ups=9.2, wpb=3523.3, bsz=129.8, num_updates=165900, lr=7.76384e-05, gnorm=2.268, loss_scale=4, train_wall=11, gb_free=19.6, wall=707
2022-12-09 20:12:57 | INFO | train_inner | epoch 151:    712 / 1102 loss=6.923, nll_loss=3.071, ppl=8.4, wps=32663.3, ups=9.17, wpb=3562.5, bsz=137.7, num_updates=166000, lr=7.76151e-05, gnorm=2.221, loss_scale=4, train_wall=11, gb_free=19.4, wall=718
2022-12-09 20:13:08 | INFO | train_inner | epoch 151:    812 / 1102 loss=6.786, nll_loss=2.958, ppl=7.77, wps=32410.7, ups=9.04, wpb=3586.5, bsz=158.2, num_updates=166100, lr=7.75917e-05, gnorm=2.084, loss_scale=4, train_wall=11, gb_free=19.5, wall=729
2022-12-09 20:13:19 | INFO | train_inner | epoch 151:    912 / 1102 loss=6.813, nll_loss=2.971, ppl=7.84, wps=32479.5, ups=9.13, wpb=3559.1, bsz=152.6, num_updates=166200, lr=7.75683e-05, gnorm=2.199, loss_scale=4, train_wall=11, gb_free=19.3, wall=740
2022-12-09 20:13:30 | INFO | train_inner | epoch 151:   1012 / 1102 loss=6.88, nll_loss=3.037, ppl=8.21, wps=32680.2, ups=9.12, wpb=3583.4, bsz=146.6, num_updates=166300, lr=7.7545e-05, gnorm=2.261, loss_scale=4, train_wall=11, gb_free=19.4, wall=751
2022-12-09 20:13:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:14:20 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 3.659 | nll_loss 2.121 | ppl 4.35 | bleu 37.48 | wps 4486 | wpb 2835.3 | bsz 115.6 | num_updates 166390 | best_bleu 37.48
2022-12-09 20:14:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 166390 updates
2022-12-09 20:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint151.pt
2022-12-09 20:14:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint151.pt (epoch 151 @ 166390 updates, score 37.48) (writing took 1.5903454134240746 seconds)
2022-12-09 20:14:22 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-12-09 20:14:22 | INFO | train | epoch 151 | loss 6.827 | nll_loss 2.983 | ppl 7.9 | wps 24231.9 | ups 6.76 | wpb 3583.6 | bsz 145.4 | num_updates 166390 | lr 7.7524e-05 | gnorm 2.191 | loss_scale 4 | train_wall 118 | gb_free 19.9 | wall 802
2022-12-09 20:14:22 | INFO | fairseq.trainer | begin training epoch 152
2022-12-09 20:14:23 | INFO | train_inner | epoch 152:     10 / 1102 loss=6.873, nll_loss=3.037, ppl=8.21, wps=6831.6, ups=1.88, wpb=3632, bsz=145.9, num_updates=166400, lr=7.75217e-05, gnorm=2.158, loss_scale=4, train_wall=11, gb_free=19.7, wall=804
2022-12-09 20:14:34 | INFO | train_inner | epoch 152:    110 / 1102 loss=6.775, nll_loss=2.921, ppl=7.57, wps=32804.5, ups=9.15, wpb=3583.3, bsz=144.2, num_updates=166500, lr=7.74984e-05, gnorm=2.147, loss_scale=4, train_wall=11, gb_free=19.3, wall=815
2022-12-09 20:14:45 | INFO | train_inner | epoch 152:    210 / 1102 loss=6.799, nll_loss=2.97, ppl=7.84, wps=33233, ups=9.1, wpb=3650, bsz=152.9, num_updates=166600, lr=7.74752e-05, gnorm=2.069, loss_scale=4, train_wall=11, gb_free=19.4, wall=826
2022-12-09 20:14:56 | INFO | train_inner | epoch 152:    310 / 1102 loss=6.8, nll_loss=2.939, ppl=7.67, wps=32443.8, ups=9.16, wpb=3542, bsz=137.7, num_updates=166700, lr=7.74519e-05, gnorm=2.157, loss_scale=4, train_wall=11, gb_free=19.6, wall=837
2022-12-09 20:15:07 | INFO | train_inner | epoch 152:    410 / 1102 loss=6.807, nll_loss=2.958, ppl=7.77, wps=32654, ups=9.12, wpb=3578.9, bsz=143.7, num_updates=166800, lr=7.74287e-05, gnorm=2.184, loss_scale=4, train_wall=11, gb_free=19.6, wall=848
2022-12-09 20:15:18 | INFO | train_inner | epoch 152:    510 / 1102 loss=6.827, nll_loss=2.99, ppl=7.95, wps=32747.4, ups=9.08, wpb=3608.1, bsz=148.2, num_updates=166900, lr=7.74055e-05, gnorm=2.126, loss_scale=4, train_wall=11, gb_free=19.4, wall=859
2022-12-09 20:15:29 | INFO | train_inner | epoch 152:    610 / 1102 loss=6.752, nll_loss=2.927, ppl=7.6, wps=32568.8, ups=9.06, wpb=3594.6, bsz=161.8, num_updates=167000, lr=7.73823e-05, gnorm=2.183, loss_scale=4, train_wall=11, gb_free=19.7, wall=870
2022-12-09 20:15:40 | INFO | train_inner | epoch 152:    710 / 1102 loss=6.842, nll_loss=2.986, ppl=7.92, wps=32370, ups=9.06, wpb=3574.8, bsz=141.2, num_updates=167100, lr=7.73592e-05, gnorm=2.292, loss_scale=4, train_wall=11, gb_free=19.6, wall=881
2022-12-09 20:15:51 | INFO | train_inner | epoch 152:    810 / 1102 loss=6.874, nll_loss=3.007, ppl=8.04, wps=32252.9, ups=9.28, wpb=3475.3, bsz=137.9, num_updates=167200, lr=7.7336e-05, gnorm=2.364, loss_scale=4, train_wall=11, gb_free=19.8, wall=892
2022-12-09 20:16:02 | INFO | train_inner | epoch 152:    910 / 1102 loss=6.873, nll_loss=3.026, ppl=8.15, wps=32534.7, ups=9.16, wpb=3551.7, bsz=144.3, num_updates=167300, lr=7.73129e-05, gnorm=2.223, loss_scale=4, train_wall=11, gb_free=20, wall=902
2022-12-09 20:16:13 | INFO | train_inner | epoch 152:   1010 / 1102 loss=6.881, nll_loss=3.03, ppl=8.17, wps=33024.5, ups=9.18, wpb=3597.3, bsz=137.4, num_updates=167400, lr=7.72898e-05, gnorm=2.201, loss_scale=4, train_wall=11, gb_free=19.3, wall=913
2022-12-09 20:16:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:17:03 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.35 | wps 4560.3 | wpb 2835.3 | bsz 115.6 | num_updates 167492 | best_bleu 37.48
2022-12-09 20:17:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 167492 updates
2022-12-09 20:17:03 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint152.pt
2022-12-09 20:17:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint152.pt (epoch 152 @ 167492 updates, score 37.35) (writing took 1.316522273235023 seconds)
2022-12-09 20:17:04 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-12-09 20:17:04 | INFO | train | epoch 152 | loss 6.826 | nll_loss 2.98 | ppl 7.89 | wps 24369.5 | ups 6.8 | wpb 3583.6 | bsz 145.4 | num_updates 167492 | lr 7.72686e-05 | gnorm 2.195 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 964
2022-12-09 20:17:04 | INFO | fairseq.trainer | begin training epoch 153
2022-12-09 20:17:05 | INFO | train_inner | epoch 153:      8 / 1102 loss=6.853, nll_loss=3.02, ppl=8.11, wps=6977.9, ups=1.91, wpb=3650.6, bsz=148.3, num_updates=167500, lr=7.72667e-05, gnorm=2.189, loss_scale=4, train_wall=11, gb_free=19.6, wall=966
2022-12-09 20:17:16 | INFO | train_inner | epoch 153:    108 / 1102 loss=6.881, nll_loss=3.039, ppl=8.22, wps=33301.2, ups=9.11, wpb=3656.7, bsz=135, num_updates=167600, lr=7.72437e-05, gnorm=2.155, loss_scale=4, train_wall=11, gb_free=19.5, wall=977
2022-12-09 20:17:27 | INFO | train_inner | epoch 153:    208 / 1102 loss=6.789, nll_loss=2.946, ppl=7.71, wps=32670.2, ups=9.15, wpb=3568.7, bsz=150.2, num_updates=167700, lr=7.72207e-05, gnorm=2.258, loss_scale=4, train_wall=11, gb_free=19.5, wall=988
2022-12-09 20:17:38 | INFO | train_inner | epoch 153:    308 / 1102 loss=6.779, nll_loss=2.939, ppl=7.67, wps=32750.1, ups=9.11, wpb=3595.9, bsz=152.1, num_updates=167800, lr=7.71976e-05, gnorm=2.158, loss_scale=4, train_wall=11, gb_free=19.4, wall=999
2022-12-09 20:17:49 | INFO | train_inner | epoch 153:    408 / 1102 loss=6.797, nll_loss=2.947, ppl=7.71, wps=31940.6, ups=9.14, wpb=3495.1, bsz=146.2, num_updates=167900, lr=7.71746e-05, gnorm=2.209, loss_scale=4, train_wall=11, gb_free=19.5, wall=1009
2022-12-09 20:18:00 | INFO | train_inner | epoch 153:    508 / 1102 loss=6.807, nll_loss=2.959, ppl=7.77, wps=33167.4, ups=9.16, wpb=3620.9, bsz=149, num_updates=168000, lr=7.71517e-05, gnorm=2.166, loss_scale=4, train_wall=11, gb_free=19.5, wall=1020
2022-12-09 20:18:11 | INFO | train_inner | epoch 153:    608 / 1102 loss=6.853, nll_loss=2.999, ppl=7.99, wps=32776.5, ups=9.17, wpb=3572.4, bsz=140.6, num_updates=168100, lr=7.71287e-05, gnorm=2.299, loss_scale=4, train_wall=11, gb_free=19.6, wall=1031
2022-12-09 20:18:22 | INFO | train_inner | epoch 153:    708 / 1102 loss=6.759, nll_loss=2.921, ppl=7.58, wps=32813.3, ups=9.16, wpb=3580.4, bsz=158.6, num_updates=168200, lr=7.71058e-05, gnorm=2.171, loss_scale=4, train_wall=11, gb_free=19.8, wall=1042
2022-12-09 20:18:33 | INFO | train_inner | epoch 153:    808 / 1102 loss=6.784, nll_loss=2.951, ppl=7.73, wps=33268.8, ups=9.13, wpb=3642.1, bsz=154.6, num_updates=168300, lr=7.70829e-05, gnorm=2.166, loss_scale=4, train_wall=11, gb_free=19.6, wall=1053
2022-12-09 20:18:44 | INFO | train_inner | epoch 153:    908 / 1102 loss=6.881, nll_loss=3.039, ppl=8.22, wps=33010.5, ups=9.07, wpb=3638.1, bsz=142.4, num_updates=168400, lr=7.706e-05, gnorm=2.171, loss_scale=4, train_wall=11, gb_free=19.3, wall=1064
2022-12-09 20:18:54 | INFO | train_inner | epoch 153:   1008 / 1102 loss=6.879, nll_loss=3.01, ppl=8.05, wps=31917.5, ups=9.21, wpb=3466.1, bsz=128, num_updates=168500, lr=7.70371e-05, gnorm=2.446, loss_scale=4, train_wall=11, gb_free=20.1, wall=1075
2022-12-09 20:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:19:43 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 3.672 | nll_loss 2.125 | ppl 4.36 | bleu 37.4 | wps 4738.5 | wpb 2835.3 | bsz 115.6 | num_updates 168594 | best_bleu 37.48
2022-12-09 20:19:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 168594 updates
2022-12-09 20:19:44 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint153.pt
2022-12-09 20:19:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint153.pt (epoch 153 @ 168594 updates, score 37.4) (writing took 1.217201380059123 seconds)
2022-12-09 20:19:44 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-12-09 20:19:44 | INFO | train | epoch 153 | loss 6.823 | nll_loss 2.977 | ppl 7.87 | wps 24633.4 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 168594 | lr 7.70156e-05 | gnorm 2.22 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 1125
2022-12-09 20:19:44 | INFO | fairseq.trainer | begin training epoch 154
2022-12-09 20:19:45 | INFO | train_inner | epoch 154:      6 / 1102 loss=6.834, nll_loss=2.988, ppl=7.94, wps=7076.2, ups=1.97, wpb=3594.9, bsz=143.8, num_updates=168600, lr=7.70143e-05, gnorm=2.209, loss_scale=4, train_wall=11, gb_free=19.5, wall=1126
2022-12-09 20:19:56 | INFO | train_inner | epoch 154:    106 / 1102 loss=6.775, nll_loss=2.935, ppl=7.65, wps=32679.4, ups=9.05, wpb=3612, bsz=150.9, num_updates=168700, lr=7.69914e-05, gnorm=2.154, loss_scale=4, train_wall=11, gb_free=19.4, wall=1137
2022-12-09 20:20:07 | INFO | train_inner | epoch 154:    206 / 1102 loss=6.794, nll_loss=2.937, ppl=7.66, wps=32692.8, ups=9.2, wpb=3552.6, bsz=136.8, num_updates=168800, lr=7.69686e-05, gnorm=2.182, loss_scale=4, train_wall=11, gb_free=19.5, wall=1148
2022-12-09 20:20:18 | INFO | train_inner | epoch 154:    306 / 1102 loss=6.81, nll_loss=2.957, ppl=7.76, wps=32560.7, ups=9.19, wpb=3544.2, bsz=139.5, num_updates=168900, lr=7.69458e-05, gnorm=2.239, loss_scale=4, train_wall=11, gb_free=19.5, wall=1159
2022-12-09 20:20:29 | INFO | train_inner | epoch 154:    406 / 1102 loss=6.827, nll_loss=2.969, ppl=7.83, wps=32673.7, ups=9.16, wpb=3567.7, bsz=134.6, num_updates=169000, lr=7.69231e-05, gnorm=2.231, loss_scale=4, train_wall=11, gb_free=19.8, wall=1170
2022-12-09 20:20:40 | INFO | train_inner | epoch 154:    506 / 1102 loss=6.794, nll_loss=2.946, ppl=7.7, wps=33319.9, ups=9.17, wpb=3634.7, bsz=146.8, num_updates=169100, lr=7.69003e-05, gnorm=2.157, loss_scale=4, train_wall=11, gb_free=19.4, wall=1180
2022-12-09 20:20:51 | INFO | train_inner | epoch 154:    606 / 1102 loss=6.783, nll_loss=2.958, ppl=7.77, wps=32856.4, ups=9.25, wpb=3551.2, bsz=164.4, num_updates=169200, lr=7.68776e-05, gnorm=2.153, loss_scale=4, train_wall=11, gb_free=19.4, wall=1191
2022-12-09 20:21:02 | INFO | train_inner | epoch 154:    706 / 1102 loss=6.823, nll_loss=2.976, ppl=7.87, wps=32995.4, ups=9.24, wpb=3572, bsz=147.2, num_updates=169300, lr=7.68549e-05, gnorm=2.174, loss_scale=4, train_wall=11, gb_free=19.7, wall=1202
2022-12-09 20:21:12 | INFO | train_inner | epoch 154:    806 / 1102 loss=6.905, nll_loss=3.057, ppl=8.32, wps=33559.7, ups=9.22, wpb=3641.1, bsz=136.4, num_updates=169400, lr=7.68322e-05, gnorm=2.199, loss_scale=4, train_wall=11, gb_free=19.8, wall=1213
2022-12-09 20:21:23 | INFO | train_inner | epoch 154:    906 / 1102 loss=6.841, nll_loss=2.982, ppl=7.9, wps=32593.4, ups=9.23, wpb=3529.6, bsz=143, num_updates=169500, lr=7.68095e-05, gnorm=2.321, loss_scale=4, train_wall=11, gb_free=19.6, wall=1224
2022-12-09 20:21:34 | INFO | train_inner | epoch 154:   1006 / 1102 loss=6.826, nll_loss=2.976, ppl=7.87, wps=32462.4, ups=9.19, wpb=3531.8, bsz=144.6, num_updates=169600, lr=7.67869e-05, gnorm=2.24, loss_scale=4, train_wall=11, gb_free=19.6, wall=1235
2022-12-09 20:21:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:22:23 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 3.666 | nll_loss 2.122 | ppl 4.35 | bleu 37.22 | wps 4648.5 | wpb 2835.3 | bsz 115.6 | num_updates 169696 | best_bleu 37.48
2022-12-09 20:22:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 169696 updates
2022-12-09 20:22:24 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint154.pt
2022-12-09 20:22:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint154.pt (epoch 154 @ 169696 updates, score 37.22) (writing took 1.1508948244154453 seconds)
2022-12-09 20:22:25 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-12-09 20:22:25 | INFO | train | epoch 154 | loss 6.821 | nll_loss 2.974 | ppl 7.85 | wps 24622.1 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 169696 | lr 7.67652e-05 | gnorm 2.198 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 1285
2022-12-09 20:22:25 | INFO | fairseq.trainer | begin training epoch 155
2022-12-09 20:22:25 | INFO | train_inner | epoch 155:      4 / 1102 loss=6.843, nll_loss=3.015, ppl=8.08, wps=7165.7, ups=1.95, wpb=3678.9, bsz=159, num_updates=169700, lr=7.67643e-05, gnorm=2.126, loss_scale=4, train_wall=11, gb_free=19.5, wall=1286
2022-12-09 20:22:36 | INFO | train_inner | epoch 155:    104 / 1102 loss=6.762, nll_loss=2.932, ppl=7.63, wps=33427, ups=9.18, wpb=3643.2, bsz=153.1, num_updates=169800, lr=7.67417e-05, gnorm=2.097, loss_scale=4, train_wall=11, gb_free=19.6, wall=1297
2022-12-09 20:22:47 | INFO | train_inner | epoch 155:    204 / 1102 loss=6.761, nll_loss=2.906, ppl=7.49, wps=32439.3, ups=9.17, wpb=3539.3, bsz=145, num_updates=169900, lr=7.67191e-05, gnorm=2.239, loss_scale=4, train_wall=11, gb_free=19.5, wall=1308
2022-12-09 20:22:58 | INFO | train_inner | epoch 155:    304 / 1102 loss=6.774, nll_loss=2.93, ppl=7.62, wps=32657.5, ups=9.15, wpb=3568.7, bsz=148.8, num_updates=170000, lr=7.66965e-05, gnorm=2.184, loss_scale=4, train_wall=11, gb_free=19.8, wall=1319
2022-12-09 20:23:09 | INFO | train_inner | epoch 155:    404 / 1102 loss=6.881, nll_loss=3.01, ppl=8.06, wps=32507.9, ups=9.16, wpb=3549.2, bsz=129.4, num_updates=170100, lr=7.6674e-05, gnorm=2.268, loss_scale=4, train_wall=11, gb_free=19.5, wall=1330
2022-12-09 20:23:20 | INFO | train_inner | epoch 155:    504 / 1102 loss=6.857, nll_loss=3.008, ppl=8.04, wps=32476.1, ups=9.12, wpb=3560.9, bsz=140.6, num_updates=170200, lr=7.66514e-05, gnorm=2.204, loss_scale=4, train_wall=11, gb_free=19.3, wall=1341
2022-12-09 20:23:31 | INFO | train_inner | epoch 155:    604 / 1102 loss=6.766, nll_loss=2.913, ppl=7.53, wps=32423.1, ups=9.13, wpb=3553, bsz=146.9, num_updates=170300, lr=7.66289e-05, gnorm=2.168, loss_scale=4, train_wall=11, gb_free=19.9, wall=1352
2022-12-09 20:23:42 | INFO | train_inner | epoch 155:    704 / 1102 loss=6.828, nll_loss=2.975, ppl=7.86, wps=32779.4, ups=9.1, wpb=3602.5, bsz=144.2, num_updates=170400, lr=7.66064e-05, gnorm=2.222, loss_scale=4, train_wall=11, gb_free=19.4, wall=1363
2022-12-09 20:23:53 | INFO | train_inner | epoch 155:    804 / 1102 loss=6.784, nll_loss=2.939, ppl=7.67, wps=32164.1, ups=9.13, wpb=3521, bsz=155.6, num_updates=170500, lr=7.6584e-05, gnorm=2.329, loss_scale=4, train_wall=11, gb_free=19.3, wall=1374
2022-12-09 20:24:04 | INFO | train_inner | epoch 155:    904 / 1102 loss=6.867, nll_loss=3.029, ppl=8.16, wps=32994.9, ups=9, wpb=3666.6, bsz=147.4, num_updates=170600, lr=7.65615e-05, gnorm=2.202, loss_scale=4, train_wall=11, gb_free=19.4, wall=1385
2022-12-09 20:24:15 | INFO | train_inner | epoch 155:   1004 / 1102 loss=6.854, nll_loss=3.003, ppl=8.02, wps=32932, ups=9.08, wpb=3626.9, bsz=139.4, num_updates=170700, lr=7.65391e-05, gnorm=2.275, loss_scale=4, train_wall=11, gb_free=19.3, wall=1396
2022-12-09 20:24:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:25:02 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 3.665 | nll_loss 2.121 | ppl 4.35 | bleu 37.34 | wps 4973.5 | wpb 2835.3 | bsz 115.6 | num_updates 170798 | best_bleu 37.48
2022-12-09 20:25:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 170798 updates
2022-12-09 20:25:03 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint155.pt
2022-12-09 20:25:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint155.pt (epoch 155 @ 170798 updates, score 37.34) (writing took 1.1661115176975727 seconds)
2022-12-09 20:25:03 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-12-09 20:25:03 | INFO | train | epoch 155 | loss 6.818 | nll_loss 2.971 | ppl 7.84 | wps 24874.3 | ups 6.94 | wpb 3583.6 | bsz 145.4 | num_updates 170798 | lr 7.65171e-05 | gnorm 2.221 | loss_scale 4 | train_wall 118 | gb_free 19.7 | wall 1444
2022-12-09 20:25:03 | INFO | fairseq.trainer | begin training epoch 156
2022-12-09 20:25:04 | INFO | train_inner | epoch 156:      2 / 1102 loss=6.87, nll_loss=3.035, ppl=8.2, wps=7323.2, ups=2.05, wpb=3575.8, bsz=145, num_updates=170800, lr=7.65167e-05, gnorm=2.263, loss_scale=4, train_wall=11, gb_free=19.4, wall=1444
2022-12-09 20:25:15 | INFO | train_inner | epoch 156:    102 / 1102 loss=6.815, nll_loss=2.953, ppl=7.74, wps=32412.7, ups=9.16, wpb=3539.6, bsz=124.2, num_updates=170900, lr=7.64943e-05, gnorm=2.189, loss_scale=4, train_wall=11, gb_free=19.5, wall=1455
2022-12-09 20:25:26 | INFO | train_inner | epoch 156:    202 / 1102 loss=6.824, nll_loss=2.981, ppl=7.9, wps=32589.8, ups=9.09, wpb=3586.3, bsz=143.3, num_updates=171000, lr=7.64719e-05, gnorm=2.184, loss_scale=4, train_wall=11, gb_free=19.8, wall=1466
2022-12-09 20:25:37 | INFO | train_inner | epoch 156:    302 / 1102 loss=6.797, nll_loss=2.955, ppl=7.76, wps=32875.5, ups=9.13, wpb=3599.8, bsz=151.9, num_updates=171100, lr=7.64496e-05, gnorm=2.213, loss_scale=4, train_wall=11, gb_free=19.4, wall=1477
2022-12-09 20:25:48 | INFO | train_inner | epoch 156:    402 / 1102 loss=6.786, nll_loss=2.922, ppl=7.58, wps=32820.3, ups=9.16, wpb=3581.7, bsz=139.2, num_updates=171200, lr=7.64272e-05, gnorm=2.369, loss_scale=4, train_wall=11, gb_free=19.7, wall=1488
2022-12-09 20:25:59 | INFO | train_inner | epoch 156:    502 / 1102 loss=6.757, nll_loss=2.92, ppl=7.57, wps=33038.4, ups=9.05, wpb=3652, bsz=157.7, num_updates=171300, lr=7.64049e-05, gnorm=2.224, loss_scale=4, train_wall=11, gb_free=19.6, wall=1499
2022-12-09 20:26:10 | INFO | train_inner | epoch 156:    602 / 1102 loss=6.833, nll_loss=2.98, ppl=7.89, wps=32573.3, ups=9.19, wpb=3544.3, bsz=139.4, num_updates=171400, lr=7.63826e-05, gnorm=2.241, loss_scale=4, train_wall=11, gb_free=19.6, wall=1510
2022-12-09 20:26:21 | INFO | train_inner | epoch 156:    702 / 1102 loss=6.849, nll_loss=2.997, ppl=7.98, wps=32965, ups=9.14, wpb=3605.4, bsz=144.8, num_updates=171500, lr=7.63604e-05, gnorm=2.329, loss_scale=4, train_wall=11, gb_free=19.6, wall=1521
2022-12-09 20:26:31 | INFO | train_inner | epoch 156:    802 / 1102 loss=6.809, nll_loss=2.963, ppl=7.8, wps=33115.7, ups=9.15, wpb=3620.2, bsz=150.3, num_updates=171600, lr=7.63381e-05, gnorm=2.221, loss_scale=4, train_wall=11, gb_free=19.4, wall=1532
2022-12-09 20:26:42 | INFO | train_inner | epoch 156:    902 / 1102 loss=6.767, nll_loss=2.936, ppl=7.65, wps=32769.9, ups=9.11, wpb=3597.2, bsz=157.6, num_updates=171700, lr=7.63159e-05, gnorm=2.162, loss_scale=4, train_wall=11, gb_free=19.3, wall=1543
2022-12-09 20:26:53 | INFO | train_inner | epoch 156:   1002 / 1102 loss=6.881, nll_loss=3.028, ppl=8.16, wps=32859.2, ups=9.17, wpb=3584.5, bsz=133, num_updates=171800, lr=7.62937e-05, gnorm=2.267, loss_scale=4, train_wall=11, gb_free=19.4, wall=1554
2022-12-09 20:27:04 | INFO | train_inner | epoch 156:   1102 / 1102 loss=6.834, nll_loss=3, ppl=8, wps=32349.4, ups=9.21, wpb=3511.9, bsz=159.4, num_updates=171900, lr=7.62715e-05, gnorm=2.242, loss_scale=4, train_wall=11, gb_free=19.7, wall=1565
2022-12-09 20:27:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:27:43 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 3.67 | nll_loss 2.124 | ppl 4.36 | bleu 37.21 | wps 4692.1 | wpb 2835.3 | bsz 115.6 | num_updates 171900 | best_bleu 37.48
2022-12-09 20:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 171900 updates
2022-12-09 20:27:44 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint156.pt
2022-12-09 20:27:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint156.pt (epoch 156 @ 171900 updates, score 37.21) (writing took 1.1736547369509935 seconds)
2022-12-09 20:27:44 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-12-09 20:27:44 | INFO | train | epoch 156 | loss 6.814 | nll_loss 2.967 | ppl 7.82 | wps 24573.9 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 171900 | lr 7.62715e-05 | gnorm 2.241 | loss_scale 4 | train_wall 118 | gb_free 19.7 | wall 1605
2022-12-09 20:27:44 | INFO | fairseq.trainer | begin training epoch 157
2022-12-09 20:27:55 | INFO | train_inner | epoch 157:    100 / 1102 loss=6.758, nll_loss=2.912, ppl=7.53, wps=7124.5, ups=1.96, wpb=3628.6, bsz=143.8, num_updates=172000, lr=7.62493e-05, gnorm=2.139, loss_scale=4, train_wall=11, gb_free=19.3, wall=1616
2022-12-09 20:28:06 | INFO | train_inner | epoch 157:    200 / 1102 loss=6.788, nll_loss=2.923, ppl=7.59, wps=32805.6, ups=9.29, wpb=3531.4, bsz=135.8, num_updates=172100, lr=7.62271e-05, gnorm=2.234, loss_scale=4, train_wall=11, gb_free=19.5, wall=1627
2022-12-09 20:28:17 | INFO | train_inner | epoch 157:    300 / 1102 loss=6.874, nll_loss=3.012, ppl=8.07, wps=32746.7, ups=9.23, wpb=3549.4, bsz=133.5, num_updates=172200, lr=7.6205e-05, gnorm=2.314, loss_scale=4, train_wall=11, gb_free=19.8, wall=1637
2022-12-09 20:28:28 | INFO | train_inner | epoch 157:    400 / 1102 loss=6.819, nll_loss=2.975, ppl=7.86, wps=32578.4, ups=9.09, wpb=3584.6, bsz=149.8, num_updates=172300, lr=7.61829e-05, gnorm=2.138, loss_scale=4, train_wall=11, gb_free=19.6, wall=1648
2022-12-09 20:28:39 | INFO | train_inner | epoch 157:    500 / 1102 loss=6.792, nll_loss=2.933, ppl=7.63, wps=32342.5, ups=9.13, wpb=3543.5, bsz=137.8, num_updates=172400, lr=7.61608e-05, gnorm=2.235, loss_scale=4, train_wall=11, gb_free=19.4, wall=1659
2022-12-09 20:28:50 | INFO | train_inner | epoch 157:    600 / 1102 loss=6.791, nll_loss=2.93, ppl=7.62, wps=32440.6, ups=9.04, wpb=3590.4, bsz=143, num_updates=172500, lr=7.61387e-05, gnorm=2.296, loss_scale=4, train_wall=11, gb_free=19.5, wall=1670
2022-12-09 20:29:01 | INFO | train_inner | epoch 157:    700 / 1102 loss=6.825, nll_loss=2.986, ppl=7.92, wps=32497.1, ups=9.05, wpb=3589.5, bsz=153.4, num_updates=172600, lr=7.61166e-05, gnorm=2.259, loss_scale=4, train_wall=11, gb_free=19.3, wall=1681
2022-12-09 20:29:12 | INFO | train_inner | epoch 157:    800 / 1102 loss=6.815, nll_loss=2.98, ppl=7.89, wps=33399, ups=9.12, wpb=3663.3, bsz=156.1, num_updates=172700, lr=7.60946e-05, gnorm=2.171, loss_scale=4, train_wall=11, gb_free=19.2, wall=1692
2022-12-09 20:29:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-12-09 20:29:23 | INFO | train_inner | epoch 157:    901 / 1102 loss=6.746, nll_loss=2.905, ppl=7.49, wps=32036.4, ups=9.07, wpb=3531.7, bsz=162.8, num_updates=172800, lr=7.60726e-05, gnorm=2.226, loss_scale=2, train_wall=11, gb_free=19.5, wall=1703
2022-12-09 20:29:34 | INFO | train_inner | epoch 157:   1001 / 1102 loss=6.913, nll_loss=3.077, ppl=8.44, wps=33296.9, ups=9.16, wpb=3634.4, bsz=138.9, num_updates=172900, lr=7.60506e-05, gnorm=2.185, loss_scale=2, train_wall=11, gb_free=19.5, wall=1714
2022-12-09 20:29:45 | INFO | train_inner | epoch 157:   1101 / 1102 loss=6.829, nll_loss=2.976, ppl=7.87, wps=32471.9, ups=9.08, wpb=3574.2, bsz=145, num_updates=173000, lr=7.60286e-05, gnorm=2.376, loss_scale=2, train_wall=11, gb_free=19.8, wall=1725
2022-12-09 20:29:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:30:25 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 3.662 | nll_loss 2.12 | ppl 4.35 | bleu 37.26 | wps 4536.2 | wpb 2835.3 | bsz 115.6 | num_updates 173001 | best_bleu 37.48
2022-12-09 20:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 173001 updates
2022-12-09 20:30:25 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint157.pt
2022-12-09 20:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint157.pt (epoch 157 @ 173001 updates, score 37.26) (writing took 1.15621018409729 seconds)
2022-12-09 20:30:26 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-12-09 20:30:26 | INFO | train | epoch 157 | loss 6.814 | nll_loss 2.965 | ppl 7.81 | wps 24400.8 | ups 6.81 | wpb 3584.1 | bsz 145.5 | num_updates 173001 | lr 7.60284e-05 | gnorm 2.234 | loss_scale 2 | train_wall 118 | gb_free 19.4 | wall 1766
2022-12-09 20:30:26 | INFO | fairseq.trainer | begin training epoch 158
2022-12-09 20:30:37 | INFO | train_inner | epoch 158:     99 / 1102 loss=6.732, nll_loss=2.896, ppl=7.44, wps=6943.7, ups=1.92, wpb=3625.4, bsz=153.3, num_updates=173100, lr=7.60066e-05, gnorm=2.082, loss_scale=2, train_wall=11, gb_free=19.4, wall=1778
2022-12-09 20:30:48 | INFO | train_inner | epoch 158:    199 / 1102 loss=6.737, nll_loss=2.882, ppl=7.37, wps=32975, ups=9.27, wpb=3556.1, bsz=146, num_updates=173200, lr=7.59847e-05, gnorm=2.191, loss_scale=2, train_wall=11, gb_free=19.5, wall=1788
2022-12-09 20:30:59 | INFO | train_inner | epoch 158:    299 / 1102 loss=6.788, nll_loss=2.932, ppl=7.63, wps=32525.3, ups=9.17, wpb=3547.5, bsz=142.2, num_updates=173300, lr=7.59628e-05, gnorm=2.194, loss_scale=2, train_wall=11, gb_free=19.9, wall=1799
2022-12-09 20:31:10 | INFO | train_inner | epoch 158:    399 / 1102 loss=6.832, nll_loss=2.976, ppl=7.87, wps=32663.7, ups=9.16, wpb=3564, bsz=141.6, num_updates=173400, lr=7.59408e-05, gnorm=2.281, loss_scale=2, train_wall=11, gb_free=19.6, wall=1810
2022-12-09 20:31:21 | INFO | train_inner | epoch 158:    499 / 1102 loss=6.816, nll_loss=2.969, ppl=7.83, wps=32581.5, ups=9.08, wpb=3588.8, bsz=146.2, num_updates=173500, lr=7.5919e-05, gnorm=2.198, loss_scale=2, train_wall=11, gb_free=19.3, wall=1821
2022-12-09 20:31:31 | INFO | train_inner | epoch 158:    599 / 1102 loss=6.806, nll_loss=2.94, ppl=7.67, wps=32601.3, ups=9.18, wpb=3551.7, bsz=136.7, num_updates=173600, lr=7.58971e-05, gnorm=2.276, loss_scale=2, train_wall=11, gb_free=19.4, wall=1832
2022-12-09 20:31:42 | INFO | train_inner | epoch 158:    699 / 1102 loss=6.818, nll_loss=2.973, ppl=7.85, wps=32538.4, ups=9.15, wpb=3554.7, bsz=148.1, num_updates=173700, lr=7.58752e-05, gnorm=2.245, loss_scale=2, train_wall=11, gb_free=19.6, wall=1843
2022-12-09 20:31:53 | INFO | train_inner | epoch 158:    799 / 1102 loss=6.842, nll_loss=3.003, ppl=8.02, wps=33292.3, ups=9.08, wpb=3667.1, bsz=144.6, num_updates=173800, lr=7.58534e-05, gnorm=2.085, loss_scale=2, train_wall=11, gb_free=19.5, wall=1854
2022-12-09 20:32:04 | INFO | train_inner | epoch 158:    899 / 1102 loss=6.868, nll_loss=3.017, ppl=8.09, wps=32806.9, ups=9.14, wpb=3589.3, bsz=145.1, num_updates=173900, lr=7.58316e-05, gnorm=2.248, loss_scale=2, train_wall=11, gb_free=19.8, wall=1865
2022-12-09 20:32:15 | INFO | train_inner | epoch 158:    999 / 1102 loss=6.808, nll_loss=2.981, ppl=7.9, wps=33166, ups=9.14, wpb=3627.2, bsz=157.9, num_updates=174000, lr=7.58098e-05, gnorm=2.115, loss_scale=2, train_wall=11, gb_free=19.5, wall=1876
2022-12-09 20:32:26 | INFO | train_inner | epoch 158:   1099 / 1102 loss=6.85, nll_loss=2.997, ppl=7.98, wps=32572.4, ups=9.14, wpb=3562.3, bsz=140.9, num_updates=174100, lr=7.5788e-05, gnorm=2.247, loss_scale=2, train_wall=11, gb_free=20.1, wall=1887
2022-12-09 20:32:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:33:07 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 3.662 | nll_loss 2.119 | ppl 4.34 | bleu 37.29 | wps 4525.5 | wpb 2835.3 | bsz 115.6 | num_updates 174103 | best_bleu 37.48
2022-12-09 20:33:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 174103 updates
2022-12-09 20:33:07 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint158.pt
2022-12-09 20:33:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint158.pt (epoch 158 @ 174103 updates, score 37.29) (writing took 1.144013687968254 seconds)
2022-12-09 20:33:08 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-12-09 20:33:08 | INFO | train | epoch 158 | loss 6.809 | nll_loss 2.96 | ppl 7.78 | wps 24383.1 | ups 6.8 | wpb 3583.6 | bsz 145.4 | num_updates 174103 | lr 7.57874e-05 | gnorm 2.199 | loss_scale 2 | train_wall 118 | gb_free 19.9 | wall 1928
2022-12-09 20:33:08 | INFO | fairseq.trainer | begin training epoch 159
2022-12-09 20:33:19 | INFO | train_inner | epoch 159:     97 / 1102 loss=6.715, nll_loss=2.858, ppl=7.25, wps=6713.5, ups=1.91, wpb=3514.9, bsz=144.6, num_updates=174200, lr=7.57663e-05, gnorm=2.258, loss_scale=2, train_wall=11, gb_free=19.9, wall=1939
2022-12-09 20:33:30 | INFO | train_inner | epoch 159:    197 / 1102 loss=6.758, nll_loss=2.9, ppl=7.47, wps=33287.1, ups=9.1, wpb=3659.5, bsz=136.5, num_updates=174300, lr=7.57445e-05, gnorm=2.149, loss_scale=2, train_wall=11, gb_free=19.3, wall=1950
2022-12-09 20:33:41 | INFO | train_inner | epoch 159:    297 / 1102 loss=6.835, nll_loss=2.978, ppl=7.88, wps=32835.1, ups=9.1, wpb=3609.5, bsz=137.1, num_updates=174400, lr=7.57228e-05, gnorm=2.284, loss_scale=2, train_wall=11, gb_free=19.8, wall=1961
2022-12-09 20:33:51 | INFO | train_inner | epoch 159:    397 / 1102 loss=6.8, nll_loss=2.952, ppl=7.74, wps=33044.9, ups=9.23, wpb=3579.6, bsz=146.9, num_updates=174500, lr=7.57011e-05, gnorm=2.214, loss_scale=2, train_wall=11, gb_free=19.7, wall=1972
2022-12-09 20:34:02 | INFO | train_inner | epoch 159:    497 / 1102 loss=6.815, nll_loss=2.985, ppl=7.92, wps=33029, ups=9.1, wpb=3628.9, bsz=153.9, num_updates=174600, lr=7.56794e-05, gnorm=2.172, loss_scale=2, train_wall=11, gb_free=19.6, wall=1983
2022-12-09 20:34:13 | INFO | train_inner | epoch 159:    597 / 1102 loss=6.756, nll_loss=2.898, ppl=7.46, wps=32599.8, ups=9.15, wpb=3562, bsz=144.6, num_updates=174700, lr=7.56578e-05, gnorm=2.265, loss_scale=2, train_wall=11, gb_free=19.6, wall=1994
2022-12-09 20:34:24 | INFO | train_inner | epoch 159:    697 / 1102 loss=6.892, nll_loss=3.038, ppl=8.21, wps=32205.1, ups=9.12, wpb=3530.6, bsz=137.9, num_updates=174800, lr=7.56361e-05, gnorm=2.262, loss_scale=2, train_wall=11, gb_free=19.3, wall=2005
2022-12-09 20:34:36 | INFO | train_inner | epoch 159:    797 / 1102 loss=6.746, nll_loss=2.922, ppl=7.58, wps=32536.4, ups=8.9, wpb=3656.8, bsz=167.4, num_updates=174900, lr=7.56145e-05, gnorm=2.14, loss_scale=2, train_wall=11, gb_free=20, wall=2016
2022-12-09 20:34:47 | INFO | train_inner | epoch 159:    897 / 1102 loss=6.851, nll_loss=3.007, ppl=8.04, wps=32906, ups=9.1, wpb=3614.4, bsz=146.7, num_updates=175000, lr=7.55929e-05, gnorm=2.196, loss_scale=2, train_wall=11, gb_free=19.6, wall=2027
2022-12-09 20:34:58 | INFO | train_inner | epoch 159:    997 / 1102 loss=6.886, nll_loss=3.017, ppl=8.1, wps=31907.2, ups=9.06, wpb=3523, bsz=130.4, num_updates=175100, lr=7.55713e-05, gnorm=2.35, loss_scale=2, train_wall=11, gb_free=19.3, wall=2038
2022-12-09 20:35:08 | INFO | train_inner | epoch 159:   1097 / 1102 loss=6.827, nll_loss=2.968, ppl=7.83, wps=32182, ups=9.15, wpb=3516.1, bsz=147.5, num_updates=175200, lr=7.55497e-05, gnorm=2.286, loss_scale=2, train_wall=11, gb_free=19.6, wall=2049
2022-12-09 20:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:35:48 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 3.663 | nll_loss 2.121 | ppl 4.35 | bleu 37.47 | wps 4651.5 | wpb 2835.3 | bsz 115.6 | num_updates 175205 | best_bleu 37.48
2022-12-09 20:35:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 175205 updates
2022-12-09 20:35:49 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint159.pt
2022-12-09 20:35:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint159.pt (epoch 159 @ 175205 updates, score 37.47) (writing took 1.1041698595508933 seconds)
2022-12-09 20:35:49 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-12-09 20:35:49 | INFO | train | epoch 159 | loss 6.807 | nll_loss 2.957 | ppl 7.76 | wps 24489.5 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 175205 | lr 7.55487e-05 | gnorm 2.23 | loss_scale 2 | train_wall 118 | gb_free 19.4 | wall 2090
2022-12-09 20:35:49 | INFO | fairseq.trainer | begin training epoch 160
2022-12-09 20:36:00 | INFO | train_inner | epoch 160:     95 / 1102 loss=6.761, nll_loss=2.924, ppl=7.59, wps=7006.3, ups=1.95, wpb=3596.3, bsz=156.4, num_updates=175300, lr=7.55282e-05, gnorm=2.144, loss_scale=2, train_wall=11, gb_free=19.4, wall=2100
2022-12-09 20:36:11 | INFO | train_inner | epoch 160:    195 / 1102 loss=6.845, nll_loss=2.975, ppl=7.86, wps=32672.4, ups=9.18, wpb=3560, bsz=127.8, num_updates=175400, lr=7.55067e-05, gnorm=2.265, loss_scale=2, train_wall=11, gb_free=19.7, wall=2111
2022-12-09 20:36:22 | INFO | train_inner | epoch 160:    295 / 1102 loss=6.697, nll_loss=2.857, ppl=7.25, wps=32888.3, ups=9.18, wpb=3583.8, bsz=161.2, num_updates=175500, lr=7.54851e-05, gnorm=2.15, loss_scale=2, train_wall=11, gb_free=19.3, wall=2122
2022-12-09 20:36:32 | INFO | train_inner | epoch 160:    395 / 1102 loss=6.821, nll_loss=2.954, ppl=7.75, wps=32439.9, ups=9.18, wpb=3532.3, bsz=135.2, num_updates=175600, lr=7.54636e-05, gnorm=2.254, loss_scale=2, train_wall=11, gb_free=19.8, wall=2133
2022-12-09 20:36:43 | INFO | train_inner | epoch 160:    495 / 1102 loss=6.838, nll_loss=2.975, ppl=7.86, wps=32434.9, ups=9.18, wpb=3532.8, bsz=137.9, num_updates=175700, lr=7.54422e-05, gnorm=2.327, loss_scale=2, train_wall=11, gb_free=19.5, wall=2144
2022-12-09 20:36:54 | INFO | train_inner | epoch 160:    595 / 1102 loss=6.805, nll_loss=2.959, ppl=7.78, wps=32940.5, ups=9.02, wpb=3650.2, bsz=147.2, num_updates=175800, lr=7.54207e-05, gnorm=2.172, loss_scale=2, train_wall=11, gb_free=19.5, wall=2155
2022-12-09 20:37:05 | INFO | train_inner | epoch 160:    695 / 1102 loss=6.805, nll_loss=2.97, ppl=7.83, wps=33026.2, ups=9.06, wpb=3644.4, bsz=150.6, num_updates=175900, lr=7.53993e-05, gnorm=2.166, loss_scale=2, train_wall=11, gb_free=19.6, wall=2166
2022-12-09 20:37:16 | INFO | train_inner | epoch 160:    795 / 1102 loss=6.758, nll_loss=2.911, ppl=7.52, wps=33122, ups=9.12, wpb=3630.3, bsz=155.5, num_updates=176000, lr=7.53778e-05, gnorm=2.211, loss_scale=2, train_wall=11, gb_free=20, wall=2177
2022-12-09 20:37:28 | INFO | train_inner | epoch 160:    895 / 1102 loss=6.805, nll_loss=2.961, ppl=7.79, wps=32092.5, ups=8.99, wpb=3569.8, bsz=144.7, num_updates=176100, lr=7.53564e-05, gnorm=2.148, loss_scale=2, train_wall=11, gb_free=19.3, wall=2188
2022-12-09 20:37:38 | INFO | train_inner | epoch 160:    995 / 1102 loss=6.84, nll_loss=2.976, ppl=7.87, wps=32190.1, ups=9.19, wpb=3501.9, bsz=135.9, num_updates=176200, lr=7.5335e-05, gnorm=2.312, loss_scale=2, train_wall=11, gb_free=19.3, wall=2199
2022-12-09 20:37:49 | INFO | train_inner | epoch 160:   1095 / 1102 loss=6.864, nll_loss=3.027, ppl=8.15, wps=33116.8, ups=9.11, wpb=3633.7, bsz=150, num_updates=176300, lr=7.53137e-05, gnorm=2.266, loss_scale=2, train_wall=11, gb_free=19.3, wall=2210
2022-12-09 20:37:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:38:29 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 3.663 | nll_loss 2.119 | ppl 4.34 | bleu 37.38 | wps 4659.9 | wpb 2835.3 | bsz 115.6 | num_updates 176307 | best_bleu 37.48
2022-12-09 20:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 176307 updates
2022-12-09 20:38:30 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint160.pt
2022-12-09 20:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint160.pt (epoch 160 @ 176307 updates, score 37.38) (writing took 1.1603893591091037 seconds)
2022-12-09 20:38:30 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-12-09 20:38:30 | INFO | train | epoch 160 | loss 6.805 | nll_loss 2.954 | ppl 7.75 | wps 24512.4 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 176307 | lr 7.53122e-05 | gnorm 2.222 | loss_scale 2 | train_wall 118 | gb_free 19.6 | wall 2251
2022-12-09 20:38:30 | INFO | fairseq.trainer | begin training epoch 161
2022-12-09 20:38:41 | INFO | train_inner | epoch 161:     93 / 1102 loss=6.696, nll_loss=2.854, ppl=7.23, wps=7027.4, ups=1.96, wpb=3593.9, bsz=157.6, num_updates=176400, lr=7.52923e-05, gnorm=2.153, loss_scale=2, train_wall=11, gb_free=19.5, wall=2261
2022-12-09 20:38:51 | INFO | train_inner | epoch 161:    193 / 1102 loss=6.785, nll_loss=2.901, ppl=7.47, wps=32358.2, ups=9.18, wpb=3525.8, bsz=130.5, num_updates=176500, lr=7.5271e-05, gnorm=2.306, loss_scale=2, train_wall=11, gb_free=19.4, wall=2272
2022-12-09 20:39:02 | INFO | train_inner | epoch 161:    293 / 1102 loss=6.839, nll_loss=2.982, ppl=7.9, wps=32629.9, ups=9.16, wpb=3562.9, bsz=131.2, num_updates=176600, lr=7.52497e-05, gnorm=2.174, loss_scale=2, train_wall=11, gb_free=19.8, wall=2283
2022-12-09 20:39:13 | INFO | train_inner | epoch 161:    393 / 1102 loss=6.77, nll_loss=2.922, ppl=7.58, wps=32334.9, ups=9.11, wpb=3549.6, bsz=146, num_updates=176700, lr=7.52284e-05, gnorm=2.215, loss_scale=2, train_wall=11, gb_free=19.4, wall=2294
2022-12-09 20:39:24 | INFO | train_inner | epoch 161:    493 / 1102 loss=6.771, nll_loss=2.927, ppl=7.61, wps=33001.2, ups=9.15, wpb=3607.1, bsz=155, num_updates=176800, lr=7.52071e-05, gnorm=2.143, loss_scale=2, train_wall=11, gb_free=19.7, wall=2305
2022-12-09 20:39:35 | INFO | train_inner | epoch 161:    593 / 1102 loss=6.849, nll_loss=2.992, ppl=7.95, wps=32920.9, ups=9.28, wpb=3547.1, bsz=135.3, num_updates=176900, lr=7.51858e-05, gnorm=2.324, loss_scale=2, train_wall=11, gb_free=19.4, wall=2316
2022-12-09 20:39:46 | INFO | train_inner | epoch 161:    693 / 1102 loss=6.802, nll_loss=2.962, ppl=7.79, wps=33037.5, ups=9.01, wpb=3666.3, bsz=149, num_updates=177000, lr=7.51646e-05, gnorm=2.176, loss_scale=2, train_wall=11, gb_free=19.4, wall=2327
2022-12-09 20:39:57 | INFO | train_inner | epoch 161:    793 / 1102 loss=6.848, nll_loss=2.989, ppl=7.94, wps=32172.2, ups=9.21, wpb=3491.9, bsz=141, num_updates=177100, lr=7.51434e-05, gnorm=2.326, loss_scale=2, train_wall=11, gb_free=19.5, wall=2338
2022-12-09 20:40:08 | INFO | train_inner | epoch 161:    893 / 1102 loss=6.76, nll_loss=2.921, ppl=7.58, wps=33277.3, ups=9.2, wpb=3617.1, bsz=156.5, num_updates=177200, lr=7.51222e-05, gnorm=2.172, loss_scale=2, train_wall=11, gb_free=19.3, wall=2348
2022-12-09 20:40:19 | INFO | train_inner | epoch 161:    993 / 1102 loss=6.846, nll_loss=3.006, ppl=8.03, wps=33080.3, ups=9.21, wpb=3590.2, bsz=152.2, num_updates=177300, lr=7.5101e-05, gnorm=2.228, loss_scale=2, train_wall=11, gb_free=19.7, wall=2359
2022-12-09 20:40:30 | INFO | train_inner | epoch 161:   1093 / 1102 loss=6.849, nll_loss=3.007, ppl=8.04, wps=33521.5, ups=9.19, wpb=3647.6, bsz=143.2, num_updates=177400, lr=7.50798e-05, gnorm=2.145, loss_scale=2, train_wall=11, gb_free=19.7, wall=2370
2022-12-09 20:40:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:41:10 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 3.663 | nll_loss 2.12 | ppl 4.35 | bleu 37.58 | wps 4524.6 | wpb 2835.3 | bsz 115.6 | num_updates 177409 | best_bleu 37.58
2022-12-09 20:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 177409 updates
2022-12-09 20:41:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint161.pt
2022-12-09 20:41:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint161.pt (epoch 161 @ 177409 updates, score 37.58) (writing took 1.527777036651969 seconds)
2022-12-09 20:41:12 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-12-09 20:41:12 | INFO | train | epoch 161 | loss 6.801 | nll_loss 2.951 | ppl 7.73 | wps 24398.3 | ups 6.81 | wpb 3583.6 | bsz 145.4 | num_updates 177409 | lr 7.50779e-05 | gnorm 2.215 | loss_scale 2 | train_wall 117 | gb_free 19.9 | wall 2413
2022-12-09 20:41:12 | INFO | fairseq.trainer | begin training epoch 162
2022-12-09 20:41:22 | INFO | train_inner | epoch 162:     91 / 1102 loss=6.757, nll_loss=2.905, ppl=7.49, wps=6816.2, ups=1.91, wpb=3574.3, bsz=150.4, num_updates=177500, lr=7.50587e-05, gnorm=2.23, loss_scale=2, train_wall=11, gb_free=19.8, wall=2423
2022-12-09 20:41:33 | INFO | train_inner | epoch 162:    191 / 1102 loss=6.76, nll_loss=2.914, ppl=7.54, wps=33571.2, ups=9.29, wpb=3613, bsz=138.4, num_updates=177600, lr=7.50375e-05, gnorm=2.145, loss_scale=2, train_wall=11, gb_free=19.3, wall=2433
2022-12-09 20:41:44 | INFO | train_inner | epoch 162:    291 / 1102 loss=6.773, nll_loss=2.928, ppl=7.61, wps=32640.7, ups=9.19, wpb=3553, bsz=155.8, num_updates=177700, lr=7.50164e-05, gnorm=2.292, loss_scale=2, train_wall=11, gb_free=19.8, wall=2444
2022-12-09 20:41:55 | INFO | train_inner | epoch 162:    391 / 1102 loss=6.795, nll_loss=2.932, ppl=7.63, wps=33380.5, ups=9.27, wpb=3600.5, bsz=132.1, num_updates=177800, lr=7.49953e-05, gnorm=2.237, loss_scale=2, train_wall=11, gb_free=19.8, wall=2455
2022-12-09 20:42:05 | INFO | train_inner | epoch 162:    491 / 1102 loss=6.749, nll_loss=2.912, ppl=7.53, wps=33427.5, ups=9.19, wpb=3637, bsz=159.4, num_updates=177900, lr=7.49742e-05, gnorm=2.152, loss_scale=2, train_wall=11, gb_free=19.6, wall=2466
2022-12-09 20:42:16 | INFO | train_inner | epoch 162:    591 / 1102 loss=6.831, nll_loss=2.973, ppl=7.85, wps=32864.2, ups=9.24, wpb=3555.1, bsz=137.6, num_updates=178000, lr=7.49532e-05, gnorm=2.348, loss_scale=2, train_wall=11, gb_free=20, wall=2477
2022-12-09 20:42:27 | INFO | train_inner | epoch 162:    691 / 1102 loss=6.834, nll_loss=2.984, ppl=7.91, wps=33345.3, ups=9.22, wpb=3616.7, bsz=145.3, num_updates=178100, lr=7.49321e-05, gnorm=2.354, loss_scale=2, train_wall=11, gb_free=19.4, wall=2488
2022-12-09 20:42:38 | INFO | train_inner | epoch 162:    791 / 1102 loss=6.821, nll_loss=2.977, ppl=7.87, wps=33038.4, ups=9.26, wpb=3567.1, bsz=151.2, num_updates=178200, lr=7.49111e-05, gnorm=2.22, loss_scale=2, train_wall=11, gb_free=19.3, wall=2498
2022-12-09 20:42:49 | INFO | train_inner | epoch 162:    891 / 1102 loss=6.768, nll_loss=2.918, ppl=7.56, wps=32357.4, ups=9.23, wpb=3504.1, bsz=156.6, num_updates=178300, lr=7.48901e-05, gnorm=2.36, loss_scale=2, train_wall=11, gb_free=19.4, wall=2509
2022-12-09 20:42:59 | INFO | train_inner | epoch 162:    991 / 1102 loss=6.865, nll_loss=3.005, ppl=8.03, wps=33383.2, ups=9.36, wpb=3565.8, bsz=135, num_updates=178400, lr=7.48691e-05, gnorm=2.25, loss_scale=2, train_wall=10, gb_free=20, wall=2520
2022-12-09 20:43:10 | INFO | train_inner | epoch 162:   1091 / 1102 loss=6.843, nll_loss=2.993, ppl=7.96, wps=33555.2, ups=9.21, wpb=3642.9, bsz=140.7, num_updates=178500, lr=7.48481e-05, gnorm=2.188, loss_scale=2, train_wall=11, gb_free=19.6, wall=2531
2022-12-09 20:43:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:43:50 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.37 | wps 4661.8 | wpb 2835.3 | bsz 115.6 | num_updates 178511 | best_bleu 37.58
2022-12-09 20:43:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 178511 updates
2022-12-09 20:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint162.pt
2022-12-09 20:43:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint162.pt (epoch 162 @ 178511 updates, score 37.37) (writing took 1.1170889735221863 seconds)
2022-12-09 20:43:51 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-12-09 20:43:51 | INFO | train | epoch 162 | loss 6.799 | nll_loss 2.949 | ppl 7.72 | wps 24797.9 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 178511 | lr 7.48458e-05 | gnorm 2.251 | loss_scale 2 | train_wall 116 | gb_free 19.4 | wall 2572
2022-12-09 20:43:51 | INFO | fairseq.trainer | begin training epoch 163
2022-12-09 20:44:01 | INFO | train_inner | epoch 163:     89 / 1102 loss=6.759, nll_loss=2.911, ppl=7.52, wps=7212.1, ups=1.96, wpb=3670.7, bsz=144.5, num_updates=178600, lr=7.48272e-05, gnorm=2.212, loss_scale=2, train_wall=11, gb_free=19.3, wall=2582
2022-12-09 20:44:12 | INFO | train_inner | epoch 163:    189 / 1102 loss=6.766, nll_loss=2.91, ppl=7.51, wps=33369.3, ups=9.28, wpb=3595.8, bsz=144, num_updates=178700, lr=7.48062e-05, gnorm=2.159, loss_scale=2, train_wall=11, gb_free=19.4, wall=2592
2022-12-09 20:44:23 | INFO | train_inner | epoch 163:    289 / 1102 loss=6.798, nll_loss=2.939, ppl=7.67, wps=33565.8, ups=9.35, wpb=3588.7, bsz=147.2, num_updates=178800, lr=7.47853e-05, gnorm=2.275, loss_scale=2, train_wall=10, gb_free=19.7, wall=2603
2022-12-09 20:44:33 | INFO | train_inner | epoch 163:    389 / 1102 loss=6.758, nll_loss=2.908, ppl=7.5, wps=33003.6, ups=9.34, wpb=3533.1, bsz=150.2, num_updates=178900, lr=7.47644e-05, gnorm=2.231, loss_scale=2, train_wall=10, gb_free=19.4, wall=2614
2022-12-09 20:44:44 | INFO | train_inner | epoch 163:    489 / 1102 loss=6.763, nll_loss=2.921, ppl=7.57, wps=33749.9, ups=9.36, wpb=3604.1, bsz=154.2, num_updates=179000, lr=7.47435e-05, gnorm=2.193, loss_scale=2, train_wall=10, gb_free=19.5, wall=2625
2022-12-09 20:44:55 | INFO | train_inner | epoch 163:    589 / 1102 loss=6.812, nll_loss=2.938, ppl=7.66, wps=32347.8, ups=9.41, wpb=3438.3, bsz=134, num_updates=179100, lr=7.47226e-05, gnorm=2.336, loss_scale=2, train_wall=10, gb_free=19.8, wall=2635
2022-12-09 20:45:05 | INFO | train_inner | epoch 163:    689 / 1102 loss=6.719, nll_loss=2.88, ppl=7.36, wps=32908.2, ups=9.31, wpb=3535.8, bsz=160.7, num_updates=179200, lr=7.47018e-05, gnorm=2.233, loss_scale=2, train_wall=11, gb_free=19.3, wall=2646
2022-12-09 20:45:16 | INFO | train_inner | epoch 163:    789 / 1102 loss=6.902, nll_loss=3.047, ppl=8.26, wps=33310.2, ups=9.21, wpb=3617.5, bsz=128.9, num_updates=179300, lr=7.4681e-05, gnorm=2.317, loss_scale=2, train_wall=11, gb_free=19.4, wall=2657
2022-12-09 20:45:27 | INFO | train_inner | epoch 163:    889 / 1102 loss=6.851, nll_loss=3.006, ppl=8.03, wps=33459.7, ups=9.2, wpb=3635.8, bsz=140.7, num_updates=179400, lr=7.46601e-05, gnorm=2.228, loss_scale=2, train_wall=11, gb_free=19.8, wall=2668
2022-12-09 20:45:38 | INFO | train_inner | epoch 163:    989 / 1102 loss=6.821, nll_loss=2.974, ppl=7.86, wps=32950.2, ups=9.19, wpb=3585.3, bsz=142.1, num_updates=179500, lr=7.46393e-05, gnorm=2.173, loss_scale=2, train_wall=11, gb_free=19.3, wall=2679
2022-12-09 20:45:49 | INFO | train_inner | epoch 163:   1089 / 1102 loss=6.794, nll_loss=2.949, ppl=7.72, wps=33235.7, ups=9.21, wpb=3608, bsz=154.2, num_updates=179600, lr=7.46186e-05, gnorm=2.259, loss_scale=2, train_wall=11, gb_free=19.4, wall=2689
2022-12-09 20:45:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:46:29 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 3.658 | nll_loss 2.118 | ppl 4.34 | bleu 37.46 | wps 4654.7 | wpb 2835.3 | bsz 115.6 | num_updates 179613 | best_bleu 37.58
2022-12-09 20:46:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 179613 updates
2022-12-09 20:46:30 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint163.pt
2022-12-09 20:46:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint163.pt (epoch 163 @ 179613 updates, score 37.46) (writing took 1.1925366949290037 seconds)
2022-12-09 20:46:30 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-12-09 20:46:30 | INFO | train | epoch 163 | loss 6.796 | nll_loss 2.945 | ppl 7.7 | wps 24831 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 179613 | lr 7.46159e-05 | gnorm 2.237 | loss_scale 2 | train_wall 116 | gb_free 19.4 | wall 2731
2022-12-09 20:46:30 | INFO | fairseq.trainer | begin training epoch 164
2022-12-09 20:46:40 | INFO | train_inner | epoch 164:     87 / 1102 loss=6.804, nll_loss=2.953, ppl=7.74, wps=7102, ups=1.95, wpb=3638.3, bsz=140.6, num_updates=179700, lr=7.45978e-05, gnorm=2.179, loss_scale=2, train_wall=11, gb_free=19.6, wall=2741
2022-12-09 20:46:51 | INFO | train_inner | epoch 164:    187 / 1102 loss=6.823, nll_loss=2.96, ppl=7.78, wps=32888.5, ups=9.29, wpb=3539.2, bsz=139.5, num_updates=179800, lr=7.4577e-05, gnorm=2.251, loss_scale=2, train_wall=11, gb_free=19.5, wall=2751
2022-12-09 20:47:02 | INFO | train_inner | epoch 164:    287 / 1102 loss=6.777, nll_loss=2.908, ppl=7.51, wps=32848.5, ups=9.23, wpb=3559.6, bsz=133.8, num_updates=179900, lr=7.45563e-05, gnorm=2.281, loss_scale=2, train_wall=11, gb_free=19.3, wall=2762
2022-12-09 20:47:12 | INFO | train_inner | epoch 164:    387 / 1102 loss=6.76, nll_loss=2.902, ppl=7.47, wps=32904.4, ups=9.3, wpb=3537.8, bsz=144.1, num_updates=180000, lr=7.45356e-05, gnorm=2.239, loss_scale=2, train_wall=11, gb_free=19.3, wall=2773
2022-12-09 20:47:23 | INFO | train_inner | epoch 164:    487 / 1102 loss=6.816, nll_loss=2.946, ppl=7.7, wps=33030.7, ups=9.2, wpb=3588.7, bsz=129.8, num_updates=180100, lr=7.45149e-05, gnorm=2.26, loss_scale=2, train_wall=11, gb_free=19.5, wall=2784
2022-12-09 20:47:34 | INFO | train_inner | epoch 164:    587 / 1102 loss=6.817, nll_loss=2.957, ppl=7.76, wps=32895.7, ups=9.16, wpb=3589.9, bsz=140.3, num_updates=180200, lr=7.44942e-05, gnorm=2.241, loss_scale=2, train_wall=11, gb_free=19.3, wall=2795
2022-12-09 20:47:45 | INFO | train_inner | epoch 164:    687 / 1102 loss=6.771, nll_loss=2.902, ppl=7.48, wps=32488.1, ups=9.23, wpb=3521.3, bsz=135, num_updates=180300, lr=7.44736e-05, gnorm=2.266, loss_scale=2, train_wall=11, gb_free=19.3, wall=2806
2022-12-09 20:47:56 | INFO | train_inner | epoch 164:    787 / 1102 loss=6.83, nll_loss=2.993, ppl=7.96, wps=33247.8, ups=9.18, wpb=3621.2, bsz=152, num_updates=180400, lr=7.44529e-05, gnorm=2.196, loss_scale=2, train_wall=11, gb_free=19.6, wall=2816
2022-12-09 20:48:07 | INFO | train_inner | epoch 164:    887 / 1102 loss=6.773, nll_loss=2.941, ppl=7.68, wps=33418.2, ups=9.15, wpb=3653.9, bsz=164.6, num_updates=180500, lr=7.44323e-05, gnorm=2.244, loss_scale=2, train_wall=11, gb_free=19.4, wall=2827
2022-12-09 20:48:18 | INFO | train_inner | epoch 164:    987 / 1102 loss=6.823, nll_loss=2.982, ppl=7.9, wps=33162.2, ups=9.27, wpb=3576.6, bsz=158.1, num_updates=180600, lr=7.44117e-05, gnorm=2.241, loss_scale=2, train_wall=11, gb_free=19.5, wall=2838
2022-12-09 20:48:28 | INFO | train_inner | epoch 164:   1087 / 1102 loss=6.786, nll_loss=2.939, ppl=7.67, wps=33185.4, ups=9.25, wpb=3589.3, bsz=155, num_updates=180700, lr=7.43911e-05, gnorm=2.22, loss_scale=2, train_wall=11, gb_free=19.3, wall=2849
2022-12-09 20:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:49:11 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 3.662 | nll_loss 2.121 | ppl 4.35 | bleu 37.53 | wps 4416.6 | wpb 2835.3 | bsz 115.6 | num_updates 180715 | best_bleu 37.58
2022-12-09 20:49:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 180715 updates
2022-12-09 20:49:12 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint164.pt
2022-12-09 20:49:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint164.pt (epoch 164 @ 180715 updates, score 37.53) (writing took 1.2960555702447891 seconds)
2022-12-09 20:49:12 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-12-09 20:49:12 | INFO | train | epoch 164 | loss 6.796 | nll_loss 2.942 | ppl 7.69 | wps 24378.4 | ups 6.8 | wpb 3583.6 | bsz 145.4 | num_updates 180715 | lr 7.4388e-05 | gnorm 2.236 | loss_scale 2 | train_wall 117 | gb_free 19.2 | wall 2893
2022-12-09 20:49:12 | INFO | fairseq.trainer | begin training epoch 165
2022-12-09 20:49:22 | INFO | train_inner | epoch 165:     85 / 1102 loss=6.79, nll_loss=2.932, ppl=7.63, wps=6748.7, ups=1.88, wpb=3596.4, bsz=136.2, num_updates=180800, lr=7.43705e-05, gnorm=2.183, loss_scale=2, train_wall=11, gb_free=19.2, wall=2902
2022-12-09 20:49:33 | INFO | train_inner | epoch 165:    185 / 1102 loss=6.763, nll_loss=2.905, ppl=7.49, wps=32682, ups=9.26, wpb=3529.3, bsz=141.6, num_updates=180900, lr=7.435e-05, gnorm=2.365, loss_scale=2, train_wall=11, gb_free=19.8, wall=2913
2022-12-09 20:49:43 | INFO | train_inner | epoch 165:    285 / 1102 loss=6.806, nll_loss=2.941, ppl=7.68, wps=33176, ups=9.23, wpb=3594, bsz=136.7, num_updates=181000, lr=7.43294e-05, gnorm=2.252, loss_scale=2, train_wall=11, gb_free=20, wall=2924
2022-12-09 20:49:54 | INFO | train_inner | epoch 165:    385 / 1102 loss=6.701, nll_loss=2.857, ppl=7.25, wps=32826.5, ups=9.07, wpb=3620.2, bsz=154.7, num_updates=181100, lr=7.43089e-05, gnorm=2.14, loss_scale=2, train_wall=11, gb_free=19.4, wall=2935
2022-12-09 20:50:05 | INFO | train_inner | epoch 165:    485 / 1102 loss=6.757, nll_loss=2.912, ppl=7.53, wps=32404.5, ups=9.11, wpb=3558.1, bsz=152.3, num_updates=181200, lr=7.42884e-05, gnorm=2.252, loss_scale=2, train_wall=11, gb_free=19.5, wall=2946
2022-12-09 20:50:16 | INFO | train_inner | epoch 165:    585 / 1102 loss=6.802, nll_loss=2.947, ppl=7.71, wps=33120.4, ups=9.19, wpb=3604.9, bsz=146.2, num_updates=181300, lr=7.42679e-05, gnorm=2.265, loss_scale=2, train_wall=11, gb_free=19.7, wall=2957
2022-12-09 20:50:27 | INFO | train_inner | epoch 165:    685 / 1102 loss=6.797, nll_loss=2.94, ppl=7.67, wps=32884.7, ups=9.19, wpb=3578, bsz=144.3, num_updates=181400, lr=7.42474e-05, gnorm=2.23, loss_scale=2, train_wall=11, gb_free=19.4, wall=2968
2022-12-09 20:50:38 | INFO | train_inner | epoch 165:    785 / 1102 loss=6.854, nll_loss=2.992, ppl=7.96, wps=32838.8, ups=9.1, wpb=3610.1, bsz=134.2, num_updates=181500, lr=7.4227e-05, gnorm=2.267, loss_scale=2, train_wall=11, gb_free=19.5, wall=2979
2022-12-09 20:50:49 | INFO | train_inner | epoch 165:    885 / 1102 loss=6.812, nll_loss=2.971, ppl=7.84, wps=32498.1, ups=9.12, wpb=3564.2, bsz=153.4, num_updates=181600, lr=7.42065e-05, gnorm=2.211, loss_scale=2, train_wall=11, gb_free=19.7, wall=2990
2022-12-09 20:51:00 | INFO | train_inner | epoch 165:    985 / 1102 loss=6.799, nll_loss=2.961, ppl=7.79, wps=32789.2, ups=9.13, wpb=3590.9, bsz=162.2, num_updates=181700, lr=7.41861e-05, gnorm=2.213, loss_scale=2, train_wall=11, gb_free=19.4, wall=3001
2022-12-09 20:51:11 | INFO | train_inner | epoch 165:   1085 / 1102 loss=6.837, nll_loss=2.984, ppl=7.91, wps=32363.3, ups=9.05, wpb=3575.5, bsz=138.6, num_updates=181800, lr=7.41657e-05, gnorm=2.223, loss_scale=2, train_wall=11, gb_free=19.3, wall=3012
2022-12-09 20:51:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:51:50 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 3.664 | nll_loss 2.119 | ppl 4.35 | bleu 37.37 | wps 4860.1 | wpb 2835.3 | bsz 115.6 | num_updates 181817 | best_bleu 37.58
2022-12-09 20:51:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 181817 updates
2022-12-09 20:51:51 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint165.pt
2022-12-09 20:51:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint165.pt (epoch 165 @ 181817 updates, score 37.37) (writing took 1.4147154623642564 seconds)
2022-12-09 20:51:52 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-12-09 20:51:52 | INFO | train | epoch 165 | loss 6.793 | nll_loss 2.94 | ppl 7.68 | wps 24770.6 | ups 6.91 | wpb 3583.6 | bsz 145.4 | num_updates 181817 | lr 7.41622e-05 | gnorm 2.237 | loss_scale 2 | train_wall 118 | gb_free 19.4 | wall 3052
2022-12-09 20:51:52 | INFO | fairseq.trainer | begin training epoch 166
2022-12-09 20:52:01 | INFO | train_inner | epoch 166:     83 / 1102 loss=6.772, nll_loss=2.92, ppl=7.57, wps=7161.2, ups=2, wpb=3580.9, bsz=146.6, num_updates=181900, lr=7.41453e-05, gnorm=2.254, loss_scale=2, train_wall=11, gb_free=19.3, wall=3062
2022-12-09 20:52:12 | INFO | train_inner | epoch 166:    183 / 1102 loss=6.768, nll_loss=2.909, ppl=7.51, wps=32596, ups=9.14, wpb=3565.9, bsz=147, num_updates=182000, lr=7.41249e-05, gnorm=2.409, loss_scale=2, train_wall=11, gb_free=19.5, wall=3073
2022-12-09 20:52:23 | INFO | train_inner | epoch 166:    283 / 1102 loss=6.67, nll_loss=2.818, ppl=7.05, wps=32241.7, ups=9.18, wpb=3513.1, bsz=155, num_updates=182100, lr=7.41046e-05, gnorm=2.204, loss_scale=2, train_wall=11, gb_free=19.3, wall=3084
2022-12-09 20:52:34 | INFO | train_inner | epoch 166:    383 / 1102 loss=6.853, nll_loss=2.988, ppl=7.93, wps=33114, ups=9.15, wpb=3619, bsz=126.3, num_updates=182200, lr=7.40842e-05, gnorm=2.164, loss_scale=2, train_wall=11, gb_free=19.4, wall=3094
2022-12-09 20:52:45 | INFO | train_inner | epoch 166:    483 / 1102 loss=6.806, nll_loss=2.948, ppl=7.72, wps=32729.5, ups=8.99, wpb=3640.9, bsz=146.4, num_updates=182300, lr=7.40639e-05, gnorm=2.236, loss_scale=2, train_wall=11, gb_free=19.5, wall=3106
2022-12-09 20:52:56 | INFO | train_inner | epoch 166:    583 / 1102 loss=6.788, nll_loss=2.942, ppl=7.69, wps=32771.8, ups=9.1, wpb=3602.9, bsz=154.7, num_updates=182400, lr=7.40436e-05, gnorm=2.27, loss_scale=2, train_wall=11, gb_free=19.4, wall=3117
2022-12-09 20:53:07 | INFO | train_inner | epoch 166:    683 / 1102 loss=6.749, nll_loss=2.887, ppl=7.4, wps=32511.2, ups=9.09, wpb=3577, bsz=141.4, num_updates=182500, lr=7.40233e-05, gnorm=2.293, loss_scale=2, train_wall=11, gb_free=19.4, wall=3128
2022-12-09 20:53:18 | INFO | train_inner | epoch 166:    783 / 1102 loss=6.758, nll_loss=2.909, ppl=7.51, wps=32449, ups=9.06, wpb=3582.9, bsz=146, num_updates=182600, lr=7.4003e-05, gnorm=2.275, loss_scale=2, train_wall=11, gb_free=19.6, wall=3139
2022-12-09 20:53:29 | INFO | train_inner | epoch 166:    883 / 1102 loss=6.85, nll_loss=3.014, ppl=8.08, wps=32708.4, ups=8.99, wpb=3638.4, bsz=145.8, num_updates=182700, lr=7.39828e-05, gnorm=2.212, loss_scale=2, train_wall=11, gb_free=19.6, wall=3150
2022-12-09 20:53:40 | INFO | train_inner | epoch 166:    983 / 1102 loss=6.796, nll_loss=2.943, ppl=7.69, wps=32194.7, ups=9.14, wpb=3523.2, bsz=145.9, num_updates=182800, lr=7.39626e-05, gnorm=2.296, loss_scale=2, train_wall=11, gb_free=19.6, wall=3161
2022-12-09 20:53:51 | INFO | train_inner | epoch 166:   1083 / 1102 loss=6.844, nll_loss=2.996, ppl=7.98, wps=32612.9, ups=9.09, wpb=3589, bsz=148, num_updates=182900, lr=7.39423e-05, gnorm=2.235, loss_scale=2, train_wall=11, gb_free=19.8, wall=3172
2022-12-09 20:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:54:30 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 3.661 | nll_loss 2.121 | ppl 4.35 | bleu 37.47 | wps 4900.3 | wpb 2835.3 | bsz 115.6 | num_updates 182919 | best_bleu 37.58
2022-12-09 20:54:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 182919 updates
2022-12-09 20:54:31 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint166.pt
2022-12-09 20:54:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint166.pt (epoch 166 @ 182919 updates, score 37.47) (writing took 1.1466519869863987 seconds)
2022-12-09 20:54:31 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-12-09 20:54:31 | INFO | train | epoch 166 | loss 6.79 | nll_loss 2.937 | ppl 7.66 | wps 24732.4 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 182919 | lr 7.39385e-05 | gnorm 2.261 | loss_scale 2 | train_wall 118 | gb_free 19.5 | wall 3212
2022-12-09 20:54:31 | INFO | fairseq.trainer | begin training epoch 167
2022-12-09 20:54:41 | INFO | train_inner | epoch 167:     81 / 1102 loss=6.793, nll_loss=2.941, ppl=7.68, wps=7290.8, ups=2.02, wpb=3602.9, bsz=141.8, num_updates=183000, lr=7.39221e-05, gnorm=2.156, loss_scale=2, train_wall=11, gb_free=19.5, wall=3221
2022-12-09 20:54:51 | INFO | train_inner | epoch 167:    181 / 1102 loss=6.831, nll_loss=2.964, ppl=7.8, wps=32988.2, ups=9.11, wpb=3619.9, bsz=131, num_updates=183100, lr=7.39019e-05, gnorm=2.299, loss_scale=2, train_wall=11, gb_free=19.6, wall=3232
2022-12-09 20:55:02 | INFO | train_inner | epoch 167:    281 / 1102 loss=6.659, nll_loss=2.809, ppl=7.01, wps=32452.8, ups=9.12, wpb=3558.2, bsz=149.8, num_updates=183200, lr=7.38818e-05, gnorm=2.165, loss_scale=2, train_wall=11, gb_free=20, wall=3243
2022-12-09 20:55:13 | INFO | train_inner | epoch 167:    381 / 1102 loss=6.858, nll_loss=2.979, ppl=7.89, wps=32272.3, ups=9.08, wpb=3555.5, bsz=127.2, num_updates=183300, lr=7.38616e-05, gnorm=2.401, loss_scale=2, train_wall=11, gb_free=19.5, wall=3254
2022-12-09 20:55:24 | INFO | train_inner | epoch 167:    481 / 1102 loss=6.813, nll_loss=2.971, ppl=7.84, wps=33365.4, ups=9.2, wpb=3625.3, bsz=149.8, num_updates=183400, lr=7.38415e-05, gnorm=2.271, loss_scale=2, train_wall=11, gb_free=19.3, wall=3265
2022-12-09 20:55:35 | INFO | train_inner | epoch 167:    581 / 1102 loss=6.729, nll_loss=2.88, ppl=7.36, wps=32478.6, ups=9.09, wpb=3571.6, bsz=159.7, num_updates=183500, lr=7.38213e-05, gnorm=2.227, loss_scale=2, train_wall=11, gb_free=19.2, wall=3276
2022-12-09 20:55:46 | INFO | train_inner | epoch 167:    681 / 1102 loss=6.788, nll_loss=2.948, ppl=7.72, wps=33278.4, ups=9.1, wpb=3658, bsz=153, num_updates=183600, lr=7.38012e-05, gnorm=2.212, loss_scale=2, train_wall=11, gb_free=19.4, wall=3287
2022-12-09 20:55:57 | INFO | train_inner | epoch 167:    781 / 1102 loss=6.753, nll_loss=2.908, ppl=7.5, wps=32823.3, ups=9.12, wpb=3599.3, bsz=155.1, num_updates=183700, lr=7.37812e-05, gnorm=2.25, loss_scale=2, train_wall=11, gb_free=19.3, wall=3298
2022-12-09 20:56:08 | INFO | train_inner | epoch 167:    881 / 1102 loss=6.744, nll_loss=2.879, ppl=7.36, wps=32187.4, ups=9.26, wpb=3477, bsz=147, num_updates=183800, lr=7.37611e-05, gnorm=2.306, loss_scale=2, train_wall=11, gb_free=20.4, wall=3309
2022-12-09 20:56:19 | INFO | train_inner | epoch 167:    981 / 1102 loss=6.931, nll_loss=3.066, ppl=8.38, wps=32702.1, ups=9.1, wpb=3592.1, bsz=128.1, num_updates=183900, lr=7.3741e-05, gnorm=2.289, loss_scale=2, train_wall=11, gb_free=19.7, wall=3320
2022-12-09 20:56:30 | INFO | train_inner | epoch 167:   1081 / 1102 loss=6.751, nll_loss=2.913, ppl=7.53, wps=32343.2, ups=9.11, wpb=3549.3, bsz=161.4, num_updates=184000, lr=7.3721e-05, gnorm=2.288, loss_scale=2, train_wall=11, gb_free=19.4, wall=3331
2022-12-09 20:56:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:57:13 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 3.663 | nll_loss 2.12 | ppl 4.35 | bleu 37.39 | wps 4486.7 | wpb 2835.3 | bsz 115.6 | num_updates 184021 | best_bleu 37.58
2022-12-09 20:57:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 184021 updates
2022-12-09 20:57:13 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint167.pt
2022-12-09 20:57:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint167.pt (epoch 167 @ 184021 updates, score 37.39) (writing took 1.163632744923234 seconds)
2022-12-09 20:57:14 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-12-09 20:57:14 | INFO | train | epoch 167 | loss 6.788 | nll_loss 2.934 | ppl 7.64 | wps 24312.2 | ups 6.78 | wpb 3583.6 | bsz 145.4 | num_updates 184021 | lr 7.37168e-05 | gnorm 2.26 | loss_scale 2 | train_wall 118 | gb_free 19.3 | wall 3374
2022-12-09 20:57:14 | INFO | fairseq.trainer | begin training epoch 168
2022-12-09 20:57:23 | INFO | train_inner | epoch 168:     79 / 1102 loss=6.756, nll_loss=2.919, ppl=7.56, wps=6948.6, ups=1.9, wpb=3659.3, bsz=157.2, num_updates=184100, lr=7.3701e-05, gnorm=2.144, loss_scale=2, train_wall=11, gb_free=19.7, wall=3383
2022-12-09 20:57:34 | INFO | train_inner | epoch 168:    179 / 1102 loss=6.796, nll_loss=2.935, ppl=7.65, wps=33143.7, ups=9.13, wpb=3628.6, bsz=132.5, num_updates=184200, lr=7.36809e-05, gnorm=2.182, loss_scale=2, train_wall=11, gb_free=19.3, wall=3394
2022-12-09 20:57:44 | INFO | train_inner | epoch 168:    279 / 1102 loss=6.768, nll_loss=2.897, ppl=7.45, wps=32769.1, ups=9.24, wpb=3546.2, bsz=138.5, num_updates=184300, lr=7.3661e-05, gnorm=2.25, loss_scale=2, train_wall=11, gb_free=19.9, wall=3405
2022-12-09 20:57:55 | INFO | train_inner | epoch 168:    379 / 1102 loss=6.854, nll_loss=2.985, ppl=7.92, wps=32358.8, ups=9.21, wpb=3511.7, bsz=129, num_updates=184400, lr=7.3641e-05, gnorm=2.364, loss_scale=2, train_wall=11, gb_free=19.8, wall=3416
2022-12-09 20:58:06 | INFO | train_inner | epoch 168:    479 / 1102 loss=6.767, nll_loss=2.913, ppl=7.53, wps=32452.3, ups=9.04, wpb=3588.2, bsz=146.7, num_updates=184500, lr=7.3621e-05, gnorm=2.25, loss_scale=2, train_wall=11, gb_free=19.7, wall=3427
2022-12-09 20:58:17 | INFO | train_inner | epoch 168:    579 / 1102 loss=6.684, nll_loss=2.836, ppl=7.14, wps=32756.6, ups=9.2, wpb=3558.9, bsz=156.6, num_updates=184600, lr=7.36011e-05, gnorm=2.188, loss_scale=2, train_wall=11, gb_free=19.3, wall=3438
2022-12-09 20:58:28 | INFO | train_inner | epoch 168:    679 / 1102 loss=6.817, nll_loss=2.967, ppl=7.82, wps=32508.7, ups=9.08, wpb=3581.7, bsz=150.6, num_updates=184700, lr=7.35811e-05, gnorm=2.239, loss_scale=2, train_wall=11, gb_free=19.5, wall=3449
2022-12-09 20:58:39 | INFO | train_inner | epoch 168:    779 / 1102 loss=6.807, nll_loss=2.94, ppl=7.68, wps=32652.3, ups=9.24, wpb=3533.5, bsz=140.4, num_updates=184800, lr=7.35612e-05, gnorm=2.268, loss_scale=2, train_wall=11, gb_free=19.4, wall=3460
2022-12-09 20:58:50 | INFO | train_inner | epoch 168:    879 / 1102 loss=6.83, nll_loss=2.987, ppl=7.93, wps=33623.7, ups=9.17, wpb=3667.5, bsz=148.4, num_updates=184900, lr=7.35413e-05, gnorm=2.189, loss_scale=2, train_wall=11, gb_free=19.5, wall=3471
2022-12-09 20:59:01 | INFO | train_inner | epoch 168:    979 / 1102 loss=6.847, nll_loss=2.986, ppl=7.92, wps=32947.2, ups=9.3, wpb=3544.5, bsz=138, num_updates=185000, lr=7.35215e-05, gnorm=2.258, loss_scale=2, train_wall=11, gb_free=19.6, wall=3481
2022-12-09 20:59:12 | INFO | train_inner | epoch 168:   1079 / 1102 loss=6.731, nll_loss=2.89, ppl=7.41, wps=32826.9, ups=9.15, wpb=3589, bsz=158.9, num_updates=185100, lr=7.35016e-05, gnorm=2.232, loss_scale=2, train_wall=11, gb_free=19.3, wall=3492
2022-12-09 20:59:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 20:59:53 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 3.662 | nll_loss 2.122 | ppl 4.35 | bleu 37.38 | wps 4615.3 | wpb 2835.3 | bsz 115.6 | num_updates 185123 | best_bleu 37.58
2022-12-09 20:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 185123 updates
2022-12-09 20:59:54 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint168.pt
2022-12-09 20:59:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint168.pt (epoch 168 @ 185123 updates, score 37.38) (writing took 1.2912774085998535 seconds)
2022-12-09 20:59:55 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-12-09 20:59:55 | INFO | train | epoch 168 | loss 6.786 | nll_loss 2.932 | ppl 7.63 | wps 24533.6 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 185123 | lr 7.3497e-05 | gnorm 2.233 | loss_scale 2 | train_wall 117 | gb_free 19.2 | wall 3535
2022-12-09 20:59:55 | INFO | fairseq.trainer | begin training epoch 169
2022-12-09 21:00:03 | INFO | train_inner | epoch 169:     77 / 1102 loss=6.784, nll_loss=2.925, ppl=7.59, wps=6967.2, ups=1.93, wpb=3603.1, bsz=136.6, num_updates=185200, lr=7.34818e-05, gnorm=2.302, loss_scale=2, train_wall=11, gb_free=19.7, wall=3544
2022-12-09 21:00:14 | INFO | train_inner | epoch 169:    177 / 1102 loss=6.771, nll_loss=2.918, ppl=7.56, wps=33116.5, ups=9.25, wpb=3580.4, bsz=148.4, num_updates=185300, lr=7.34619e-05, gnorm=2.244, loss_scale=2, train_wall=11, gb_free=19.4, wall=3555
2022-12-09 21:00:25 | INFO | train_inner | epoch 169:    277 / 1102 loss=6.795, nll_loss=2.925, ppl=7.6, wps=32552.8, ups=9.21, wpb=3532.8, bsz=142.5, num_updates=185400, lr=7.34421e-05, gnorm=2.258, loss_scale=2, train_wall=11, gb_free=19.4, wall=3566
2022-12-09 21:00:36 | INFO | train_inner | epoch 169:    377 / 1102 loss=6.723, nll_loss=2.884, ppl=7.38, wps=33471.1, ups=9.15, wpb=3658.7, bsz=158.4, num_updates=185500, lr=7.34223e-05, gnorm=2.14, loss_scale=2, train_wall=11, gb_free=19.6, wall=3577
2022-12-09 21:00:47 | INFO | train_inner | epoch 169:    477 / 1102 loss=6.695, nll_loss=2.869, ppl=7.31, wps=33405.3, ups=9.2, wpb=3632.4, bsz=174.2, num_updates=185600, lr=7.34025e-05, gnorm=2.124, loss_scale=2, train_wall=11, gb_free=19.3, wall=3587
2022-12-09 21:00:58 | INFO | train_inner | epoch 169:    577 / 1102 loss=6.776, nll_loss=2.907, ppl=7.5, wps=33097.4, ups=9.32, wpb=3551.5, bsz=140.7, num_updates=185700, lr=7.33828e-05, gnorm=2.276, loss_scale=2, train_wall=10, gb_free=19.2, wall=3598
2022-12-09 21:01:08 | INFO | train_inner | epoch 169:    677 / 1102 loss=6.774, nll_loss=2.916, ppl=7.55, wps=32839.9, ups=9.23, wpb=3558.7, bsz=137.1, num_updates=185800, lr=7.3363e-05, gnorm=2.278, loss_scale=2, train_wall=11, gb_free=19.7, wall=3609
2022-12-09 21:01:19 | INFO | train_inner | epoch 169:    777 / 1102 loss=6.832, nll_loss=2.952, ppl=7.74, wps=32757.7, ups=9.31, wpb=3519.8, bsz=127.9, num_updates=185900, lr=7.33433e-05, gnorm=2.341, loss_scale=2, train_wall=11, gb_free=19.7, wall=3620
2022-12-09 21:01:30 | INFO | train_inner | epoch 169:    877 / 1102 loss=6.775, nll_loss=2.925, ppl=7.59, wps=33313.8, ups=9.21, wpb=3617.3, bsz=145.6, num_updates=186000, lr=7.33236e-05, gnorm=2.167, loss_scale=2, train_wall=11, gb_free=19.4, wall=3631
2022-12-09 21:01:41 | INFO | train_inner | epoch 169:    977 / 1102 loss=6.79, nll_loss=2.946, ppl=7.71, wps=33098.2, ups=9.24, wpb=3581.2, bsz=149.7, num_updates=186100, lr=7.33039e-05, gnorm=2.187, loss_scale=2, train_wall=11, gb_free=19.6, wall=3641
2022-12-09 21:01:52 | INFO | train_inner | epoch 169:   1077 / 1102 loss=6.888, nll_loss=3.038, ppl=8.21, wps=33342.4, ups=9.34, wpb=3570.4, bsz=140.6, num_updates=186200, lr=7.32842e-05, gnorm=2.19, loss_scale=2, train_wall=10, gb_free=19.6, wall=3652
2022-12-09 21:01:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:02:34 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 3.66 | nll_loss 2.12 | ppl 4.35 | bleu 37.48 | wps 4591 | wpb 2835.3 | bsz 115.6 | num_updates 186225 | best_bleu 37.58
2022-12-09 21:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 186225 updates
2022-12-09 21:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint169.pt
2022-12-09 21:02:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint169.pt (epoch 169 @ 186225 updates, score 37.48) (writing took 1.2539216987788677 seconds)
2022-12-09 21:02:35 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-12-09 21:02:35 | INFO | train | epoch 169 | loss 6.783 | nll_loss 2.929 | ppl 7.62 | wps 24672.9 | ups 6.88 | wpb 3583.6 | bsz 145.4 | num_updates 186225 | lr 7.32792e-05 | gnorm 2.227 | loss_scale 2 | train_wall 117 | gb_free 19.5 | wall 3695
2022-12-09 21:02:35 | INFO | fairseq.trainer | begin training epoch 170
2022-12-09 21:02:43 | INFO | train_inner | epoch 170:     75 / 1102 loss=6.851, nll_loss=2.982, ppl=7.9, wps=6990.3, ups=1.93, wpb=3617.4, bsz=127.7, num_updates=186300, lr=7.32645e-05, gnorm=2.219, loss_scale=2, train_wall=11, gb_free=19.6, wall=3704
2022-12-09 21:02:54 | INFO | train_inner | epoch 170:    175 / 1102 loss=6.749, nll_loss=2.901, ppl=7.47, wps=33269, ups=9.24, wpb=3599.7, bsz=143.4, num_updates=186400, lr=7.32448e-05, gnorm=2.197, loss_scale=2, train_wall=11, gb_free=19.9, wall=3715
2022-12-09 21:03:05 | INFO | train_inner | epoch 170:    275 / 1102 loss=6.687, nll_loss=2.837, ppl=7.14, wps=32972.8, ups=9.19, wpb=3586.1, bsz=158.9, num_updates=186500, lr=7.32252e-05, gnorm=2.188, loss_scale=2, train_wall=11, gb_free=19.4, wall=3726
2022-12-09 21:03:16 | INFO | train_inner | epoch 170:    375 / 1102 loss=6.768, nll_loss=2.922, ppl=7.58, wps=33247.5, ups=9.13, wpb=3639.7, bsz=151.1, num_updates=186600, lr=7.32056e-05, gnorm=2.155, loss_scale=2, train_wall=11, gb_free=19.4, wall=3737
2022-12-09 21:03:27 | INFO | train_inner | epoch 170:    475 / 1102 loss=6.79, nll_loss=2.919, ppl=7.56, wps=32996, ups=9.29, wpb=3550.5, bsz=141.1, num_updates=186700, lr=7.3186e-05, gnorm=2.257, loss_scale=2, train_wall=11, gb_free=19.4, wall=3747
2022-12-09 21:03:38 | INFO | train_inner | epoch 170:    575 / 1102 loss=6.753, nll_loss=2.904, ppl=7.48, wps=32792.1, ups=9.18, wpb=3571.9, bsz=152.3, num_updates=186800, lr=7.31664e-05, gnorm=2.256, loss_scale=2, train_wall=11, gb_free=19.3, wall=3758
2022-12-09 21:03:48 | INFO | train_inner | epoch 170:    675 / 1102 loss=6.748, nll_loss=2.887, ppl=7.4, wps=32925.8, ups=9.23, wpb=3569.1, bsz=145.4, num_updates=186900, lr=7.31468e-05, gnorm=2.388, loss_scale=2, train_wall=11, gb_free=19.5, wall=3769
2022-12-09 21:03:59 | INFO | train_inner | epoch 170:    775 / 1102 loss=6.853, nll_loss=2.987, ppl=7.93, wps=33285.4, ups=9.26, wpb=3593.3, bsz=131.3, num_updates=187000, lr=7.31272e-05, gnorm=2.273, loss_scale=2, train_wall=11, gb_free=19.4, wall=3780
2022-12-09 21:04:10 | INFO | train_inner | epoch 170:    875 / 1102 loss=6.794, nll_loss=2.949, ppl=7.72, wps=33084.5, ups=9.26, wpb=3573.8, bsz=154.9, num_updates=187100, lr=7.31077e-05, gnorm=2.24, loss_scale=2, train_wall=11, gb_free=19.2, wall=3791
2022-12-09 21:04:21 | INFO | train_inner | epoch 170:    975 / 1102 loss=6.822, nll_loss=2.964, ppl=7.8, wps=32916.9, ups=9.3, wpb=3540.6, bsz=144.2, num_updates=187200, lr=7.30882e-05, gnorm=2.268, loss_scale=2, train_wall=11, gb_free=19.3, wall=3801
2022-12-09 21:04:32 | INFO | train_inner | epoch 170:   1075 / 1102 loss=6.815, nll_loss=2.971, ppl=7.84, wps=33611.3, ups=9.21, wpb=3649.7, bsz=143.9, num_updates=187300, lr=7.30687e-05, gnorm=2.166, loss_scale=2, train_wall=11, gb_free=19.5, wall=3812
2022-12-09 21:04:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:05:14 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 3.664 | nll_loss 2.121 | ppl 4.35 | bleu 37.47 | wps 4627 | wpb 2835.3 | bsz 115.6 | num_updates 187327 | best_bleu 37.58
2022-12-09 21:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 187327 updates
2022-12-09 21:05:14 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint170.pt
2022-12-09 21:05:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint170.pt (epoch 170 @ 187327 updates, score 37.47) (writing took 1.1373131005093455 seconds)
2022-12-09 21:05:15 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-12-09 21:05:15 | INFO | train | epoch 170 | loss 6.781 | nll_loss 2.926 | ppl 7.6 | wps 24680.8 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 187327 | lr 7.30634e-05 | gnorm 2.241 | loss_scale 2 | train_wall 117 | gb_free 19.7 | wall 3855
2022-12-09 21:05:15 | INFO | fairseq.trainer | begin training epoch 171
2022-12-09 21:05:23 | INFO | train_inner | epoch 171:     73 / 1102 loss=6.733, nll_loss=2.873, ppl=7.33, wps=6864.9, ups=1.95, wpb=3526.3, bsz=141.6, num_updates=187400, lr=7.30492e-05, gnorm=2.237, loss_scale=2, train_wall=11, gb_free=19.3, wall=3864
2022-12-09 21:05:34 | INFO | train_inner | epoch 171:    173 / 1102 loss=6.816, nll_loss=2.954, ppl=7.75, wps=32804.8, ups=9.17, wpb=3577.3, bsz=139.1, num_updates=187500, lr=7.30297e-05, gnorm=2.271, loss_scale=2, train_wall=11, gb_free=19.2, wall=3875
2022-12-09 21:05:45 | INFO | train_inner | epoch 171:    273 / 1102 loss=6.684, nll_loss=2.846, ppl=7.19, wps=32892.5, ups=9.03, wpb=3642.8, bsz=162.1, num_updates=187600, lr=7.30102e-05, gnorm=2.109, loss_scale=2, train_wall=11, gb_free=19.8, wall=3886
2022-12-09 21:05:56 | INFO | train_inner | epoch 171:    373 / 1102 loss=6.73, nll_loss=2.865, ppl=7.28, wps=32520.8, ups=9.12, wpb=3566.2, bsz=138.6, num_updates=187700, lr=7.29908e-05, gnorm=2.223, loss_scale=2, train_wall=11, gb_free=19.5, wall=3897
2022-12-09 21:06:07 | INFO | train_inner | epoch 171:    473 / 1102 loss=6.831, nll_loss=2.964, ppl=7.8, wps=32730.8, ups=9.14, wpb=3580.7, bsz=133.5, num_updates=187800, lr=7.29713e-05, gnorm=2.341, loss_scale=2, train_wall=11, gb_free=19.4, wall=3908
2022-12-09 21:06:18 | INFO | train_inner | epoch 171:    573 / 1102 loss=6.817, nll_loss=2.964, ppl=7.8, wps=33409.4, ups=9.13, wpb=3660.5, bsz=137.8, num_updates=187900, lr=7.29519e-05, gnorm=2.199, loss_scale=2, train_wall=11, gb_free=19.8, wall=3919
2022-12-09 21:06:29 | INFO | train_inner | epoch 171:    673 / 1102 loss=6.771, nll_loss=2.922, ppl=7.58, wps=32176.4, ups=9.12, wpb=3527.6, bsz=156.6, num_updates=188000, lr=7.29325e-05, gnorm=2.288, loss_scale=2, train_wall=11, gb_free=19.5, wall=3929
2022-12-09 21:06:40 | INFO | train_inner | epoch 171:    773 / 1102 loss=6.8, nll_loss=2.933, ppl=7.64, wps=32232.4, ups=9.17, wpb=3515.6, bsz=139.1, num_updates=188100, lr=7.29131e-05, gnorm=2.329, loss_scale=2, train_wall=11, gb_free=19.6, wall=3940
2022-12-09 21:06:51 | INFO | train_inner | epoch 171:    873 / 1102 loss=6.75, nll_loss=2.904, ppl=7.48, wps=32655.5, ups=9.2, wpb=3549.7, bsz=161.8, num_updates=188200, lr=7.28937e-05, gnorm=2.243, loss_scale=2, train_wall=11, gb_free=19.5, wall=3951
2022-12-09 21:07:02 | INFO | train_inner | epoch 171:    973 / 1102 loss=6.836, nll_loss=2.983, ppl=7.91, wps=32346.8, ups=9.04, wpb=3579.8, bsz=144.8, num_updates=188300, lr=7.28744e-05, gnorm=2.235, loss_scale=2, train_wall=11, gb_free=19.5, wall=3962
2022-12-09 21:07:13 | INFO | train_inner | epoch 171:   1073 / 1102 loss=6.788, nll_loss=2.933, ppl=7.64, wps=33109.2, ups=9.12, wpb=3631.8, bsz=150.2, num_updates=188400, lr=7.2855e-05, gnorm=2.268, loss_scale=2, train_wall=11, gb_free=19.2, wall=3973
2022-12-09 21:07:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:07:55 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 3.667 | nll_loss 2.124 | ppl 4.36 | bleu 37.29 | wps 4596 | wpb 2835.3 | bsz 115.6 | num_updates 188429 | best_bleu 37.58
2022-12-09 21:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 188429 updates
2022-12-09 21:07:56 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint171.pt
2022-12-09 21:07:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint171.pt (epoch 171 @ 188429 updates, score 37.29) (writing took 1.1039861608296633 seconds)
2022-12-09 21:07:56 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-12-09 21:07:56 | INFO | train | epoch 171 | loss 6.779 | nll_loss 2.922 | ppl 7.58 | wps 24480.2 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 188429 | lr 7.28494e-05 | gnorm 2.249 | loss_scale 2 | train_wall 118 | gb_free 19.3 | wall 4017
2022-12-09 21:07:56 | INFO | fairseq.trainer | begin training epoch 172
2022-12-09 21:08:04 | INFO | train_inner | epoch 172:     71 / 1102 loss=6.733, nll_loss=2.888, ppl=7.4, wps=7059.9, ups=1.95, wpb=3625.6, bsz=160.2, num_updates=188500, lr=7.28357e-05, gnorm=2.246, loss_scale=2, train_wall=10, gb_free=19.4, wall=4025
2022-12-09 21:08:15 | INFO | train_inner | epoch 172:    171 / 1102 loss=6.74, nll_loss=2.875, ppl=7.33, wps=32453.2, ups=9.15, wpb=3545, bsz=133.3, num_updates=188600, lr=7.28164e-05, gnorm=2.194, loss_scale=2, train_wall=11, gb_free=19.5, wall=4036
2022-12-09 21:08:26 | INFO | train_inner | epoch 172:    271 / 1102 loss=6.693, nll_loss=2.842, ppl=7.17, wps=32675.1, ups=9.16, wpb=3565.9, bsz=153, num_updates=188700, lr=7.27971e-05, gnorm=2.227, loss_scale=2, train_wall=11, gb_free=19.6, wall=4046
2022-12-09 21:08:37 | INFO | train_inner | epoch 172:    371 / 1102 loss=6.755, nll_loss=2.9, ppl=7.46, wps=33338.6, ups=9.31, wpb=3582, bsz=153.1, num_updates=188800, lr=7.27778e-05, gnorm=2.269, loss_scale=2, train_wall=11, gb_free=20, wall=4057
2022-12-09 21:08:48 | INFO | train_inner | epoch 172:    471 / 1102 loss=6.72, nll_loss=2.87, ppl=7.31, wps=33015.2, ups=9.19, wpb=3591.4, bsz=151.5, num_updates=188900, lr=7.27585e-05, gnorm=2.221, loss_scale=2, train_wall=11, gb_free=19.4, wall=4068
2022-12-09 21:08:58 | INFO | train_inner | epoch 172:    571 / 1102 loss=6.76, nll_loss=2.91, ppl=7.52, wps=33380.1, ups=9.29, wpb=3592.8, bsz=151.9, num_updates=189000, lr=7.27393e-05, gnorm=2.427, loss_scale=2, train_wall=11, gb_free=19.3, wall=4079
2022-12-09 21:09:09 | INFO | train_inner | epoch 172:    671 / 1102 loss=6.799, nll_loss=2.928, ppl=7.61, wps=33012.5, ups=9.25, wpb=3568.8, bsz=134.2, num_updates=189100, lr=7.27201e-05, gnorm=2.379, loss_scale=2, train_wall=11, gb_free=19.4, wall=4090
2022-12-09 21:09:20 | INFO | train_inner | epoch 172:    771 / 1102 loss=6.853, nll_loss=2.996, ppl=7.98, wps=33182.7, ups=9.12, wpb=3640.3, bsz=140.3, num_updates=189200, lr=7.27008e-05, gnorm=2.284, loss_scale=4, train_wall=11, gb_free=19.2, wall=4101
2022-12-09 21:09:31 | INFO | train_inner | epoch 172:    871 / 1102 loss=6.777, nll_loss=2.922, ppl=7.58, wps=32414.1, ups=9.22, wpb=3516.9, bsz=143.4, num_updates=189300, lr=7.26816e-05, gnorm=2.22, loss_scale=4, train_wall=11, gb_free=19.6, wall=4111
2022-12-09 21:09:42 | INFO | train_inner | epoch 172:    971 / 1102 loss=6.888, nll_loss=3.022, ppl=8.13, wps=32824.6, ups=9.07, wpb=3617.2, bsz=132.8, num_updates=189400, lr=7.26624e-05, gnorm=2.325, loss_scale=4, train_wall=11, gb_free=19.4, wall=4123
2022-12-09 21:09:53 | INFO | train_inner | epoch 172:   1071 / 1102 loss=6.788, nll_loss=2.95, ppl=7.73, wps=32795.2, ups=9.11, wpb=3600.9, bsz=151, num_updates=189500, lr=7.26433e-05, gnorm=2.206, loss_scale=4, train_wall=11, gb_free=19.4, wall=4133
2022-12-09 21:09:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:10:36 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 3.661 | nll_loss 2.12 | ppl 4.35 | bleu 37.41 | wps 4549.9 | wpb 2835.3 | bsz 115.6 | num_updates 189531 | best_bleu 37.58
2022-12-09 21:10:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 189531 updates
2022-12-09 21:10:37 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint172.pt
2022-12-09 21:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint172.pt (epoch 172 @ 189531 updates, score 37.41) (writing took 1.0822250517085195 seconds)
2022-12-09 21:10:37 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-12-09 21:10:37 | INFO | train | epoch 172 | loss 6.776 | nll_loss 2.921 | ppl 7.57 | wps 24547.7 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 189531 | lr 7.26373e-05 | gnorm 2.275 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 4178
2022-12-09 21:10:37 | INFO | fairseq.trainer | begin training epoch 173
2022-12-09 21:10:45 | INFO | train_inner | epoch 173:     69 / 1102 loss=6.774, nll_loss=2.914, ppl=7.53, wps=6887.9, ups=1.92, wpb=3581.5, bsz=141, num_updates=189600, lr=7.26241e-05, gnorm=2.248, loss_scale=4, train_wall=11, gb_free=19.4, wall=4185
2022-12-09 21:10:56 | INFO | train_inner | epoch 173:    169 / 1102 loss=6.735, nll_loss=2.878, ppl=7.35, wps=32749.3, ups=9.18, wpb=3566.1, bsz=147.8, num_updates=189700, lr=7.2605e-05, gnorm=2.255, loss_scale=4, train_wall=11, gb_free=19.4, wall=4196
2022-12-09 21:11:07 | INFO | train_inner | epoch 173:    269 / 1102 loss=6.767, nll_loss=2.912, ppl=7.53, wps=32882, ups=9.11, wpb=3608.3, bsz=137.6, num_updates=189800, lr=7.25858e-05, gnorm=2.165, loss_scale=4, train_wall=11, gb_free=19.6, wall=4207
2022-12-09 21:11:18 | INFO | train_inner | epoch 173:    369 / 1102 loss=6.771, nll_loss=2.902, ppl=7.47, wps=32153, ups=9.13, wpb=3521.4, bsz=131.4, num_updates=189900, lr=7.25667e-05, gnorm=2.3, loss_scale=4, train_wall=11, gb_free=19.4, wall=4218
2022-12-09 21:11:29 | INFO | train_inner | epoch 173:    469 / 1102 loss=6.693, nll_loss=2.831, ppl=7.12, wps=32135.4, ups=9.12, wpb=3524.6, bsz=155, num_updates=190000, lr=7.25476e-05, gnorm=2.321, loss_scale=4, train_wall=11, gb_free=19.5, wall=4229
2022-12-09 21:11:40 | INFO | train_inner | epoch 173:    569 / 1102 loss=6.765, nll_loss=2.899, ppl=7.46, wps=32631.2, ups=9.13, wpb=3575.6, bsz=140.3, num_updates=190100, lr=7.25285e-05, gnorm=2.248, loss_scale=4, train_wall=11, gb_free=19.4, wall=4240
2022-12-09 21:11:51 | INFO | train_inner | epoch 173:    669 / 1102 loss=6.773, nll_loss=2.932, ppl=7.63, wps=32713.7, ups=9.14, wpb=3580.6, bsz=150.8, num_updates=190200, lr=7.25095e-05, gnorm=2.173, loss_scale=4, train_wall=11, gb_free=20, wall=4251
2022-12-09 21:12:02 | INFO | train_inner | epoch 173:    769 / 1102 loss=6.796, nll_loss=2.932, ppl=7.63, wps=32379.4, ups=9.06, wpb=3572.6, bsz=139, num_updates=190300, lr=7.24904e-05, gnorm=2.35, loss_scale=4, train_wall=11, gb_free=19.5, wall=4262
2022-12-09 21:12:13 | INFO | train_inner | epoch 173:    869 / 1102 loss=6.767, nll_loss=2.932, ppl=7.63, wps=33135.7, ups=9.08, wpb=3647.7, bsz=166.2, num_updates=190400, lr=7.24714e-05, gnorm=2.153, loss_scale=4, train_wall=11, gb_free=19.3, wall=4273
2022-12-09 21:12:24 | INFO | train_inner | epoch 173:    969 / 1102 loss=6.752, nll_loss=2.912, ppl=7.52, wps=32866.7, ups=9.11, wpb=3609.4, bsz=160.7, num_updates=190500, lr=7.24524e-05, gnorm=2.245, loss_scale=4, train_wall=11, gb_free=20, wall=4284
2022-12-09 21:12:35 | INFO | train_inner | epoch 173:   1069 / 1102 loss=6.896, nll_loss=3.026, ppl=8.15, wps=32760.8, ups=9.06, wpb=3615.2, bsz=130.4, num_updates=190600, lr=7.24333e-05, gnorm=2.326, loss_scale=4, train_wall=11, gb_free=19.6, wall=4295
2022-12-09 21:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:13:20 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 3.659 | nll_loss 2.117 | ppl 4.34 | bleu 37.54 | wps 4337.6 | wpb 2835.3 | bsz 115.6 | num_updates 190633 | best_bleu 37.58
2022-12-09 21:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 190633 updates
2022-12-09 21:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint173.pt
2022-12-09 21:13:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint173.pt (epoch 173 @ 190633 updates, score 37.54) (writing took 1.1209045397117734 seconds)
2022-12-09 21:13:21 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-12-09 21:13:21 | INFO | train | epoch 173 | loss 6.773 | nll_loss 2.917 | ppl 7.55 | wps 24073.1 | ups 6.72 | wpb 3583.6 | bsz 145.4 | num_updates 190633 | lr 7.24271e-05 | gnorm 2.251 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 4342
2022-12-09 21:13:21 | INFO | fairseq.trainer | begin training epoch 174
2022-12-09 21:13:29 | INFO | train_inner | epoch 174:     67 / 1102 loss=6.7, nll_loss=2.864, ppl=7.28, wps=6793.9, ups=1.85, wpb=3673.5, bsz=161.7, num_updates=190700, lr=7.24144e-05, gnorm=2.264, loss_scale=4, train_wall=11, gb_free=19.6, wall=4349
2022-12-09 21:13:40 | INFO | train_inner | epoch 174:    167 / 1102 loss=6.699, nll_loss=2.835, ppl=7.14, wps=33082.7, ups=9.26, wpb=3571.1, bsz=147.6, num_updates=190800, lr=7.23954e-05, gnorm=2.274, loss_scale=4, train_wall=11, gb_free=19.4, wall=4360
2022-12-09 21:13:50 | INFO | train_inner | epoch 174:    267 / 1102 loss=6.716, nll_loss=2.848, ppl=7.2, wps=32519.7, ups=9.26, wpb=3513.6, bsz=143.7, num_updates=190900, lr=7.23764e-05, gnorm=2.339, loss_scale=4, train_wall=11, gb_free=19.8, wall=4371
2022-12-09 21:14:01 | INFO | train_inner | epoch 174:    367 / 1102 loss=6.757, nll_loss=2.912, ppl=7.52, wps=33270.1, ups=9.3, wpb=3578.8, bsz=152.4, num_updates=191000, lr=7.23575e-05, gnorm=2.252, loss_scale=4, train_wall=11, gb_free=19.6, wall=4382
2022-12-09 21:14:12 | INFO | train_inner | epoch 174:    467 / 1102 loss=6.803, nll_loss=2.937, ppl=7.66, wps=32974.1, ups=9.2, wpb=3586, bsz=138.5, num_updates=191100, lr=7.23385e-05, gnorm=2.27, loss_scale=4, train_wall=11, gb_free=19.5, wall=4393
2022-12-09 21:14:23 | INFO | train_inner | epoch 174:    567 / 1102 loss=6.767, nll_loss=2.915, ppl=7.54, wps=33531.8, ups=9.28, wpb=3613.8, bsz=148.4, num_updates=191200, lr=7.23196e-05, gnorm=2.222, loss_scale=4, train_wall=11, gb_free=19.4, wall=4403
2022-12-09 21:14:34 | INFO | train_inner | epoch 174:    667 / 1102 loss=6.811, nll_loss=2.956, ppl=7.76, wps=33202.1, ups=9.14, wpb=3633.3, bsz=137.1, num_updates=191300, lr=7.23007e-05, gnorm=2.257, loss_scale=4, train_wall=11, gb_free=19.7, wall=4414
2022-12-09 21:14:45 | INFO | train_inner | epoch 174:    767 / 1102 loss=6.791, nll_loss=2.932, ppl=7.63, wps=32935.1, ups=9.23, wpb=3568.3, bsz=146.4, num_updates=191400, lr=7.22818e-05, gnorm=2.288, loss_scale=4, train_wall=11, gb_free=19.3, wall=4425
2022-12-09 21:14:55 | INFO | train_inner | epoch 174:    867 / 1102 loss=6.904, nll_loss=3.044, ppl=8.25, wps=33020.3, ups=9.23, wpb=3577.4, bsz=137.3, num_updates=191500, lr=7.22629e-05, gnorm=2.326, loss_scale=4, train_wall=11, gb_free=19.5, wall=4436
2022-12-09 21:15:06 | INFO | train_inner | epoch 174:    967 / 1102 loss=6.78, nll_loss=2.919, ppl=7.56, wps=33100.7, ups=9.27, wpb=3572.2, bsz=141.4, num_updates=191600, lr=7.22441e-05, gnorm=2.268, loss_scale=4, train_wall=11, gb_free=19.4, wall=4447
2022-12-09 21:15:17 | INFO | train_inner | epoch 174:   1067 / 1102 loss=6.814, nll_loss=2.947, ppl=7.71, wps=32986.5, ups=9.25, wpb=3567.8, bsz=136.4, num_updates=191700, lr=7.22252e-05, gnorm=2.291, loss_scale=4, train_wall=11, gb_free=19.3, wall=4458
2022-12-09 21:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:15:59 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 3.658 | nll_loss 2.118 | ppl 4.34 | bleu 37.38 | wps 4675.8 | wpb 2835.3 | bsz 115.6 | num_updates 191735 | best_bleu 37.58
2022-12-09 21:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 191735 updates
2022-12-09 21:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint174.pt
2022-12-09 21:16:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint174.pt (epoch 174 @ 191735 updates, score 37.38) (writing took 1.1226694993674755 seconds)
2022-12-09 21:16:00 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-12-09 21:16:00 | INFO | train | epoch 174 | loss 6.772 | nll_loss 2.915 | ppl 7.54 | wps 24781.9 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 191735 | lr 7.22186e-05 | gnorm 2.276 | loss_scale 4 | train_wall 117 | gb_free 19.8 | wall 4501
2022-12-09 21:16:01 | INFO | fairseq.trainer | begin training epoch 175
2022-12-09 21:16:08 | INFO | train_inner | epoch 175:     65 / 1102 loss=6.776, nll_loss=2.898, ppl=7.46, wps=6892.9, ups=1.97, wpb=3503.9, bsz=134, num_updates=191800, lr=7.22064e-05, gnorm=2.422, loss_scale=4, train_wall=11, gb_free=19.8, wall=4508
2022-12-09 21:16:19 | INFO | train_inner | epoch 175:    165 / 1102 loss=6.719, nll_loss=2.874, ppl=7.33, wps=33106.1, ups=9.1, wpb=3637.7, bsz=151.4, num_updates=191900, lr=7.21876e-05, gnorm=2.143, loss_scale=4, train_wall=11, gb_free=19.3, wall=4519
2022-12-09 21:16:30 | INFO | train_inner | epoch 175:    265 / 1102 loss=6.81, nll_loss=2.923, ppl=7.59, wps=32723.9, ups=9.17, wpb=3568.8, bsz=122.8, num_updates=192000, lr=7.21688e-05, gnorm=2.497, loss_scale=4, train_wall=11, gb_free=19.9, wall=4530
2022-12-09 21:16:41 | INFO | train_inner | epoch 175:    365 / 1102 loss=6.725, nll_loss=2.874, ppl=7.33, wps=32377.9, ups=9.21, wpb=3513.7, bsz=162.2, num_updates=192100, lr=7.215e-05, gnorm=2.269, loss_scale=4, train_wall=11, gb_free=19.6, wall=4541
2022-12-09 21:16:51 | INFO | train_inner | epoch 175:    465 / 1102 loss=6.783, nll_loss=2.921, ppl=7.57, wps=33302.6, ups=9.21, wpb=3614.9, bsz=143.7, num_updates=192200, lr=7.21312e-05, gnorm=2.247, loss_scale=4, train_wall=11, gb_free=19.6, wall=4552
2022-12-09 21:17:02 | INFO | train_inner | epoch 175:    565 / 1102 loss=6.771, nll_loss=2.917, ppl=7.55, wps=33075.8, ups=9.23, wpb=3584, bsz=144.5, num_updates=192300, lr=7.21125e-05, gnorm=2.21, loss_scale=4, train_wall=11, gb_free=19.4, wall=4563
2022-12-09 21:17:13 | INFO | train_inner | epoch 175:    665 / 1102 loss=6.754, nll_loss=2.902, ppl=7.48, wps=33809.7, ups=9.27, wpb=3649, bsz=144.7, num_updates=192400, lr=7.20937e-05, gnorm=2.204, loss_scale=4, train_wall=11, gb_free=19.7, wall=4574
2022-12-09 21:17:24 | INFO | train_inner | epoch 175:    765 / 1102 loss=6.789, nll_loss=2.944, ppl=7.69, wps=33072.5, ups=9.1, wpb=3633.5, bsz=149.2, num_updates=192500, lr=7.2075e-05, gnorm=2.354, loss_scale=4, train_wall=11, gb_free=19.5, wall=4585
2022-12-09 21:17:35 | INFO | train_inner | epoch 175:    865 / 1102 loss=6.762, nll_loss=2.915, ppl=7.54, wps=32646.6, ups=9.04, wpb=3611.8, bsz=149.7, num_updates=192600, lr=7.20563e-05, gnorm=2.184, loss_scale=4, train_wall=11, gb_free=19.3, wall=4596
2022-12-09 21:17:46 | INFO | train_inner | epoch 175:    965 / 1102 loss=6.845, nll_loss=2.979, ppl=7.88, wps=32337.1, ups=9.16, wpb=3530.7, bsz=135.2, num_updates=192700, lr=7.20376e-05, gnorm=2.381, loss_scale=4, train_wall=11, gb_free=19.5, wall=4607
2022-12-09 21:17:57 | INFO | train_inner | epoch 175:   1065 / 1102 loss=6.755, nll_loss=2.902, ppl=7.47, wps=32529.8, ups=9.15, wpb=3556.6, bsz=157.8, num_updates=192800, lr=7.20189e-05, gnorm=2.323, loss_scale=4, train_wall=11, gb_free=19.3, wall=4618
2022-12-09 21:18:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:18:39 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 3.662 | nll_loss 2.122 | ppl 4.35 | bleu 37.45 | wps 4728.4 | wpb 2835.3 | bsz 115.6 | num_updates 192837 | best_bleu 37.58
2022-12-09 21:18:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 192837 updates
2022-12-09 21:18:40 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint175.pt
2022-12-09 21:18:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint175.pt (epoch 175 @ 192837 updates, score 37.45) (writing took 1.1189663531258702 seconds)
2022-12-09 21:18:40 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-12-09 21:18:40 | INFO | train | epoch 175 | loss 6.769 | nll_loss 2.912 | ppl 7.52 | wps 24707.3 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 192837 | lr 7.2012e-05 | gnorm 2.297 | loss_scale 4 | train_wall 117 | gb_free 19.5 | wall 4661
2022-12-09 21:18:40 | INFO | fairseq.trainer | begin training epoch 176
2022-12-09 21:18:47 | INFO | train_inner | epoch 176:     63 / 1102 loss=6.74, nll_loss=2.876, ppl=7.34, wps=7066.8, ups=1.98, wpb=3566.5, bsz=147.2, num_updates=192900, lr=7.20002e-05, gnorm=2.315, loss_scale=4, train_wall=11, gb_free=19.8, wall=4668
2022-12-09 21:18:58 | INFO | train_inner | epoch 176:    163 / 1102 loss=6.75, nll_loss=2.881, ppl=7.37, wps=33068.8, ups=9.15, wpb=3614.5, bsz=135.5, num_updates=193000, lr=7.19816e-05, gnorm=2.245, loss_scale=4, train_wall=11, gb_free=19.3, wall=4679
2022-12-09 21:19:09 | INFO | train_inner | epoch 176:    263 / 1102 loss=6.655, nll_loss=2.805, ppl=6.99, wps=32589.9, ups=9.17, wpb=3552.2, bsz=154.9, num_updates=193100, lr=7.19629e-05, gnorm=2.212, loss_scale=4, train_wall=11, gb_free=19.7, wall=4690
2022-12-09 21:19:20 | INFO | train_inner | epoch 176:    363 / 1102 loss=6.774, nll_loss=2.908, ppl=7.51, wps=33035.8, ups=9.18, wpb=3600, bsz=144.6, num_updates=193200, lr=7.19443e-05, gnorm=2.271, loss_scale=4, train_wall=11, gb_free=19.6, wall=4701
2022-12-09 21:19:31 | INFO | train_inner | epoch 176:    463 / 1102 loss=6.785, nll_loss=2.921, ppl=7.57, wps=33042, ups=9.32, wpb=3547.2, bsz=141.6, num_updates=193300, lr=7.19257e-05, gnorm=2.313, loss_scale=4, train_wall=10, gb_free=19.5, wall=4711
2022-12-09 21:19:42 | INFO | train_inner | epoch 176:    563 / 1102 loss=6.733, nll_loss=2.881, ppl=7.36, wps=32749.6, ups=9.18, wpb=3569.4, bsz=152.8, num_updates=193400, lr=7.19071e-05, gnorm=2.254, loss_scale=4, train_wall=11, gb_free=19.8, wall=4722
2022-12-09 21:19:53 | INFO | train_inner | epoch 176:    663 / 1102 loss=6.765, nll_loss=2.914, ppl=7.54, wps=32921.8, ups=9.14, wpb=3603.1, bsz=148.8, num_updates=193500, lr=7.18885e-05, gnorm=2.195, loss_scale=4, train_wall=11, gb_free=19.5, wall=4733
2022-12-09 21:20:04 | INFO | train_inner | epoch 176:    763 / 1102 loss=6.857, nll_loss=2.981, ppl=7.89, wps=32674.5, ups=9.22, wpb=3545.6, bsz=130.7, num_updates=193600, lr=7.18699e-05, gnorm=2.362, loss_scale=4, train_wall=11, gb_free=19.4, wall=4744
2022-12-09 21:20:14 | INFO | train_inner | epoch 176:    863 / 1102 loss=6.812, nll_loss=2.965, ppl=7.81, wps=33634.8, ups=9.24, wpb=3640.2, bsz=152.9, num_updates=193700, lr=7.18514e-05, gnorm=2.247, loss_scale=4, train_wall=11, gb_free=19.5, wall=4755
2022-12-09 21:20:25 | INFO | train_inner | epoch 176:    963 / 1102 loss=6.754, nll_loss=2.895, ppl=7.44, wps=33126.7, ups=9.18, wpb=3606.7, bsz=145.8, num_updates=193800, lr=7.18329e-05, gnorm=2.315, loss_scale=4, train_wall=11, gb_free=19.5, wall=4766
2022-12-09 21:20:36 | INFO | train_inner | epoch 176:   1063 / 1102 loss=6.794, nll_loss=2.947, ppl=7.71, wps=33338.9, ups=9.18, wpb=3631.2, bsz=142.5, num_updates=193900, lr=7.18143e-05, gnorm=2.251, loss_scale=4, train_wall=11, gb_free=19.8, wall=4777
2022-12-09 21:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:21:22 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 3.666 | nll_loss 2.122 | ppl 4.35 | bleu 37.29 | wps 4336.2 | wpb 2835.3 | bsz 115.6 | num_updates 193939 | best_bleu 37.58
2022-12-09 21:21:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 193939 updates
2022-12-09 21:21:23 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint176.pt
2022-12-09 21:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint176.pt (epoch 176 @ 193939 updates, score 37.29) (writing took 1.1049223793670535 seconds)
2022-12-09 21:21:23 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-12-09 21:21:23 | INFO | train | epoch 176 | loss 6.767 | nll_loss 2.909 | ppl 7.51 | wps 24267.3 | ups 6.77 | wpb 3583.6 | bsz 145.4 | num_updates 193939 | lr 7.18071e-05 | gnorm 2.272 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 4824
2022-12-09 21:21:23 | INFO | fairseq.trainer | begin training epoch 177
2022-12-09 21:21:30 | INFO | train_inner | epoch 177:     61 / 1102 loss=6.787, nll_loss=2.94, ppl=7.67, wps=6612.3, ups=1.86, wpb=3557.4, bsz=150.4, num_updates=194000, lr=7.17958e-05, gnorm=2.278, loss_scale=4, train_wall=11, gb_free=19.5, wall=4831
2022-12-09 21:21:41 | INFO | train_inner | epoch 177:    161 / 1102 loss=6.749, nll_loss=2.889, ppl=7.41, wps=33250.1, ups=9.22, wpb=3608, bsz=147.9, num_updates=194100, lr=7.17773e-05, gnorm=2.207, loss_scale=4, train_wall=11, gb_free=19.7, wall=4841
2022-12-09 21:21:52 | INFO | train_inner | epoch 177:    261 / 1102 loss=6.743, nll_loss=2.865, ppl=7.29, wps=32369.8, ups=9.29, wpb=3483.9, bsz=140.6, num_updates=194200, lr=7.17588e-05, gnorm=2.335, loss_scale=4, train_wall=11, gb_free=19.3, wall=4852
2022-12-09 21:22:02 | INFO | train_inner | epoch 177:    361 / 1102 loss=6.778, nll_loss=2.905, ppl=7.49, wps=32931, ups=9.33, wpb=3529, bsz=133.5, num_updates=194300, lr=7.17404e-05, gnorm=2.296, loss_scale=4, train_wall=10, gb_free=19.7, wall=4863
2022-12-09 21:22:13 | INFO | train_inner | epoch 177:    461 / 1102 loss=6.722, nll_loss=2.861, ppl=7.27, wps=33203.2, ups=9.26, wpb=3584.2, bsz=143.2, num_updates=194400, lr=7.17219e-05, gnorm=2.238, loss_scale=4, train_wall=11, gb_free=19.7, wall=4874
2022-12-09 21:22:24 | INFO | train_inner | epoch 177:    561 / 1102 loss=6.771, nll_loss=2.898, ppl=7.45, wps=33153.3, ups=9.25, wpb=3583.4, bsz=135.1, num_updates=194500, lr=7.17035e-05, gnorm=2.269, loss_scale=4, train_wall=11, gb_free=19.6, wall=4884
2022-12-09 21:22:35 | INFO | train_inner | epoch 177:    661 / 1102 loss=6.812, nll_loss=2.954, ppl=7.75, wps=33524.2, ups=9.22, wpb=3635.1, bsz=142.7, num_updates=194600, lr=7.1685e-05, gnorm=2.284, loss_scale=4, train_wall=11, gb_free=19.5, wall=4895
2022-12-09 21:22:45 | INFO | train_inner | epoch 177:    761 / 1102 loss=6.806, nll_loss=2.946, ppl=7.71, wps=33251.8, ups=9.31, wpb=3572.1, bsz=138.4, num_updates=194700, lr=7.16666e-05, gnorm=2.295, loss_scale=4, train_wall=10, gb_free=19.3, wall=4906
2022-12-09 21:22:56 | INFO | train_inner | epoch 177:    861 / 1102 loss=6.695, nll_loss=2.853, ppl=7.23, wps=33040.1, ups=9.23, wpb=3580.9, bsz=166, num_updates=194800, lr=7.16482e-05, gnorm=2.287, loss_scale=4, train_wall=11, gb_free=20, wall=4917
2022-12-09 21:23:07 | INFO | train_inner | epoch 177:    961 / 1102 loss=6.762, nll_loss=2.925, ppl=7.59, wps=33310.1, ups=9.14, wpb=3645.8, bsz=160.6, num_updates=194900, lr=7.16299e-05, gnorm=2.188, loss_scale=4, train_wall=11, gb_free=19.4, wall=4928
2022-12-09 21:23:18 | INFO | train_inner | epoch 177:   1061 / 1102 loss=6.806, nll_loss=2.944, ppl=7.7, wps=33152.9, ups=9.29, wpb=3569.6, bsz=144.7, num_updates=195000, lr=7.16115e-05, gnorm=2.381, loss_scale=4, train_wall=11, gb_free=19.5, wall=4939
2022-12-09 21:23:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:24:02 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 3.667 | nll_loss 2.123 | ppl 4.36 | bleu 37.37 | wps 4548.3 | wpb 2835.3 | bsz 115.6 | num_updates 195041 | best_bleu 37.58
2022-12-09 21:24:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 195041 updates
2022-12-09 21:24:03 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint177.pt
2022-12-09 21:24:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint177.pt (epoch 177 @ 195041 updates, score 37.37) (writing took 1.1349737802520394 seconds)
2022-12-09 21:24:03 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-12-09 21:24:03 | INFO | train | epoch 177 | loss 6.766 | nll_loss 2.907 | ppl 7.5 | wps 24634.2 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 195041 | lr 7.1604e-05 | gnorm 2.273 | loss_scale 4 | train_wall 117 | gb_free 19.3 | wall 4984
2022-12-09 21:24:03 | INFO | fairseq.trainer | begin training epoch 178
2022-12-09 21:24:10 | INFO | train_inner | epoch 178:     59 / 1102 loss=6.726, nll_loss=2.868, ppl=7.3, wps=6827.5, ups=1.93, wpb=3546.3, bsz=144.8, num_updates=195100, lr=7.15931e-05, gnorm=2.281, loss_scale=4, train_wall=11, gb_free=19.5, wall=4991
2022-12-09 21:24:21 | INFO | train_inner | epoch 178:    159 / 1102 loss=6.716, nll_loss=2.869, ppl=7.3, wps=33591, ups=9.18, wpb=3660.1, bsz=156.9, num_updates=195200, lr=7.15748e-05, gnorm=2.205, loss_scale=4, train_wall=11, gb_free=19.5, wall=5001
2022-12-09 21:24:32 | INFO | train_inner | epoch 178:    259 / 1102 loss=6.715, nll_loss=2.85, ppl=7.21, wps=33186.3, ups=9.34, wpb=3553.6, bsz=146.5, num_updates=195300, lr=7.15565e-05, gnorm=2.245, loss_scale=4, train_wall=10, gb_free=19.5, wall=5012
2022-12-09 21:24:42 | INFO | train_inner | epoch 178:    359 / 1102 loss=6.757, nll_loss=2.887, ppl=7.4, wps=32721.3, ups=9.28, wpb=3526.3, bsz=140.8, num_updates=195400, lr=7.15382e-05, gnorm=2.401, loss_scale=4, train_wall=11, gb_free=19.8, wall=5023
2022-12-09 21:24:53 | INFO | train_inner | epoch 178:    459 / 1102 loss=6.737, nll_loss=2.871, ppl=7.32, wps=33092.7, ups=9.22, wpb=3589.8, bsz=150.2, num_updates=195500, lr=7.15199e-05, gnorm=2.336, loss_scale=4, train_wall=11, gb_free=19.5, wall=5034
2022-12-09 21:25:04 | INFO | train_inner | epoch 178:    559 / 1102 loss=6.859, nll_loss=2.987, ppl=7.93, wps=33420, ups=9.31, wpb=3590.9, bsz=125.3, num_updates=195600, lr=7.15016e-05, gnorm=2.275, loss_scale=4, train_wall=10, gb_free=19.6, wall=5045
2022-12-09 21:25:15 | INFO | train_inner | epoch 178:    659 / 1102 loss=6.742, nll_loss=2.901, ppl=7.47, wps=33532.5, ups=9.26, wpb=3619.3, bsz=158.1, num_updates=195700, lr=7.14833e-05, gnorm=2.209, loss_scale=4, train_wall=11, gb_free=19.6, wall=5055
2022-12-09 21:25:25 | INFO | train_inner | epoch 178:    759 / 1102 loss=6.83, nll_loss=2.98, ppl=7.89, wps=33786.7, ups=9.31, wpb=3628.3, bsz=145.8, num_updates=195800, lr=7.1465e-05, gnorm=2.241, loss_scale=4, train_wall=10, gb_free=19.5, wall=5066
2022-12-09 21:25:36 | INFO | train_inner | epoch 178:    859 / 1102 loss=6.766, nll_loss=2.909, ppl=7.51, wps=33324.3, ups=9.28, wpb=3589.8, bsz=146.6, num_updates=195900, lr=7.14468e-05, gnorm=2.236, loss_scale=4, train_wall=11, gb_free=19.3, wall=5077
2022-12-09 21:25:47 | INFO | train_inner | epoch 178:    959 / 1102 loss=6.796, nll_loss=2.934, ppl=7.64, wps=33346.4, ups=9.35, wpb=3567.1, bsz=135.3, num_updates=196000, lr=7.14286e-05, gnorm=2.319, loss_scale=4, train_wall=10, gb_free=19.3, wall=5088
2022-12-09 21:25:58 | INFO | train_inner | epoch 178:   1059 / 1102 loss=6.734, nll_loss=2.893, ppl=7.43, wps=33428.3, ups=9.32, wpb=3586.2, bsz=155.2, num_updates=196100, lr=7.14104e-05, gnorm=2.181, loss_scale=4, train_wall=11, gb_free=20, wall=5098
2022-12-09 21:26:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:26:40 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 3.663 | nll_loss 2.121 | ppl 4.35 | bleu 37.49 | wps 4777.8 | wpb 2835.3 | bsz 115.6 | num_updates 196143 | best_bleu 37.58
2022-12-09 21:26:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 196143 updates
2022-12-09 21:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint178.pt
2022-12-09 21:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint178.pt (epoch 178 @ 196143 updates, score 37.49) (writing took 1.0429084748029709 seconds)
2022-12-09 21:26:41 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-12-09 21:26:41 | INFO | train | epoch 178 | loss 6.762 | nll_loss 2.904 | ppl 7.48 | wps 25032 | ups 6.99 | wpb 3583.6 | bsz 145.4 | num_updates 196143 | lr 7.14025e-05 | gnorm 2.272 | loss_scale 4 | train_wall 116 | gb_free 19.3 | wall 5142
2022-12-09 21:26:41 | INFO | fairseq.trainer | begin training epoch 179
2022-12-09 21:26:47 | INFO | train_inner | epoch 179:     57 / 1102 loss=6.729, nll_loss=2.861, ppl=7.26, wps=7119.9, ups=2.01, wpb=3541.1, bsz=137.1, num_updates=196200, lr=7.13922e-05, gnorm=2.302, loss_scale=4, train_wall=10, gb_free=19.4, wall=5148
2022-12-09 21:26:58 | INFO | train_inner | epoch 179:    157 / 1102 loss=6.827, nll_loss=2.968, ppl=7.83, wps=33957.9, ups=9.48, wpb=3582, bsz=133.6, num_updates=196300, lr=7.1374e-05, gnorm=2.298, loss_scale=4, train_wall=10, gb_free=19.5, wall=5159
2022-12-09 21:27:09 | INFO | train_inner | epoch 179:    257 / 1102 loss=6.735, nll_loss=2.868, ppl=7.3, wps=33950.2, ups=9.29, wpb=3653, bsz=136.6, num_updates=196400, lr=7.13558e-05, gnorm=2.226, loss_scale=4, train_wall=11, gb_free=19.7, wall=5169
2022-12-09 21:27:19 | INFO | train_inner | epoch 179:    357 / 1102 loss=6.805, nll_loss=2.941, ppl=7.68, wps=33575.9, ups=9.33, wpb=3600.4, bsz=134.8, num_updates=196500, lr=7.13376e-05, gnorm=2.28, loss_scale=4, train_wall=10, gb_free=19.3, wall=5180
2022-12-09 21:27:30 | INFO | train_inner | epoch 179:    457 / 1102 loss=6.721, nll_loss=2.849, ppl=7.2, wps=32785, ups=9.28, wpb=3533, bsz=145.4, num_updates=196600, lr=7.13195e-05, gnorm=2.28, loss_scale=4, train_wall=11, gb_free=19.4, wall=5191
2022-12-09 21:27:41 | INFO | train_inner | epoch 179:    557 / 1102 loss=6.761, nll_loss=2.893, ppl=7.43, wps=32625.2, ups=9.2, wpb=3546.4, bsz=141.3, num_updates=196700, lr=7.13014e-05, gnorm=2.33, loss_scale=4, train_wall=11, gb_free=19.4, wall=5202
2022-12-09 21:27:52 | INFO | train_inner | epoch 179:    657 / 1102 loss=6.772, nll_loss=2.922, ppl=7.58, wps=32913.2, ups=9.21, wpb=3575.2, bsz=153.8, num_updates=196800, lr=7.12832e-05, gnorm=2.268, loss_scale=4, train_wall=11, gb_free=19.5, wall=5213
2022-12-09 21:28:03 | INFO | train_inner | epoch 179:    757 / 1102 loss=6.77, nll_loss=2.911, ppl=7.52, wps=33130.6, ups=9.29, wpb=3568.1, bsz=144.7, num_updates=196900, lr=7.12651e-05, gnorm=2.345, loss_scale=4, train_wall=11, gb_free=19.6, wall=5223
2022-12-09 21:28:14 | INFO | train_inner | epoch 179:    857 / 1102 loss=6.775, nll_loss=2.931, ppl=7.63, wps=33486.5, ups=9.19, wpb=3645.7, bsz=154.2, num_updates=197000, lr=7.1247e-05, gnorm=2.213, loss_scale=4, train_wall=11, gb_free=19.5, wall=5234
2022-12-09 21:28:24 | INFO | train_inner | epoch 179:    957 / 1102 loss=6.768, nll_loss=2.897, ppl=7.45, wps=32531.6, ups=9.25, wpb=3516.6, bsz=146.2, num_updates=197100, lr=7.1229e-05, gnorm=2.324, loss_scale=4, train_wall=11, gb_free=19.6, wall=5245
2022-12-09 21:28:35 | INFO | train_inner | epoch 179:   1057 / 1102 loss=6.725, nll_loss=2.882, ppl=7.37, wps=33070, ups=9.12, wpb=3626.9, bsz=160.8, num_updates=197200, lr=7.12109e-05, gnorm=2.219, loss_scale=4, train_wall=11, gb_free=19.7, wall=5256
2022-12-09 21:28:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:29:17 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 3.665 | nll_loss 2.122 | ppl 4.35 | bleu 37.46 | wps 4924.6 | wpb 2835.3 | bsz 115.6 | num_updates 197245 | best_bleu 37.58
2022-12-09 21:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 197245 updates
2022-12-09 21:29:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint179.pt
2022-12-09 21:29:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint179.pt (epoch 179 @ 197245 updates, score 37.46) (writing took 1.1274062916636467 seconds)
2022-12-09 21:29:18 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-12-09 21:29:18 | INFO | train | epoch 179 | loss 6.759 | nll_loss 2.9 | ppl 7.47 | wps 25148.2 | ups 7.02 | wpb 3583.6 | bsz 145.4 | num_updates 197245 | lr 7.12028e-05 | gnorm 2.274 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 5299
2022-12-09 21:29:18 | INFO | fairseq.trainer | begin training epoch 180
2022-12-09 21:29:24 | INFO | train_inner | epoch 180:     55 / 1102 loss=6.655, nll_loss=2.805, ppl=6.99, wps=7315.1, ups=2.04, wpb=3584, bsz=157.1, num_updates=197300, lr=7.11929e-05, gnorm=2.27, loss_scale=4, train_wall=11, gb_free=19.8, wall=5305
2022-12-09 21:29:35 | INFO | train_inner | epoch 180:    155 / 1102 loss=6.735, nll_loss=2.881, ppl=7.37, wps=33206, ups=9.12, wpb=3640.5, bsz=145.2, num_updates=197400, lr=7.11748e-05, gnorm=2.201, loss_scale=4, train_wall=11, gb_free=19.5, wall=5316
2022-12-09 21:29:46 | INFO | train_inner | epoch 180:    255 / 1102 loss=6.766, nll_loss=2.907, ppl=7.5, wps=33085.1, ups=9.23, wpb=3583.3, bsz=138.6, num_updates=197500, lr=7.11568e-05, gnorm=2.281, loss_scale=4, train_wall=11, gb_free=19.3, wall=5327
2022-12-09 21:29:57 | INFO | train_inner | epoch 180:    355 / 1102 loss=6.788, nll_loss=2.912, ppl=7.52, wps=32712.4, ups=9.21, wpb=3551.7, bsz=133, num_updates=197600, lr=7.11388e-05, gnorm=2.339, loss_scale=4, train_wall=11, gb_free=19.4, wall=5338
2022-12-09 21:30:08 | INFO | train_inner | epoch 180:    455 / 1102 loss=6.725, nll_loss=2.879, ppl=7.36, wps=33063.9, ups=9.07, wpb=3645.1, bsz=161.4, num_updates=197700, lr=7.11208e-05, gnorm=2.246, loss_scale=4, train_wall=11, gb_free=19.7, wall=5349
2022-12-09 21:30:19 | INFO | train_inner | epoch 180:    555 / 1102 loss=6.715, nll_loss=2.843, ppl=7.18, wps=32502.8, ups=9.21, wpb=3528.3, bsz=143, num_updates=197800, lr=7.11028e-05, gnorm=2.356, loss_scale=4, train_wall=11, gb_free=19.6, wall=5360
2022-12-09 21:30:30 | INFO | train_inner | epoch 180:    655 / 1102 loss=6.687, nll_loss=2.838, ppl=7.15, wps=32281.4, ups=9.16, wpb=3523.5, bsz=164.4, num_updates=197900, lr=7.10849e-05, gnorm=2.293, loss_scale=4, train_wall=11, gb_free=19.8, wall=5370
2022-12-09 21:30:41 | INFO | train_inner | epoch 180:    755 / 1102 loss=6.846, nll_loss=2.975, ppl=7.86, wps=33041.7, ups=9.24, wpb=3577, bsz=133, num_updates=198000, lr=7.10669e-05, gnorm=2.453, loss_scale=4, train_wall=11, gb_free=19.4, wall=5381
2022-12-09 21:30:52 | INFO | train_inner | epoch 180:    855 / 1102 loss=6.709, nll_loss=2.861, ppl=7.27, wps=33295.6, ups=9.17, wpb=3630, bsz=156.3, num_updates=198100, lr=7.1049e-05, gnorm=2.318, loss_scale=4, train_wall=11, gb_free=19.3, wall=5392
2022-12-09 21:31:02 | INFO | train_inner | epoch 180:    955 / 1102 loss=6.832, nll_loss=2.968, ppl=7.83, wps=32869.5, ups=9.19, wpb=3577.3, bsz=138.6, num_updates=198200, lr=7.1031e-05, gnorm=2.31, loss_scale=4, train_wall=11, gb_free=19.4, wall=5403
2022-12-09 21:31:13 | INFO | train_inner | epoch 180:   1055 / 1102 loss=6.809, nll_loss=2.945, ppl=7.7, wps=32805.3, ups=9.14, wpb=3587.5, bsz=141.8, num_updates=198300, lr=7.10131e-05, gnorm=2.331, loss_scale=4, train_wall=11, gb_free=19.4, wall=5414
2022-12-09 21:31:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:31:58 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.46 | wps 4509.9 | wpb 2835.3 | bsz 115.6 | num_updates 198347 | best_bleu 37.58
2022-12-09 21:31:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 198347 updates
2022-12-09 21:31:59 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint180.pt
2022-12-09 21:32:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint180.pt (epoch 180 @ 198347 updates, score 37.46) (writing took 1.0968266129493713 seconds)
2022-12-09 21:32:00 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-12-09 21:32:00 | INFO | train | epoch 180 | loss 6.758 | nll_loss 2.898 | ppl 7.45 | wps 24456.7 | ups 6.82 | wpb 3583.6 | bsz 145.4 | num_updates 198347 | lr 7.10047e-05 | gnorm 2.31 | loss_scale 4 | train_wall 117 | gb_free 19.3 | wall 5460
2022-12-09 21:32:00 | INFO | fairseq.trainer | begin training epoch 181
2022-12-09 21:32:05 | INFO | train_inner | epoch 181:     53 / 1102 loss=6.735, nll_loss=2.869, ppl=7.31, wps=6799.2, ups=1.92, wpb=3539.6, bsz=138.9, num_updates=198400, lr=7.09952e-05, gnorm=2.267, loss_scale=4, train_wall=10, gb_free=19.6, wall=5466
2022-12-09 21:32:16 | INFO | train_inner | epoch 181:    153 / 1102 loss=6.714, nll_loss=2.843, ppl=7.17, wps=33047.2, ups=9.22, wpb=3582.8, bsz=142.2, num_updates=198500, lr=7.09773e-05, gnorm=2.245, loss_scale=4, train_wall=11, gb_free=19.3, wall=5477
2022-12-09 21:32:27 | INFO | train_inner | epoch 181:    253 / 1102 loss=6.754, nll_loss=2.904, ppl=7.48, wps=33739.1, ups=9.28, wpb=3634.2, bsz=154.7, num_updates=198600, lr=7.09595e-05, gnorm=2.305, loss_scale=4, train_wall=11, gb_free=19.5, wall=5488
2022-12-09 21:32:38 | INFO | train_inner | epoch 181:    353 / 1102 loss=6.746, nll_loss=2.883, ppl=7.38, wps=32086.7, ups=9.16, wpb=3501.6, bsz=146, num_updates=198700, lr=7.09416e-05, gnorm=2.342, loss_scale=4, train_wall=11, gb_free=19.6, wall=5499
2022-12-09 21:32:49 | INFO | train_inner | epoch 181:    453 / 1102 loss=6.73, nll_loss=2.875, ppl=7.33, wps=33332.5, ups=9.22, wpb=3614.9, bsz=154.2, num_updates=198800, lr=7.09238e-05, gnorm=2.262, loss_scale=4, train_wall=11, gb_free=19.7, wall=5509
2022-12-09 21:33:00 | INFO | train_inner | epoch 181:    553 / 1102 loss=6.719, nll_loss=2.863, ppl=7.27, wps=33216.6, ups=9.23, wpb=3599.7, bsz=146.6, num_updates=198900, lr=7.09059e-05, gnorm=2.238, loss_scale=4, train_wall=11, gb_free=19.4, wall=5520
2022-12-09 21:33:11 | INFO | train_inner | epoch 181:    653 / 1102 loss=6.795, nll_loss=2.923, ppl=7.58, wps=32698, ups=9.1, wpb=3594.1, bsz=136, num_updates=199000, lr=7.08881e-05, gnorm=2.403, loss_scale=4, train_wall=11, gb_free=19.2, wall=5531
2022-12-09 21:33:22 | INFO | train_inner | epoch 181:    753 / 1102 loss=6.782, nll_loss=2.932, ppl=7.63, wps=33428.7, ups=9.21, wpb=3631.6, bsz=147.1, num_updates=199100, lr=7.08703e-05, gnorm=2.238, loss_scale=4, train_wall=11, gb_free=19.7, wall=5542
2022-12-09 21:33:32 | INFO | train_inner | epoch 181:    853 / 1102 loss=6.829, nll_loss=2.971, ppl=7.84, wps=33015.4, ups=9.23, wpb=3576.5, bsz=138, num_updates=199200, lr=7.08525e-05, gnorm=2.288, loss_scale=4, train_wall=11, gb_free=19.6, wall=5553
2022-12-09 21:33:43 | INFO | train_inner | epoch 181:    953 / 1102 loss=6.794, nll_loss=2.926, ppl=7.6, wps=33258.3, ups=9.36, wpb=3553.4, bsz=147.8, num_updates=199300, lr=7.08347e-05, gnorm=2.349, loss_scale=4, train_wall=10, gb_free=19.7, wall=5564
2022-12-09 21:33:54 | INFO | train_inner | epoch 181:   1053 / 1102 loss=6.788, nll_loss=2.922, ppl=7.58, wps=33479.5, ups=9.21, wpb=3636.3, bsz=137.7, num_updates=199400, lr=7.0817e-05, gnorm=2.3, loss_scale=4, train_wall=11, gb_free=19.4, wall=5574
2022-12-09 21:33:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:34:37 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 3.665 | nll_loss 2.121 | ppl 4.35 | bleu 37.57 | wps 4808.5 | wpb 2835.3 | bsz 115.6 | num_updates 199449 | best_bleu 37.58
2022-12-09 21:34:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 199449 updates
2022-12-09 21:34:38 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint181.pt
2022-12-09 21:34:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint181.pt (epoch 181 @ 199449 updates, score 37.57) (writing took 1.1036011120304465 seconds)
2022-12-09 21:34:38 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-12-09 21:34:38 | INFO | train | epoch 181 | loss 6.757 | nll_loss 2.896 | ppl 7.44 | wps 24944.2 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 199449 | lr 7.08083e-05 | gnorm 2.298 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 5618
2022-12-09 21:34:38 | INFO | fairseq.trainer | begin training epoch 182
2022-12-09 21:34:44 | INFO | train_inner | epoch 182:     51 / 1102 loss=6.691, nll_loss=2.831, ppl=7.12, wps=7103.8, ups=2.01, wpb=3534, bsz=147, num_updates=199500, lr=7.07992e-05, gnorm=2.27, loss_scale=4, train_wall=11, gb_free=19.8, wall=5624
2022-12-09 21:34:54 | INFO | train_inner | epoch 182:    151 / 1102 loss=6.744, nll_loss=2.874, ppl=7.33, wps=32889.1, ups=9.23, wpb=3562.2, bsz=136.6, num_updates=199600, lr=7.07815e-05, gnorm=2.284, loss_scale=4, train_wall=11, gb_free=19.5, wall=5635
2022-12-09 21:35:05 | INFO | train_inner | epoch 182:    251 / 1102 loss=6.737, nll_loss=2.869, ppl=7.31, wps=33044.3, ups=9.24, wpb=3575, bsz=145.8, num_updates=199700, lr=7.07638e-05, gnorm=2.368, loss_scale=4, train_wall=11, gb_free=19.4, wall=5646
2022-12-09 21:35:16 | INFO | train_inner | epoch 182:    351 / 1102 loss=6.71, nll_loss=2.851, ppl=7.22, wps=32525.5, ups=9.17, wpb=3545.6, bsz=160.2, num_updates=199800, lr=7.07461e-05, gnorm=2.299, loss_scale=4, train_wall=11, gb_free=19.4, wall=5657
2022-12-09 21:35:27 | INFO | train_inner | epoch 182:    451 / 1102 loss=6.781, nll_loss=2.937, ppl=7.66, wps=33338.4, ups=9.25, wpb=3603.2, bsz=148.4, num_updates=199900, lr=7.07284e-05, gnorm=2.209, loss_scale=4, train_wall=11, gb_free=19.7, wall=5668
2022-12-09 21:35:38 | INFO | train_inner | epoch 182:    551 / 1102 loss=6.801, nll_loss=2.918, ppl=7.56, wps=32151.6, ups=9.03, wpb=3562.3, bsz=129.8, num_updates=200000, lr=7.07107e-05, gnorm=2.346, loss_scale=4, train_wall=11, gb_free=19.7, wall=5679
2022-12-09 21:35:49 | INFO | train_inner | epoch 182:    651 / 1102 loss=6.727, nll_loss=2.872, ppl=7.32, wps=33036.3, ups=9.12, wpb=3622.7, bsz=155.1, num_updates=200100, lr=7.0693e-05, gnorm=2.274, loss_scale=4, train_wall=11, gb_free=19.4, wall=5690
2022-12-09 21:36:00 | INFO | train_inner | epoch 182:    751 / 1102 loss=6.788, nll_loss=2.922, ppl=7.58, wps=33300.1, ups=9.23, wpb=3606, bsz=134.2, num_updates=200200, lr=7.06753e-05, gnorm=2.244, loss_scale=4, train_wall=11, gb_free=19.3, wall=5700
2022-12-09 21:36:11 | INFO | train_inner | epoch 182:    851 / 1102 loss=6.766, nll_loss=2.914, ppl=7.53, wps=32811.6, ups=9.1, wpb=3605.3, bsz=149, num_updates=200300, lr=7.06577e-05, gnorm=2.292, loss_scale=4, train_wall=11, gb_free=19.4, wall=5711
2022-12-09 21:36:22 | INFO | train_inner | epoch 182:    951 / 1102 loss=6.733, nll_loss=2.885, ppl=7.38, wps=32854.5, ups=9.19, wpb=3575.8, bsz=159.9, num_updates=200400, lr=7.06401e-05, gnorm=2.355, loss_scale=4, train_wall=11, gb_free=19.2, wall=5722
2022-12-09 21:36:33 | INFO | train_inner | epoch 182:   1051 / 1102 loss=6.769, nll_loss=2.904, ppl=7.49, wps=32771.3, ups=9.19, wpb=3565.2, bsz=142.1, num_updates=200500, lr=7.06225e-05, gnorm=2.343, loss_scale=4, train_wall=11, gb_free=19.4, wall=5733
2022-12-09 21:36:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:37:17 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 3.662 | nll_loss 2.121 | ppl 4.35 | bleu 37.48 | wps 4706.6 | wpb 2835.3 | bsz 115.6 | num_updates 200551 | best_bleu 37.58
2022-12-09 21:37:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 200551 updates
2022-12-09 21:37:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint182.pt
2022-12-09 21:37:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint182.pt (epoch 182 @ 200551 updates, score 37.48) (writing took 1.4369200002402067 seconds)
2022-12-09 21:37:18 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-12-09 21:37:18 | INFO | train | epoch 182 | loss 6.754 | nll_loss 2.893 | ppl 7.43 | wps 24673.2 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 200551 | lr 7.06135e-05 | gnorm 2.296 | loss_scale 4 | train_wall 117 | gb_free 19.5 | wall 5779
2022-12-09 21:37:18 | INFO | fairseq.trainer | begin training epoch 183
2022-12-09 21:37:24 | INFO | train_inner | epoch 183:     49 / 1102 loss=6.79, nll_loss=2.925, ppl=7.6, wps=7035.1, ups=1.96, wpb=3581.3, bsz=134.8, num_updates=200600, lr=7.06049e-05, gnorm=2.325, loss_scale=4, train_wall=11, gb_free=19.6, wall=5784
2022-12-09 21:37:34 | INFO | train_inner | epoch 183:    149 / 1102 loss=6.631, nll_loss=2.769, ppl=6.82, wps=32835.5, ups=9.22, wpb=3559.5, bsz=156.2, num_updates=200700, lr=7.05873e-05, gnorm=2.338, loss_scale=4, train_wall=11, gb_free=19.7, wall=5795
2022-12-09 21:37:45 | INFO | train_inner | epoch 183:    249 / 1102 loss=6.722, nll_loss=2.87, ppl=7.31, wps=33037.8, ups=9.2, wpb=3589.3, bsz=157.8, num_updates=200800, lr=7.05697e-05, gnorm=2.306, loss_scale=4, train_wall=11, gb_free=19.4, wall=5806
2022-12-09 21:37:56 | INFO | train_inner | epoch 183:    349 / 1102 loss=6.686, nll_loss=2.84, ppl=7.16, wps=33157.6, ups=9.14, wpb=3627.8, bsz=164.8, num_updates=200900, lr=7.05521e-05, gnorm=2.22, loss_scale=4, train_wall=11, gb_free=19.3, wall=5817
2022-12-09 21:38:07 | INFO | train_inner | epoch 183:    449 / 1102 loss=6.803, nll_loss=2.928, ppl=7.61, wps=33411.1, ups=9.37, wpb=3564.8, bsz=135.1, num_updates=201000, lr=7.05346e-05, gnorm=2.422, loss_scale=4, train_wall=10, gb_free=19.4, wall=5827
2022-12-09 21:38:18 | INFO | train_inner | epoch 183:    549 / 1102 loss=6.768, nll_loss=2.895, ppl=7.44, wps=33098.8, ups=9.29, wpb=3561.8, bsz=137.7, num_updates=201100, lr=7.0517e-05, gnorm=2.346, loss_scale=4, train_wall=11, gb_free=19.4, wall=5838
2022-12-09 21:38:28 | INFO | train_inner | epoch 183:    649 / 1102 loss=6.782, nll_loss=2.914, ppl=7.54, wps=33168.6, ups=9.28, wpb=3574.5, bsz=137.4, num_updates=201200, lr=7.04995e-05, gnorm=2.326, loss_scale=4, train_wall=11, gb_free=19.6, wall=5849
2022-12-09 21:38:39 | INFO | train_inner | epoch 183:    749 / 1102 loss=6.826, nll_loss=2.963, ppl=7.8, wps=33344.7, ups=9.16, wpb=3639.7, bsz=136.4, num_updates=201300, lr=7.0482e-05, gnorm=2.405, loss_scale=4, train_wall=11, gb_free=19.3, wall=5860
2022-12-09 21:38:50 | INFO | train_inner | epoch 183:    849 / 1102 loss=6.893, nll_loss=3.006, ppl=8.03, wps=32689.5, ups=9.23, wpb=3541.9, bsz=116.3, num_updates=201400, lr=7.04645e-05, gnorm=2.352, loss_scale=4, train_wall=11, gb_free=19.3, wall=5871
2022-12-09 21:39:01 | INFO | train_inner | epoch 183:    949 / 1102 loss=6.67, nll_loss=2.84, ppl=7.16, wps=32644.3, ups=9.1, wpb=3586.6, bsz=171, num_updates=201500, lr=7.0447e-05, gnorm=2.235, loss_scale=4, train_wall=11, gb_free=19.5, wall=5882
2022-12-09 21:39:12 | INFO | train_inner | epoch 183:   1049 / 1102 loss=6.728, nll_loss=2.873, ppl=7.33, wps=33138.7, ups=9.16, wpb=3617.9, bsz=147.5, num_updates=201600, lr=7.04295e-05, gnorm=2.334, loss_scale=4, train_wall=11, gb_free=19.4, wall=5893
2022-12-09 21:39:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:40:01 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 3.667 | nll_loss 2.123 | ppl 4.36 | bleu 37.35 | wps 4172 | wpb 2835.3 | bsz 115.6 | num_updates 201653 | best_bleu 37.58
2022-12-09 21:40:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 201653 updates
2022-12-09 21:40:02 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint183.pt
2022-12-09 21:40:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint183.pt (epoch 183 @ 201653 updates, score 37.35) (writing took 1.5239623496308923 seconds)
2022-12-09 21:40:03 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-12-09 21:40:03 | INFO | train | epoch 183 | loss 6.752 | nll_loss 2.891 | ppl 7.42 | wps 23990.8 | ups 6.69 | wpb 3583.6 | bsz 145.4 | num_updates 201653 | lr 7.04203e-05 | gnorm 2.325 | loss_scale 4 | train_wall 117 | gb_free 19.9 | wall 5943
2022-12-09 21:40:03 | INFO | fairseq.trainer | begin training epoch 184
2022-12-09 21:40:08 | INFO | train_inner | epoch 184:     47 / 1102 loss=6.737, nll_loss=2.883, ppl=7.38, wps=6360.1, ups=1.78, wpb=3566.6, bsz=152, num_updates=201700, lr=7.04121e-05, gnorm=2.245, loss_scale=4, train_wall=11, gb_free=19.4, wall=5949
2022-12-09 21:40:19 | INFO | train_inner | epoch 184:    147 / 1102 loss=6.666, nll_loss=2.823, ppl=7.08, wps=32626.1, ups=9.08, wpb=3593.3, bsz=161.9, num_updates=201800, lr=7.03946e-05, gnorm=2.258, loss_scale=4, train_wall=11, gb_free=19.3, wall=5960
2022-12-09 21:40:30 | INFO | train_inner | epoch 184:    247 / 1102 loss=6.701, nll_loss=2.835, ppl=7.14, wps=32908.5, ups=9.33, wpb=3528.4, bsz=152.6, num_updates=201900, lr=7.03772e-05, gnorm=2.321, loss_scale=4, train_wall=10, gb_free=19.3, wall=5970
2022-12-09 21:40:41 | INFO | train_inner | epoch 184:    347 / 1102 loss=6.78, nll_loss=2.9, ppl=7.46, wps=33431.8, ups=9.25, wpb=3615.7, bsz=128.5, num_updates=202000, lr=7.03598e-05, gnorm=2.372, loss_scale=4, train_wall=11, gb_free=19.5, wall=5981
2022-12-09 21:40:51 | INFO | train_inner | epoch 184:    447 / 1102 loss=6.807, nll_loss=2.932, ppl=7.63, wps=32785.1, ups=9.32, wpb=3517.7, bsz=134.2, num_updates=202100, lr=7.03423e-05, gnorm=2.324, loss_scale=4, train_wall=10, gb_free=19.4, wall=5992
2022-12-09 21:41:02 | INFO | train_inner | epoch 184:    547 / 1102 loss=6.79, nll_loss=2.914, ppl=7.54, wps=33026.1, ups=9.25, wpb=3571.1, bsz=136, num_updates=202200, lr=7.03249e-05, gnorm=2.359, loss_scale=4, train_wall=11, gb_free=19.8, wall=6003
2022-12-09 21:41:13 | INFO | train_inner | epoch 184:    647 / 1102 loss=6.782, nll_loss=2.907, ppl=7.5, wps=32582.8, ups=9.12, wpb=3570.8, bsz=133.6, num_updates=202300, lr=7.03076e-05, gnorm=2.275, loss_scale=4, train_wall=11, gb_free=19.4, wall=6014
2022-12-09 21:41:24 | INFO | train_inner | epoch 184:    747 / 1102 loss=6.724, nll_loss=2.882, ppl=7.37, wps=33138.4, ups=9.06, wpb=3656.1, bsz=157.3, num_updates=202400, lr=7.02902e-05, gnorm=2.162, loss_scale=4, train_wall=11, gb_free=19.6, wall=6025
2022-12-09 21:41:35 | INFO | train_inner | epoch 184:    847 / 1102 loss=6.747, nll_loss=2.876, ppl=7.34, wps=32993.8, ups=9.18, wpb=3592.7, bsz=139.6, num_updates=202500, lr=7.02728e-05, gnorm=2.271, loss_scale=4, train_wall=11, gb_free=19.5, wall=6036
2022-12-09 21:41:46 | INFO | train_inner | epoch 184:    947 / 1102 loss=6.795, nll_loss=2.944, ppl=7.69, wps=32893.5, ups=9.15, wpb=3593.8, bsz=151.9, num_updates=202600, lr=7.02555e-05, gnorm=2.294, loss_scale=4, train_wall=11, gb_free=19.5, wall=6047
2022-12-09 21:41:57 | INFO | train_inner | epoch 184:   1047 / 1102 loss=6.716, nll_loss=2.874, ppl=7.33, wps=33081.1, ups=9.16, wpb=3611.2, bsz=156.3, num_updates=202700, lr=7.02382e-05, gnorm=2.226, loss_scale=4, train_wall=11, gb_free=19.3, wall=6058
2022-12-09 21:42:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:42:42 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 3.662 | nll_loss 2.119 | ppl 4.34 | bleu 37.53 | wps 4652.2 | wpb 2835.3 | bsz 115.6 | num_updates 202755 | best_bleu 37.58
2022-12-09 21:42:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 202755 updates
2022-12-09 21:42:43 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint184.pt
2022-12-09 21:42:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint184.pt (epoch 184 @ 202755 updates, score 37.53) (writing took 1.478141512721777 seconds)
2022-12-09 21:42:43 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-12-09 21:42:43 | INFO | train | epoch 184 | loss 6.751 | nll_loss 2.889 | ppl 7.41 | wps 24574.6 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 202755 | lr 7.02286e-05 | gnorm 2.293 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 6104
2022-12-09 21:42:43 | INFO | fairseq.trainer | begin training epoch 185
2022-12-09 21:42:48 | INFO | train_inner | epoch 185:     45 / 1102 loss=6.747, nll_loss=2.878, ppl=7.35, wps=6955.3, ups=1.94, wpb=3580.7, bsz=136.9, num_updates=202800, lr=7.02208e-05, gnorm=2.377, loss_scale=4, train_wall=11, gb_free=19.3, wall=6109
2022-12-09 21:42:59 | INFO | train_inner | epoch 185:    145 / 1102 loss=6.76, nll_loss=2.892, ppl=7.42, wps=33150.6, ups=9.21, wpb=3599.7, bsz=138.6, num_updates=202900, lr=7.02035e-05, gnorm=2.279, loss_scale=4, train_wall=11, gb_free=20, wall=6120
2022-12-09 21:43:10 | INFO | train_inner | epoch 185:    245 / 1102 loss=6.74, nll_loss=2.866, ppl=7.29, wps=32628.8, ups=9.28, wpb=3517.2, bsz=140.4, num_updates=203000, lr=7.01862e-05, gnorm=2.297, loss_scale=4, train_wall=11, gb_free=19.7, wall=6131
2022-12-09 21:43:21 | INFO | train_inner | epoch 185:    345 / 1102 loss=6.724, nll_loss=2.86, ppl=7.26, wps=32452.9, ups=9.12, wpb=3559.7, bsz=150.9, num_updates=203100, lr=7.0169e-05, gnorm=2.264, loss_scale=4, train_wall=11, gb_free=19.4, wall=6142
2022-12-09 21:43:32 | INFO | train_inner | epoch 185:    445 / 1102 loss=6.738, nll_loss=2.887, ppl=7.4, wps=33200.2, ups=9.12, wpb=3641.4, bsz=151.7, num_updates=203200, lr=7.01517e-05, gnorm=2.284, loss_scale=4, train_wall=11, gb_free=19.6, wall=6153
2022-12-09 21:43:43 | INFO | train_inner | epoch 185:    545 / 1102 loss=6.763, nll_loss=2.895, ppl=7.44, wps=32868.9, ups=9.25, wpb=3555.1, bsz=135.4, num_updates=203300, lr=7.01344e-05, gnorm=2.427, loss_scale=4, train_wall=11, gb_free=19.7, wall=6163
2022-12-09 21:43:54 | INFO | train_inner | epoch 185:    645 / 1102 loss=6.751, nll_loss=2.874, ppl=7.33, wps=33013.3, ups=9.22, wpb=3578.8, bsz=137.9, num_updates=203400, lr=7.01172e-05, gnorm=2.37, loss_scale=4, train_wall=11, gb_free=19.3, wall=6174
2022-12-09 21:44:05 | INFO | train_inner | epoch 185:    745 / 1102 loss=6.759, nll_loss=2.887, ppl=7.4, wps=32729.8, ups=9.21, wpb=3553.5, bsz=139.4, num_updates=203500, lr=7.01e-05, gnorm=2.292, loss_scale=4, train_wall=11, gb_free=19.9, wall=6185
2022-12-09 21:44:15 | INFO | train_inner | epoch 185:    845 / 1102 loss=6.8, nll_loss=2.938, ppl=7.66, wps=32918.6, ups=9.15, wpb=3597.1, bsz=139.8, num_updates=203600, lr=7.00827e-05, gnorm=2.314, loss_scale=4, train_wall=11, gb_free=19.3, wall=6196
2022-12-09 21:44:26 | INFO | train_inner | epoch 185:    945 / 1102 loss=6.734, nll_loss=2.885, ppl=7.38, wps=32804.9, ups=9.09, wpb=3607.1, bsz=156.9, num_updates=203700, lr=7.00655e-05, gnorm=2.24, loss_scale=4, train_wall=11, gb_free=19.6, wall=6207
2022-12-09 21:44:37 | INFO | train_inner | epoch 185:   1045 / 1102 loss=6.746, nll_loss=2.898, ppl=7.45, wps=33388, ups=9.19, wpb=3632, bsz=163.4, num_updates=203800, lr=7.00484e-05, gnorm=2.325, loss_scale=4, train_wall=11, gb_free=19.5, wall=6218
2022-12-09 21:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:45:28 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 3.662 | nll_loss 2.12 | ppl 4.35 | bleu 37.46 | wps 4070.6 | wpb 2835.3 | bsz 115.6 | num_updates 203857 | best_bleu 37.58
2022-12-09 21:45:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 203857 updates
2022-12-09 21:45:29 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint185.pt
2022-12-09 21:45:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint185.pt (epoch 185 @ 203857 updates, score 37.46) (writing took 1.7540423814207315 seconds)
2022-12-09 21:45:30 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-12-09 21:45:30 | INFO | train | epoch 185 | loss 6.748 | nll_loss 2.885 | ppl 7.39 | wps 23743.8 | ups 6.63 | wpb 3583.6 | bsz 145.4 | num_updates 203857 | lr 7.00386e-05 | gnorm 2.308 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 6270
2022-12-09 21:45:30 | INFO | fairseq.trainer | begin training epoch 186
2022-12-09 21:45:35 | INFO | train_inner | epoch 186:     43 / 1102 loss=6.639, nll_loss=2.8, ppl=6.96, wps=6267.1, ups=1.74, wpb=3597.9, bsz=168.2, num_updates=203900, lr=7.00312e-05, gnorm=2.202, loss_scale=4, train_wall=11, gb_free=19.3, wall=6275
2022-12-09 21:45:46 | INFO | train_inner | epoch 186:    143 / 1102 loss=6.704, nll_loss=2.845, ppl=7.18, wps=32709.6, ups=9.08, wpb=3604.2, bsz=146.2, num_updates=204000, lr=7.0014e-05, gnorm=2.305, loss_scale=4, train_wall=11, gb_free=19.3, wall=6286
2022-12-09 21:45:57 | INFO | train_inner | epoch 186:    243 / 1102 loss=6.709, nll_loss=2.857, ppl=7.24, wps=32791.9, ups=9.18, wpb=3572.5, bsz=149.8, num_updates=204100, lr=6.99969e-05, gnorm=2.254, loss_scale=4, train_wall=11, gb_free=19.3, wall=6297
2022-12-09 21:46:07 | INFO | train_inner | epoch 186:    343 / 1102 loss=6.796, nll_loss=2.913, ppl=7.53, wps=32937.9, ups=9.32, wpb=3534.1, bsz=130.3, num_updates=204200, lr=6.99797e-05, gnorm=2.364, loss_scale=4, train_wall=10, gb_free=19.4, wall=6308
2022-12-09 21:46:18 | INFO | train_inner | epoch 186:    443 / 1102 loss=6.739, nll_loss=2.882, ppl=7.37, wps=32992.7, ups=9.12, wpb=3618.4, bsz=154.7, num_updates=204300, lr=6.99626e-05, gnorm=2.252, loss_scale=4, train_wall=11, gb_free=19.4, wall=6319
2022-12-09 21:46:29 | INFO | train_inner | epoch 186:    543 / 1102 loss=6.753, nll_loss=2.882, ppl=7.37, wps=32972.3, ups=9.28, wpb=3551.7, bsz=136.4, num_updates=204400, lr=6.99455e-05, gnorm=2.313, loss_scale=4, train_wall=11, gb_free=19.5, wall=6330
2022-12-09 21:46:40 | INFO | train_inner | epoch 186:    643 / 1102 loss=6.753, nll_loss=2.893, ppl=7.43, wps=33328, ups=9.34, wpb=3567.7, bsz=148, num_updates=204500, lr=6.99284e-05, gnorm=2.265, loss_scale=4, train_wall=10, gb_free=19.3, wall=6340
2022-12-09 21:46:51 | INFO | train_inner | epoch 186:    743 / 1102 loss=6.732, nll_loss=2.885, ppl=7.38, wps=33211.5, ups=9.17, wpb=3622.8, bsz=154.7, num_updates=204600, lr=6.99113e-05, gnorm=2.279, loss_scale=4, train_wall=11, gb_free=19.4, wall=6351
2022-12-09 21:47:01 | INFO | train_inner | epoch 186:    843 / 1102 loss=6.786, nll_loss=2.914, ppl=7.54, wps=33012.6, ups=9.33, wpb=3537.5, bsz=135.3, num_updates=204700, lr=6.98942e-05, gnorm=2.39, loss_scale=4, train_wall=10, gb_free=19.5, wall=6362
2022-12-09 21:47:12 | INFO | train_inner | epoch 186:    943 / 1102 loss=6.742, nll_loss=2.892, ppl=7.43, wps=33685.6, ups=9.21, wpb=3656.2, bsz=149.9, num_updates=204800, lr=6.98771e-05, gnorm=2.266, loss_scale=4, train_wall=11, gb_free=19.5, wall=6373
2022-12-09 21:47:23 | INFO | train_inner | epoch 186:   1043 / 1102 loss=6.816, nll_loss=2.935, ppl=7.65, wps=32545.1, ups=9.16, wpb=3552.8, bsz=138.7, num_updates=204900, lr=6.98601e-05, gnorm=2.374, loss_scale=4, train_wall=11, gb_free=19.7, wall=6384
2022-12-09 21:47:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:48:11 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 3.665 | nll_loss 2.123 | ppl 4.35 | bleu 37.43 | wps 4357.2 | wpb 2835.3 | bsz 115.6 | num_updates 204959 | best_bleu 37.58
2022-12-09 21:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 204959 updates
2022-12-09 21:48:12 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint186.pt
2022-12-09 21:48:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint186.pt (epoch 186 @ 204959 updates, score 37.43) (writing took 1.2975113028660417 seconds)
2022-12-09 21:48:13 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-12-09 21:48:13 | INFO | train | epoch 186 | loss 6.746 | nll_loss 2.884 | ppl 7.38 | wps 24226.8 | ups 6.76 | wpb 3583.6 | bsz 145.4 | num_updates 204959 | lr 6.985e-05 | gnorm 2.305 | loss_scale 4 | train_wall 117 | gb_free 19.7 | wall 6433
2022-12-09 21:48:13 | INFO | fairseq.trainer | begin training epoch 187
2022-12-09 21:48:17 | INFO | train_inner | epoch 187:     41 / 1102 loss=6.751, nll_loss=2.884, ppl=7.38, wps=6550.8, ups=1.85, wpb=3546.6, bsz=137.4, num_updates=205000, lr=6.9843e-05, gnorm=2.349, loss_scale=4, train_wall=11, gb_free=19.2, wall=6438
2022-12-09 21:48:28 | INFO | train_inner | epoch 187:    141 / 1102 loss=6.69, nll_loss=2.844, ppl=7.18, wps=33701.6, ups=9.18, wpb=3669.3, bsz=150.8, num_updates=205100, lr=6.9826e-05, gnorm=2.197, loss_scale=4, train_wall=11, gb_free=19.4, wall=6449
2022-12-09 21:48:39 | INFO | train_inner | epoch 187:    241 / 1102 loss=6.687, nll_loss=2.83, ppl=7.11, wps=33052.3, ups=9.17, wpb=3604.8, bsz=152.8, num_updates=205200, lr=6.9809e-05, gnorm=2.191, loss_scale=4, train_wall=11, gb_free=19.3, wall=6460
2022-12-09 21:48:50 | INFO | train_inner | epoch 187:    341 / 1102 loss=6.717, nll_loss=2.85, ppl=7.21, wps=33331.9, ups=9.3, wpb=3585.5, bsz=136.5, num_updates=205300, lr=6.9792e-05, gnorm=2.355, loss_scale=4, train_wall=11, gb_free=19.9, wall=6471
2022-12-09 21:49:01 | INFO | train_inner | epoch 187:    441 / 1102 loss=6.768, nll_loss=2.886, ppl=7.39, wps=32727.1, ups=9.35, wpb=3500.9, bsz=135.3, num_updates=205400, lr=6.9775e-05, gnorm=2.433, loss_scale=4, train_wall=10, gb_free=19.5, wall=6481
2022-12-09 21:49:12 | INFO | train_inner | epoch 187:    541 / 1102 loss=6.768, nll_loss=2.891, ppl=7.42, wps=32394.8, ups=9.1, wpb=3559.5, bsz=135, num_updates=205500, lr=6.9758e-05, gnorm=2.474, loss_scale=8, train_wall=11, gb_free=20.1, wall=6492
2022-12-09 21:49:23 | INFO | train_inner | epoch 187:    641 / 1102 loss=6.782, nll_loss=2.911, ppl=7.52, wps=32923.2, ups=9.09, wpb=3620, bsz=140.2, num_updates=205600, lr=6.9741e-05, gnorm=2.291, loss_scale=8, train_wall=11, gb_free=19.5, wall=6503
2022-12-09 21:49:34 | INFO | train_inner | epoch 187:    741 / 1102 loss=6.69, nll_loss=2.849, ppl=7.2, wps=33135.9, ups=9.07, wpb=3655.3, bsz=163.6, num_updates=205700, lr=6.97241e-05, gnorm=2.211, loss_scale=8, train_wall=11, gb_free=19.4, wall=6514
2022-12-09 21:49:44 | INFO | train_inner | epoch 187:    841 / 1102 loss=6.867, nll_loss=2.978, ppl=7.88, wps=33080.4, ups=9.32, wpb=3547.9, bsz=124.4, num_updates=205800, lr=6.97071e-05, gnorm=2.356, loss_scale=8, train_wall=10, gb_free=19.4, wall=6525
2022-12-09 21:49:55 | INFO | train_inner | epoch 187:    941 / 1102 loss=6.742, nll_loss=2.894, ppl=7.43, wps=32838.5, ups=9.18, wpb=3577.7, bsz=155.8, num_updates=205900, lr=6.96902e-05, gnorm=2.239, loss_scale=8, train_wall=11, gb_free=19.5, wall=6536
2022-12-09 21:50:06 | INFO | train_inner | epoch 187:   1041 / 1102 loss=6.727, nll_loss=2.872, ppl=7.32, wps=32030.5, ups=8.99, wpb=3563.3, bsz=161.1, num_updates=206000, lr=6.96733e-05, gnorm=2.27, loss_scale=8, train_wall=11, gb_free=19.4, wall=6547
2022-12-09 21:50:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:50:52 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 3.661 | nll_loss 2.121 | ppl 4.35 | bleu 37.55 | wps 4669.9 | wpb 2835.3 | bsz 115.6 | num_updates 206061 | best_bleu 37.58
2022-12-09 21:50:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 206061 updates
2022-12-09 21:50:53 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint187.pt
2022-12-09 21:50:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint187.pt (epoch 187 @ 206061 updates, score 37.55) (writing took 1.1547143459320068 seconds)
2022-12-09 21:50:53 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-12-09 21:50:53 | INFO | train | epoch 187 | loss 6.744 | nll_loss 2.881 | ppl 7.36 | wps 24610.3 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 206061 | lr 6.9663e-05 | gnorm 2.297 | loss_scale 8 | train_wall 117 | gb_free 19.4 | wall 6594
2022-12-09 21:50:53 | INFO | fairseq.trainer | begin training epoch 188
2022-12-09 21:50:58 | INFO | train_inner | epoch 188:     39 / 1102 loss=6.743, nll_loss=2.879, ppl=7.36, wps=6949.4, ups=1.96, wpb=3553.8, bsz=138.6, num_updates=206100, lr=6.96564e-05, gnorm=2.241, loss_scale=8, train_wall=11, gb_free=19.2, wall=6598
2022-12-09 21:51:08 | INFO | train_inner | epoch 188:    139 / 1102 loss=6.73, nll_loss=2.887, ppl=7.4, wps=33956.9, ups=9.22, wpb=3682.8, bsz=152, num_updates=206200, lr=6.96395e-05, gnorm=2.169, loss_scale=8, train_wall=11, gb_free=19.3, wall=6609
2022-12-09 21:51:19 | INFO | train_inner | epoch 188:    239 / 1102 loss=6.711, nll_loss=2.847, ppl=7.2, wps=33678.3, ups=9.3, wpb=3620.8, bsz=146.4, num_updates=206300, lr=6.96226e-05, gnorm=2.266, loss_scale=8, train_wall=11, gb_free=19.7, wall=6620
2022-12-09 21:51:30 | INFO | train_inner | epoch 188:    339 / 1102 loss=6.764, nll_loss=2.893, ppl=7.43, wps=32787.7, ups=9.26, wpb=3540.4, bsz=140.6, num_updates=206400, lr=6.96058e-05, gnorm=2.272, loss_scale=8, train_wall=11, gb_free=19.9, wall=6631
2022-12-09 21:51:41 | INFO | train_inner | epoch 188:    439 / 1102 loss=6.739, nll_loss=2.856, ppl=7.24, wps=32233.1, ups=9.25, wpb=3486, bsz=140.3, num_updates=206500, lr=6.95889e-05, gnorm=2.338, loss_scale=8, train_wall=11, gb_free=19.4, wall=6641
2022-12-09 21:51:51 | INFO | train_inner | epoch 188:    539 / 1102 loss=6.783, nll_loss=2.884, ppl=7.38, wps=32805.8, ups=9.34, wpb=3512.5, bsz=121.3, num_updates=206600, lr=6.95721e-05, gnorm=2.425, loss_scale=8, train_wall=10, gb_free=19.3, wall=6652
2022-12-09 21:52:02 | INFO | train_inner | epoch 188:    639 / 1102 loss=6.655, nll_loss=2.809, ppl=7.01, wps=32678.3, ups=9.19, wpb=3556.3, bsz=169.8, num_updates=206700, lr=6.95552e-05, gnorm=2.298, loss_scale=8, train_wall=11, gb_free=19.6, wall=6663
2022-12-09 21:52:13 | INFO | train_inner | epoch 188:    739 / 1102 loss=6.717, nll_loss=2.855, ppl=7.23, wps=33034.7, ups=9.31, wpb=3548.1, bsz=154.6, num_updates=206800, lr=6.95384e-05, gnorm=2.343, loss_scale=8, train_wall=11, gb_free=19.8, wall=6674
2022-12-09 21:52:24 | INFO | train_inner | epoch 188:    839 / 1102 loss=6.768, nll_loss=2.915, ppl=7.54, wps=32957.6, ups=8.99, wpb=3664.1, bsz=142.7, num_updates=206900, lr=6.95216e-05, gnorm=2.213, loss_scale=8, train_wall=11, gb_free=19.4, wall=6685
2022-12-09 21:52:35 | INFO | train_inner | epoch 188:    939 / 1102 loss=6.77, nll_loss=2.896, ppl=7.44, wps=33502.5, ups=9.31, wpb=3597.1, bsz=142.4, num_updates=207000, lr=6.95048e-05, gnorm=2.337, loss_scale=8, train_wall=10, gb_free=19.8, wall=6696
2022-12-09 21:52:46 | INFO | train_inner | epoch 188:   1039 / 1102 loss=6.759, nll_loss=2.909, ppl=7.51, wps=33107.4, ups=9.09, wpb=3643.6, bsz=151.4, num_updates=207100, lr=6.9488e-05, gnorm=2.282, loss_scale=8, train_wall=11, gb_free=19.8, wall=6707
2022-12-09 21:52:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:53:32 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 3.659 | nll_loss 2.118 | ppl 4.34 | bleu 37.51 | wps 4615.6 | wpb 2835.3 | bsz 115.6 | num_updates 207163 | best_bleu 37.58
2022-12-09 21:53:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 207163 updates
2022-12-09 21:53:33 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint188.pt
2022-12-09 21:53:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint188.pt (epoch 188 @ 207163 updates, score 37.51) (writing took 1.2145115975290537 seconds)
2022-12-09 21:53:33 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-12-09 21:53:33 | INFO | train | epoch 188 | loss 6.742 | nll_loss 2.878 | ppl 7.35 | wps 24684.6 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 207163 | lr 6.94775e-05 | gnorm 2.293 | loss_scale 8 | train_wall 117 | gb_free 19.4 | wall 6754
2022-12-09 21:53:33 | INFO | fairseq.trainer | begin training epoch 189
2022-12-09 21:53:37 | INFO | train_inner | epoch 189:     37 / 1102 loss=6.742, nll_loss=2.883, ppl=7.38, wps=6898.9, ups=1.95, wpb=3546.1, bsz=145.7, num_updates=207200, lr=6.94713e-05, gnorm=2.376, loss_scale=8, train_wall=10, gb_free=19.4, wall=6758
2022-12-09 21:53:48 | INFO | train_inner | epoch 189:    137 / 1102 loss=6.691, nll_loss=2.839, ppl=7.16, wps=33353, ups=9.31, wpb=3583.1, bsz=151.4, num_updates=207300, lr=6.94545e-05, gnorm=2.251, loss_scale=8, train_wall=11, gb_free=19.7, wall=6769
2022-12-09 21:53:59 | INFO | train_inner | epoch 189:    237 / 1102 loss=6.707, nll_loss=2.836, ppl=7.14, wps=34082.1, ups=9.48, wpb=3597, bsz=145.7, num_updates=207400, lr=6.94377e-05, gnorm=2.283, loss_scale=8, train_wall=10, gb_free=19.3, wall=6779
2022-12-09 21:54:09 | INFO | train_inner | epoch 189:    337 / 1102 loss=6.768, nll_loss=2.883, ppl=7.38, wps=33489.6, ups=9.44, wpb=3547.5, bsz=129.1, num_updates=207500, lr=6.9421e-05, gnorm=2.345, loss_scale=8, train_wall=10, gb_free=19.3, wall=6790
2022-12-09 21:54:20 | INFO | train_inner | epoch 189:    437 / 1102 loss=6.676, nll_loss=2.821, ppl=7.07, wps=33076.5, ups=9.2, wpb=3595.2, bsz=151, num_updates=207600, lr=6.94043e-05, gnorm=2.266, loss_scale=8, train_wall=11, gb_free=19.4, wall=6801
2022-12-09 21:54:31 | INFO | train_inner | epoch 189:    537 / 1102 loss=6.752, nll_loss=2.89, ppl=7.41, wps=33077.4, ups=9.27, wpb=3568.8, bsz=147.8, num_updates=207700, lr=6.93876e-05, gnorm=2.294, loss_scale=8, train_wall=11, gb_free=19.4, wall=6811
2022-12-09 21:54:42 | INFO | train_inner | epoch 189:    637 / 1102 loss=6.777, nll_loss=2.9, ppl=7.47, wps=32684, ups=9.28, wpb=3521.9, bsz=133.8, num_updates=207800, lr=6.93709e-05, gnorm=2.382, loss_scale=8, train_wall=11, gb_free=19.4, wall=6822
2022-12-09 21:54:52 | INFO | train_inner | epoch 189:    737 / 1102 loss=6.778, nll_loss=2.909, ppl=7.51, wps=33493, ups=9.36, wpb=3580.2, bsz=141.5, num_updates=207900, lr=6.93542e-05, gnorm=2.321, loss_scale=8, train_wall=10, gb_free=19.7, wall=6833
2022-12-09 21:55:03 | INFO | train_inner | epoch 189:    837 / 1102 loss=6.802, nll_loss=2.932, ppl=7.63, wps=33412.5, ups=9.25, wpb=3612, bsz=140.5, num_updates=208000, lr=6.93375e-05, gnorm=2.397, loss_scale=8, train_wall=11, gb_free=19.5, wall=6844
2022-12-09 21:55:14 | INFO | train_inner | epoch 189:    937 / 1102 loss=6.683, nll_loss=2.843, ppl=7.18, wps=33194.8, ups=9.12, wpb=3640.6, bsz=170.5, num_updates=208100, lr=6.93209e-05, gnorm=2.178, loss_scale=8, train_wall=11, gb_free=19.3, wall=6855
2022-12-09 21:55:25 | INFO | train_inner | epoch 189:   1037 / 1102 loss=6.766, nll_loss=2.907, ppl=7.5, wps=33212.4, ups=9.28, wpb=3577.6, bsz=144.2, num_updates=208200, lr=6.93042e-05, gnorm=2.352, loss_scale=8, train_wall=11, gb_free=19.5, wall=6865
2022-12-09 21:55:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:56:12 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 3.663 | nll_loss 2.12 | ppl 4.35 | bleu 37.41 | wps 4508.3 | wpb 2835.3 | bsz 115.6 | num_updates 208265 | best_bleu 37.58
2022-12-09 21:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 208265 updates
2022-12-09 21:56:13 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint189.pt
2022-12-09 21:56:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint189.pt (epoch 189 @ 208265 updates, score 37.41) (writing took 1.3590547423809767 seconds)
2022-12-09 21:56:13 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-12-09 21:56:13 | INFO | train | epoch 189 | loss 6.739 | nll_loss 2.876 | ppl 7.34 | wps 24619 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 208265 | lr 6.92934e-05 | gnorm 2.307 | loss_scale 8 | train_wall 116 | gb_free 19.6 | wall 6914
2022-12-09 21:56:14 | INFO | fairseq.trainer | begin training epoch 190
2022-12-09 21:56:18 | INFO | train_inner | epoch 190:     35 / 1102 loss=6.745, nll_loss=2.888, ppl=7.4, wps=6876, ups=1.9, wpb=3619.9, bsz=146.2, num_updates=208300, lr=6.92876e-05, gnorm=2.211, loss_scale=8, train_wall=11, gb_free=19.4, wall=6918
2022-12-09 21:56:28 | INFO | train_inner | epoch 190:    135 / 1102 loss=6.764, nll_loss=2.885, ppl=7.39, wps=33341.2, ups=9.34, wpb=3569.9, bsz=133.3, num_updates=208400, lr=6.92709e-05, gnorm=2.38, loss_scale=8, train_wall=10, gb_free=19.4, wall=6929
2022-12-09 21:56:39 | INFO | train_inner | epoch 190:    235 / 1102 loss=6.775, nll_loss=2.896, ppl=7.44, wps=32855.6, ups=9.26, wpb=3549.4, bsz=135.1, num_updates=208500, lr=6.92543e-05, gnorm=2.396, loss_scale=8, train_wall=11, gb_free=19.4, wall=6940
2022-12-09 21:56:50 | INFO | train_inner | epoch 190:    335 / 1102 loss=6.765, nll_loss=2.879, ppl=7.36, wps=33156.2, ups=9.32, wpb=3556.6, bsz=129.1, num_updates=208600, lr=6.92377e-05, gnorm=2.371, loss_scale=8, train_wall=10, gb_free=19.2, wall=6950
2022-12-09 21:57:01 | INFO | train_inner | epoch 190:    435 / 1102 loss=6.714, nll_loss=2.838, ppl=7.15, wps=33225.8, ups=9.25, wpb=3590.6, bsz=143.6, num_updates=208700, lr=6.92211e-05, gnorm=2.32, loss_scale=8, train_wall=11, gb_free=19.5, wall=6961
2022-12-09 21:57:11 | INFO | train_inner | epoch 190:    535 / 1102 loss=6.689, nll_loss=2.841, ppl=7.17, wps=33220.3, ups=9.19, wpb=3614, bsz=161.5, num_updates=208800, lr=6.92046e-05, gnorm=2.248, loss_scale=8, train_wall=11, gb_free=19.3, wall=6972
2022-12-09 21:57:22 | INFO | train_inner | epoch 190:    635 / 1102 loss=6.754, nll_loss=2.888, ppl=7.4, wps=33337.5, ups=9.28, wpb=3591.2, bsz=136.6, num_updates=208900, lr=6.9188e-05, gnorm=2.255, loss_scale=8, train_wall=11, gb_free=19.3, wall=6983
2022-12-09 21:57:33 | INFO | train_inner | epoch 190:    735 / 1102 loss=6.733, nll_loss=2.864, ppl=7.28, wps=32778.4, ups=9.17, wpb=3573.8, bsz=142.6, num_updates=209000, lr=6.91714e-05, gnorm=2.375, loss_scale=8, train_wall=11, gb_free=19.3, wall=6994
2022-12-09 21:57:44 | INFO | train_inner | epoch 190:    835 / 1102 loss=6.757, nll_loss=2.903, ppl=7.48, wps=33569.8, ups=9.23, wpb=3636.3, bsz=149.6, num_updates=209100, lr=6.91549e-05, gnorm=2.301, loss_scale=8, train_wall=11, gb_free=19.4, wall=7005
2022-12-09 21:57:55 | INFO | train_inner | epoch 190:    935 / 1102 loss=6.729, nll_loss=2.883, ppl=7.38, wps=32137.3, ups=8.99, wpb=3576.6, bsz=161, num_updates=209200, lr=6.91384e-05, gnorm=2.245, loss_scale=8, train_wall=11, gb_free=19.4, wall=7016
2022-12-09 21:58:06 | INFO | train_inner | epoch 190:   1035 / 1102 loss=6.756, nll_loss=2.901, ppl=7.47, wps=32702.7, ups=9.05, wpb=3612.2, bsz=147.7, num_updates=209300, lr=6.91219e-05, gnorm=2.281, loss_scale=8, train_wall=11, gb_free=19.7, wall=7027
2022-12-09 21:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 21:58:55 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 3.66 | nll_loss 2.12 | ppl 4.35 | bleu 37.55 | wps 4360.4 | wpb 2835.3 | bsz 115.6 | num_updates 209367 | best_bleu 37.58
2022-12-09 21:58:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 209367 updates
2022-12-09 21:58:56 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint190.pt
2022-12-09 21:58:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint190.pt (epoch 190 @ 209367 updates, score 37.55) (writing took 1.134487945586443 seconds)
2022-12-09 21:58:56 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-12-09 21:58:56 | INFO | train | epoch 190 | loss 6.738 | nll_loss 2.873 | ppl 7.33 | wps 24285.1 | ups 6.78 | wpb 3583.6 | bsz 145.4 | num_updates 209367 | lr 6.91108e-05 | gnorm 2.308 | loss_scale 8 | train_wall 117 | gb_free 19.7 | wall 7077
2022-12-09 21:58:56 | INFO | fairseq.trainer | begin training epoch 191
2022-12-09 21:59:00 | INFO | train_inner | epoch 191:     33 / 1102 loss=6.685, nll_loss=2.824, ppl=7.08, wps=6571.4, ups=1.86, wpb=3535.7, bsz=153.9, num_updates=209400, lr=6.91053e-05, gnorm=2.305, loss_scale=8, train_wall=11, gb_free=19.3, wall=7081
2022-12-09 21:59:11 | INFO | train_inner | epoch 191:    133 / 1102 loss=6.724, nll_loss=2.849, ppl=7.21, wps=33226.6, ups=9.33, wpb=3563, bsz=140.3, num_updates=209500, lr=6.90889e-05, gnorm=2.349, loss_scale=8, train_wall=10, gb_free=19.3, wall=7091
2022-12-09 21:59:22 | INFO | train_inner | epoch 191:    233 / 1102 loss=6.709, nll_loss=2.851, ppl=7.22, wps=32832.4, ups=9.1, wpb=3606.8, bsz=148, num_updates=209600, lr=6.90724e-05, gnorm=2.24, loss_scale=8, train_wall=11, gb_free=19.5, wall=7102
2022-12-09 21:59:33 | INFO | train_inner | epoch 191:    333 / 1102 loss=6.741, nll_loss=2.882, ppl=7.37, wps=32758.9, ups=9.11, wpb=3596.8, bsz=150, num_updates=209700, lr=6.90559e-05, gnorm=2.318, loss_scale=8, train_wall=11, gb_free=19.5, wall=7113
2022-12-09 21:59:44 | INFO | train_inner | epoch 191:    433 / 1102 loss=6.716, nll_loss=2.841, ppl=7.17, wps=32787.9, ups=9.12, wpb=3596.7, bsz=136.2, num_updates=209800, lr=6.90394e-05, gnorm=2.283, loss_scale=8, train_wall=11, gb_free=19.5, wall=7124
2022-12-09 21:59:54 | INFO | train_inner | epoch 191:    533 / 1102 loss=6.692, nll_loss=2.827, ppl=7.09, wps=32671.5, ups=9.27, wpb=3524.8, bsz=151.8, num_updates=209900, lr=6.9023e-05, gnorm=2.294, loss_scale=8, train_wall=11, gb_free=19.8, wall=7135
2022-12-09 22:00:05 | INFO | train_inner | epoch 191:    633 / 1102 loss=6.684, nll_loss=2.82, ppl=7.06, wps=33415.4, ups=9.25, wpb=3612.4, bsz=153, num_updates=210000, lr=6.90066e-05, gnorm=2.324, loss_scale=8, train_wall=11, gb_free=19.4, wall=7146
2022-12-09 22:00:16 | INFO | train_inner | epoch 191:    733 / 1102 loss=6.758, nll_loss=2.891, ppl=7.42, wps=33626.5, ups=9.35, wpb=3595.1, bsz=139, num_updates=210100, lr=6.89901e-05, gnorm=2.355, loss_scale=8, train_wall=10, gb_free=19.4, wall=7156
2022-12-09 22:00:27 | INFO | train_inner | epoch 191:    833 / 1102 loss=6.754, nll_loss=2.901, ppl=7.47, wps=33628.4, ups=9.27, wpb=3626, bsz=149.1, num_updates=210200, lr=6.89737e-05, gnorm=2.245, loss_scale=8, train_wall=11, gb_free=19.8, wall=7167
2022-12-09 22:00:37 | INFO | train_inner | epoch 191:    933 / 1102 loss=6.797, nll_loss=2.945, ppl=7.7, wps=33274.3, ups=9.3, wpb=3577, bsz=153.7, num_updates=210300, lr=6.89573e-05, gnorm=2.331, loss_scale=8, train_wall=11, gb_free=19.2, wall=7178
2022-12-09 22:00:48 | INFO | train_inner | epoch 191:   1033 / 1102 loss=6.753, nll_loss=2.884, ppl=7.38, wps=33498.8, ups=9.31, wpb=3597.8, bsz=137.4, num_updates=210400, lr=6.89409e-05, gnorm=2.322, loss_scale=8, train_wall=11, gb_free=19.3, wall=7189
2022-12-09 22:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:01:35 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 3.661 | nll_loss 2.12 | ppl 4.35 | bleu 37.37 | wps 4536.7 | wpb 2835.3 | bsz 115.6 | num_updates 210469 | best_bleu 37.58
2022-12-09 22:01:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 210469 updates
2022-12-09 22:01:36 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint191.pt
2022-12-09 22:01:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint191.pt (epoch 191 @ 210469 updates, score 37.37) (writing took 1.206732519902289 seconds)
2022-12-09 22:01:37 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-12-09 22:01:37 | INFO | train | epoch 191 | loss 6.736 | nll_loss 2.871 | ppl 7.31 | wps 24598.1 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 210469 | lr 6.89296e-05 | gnorm 2.318 | loss_scale 8 | train_wall 116 | gb_free 19.5 | wall 7237
2022-12-09 22:01:37 | INFO | fairseq.trainer | begin training epoch 192
2022-12-09 22:01:40 | INFO | train_inner | epoch 192:     31 / 1102 loss=6.814, nll_loss=2.946, ppl=7.71, wps=6839.6, ups=1.92, wpb=3568.5, bsz=137, num_updates=210500, lr=6.89246e-05, gnorm=2.389, loss_scale=8, train_wall=11, gb_free=19.7, wall=7241
2022-12-09 22:01:51 | INFO | train_inner | epoch 192:    131 / 1102 loss=6.62, nll_loss=2.762, ppl=6.78, wps=33126.3, ups=9.18, wpb=3607.8, bsz=155.2, num_updates=210600, lr=6.89082e-05, gnorm=2.247, loss_scale=8, train_wall=11, gb_free=19.3, wall=7252
2022-12-09 22:02:02 | INFO | train_inner | epoch 192:    231 / 1102 loss=6.752, nll_loss=2.895, ppl=7.44, wps=33749.2, ups=9.43, wpb=3579.8, bsz=145.8, num_updates=210700, lr=6.88918e-05, gnorm=2.301, loss_scale=8, train_wall=10, gb_free=19.5, wall=7262
2022-12-09 22:02:13 | INFO | train_inner | epoch 192:    331 / 1102 loss=6.677, nll_loss=2.826, ppl=7.09, wps=34225.1, ups=9.32, wpb=3673, bsz=160.7, num_updates=210800, lr=6.88755e-05, gnorm=2.32, loss_scale=8, train_wall=11, gb_free=19.2, wall=7273
2022-12-09 22:02:23 | INFO | train_inner | epoch 192:    431 / 1102 loss=6.683, nll_loss=2.816, ppl=7.04, wps=32707.7, ups=9.31, wpb=3511.4, bsz=148.6, num_updates=210900, lr=6.88592e-05, gnorm=2.32, loss_scale=8, train_wall=11, gb_free=19.4, wall=7284
2022-12-09 22:02:34 | INFO | train_inner | epoch 192:    531 / 1102 loss=6.8, nll_loss=2.92, ppl=7.57, wps=32966.8, ups=9.25, wpb=3565.4, bsz=133.4, num_updates=211000, lr=6.88428e-05, gnorm=2.354, loss_scale=8, train_wall=11, gb_free=19.6, wall=7295
2022-12-09 22:02:45 | INFO | train_inner | epoch 192:    631 / 1102 loss=6.801, nll_loss=2.909, ppl=7.51, wps=32795.5, ups=9.28, wpb=3534.7, bsz=122.6, num_updates=211100, lr=6.88265e-05, gnorm=2.426, loss_scale=8, train_wall=11, gb_free=19.7, wall=7306
2022-12-09 22:02:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-09 22:02:56 | INFO | train_inner | epoch 192:    732 / 1102 loss=6.736, nll_loss=2.866, ppl=7.29, wps=32369.9, ups=9.05, wpb=3577.6, bsz=144.6, num_updates=211200, lr=6.88102e-05, gnorm=2.341, loss_scale=4, train_wall=11, gb_free=20, wall=7317
2022-12-09 22:03:07 | INFO | train_inner | epoch 192:    832 / 1102 loss=6.801, nll_loss=2.934, ppl=7.64, wps=32626.8, ups=9.18, wpb=3553.7, bsz=139.1, num_updates=211300, lr=6.8794e-05, gnorm=2.302, loss_scale=4, train_wall=11, gb_free=19.6, wall=7327
2022-12-09 22:03:18 | INFO | train_inner | epoch 192:    932 / 1102 loss=6.716, nll_loss=2.872, ppl=7.32, wps=33250.7, ups=9.15, wpb=3634.1, bsz=159.9, num_updates=211400, lr=6.87777e-05, gnorm=2.295, loss_scale=4, train_wall=11, gb_free=19.5, wall=7338
2022-12-09 22:03:29 | INFO | train_inner | epoch 192:   1032 / 1102 loss=6.732, nll_loss=2.861, ppl=7.26, wps=33076.8, ups=9.25, wpb=3577.3, bsz=146.2, num_updates=211500, lr=6.87614e-05, gnorm=2.378, loss_scale=4, train_wall=11, gb_free=19.6, wall=7349
2022-12-09 22:03:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:04:16 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 3.666 | nll_loss 2.125 | ppl 4.36 | bleu 37.48 | wps 4521.2 | wpb 2835.3 | bsz 115.6 | num_updates 211570 | best_bleu 37.58
2022-12-09 22:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 211570 updates
2022-12-09 22:04:17 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint192.pt
2022-12-09 22:04:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint192.pt (epoch 192 @ 211570 updates, score 37.48) (writing took 1.184793015010655 seconds)
2022-12-09 22:04:17 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-12-09 22:04:17 | INFO | train | epoch 192 | loss 6.734 | nll_loss 2.869 | ppl 7.3 | wps 24554.1 | ups 6.85 | wpb 3583.7 | bsz 145.3 | num_updates 211570 | lr 6.875e-05 | gnorm 2.328 | loss_scale 4 | train_wall 117 | gb_free 19.9 | wall 7398
2022-12-09 22:04:17 | INFO | fairseq.trainer | begin training epoch 193
2022-12-09 22:04:21 | INFO | train_inner | epoch 193:     30 / 1102 loss=6.719, nll_loss=2.856, ppl=7.24, wps=6819.4, ups=1.91, wpb=3566, bsz=151.1, num_updates=211600, lr=6.87452e-05, gnorm=2.342, loss_scale=4, train_wall=11, gb_free=19.7, wall=7401
2022-12-09 22:04:32 | INFO | train_inner | epoch 193:    130 / 1102 loss=6.639, nll_loss=2.78, ppl=6.87, wps=32646, ups=9.23, wpb=3538, bsz=151.7, num_updates=211700, lr=6.87289e-05, gnorm=2.29, loss_scale=4, train_wall=11, gb_free=19.5, wall=7412
2022-12-09 22:04:43 | INFO | train_inner | epoch 193:    230 / 1102 loss=6.698, nll_loss=2.829, ppl=7.11, wps=32880.2, ups=9.18, wpb=3581.7, bsz=142.3, num_updates=211800, lr=6.87127e-05, gnorm=2.248, loss_scale=4, train_wall=11, gb_free=19.6, wall=7423
2022-12-09 22:04:53 | INFO | train_inner | epoch 193:    330 / 1102 loss=6.781, nll_loss=2.898, ppl=7.45, wps=33189.2, ups=9.31, wpb=3564.8, bsz=132.2, num_updates=211900, lr=6.86965e-05, gnorm=2.365, loss_scale=4, train_wall=11, gb_free=19.3, wall=7434
2022-12-09 22:05:04 | INFO | train_inner | epoch 193:    430 / 1102 loss=6.786, nll_loss=2.913, ppl=7.53, wps=32151.9, ups=9.05, wpb=3552.1, bsz=136.3, num_updates=212000, lr=6.86803e-05, gnorm=2.331, loss_scale=4, train_wall=11, gb_free=19.4, wall=7445
2022-12-09 22:05:15 | INFO | train_inner | epoch 193:    530 / 1102 loss=6.699, nll_loss=2.834, ppl=7.13, wps=32816.8, ups=9.17, wpb=3577, bsz=152.8, num_updates=212100, lr=6.86641e-05, gnorm=2.41, loss_scale=4, train_wall=11, gb_free=19.6, wall=7456
2022-12-09 22:05:26 | INFO | train_inner | epoch 193:    630 / 1102 loss=6.693, nll_loss=2.843, ppl=7.17, wps=32931.5, ups=9.06, wpb=3634.9, bsz=160.6, num_updates=212200, lr=6.86479e-05, gnorm=2.245, loss_scale=4, train_wall=11, gb_free=19.7, wall=7467
2022-12-09 22:05:37 | INFO | train_inner | epoch 193:    730 / 1102 loss=6.743, nll_loss=2.869, ppl=7.31, wps=32701.9, ups=9.15, wpb=3574.2, bsz=139.3, num_updates=212300, lr=6.86317e-05, gnorm=2.314, loss_scale=4, train_wall=11, gb_free=19.4, wall=7478
2022-12-09 22:05:48 | INFO | train_inner | epoch 193:    830 / 1102 loss=6.754, nll_loss=2.878, ppl=7.35, wps=32090.1, ups=9.03, wpb=3552.4, bsz=137.2, num_updates=212400, lr=6.86156e-05, gnorm=2.388, loss_scale=4, train_wall=11, gb_free=19.5, wall=7489
2022-12-09 22:05:59 | INFO | train_inner | epoch 193:    930 / 1102 loss=6.789, nll_loss=2.92, ppl=7.57, wps=32351, ups=9, wpb=3596.5, bsz=141.4, num_updates=212500, lr=6.85994e-05, gnorm=2.304, loss_scale=4, train_wall=11, gb_free=19.7, wall=7500
2022-12-09 22:06:11 | INFO | train_inner | epoch 193:   1030 / 1102 loss=6.717, nll_loss=2.864, ppl=7.28, wps=32198.9, ups=8.88, wpb=3627.9, bsz=151.4, num_updates=212600, lr=6.85833e-05, gnorm=2.275, loss_scale=4, train_wall=11, gb_free=19.7, wall=7511
2022-12-09 22:06:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:07:02 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 3.663 | nll_loss 2.121 | ppl 4.35 | bleu 37.49 | wps 4192.6 | wpb 2835.3 | bsz 115.6 | num_updates 212672 | best_bleu 37.58
2022-12-09 22:07:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 212672 updates
2022-12-09 22:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint193.pt
2022-12-09 22:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint193.pt (epoch 193 @ 212672 updates, score 37.49) (writing took 1.253624483011663 seconds)
2022-12-09 22:07:03 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-12-09 22:07:03 | INFO | train | epoch 193 | loss 6.732 | nll_loss 2.866 | ppl 7.29 | wps 23810.7 | ups 6.64 | wpb 3583.6 | bsz 145.4 | num_updates 212672 | lr 6.85717e-05 | gnorm 2.316 | loss_scale 4 | train_wall 118 | gb_free 19.6 | wall 7564
2022-12-09 22:07:03 | INFO | fairseq.trainer | begin training epoch 194
2022-12-09 22:07:06 | INFO | train_inner | epoch 194:     28 / 1102 loss=6.767, nll_loss=2.909, ppl=7.51, wps=6531.9, ups=1.79, wpb=3641.5, bsz=145.6, num_updates=212700, lr=6.85672e-05, gnorm=2.284, loss_scale=4, train_wall=11, gb_free=19.3, wall=7567
2022-12-09 22:07:18 | INFO | train_inner | epoch 194:    128 / 1102 loss=6.679, nll_loss=2.822, ppl=7.07, wps=32566.7, ups=9.01, wpb=3612.6, bsz=149.7, num_updates=212800, lr=6.85511e-05, gnorm=2.221, loss_scale=4, train_wall=11, gb_free=19.5, wall=7578
2022-12-09 22:07:28 | INFO | train_inner | epoch 194:    228 / 1102 loss=6.704, nll_loss=2.84, ppl=7.16, wps=33005.1, ups=9.23, wpb=3576.1, bsz=143.8, num_updates=212900, lr=6.8535e-05, gnorm=2.277, loss_scale=4, train_wall=11, gb_free=19.2, wall=7589
2022-12-09 22:07:39 | INFO | train_inner | epoch 194:    328 / 1102 loss=6.741, nll_loss=2.861, ppl=7.26, wps=33033.9, ups=9.18, wpb=3600.1, bsz=141.9, num_updates=213000, lr=6.85189e-05, gnorm=2.343, loss_scale=4, train_wall=11, gb_free=19.6, wall=7600
2022-12-09 22:07:50 | INFO | train_inner | epoch 194:    428 / 1102 loss=6.733, nll_loss=2.868, ppl=7.3, wps=32826.5, ups=9.13, wpb=3597, bsz=142.4, num_updates=213100, lr=6.85028e-05, gnorm=2.269, loss_scale=4, train_wall=11, gb_free=19.4, wall=7611
2022-12-09 22:08:01 | INFO | train_inner | epoch 194:    528 / 1102 loss=6.695, nll_loss=2.841, ppl=7.16, wps=32837.3, ups=9.13, wpb=3596.6, bsz=159.8, num_updates=213200, lr=6.84867e-05, gnorm=2.286, loss_scale=4, train_wall=11, gb_free=19.6, wall=7622
2022-12-09 22:08:12 | INFO | train_inner | epoch 194:    628 / 1102 loss=6.67, nll_loss=2.814, ppl=7.03, wps=32832.7, ups=9.12, wpb=3598.2, bsz=159.9, num_updates=213300, lr=6.84707e-05, gnorm=2.38, loss_scale=4, train_wall=11, gb_free=21.3, wall=7633
2022-12-09 22:08:23 | INFO | train_inner | epoch 194:    728 / 1102 loss=6.766, nll_loss=2.893, ppl=7.43, wps=33024.9, ups=9.23, wpb=3576.5, bsz=140.6, num_updates=213400, lr=6.84546e-05, gnorm=2.356, loss_scale=4, train_wall=11, gb_free=19.7, wall=7644
2022-12-09 22:08:34 | INFO | train_inner | epoch 194:    828 / 1102 loss=6.742, nll_loss=2.879, ppl=7.35, wps=33158.8, ups=9.19, wpb=3609.1, bsz=137.3, num_updates=213500, lr=6.84386e-05, gnorm=2.287, loss_scale=4, train_wall=11, gb_free=19.8, wall=7654
2022-12-09 22:08:45 | INFO | train_inner | epoch 194:    928 / 1102 loss=6.769, nll_loss=2.901, ppl=7.47, wps=33236.5, ups=9.32, wpb=3566.8, bsz=145.5, num_updates=213600, lr=6.84226e-05, gnorm=2.421, loss_scale=4, train_wall=10, gb_free=19.6, wall=7665
2022-12-09 22:08:55 | INFO | train_inner | epoch 194:   1028 / 1102 loss=6.746, nll_loss=2.876, ppl=7.34, wps=32287.2, ups=9.21, wpb=3505.3, bsz=144.6, num_updates=213700, lr=6.84066e-05, gnorm=2.363, loss_scale=4, train_wall=11, gb_free=19.5, wall=7676
2022-12-09 22:09:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:09:40 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.46 | wps 4946.6 | wpb 2835.3 | bsz 115.6 | num_updates 213774 | best_bleu 37.58
2022-12-09 22:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 213774 updates
2022-12-09 22:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint194.pt
2022-12-09 22:09:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint194.pt (epoch 194 @ 213774 updates, score 37.46) (writing took 1.083434445783496 seconds)
2022-12-09 22:09:41 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-12-09 22:09:41 | INFO | train | epoch 194 | loss 6.731 | nll_loss 2.865 | ppl 7.29 | wps 24987.1 | ups 6.97 | wpb 3583.6 | bsz 145.4 | num_updates 213774 | lr 6.83947e-05 | gnorm 2.326 | loss_scale 4 | train_wall 117 | gb_free 19.3 | wall 7722
2022-12-09 22:09:41 | INFO | fairseq.trainer | begin training epoch 195
2022-12-09 22:09:44 | INFO | train_inner | epoch 195:     26 / 1102 loss=6.847, nll_loss=2.967, ppl=7.82, wps=7323.9, ups=2.05, wpb=3579.3, bsz=126.2, num_updates=213800, lr=6.83906e-05, gnorm=2.372, loss_scale=4, train_wall=11, gb_free=19.5, wall=7725
2022-12-09 22:09:55 | INFO | train_inner | epoch 195:    126 / 1102 loss=6.691, nll_loss=2.829, ppl=7.11, wps=33376.3, ups=9.25, wpb=3606.6, bsz=144, num_updates=213900, lr=6.83746e-05, gnorm=2.252, loss_scale=4, train_wall=11, gb_free=19.7, wall=7736
2022-12-09 22:10:06 | INFO | train_inner | epoch 195:    226 / 1102 loss=6.789, nll_loss=2.913, ppl=7.53, wps=33555.3, ups=9.35, wpb=3590, bsz=132.9, num_updates=214000, lr=6.83586e-05, gnorm=2.363, loss_scale=4, train_wall=10, gb_free=19.4, wall=7746
2022-12-09 22:10:17 | INFO | train_inner | epoch 195:    326 / 1102 loss=6.689, nll_loss=2.824, ppl=7.08, wps=33383.2, ups=9.3, wpb=3591.1, bsz=148.7, num_updates=214100, lr=6.83426e-05, gnorm=2.339, loss_scale=4, train_wall=11, gb_free=19.6, wall=7757
2022-12-09 22:10:27 | INFO | train_inner | epoch 195:    426 / 1102 loss=6.661, nll_loss=2.806, ppl=6.99, wps=32828.7, ups=9.28, wpb=3536.3, bsz=160.2, num_updates=214200, lr=6.83267e-05, gnorm=2.308, loss_scale=4, train_wall=11, gb_free=19.2, wall=7768
2022-12-09 22:10:38 | INFO | train_inner | epoch 195:    526 / 1102 loss=6.772, nll_loss=2.89, ppl=7.41, wps=33563, ups=9.23, wpb=3635.4, bsz=130.5, num_updates=214300, lr=6.83107e-05, gnorm=2.317, loss_scale=4, train_wall=11, gb_free=19.6, wall=7779
2022-12-09 22:10:49 | INFO | train_inner | epoch 195:    626 / 1102 loss=6.659, nll_loss=2.796, ppl=6.94, wps=33078.4, ups=9.35, wpb=3536.6, bsz=158.4, num_updates=214400, lr=6.82948e-05, gnorm=2.29, loss_scale=4, train_wall=10, gb_free=19.4, wall=7790
2022-12-09 22:11:00 | INFO | train_inner | epoch 195:    726 / 1102 loss=6.743, nll_loss=2.867, ppl=7.3, wps=33177.7, ups=9.35, wpb=3549, bsz=140.8, num_updates=214500, lr=6.82789e-05, gnorm=2.406, loss_scale=4, train_wall=10, gb_free=19.5, wall=7800
2022-12-09 22:11:10 | INFO | train_inner | epoch 195:    826 / 1102 loss=6.723, nll_loss=2.857, ppl=7.24, wps=33423.1, ups=9.31, wpb=3590.1, bsz=150.6, num_updates=214600, lr=6.8263e-05, gnorm=2.274, loss_scale=4, train_wall=11, gb_free=19.5, wall=7811
2022-12-09 22:11:21 | INFO | train_inner | epoch 195:    926 / 1102 loss=6.696, nll_loss=2.844, ppl=7.18, wps=33212, ups=9.22, wpb=3603.4, bsz=158.6, num_updates=214700, lr=6.82471e-05, gnorm=2.316, loss_scale=4, train_wall=11, gb_free=19.4, wall=7822
2022-12-09 22:11:32 | INFO | train_inner | epoch 195:   1026 / 1102 loss=6.772, nll_loss=2.908, ppl=7.51, wps=33137.2, ups=9.33, wpb=3550, bsz=145.4, num_updates=214800, lr=6.82312e-05, gnorm=2.307, loss_scale=4, train_wall=10, gb_free=19.5, wall=7833
2022-12-09 22:11:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:12:19 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 3.657 | nll_loss 2.12 | ppl 4.35 | bleu 37.54 | wps 4710.2 | wpb 2835.3 | bsz 115.6 | num_updates 214876 | best_bleu 37.58
2022-12-09 22:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 214876 updates
2022-12-09 22:12:19 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint195.pt
2022-12-09 22:12:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint195.pt (epoch 195 @ 214876 updates, score 37.54) (writing took 1.141395965591073 seconds)
2022-12-09 22:12:20 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-12-09 22:12:20 | INFO | train | epoch 195 | loss 6.727 | nll_loss 2.86 | ppl 7.26 | wps 24927.2 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 214876 | lr 6.82191e-05 | gnorm 2.311 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 7880
2022-12-09 22:12:20 | INFO | fairseq.trainer | begin training epoch 196
2022-12-09 22:12:23 | INFO | train_inner | epoch 196:     24 / 1102 loss=6.734, nll_loss=2.858, ppl=7.25, wps=7115, ups=1.98, wpb=3598.4, bsz=134.5, num_updates=214900, lr=6.82153e-05, gnorm=2.297, loss_scale=4, train_wall=11, gb_free=19.4, wall=7883
2022-12-09 22:12:33 | INFO | train_inner | epoch 196:    124 / 1102 loss=6.653, nll_loss=2.801, ppl=6.97, wps=32932.3, ups=9.21, wpb=3573.8, bsz=161.5, num_updates=215000, lr=6.81994e-05, gnorm=2.326, loss_scale=4, train_wall=11, gb_free=19.7, wall=7894
2022-12-09 22:12:45 | INFO | train_inner | epoch 196:    224 / 1102 loss=6.67, nll_loss=2.79, ppl=6.92, wps=31100.9, ups=8.86, wpb=3510.6, bsz=141.9, num_updates=215100, lr=6.81836e-05, gnorm=2.312, loss_scale=4, train_wall=11, gb_free=19.6, wall=7905
2022-12-09 22:12:56 | INFO | train_inner | epoch 196:    324 / 1102 loss=6.725, nll_loss=2.85, ppl=7.21, wps=32841.4, ups=9.12, wpb=3602.9, bsz=139.8, num_updates=215200, lr=6.81677e-05, gnorm=2.321, loss_scale=4, train_wall=11, gb_free=19.5, wall=7916
2022-12-09 22:13:06 | INFO | train_inner | epoch 196:    424 / 1102 loss=6.646, nll_loss=2.786, ppl=6.9, wps=33987.7, ups=9.31, wpb=3650.6, bsz=156.2, num_updates=215300, lr=6.81519e-05, gnorm=2.282, loss_scale=4, train_wall=10, gb_free=19.6, wall=7927
2022-12-09 22:13:17 | INFO | train_inner | epoch 196:    524 / 1102 loss=6.764, nll_loss=2.896, ppl=7.45, wps=33151.8, ups=9.2, wpb=3604.4, bsz=141.8, num_updates=215400, lr=6.81361e-05, gnorm=2.292, loss_scale=4, train_wall=11, gb_free=20, wall=7938
2022-12-09 22:13:28 | INFO | train_inner | epoch 196:    624 / 1102 loss=6.749, nll_loss=2.883, ppl=7.38, wps=33214.2, ups=9.22, wpb=3604.3, bsz=141.3, num_updates=215500, lr=6.81203e-05, gnorm=2.308, loss_scale=4, train_wall=11, gb_free=19.6, wall=7949
2022-12-09 22:13:39 | INFO | train_inner | epoch 196:    724 / 1102 loss=6.75, nll_loss=2.881, ppl=7.37, wps=32947.3, ups=9.18, wpb=3588.1, bsz=141.4, num_updates=215600, lr=6.81045e-05, gnorm=2.329, loss_scale=4, train_wall=11, gb_free=19.6, wall=7960
2022-12-09 22:13:50 | INFO | train_inner | epoch 196:    824 / 1102 loss=6.758, nll_loss=2.899, ppl=7.46, wps=33703.4, ups=9.21, wpb=3658.8, bsz=146, num_updates=215700, lr=6.80887e-05, gnorm=2.244, loss_scale=4, train_wall=11, gb_free=19.4, wall=7970
2022-12-09 22:14:01 | INFO | train_inner | epoch 196:    924 / 1102 loss=6.776, nll_loss=2.903, ppl=7.48, wps=32411.7, ups=9.32, wpb=3477, bsz=143.5, num_updates=215800, lr=6.80729e-05, gnorm=2.4, loss_scale=4, train_wall=10, gb_free=19.7, wall=7981
2022-12-09 22:14:12 | INFO | train_inner | epoch 196:   1024 / 1102 loss=6.78, nll_loss=2.915, ppl=7.54, wps=32770.7, ups=9.12, wpb=3595.2, bsz=144.1, num_updates=215900, lr=6.80571e-05, gnorm=2.283, loss_scale=4, train_wall=11, gb_free=19.8, wall=7992
2022-12-09 22:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:15:01 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 3.664 | nll_loss 2.119 | ppl 4.34 | bleu 37.48 | wps 4469.5 | wpb 2835.3 | bsz 115.6 | num_updates 215978 | best_bleu 37.58
2022-12-09 22:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 215978 updates
2022-12-09 22:15:01 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint196.pt
2022-12-09 22:15:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint196.pt (epoch 196 @ 215978 updates, score 37.48) (writing took 1.203533073887229 seconds)
2022-12-09 22:15:02 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-12-09 22:15:02 | INFO | train | epoch 196 | loss 6.726 | nll_loss 2.859 | ppl 7.26 | wps 24357 | ups 6.8 | wpb 3583.6 | bsz 145.4 | num_updates 215978 | lr 6.80448e-05 | gnorm 2.313 | loss_scale 4 | train_wall 117 | gb_free 19.8 | wall 8042
2022-12-09 22:15:02 | INFO | fairseq.trainer | begin training epoch 197
2022-12-09 22:15:05 | INFO | train_inner | epoch 197:     22 / 1102 loss=6.693, nll_loss=2.83, ppl=7.11, wps=6704.5, ups=1.89, wpb=3555.8, bsz=151.9, num_updates=216000, lr=6.80414e-05, gnorm=2.316, loss_scale=4, train_wall=11, gb_free=19.7, wall=8045
2022-12-09 22:15:15 | INFO | train_inner | epoch 197:    122 / 1102 loss=6.717, nll_loss=2.834, ppl=7.13, wps=32911.4, ups=9.28, wpb=3547.5, bsz=132.6, num_updates=216100, lr=6.80256e-05, gnorm=2.308, loss_scale=4, train_wall=11, gb_free=19.7, wall=8056
2022-12-09 22:15:26 | INFO | train_inner | epoch 197:    222 / 1102 loss=6.704, nll_loss=2.827, ppl=7.1, wps=32963.7, ups=9.16, wpb=3600.2, bsz=137.3, num_updates=216200, lr=6.80099e-05, gnorm=2.427, loss_scale=4, train_wall=11, gb_free=19.2, wall=8067
2022-12-09 22:15:37 | INFO | train_inner | epoch 197:    322 / 1102 loss=6.713, nll_loss=2.849, ppl=7.21, wps=32577.8, ups=9.07, wpb=3591.6, bsz=144.5, num_updates=216300, lr=6.79942e-05, gnorm=2.229, loss_scale=4, train_wall=11, gb_free=19.4, wall=8078
2022-12-09 22:15:48 | INFO | train_inner | epoch 197:    422 / 1102 loss=6.741, nll_loss=2.862, ppl=7.27, wps=32454.2, ups=9.09, wpb=3570.6, bsz=138, num_updates=216400, lr=6.79785e-05, gnorm=2.404, loss_scale=4, train_wall=11, gb_free=19.5, wall=8089
2022-12-09 22:15:59 | INFO | train_inner | epoch 197:    522 / 1102 loss=6.685, nll_loss=2.818, ppl=7.05, wps=32780.7, ups=9.17, wpb=3576.2, bsz=157.4, num_updates=216500, lr=6.79628e-05, gnorm=2.387, loss_scale=4, train_wall=11, gb_free=19.2, wall=8100
2022-12-09 22:16:10 | INFO | train_inner | epoch 197:    622 / 1102 loss=6.698, nll_loss=2.831, ppl=7.12, wps=32684.5, ups=9.08, wpb=3600.7, bsz=142.2, num_updates=216600, lr=6.79471e-05, gnorm=2.361, loss_scale=4, train_wall=11, gb_free=19.5, wall=8111
2022-12-09 22:16:21 | INFO | train_inner | epoch 197:    722 / 1102 loss=6.708, nll_loss=2.847, ppl=7.19, wps=32374, ups=9.07, wpb=3567.9, bsz=152, num_updates=216700, lr=6.79314e-05, gnorm=2.343, loss_scale=4, train_wall=11, gb_free=19.6, wall=8122
2022-12-09 22:16:32 | INFO | train_inner | epoch 197:    822 / 1102 loss=6.784, nll_loss=2.931, ppl=7.62, wps=33119.8, ups=9.05, wpb=3661.3, bsz=146.8, num_updates=216800, lr=6.79157e-05, gnorm=2.266, loss_scale=4, train_wall=11, gb_free=19.4, wall=8133
2022-12-09 22:16:43 | INFO | train_inner | epoch 197:    922 / 1102 loss=6.705, nll_loss=2.858, ppl=7.25, wps=33124.7, ups=9.06, wpb=3657.5, bsz=163.3, num_updates=216900, lr=6.79001e-05, gnorm=2.243, loss_scale=4, train_wall=11, gb_free=19.3, wall=8144
2022-12-09 22:16:54 | INFO | train_inner | epoch 197:   1022 / 1102 loss=6.72, nll_loss=2.854, ppl=7.23, wps=32146.4, ups=9.18, wpb=3503.1, bsz=146.1, num_updates=217000, lr=6.78844e-05, gnorm=2.373, loss_scale=4, train_wall=11, gb_free=19.8, wall=8155
2022-12-09 22:17:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:17:44 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 3.655 | nll_loss 2.116 | ppl 4.34 | bleu 37.43 | wps 4352.1 | wpb 2835.3 | bsz 115.6 | num_updates 217080 | best_bleu 37.58
2022-12-09 22:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 217080 updates
2022-12-09 22:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint197.pt
2022-12-09 22:17:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint197.pt (epoch 197 @ 217080 updates, score 37.43) (writing took 1.2564039342105389 seconds)
2022-12-09 22:17:46 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-12-09 22:17:46 | INFO | train | epoch 197 | loss 6.724 | nll_loss 2.857 | ppl 7.24 | wps 24098.1 | ups 6.72 | wpb 3583.6 | bsz 145.4 | num_updates 217080 | lr 6.78719e-05 | gnorm 2.337 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 8206
2022-12-09 22:17:46 | INFO | fairseq.trainer | begin training epoch 198
2022-12-09 22:17:48 | INFO | train_inner | epoch 198:     20 / 1102 loss=6.815, nll_loss=2.947, ppl=7.71, wps=6632.1, ups=1.85, wpb=3582.9, bsz=139.1, num_updates=217100, lr=6.78688e-05, gnorm=2.356, loss_scale=4, train_wall=11, gb_free=19.4, wall=8209
2022-12-09 22:17:59 | INFO | train_inner | epoch 198:    120 / 1102 loss=6.706, nll_loss=2.828, ppl=7.1, wps=32661.8, ups=9.3, wpb=3510.4, bsz=135.8, num_updates=217200, lr=6.78532e-05, gnorm=2.452, loss_scale=4, train_wall=11, gb_free=19.6, wall=8220
2022-12-09 22:18:10 | INFO | train_inner | epoch 198:    220 / 1102 loss=6.638, nll_loss=2.767, ppl=6.81, wps=32056.6, ups=9.13, wpb=3512.9, bsz=155.9, num_updates=217300, lr=6.78375e-05, gnorm=2.329, loss_scale=4, train_wall=11, gb_free=19.8, wall=8231
2022-12-09 22:18:21 | INFO | train_inner | epoch 198:    320 / 1102 loss=6.7, nll_loss=2.822, ppl=7.07, wps=32767.8, ups=9.08, wpb=3610, bsz=142.4, num_updates=217400, lr=6.78219e-05, gnorm=2.323, loss_scale=4, train_wall=11, gb_free=19.2, wall=8242
2022-12-09 22:18:32 | INFO | train_inner | epoch 198:    420 / 1102 loss=6.772, nll_loss=2.895, ppl=7.44, wps=32418.2, ups=9.17, wpb=3533.7, bsz=135, num_updates=217500, lr=6.78064e-05, gnorm=2.379, loss_scale=4, train_wall=11, gb_free=19.4, wall=8252
2022-12-09 22:18:43 | INFO | train_inner | epoch 198:    520 / 1102 loss=6.725, nll_loss=2.858, ppl=7.25, wps=32500.3, ups=9.04, wpb=3595.1, bsz=146.2, num_updates=217600, lr=6.77908e-05, gnorm=2.279, loss_scale=4, train_wall=11, gb_free=19.4, wall=8264
2022-12-09 22:18:54 | INFO | train_inner | epoch 198:    620 / 1102 loss=6.736, nll_loss=2.865, ppl=7.29, wps=32830.5, ups=9.18, wpb=3577.9, bsz=134.6, num_updates=217700, lr=6.77752e-05, gnorm=2.311, loss_scale=4, train_wall=11, gb_free=19.5, wall=8274
2022-12-09 22:19:05 | INFO | train_inner | epoch 198:    720 / 1102 loss=6.745, nll_loss=2.875, ppl=7.34, wps=32854.9, ups=9.06, wpb=3627.8, bsz=143.7, num_updates=217800, lr=6.77596e-05, gnorm=2.403, loss_scale=4, train_wall=11, gb_free=19.5, wall=8285
2022-12-09 22:19:16 | INFO | train_inner | epoch 198:    820 / 1102 loss=6.75, nll_loss=2.893, ppl=7.43, wps=33232, ups=9.16, wpb=3626.1, bsz=145.5, num_updates=217900, lr=6.77441e-05, gnorm=2.252, loss_scale=4, train_wall=11, gb_free=19.5, wall=8296
2022-12-09 22:19:27 | INFO | train_inner | epoch 198:    920 / 1102 loss=6.69, nll_loss=2.829, ppl=7.11, wps=32804.1, ups=9.06, wpb=3619.9, bsz=157.2, num_updates=218000, lr=6.77285e-05, gnorm=2.381, loss_scale=4, train_wall=11, gb_free=19.4, wall=8307
2022-12-09 22:19:38 | INFO | train_inner | epoch 198:   1020 / 1102 loss=6.746, nll_loss=2.873, ppl=7.33, wps=32443.2, ups=9.17, wpb=3539.2, bsz=149.4, num_updates=218100, lr=6.7713e-05, gnorm=2.413, loss_scale=4, train_wall=11, gb_free=19.4, wall=8318
2022-12-09 22:19:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:20:30 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 3.655 | nll_loss 2.118 | ppl 4.34 | bleu 37.51 | wps 4179.1 | wpb 2835.3 | bsz 115.6 | num_updates 218182 | best_bleu 37.58
2022-12-09 22:20:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 218182 updates
2022-12-09 22:20:31 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint198.pt
2022-12-09 22:20:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint198.pt (epoch 198 @ 218182 updates, score 37.51) (writing took 1.1988787157461047 seconds)
2022-12-09 22:20:31 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-12-09 22:20:31 | INFO | train | epoch 198 | loss 6.723 | nll_loss 2.854 | ppl 7.23 | wps 23854.5 | ups 6.66 | wpb 3583.6 | bsz 145.4 | num_updates 218182 | lr 6.77003e-05 | gnorm 2.347 | loss_scale 4 | train_wall 118 | gb_free 19.7 | wall 8372
2022-12-09 22:20:31 | INFO | fairseq.trainer | begin training epoch 199
2022-12-09 22:20:34 | INFO | train_inner | epoch 199:     18 / 1102 loss=6.745, nll_loss=2.9, ppl=7.46, wps=6559.4, ups=1.79, wpb=3659, bsz=154.2, num_updates=218200, lr=6.76975e-05, gnorm=2.272, loss_scale=4, train_wall=11, gb_free=19.3, wall=8374
2022-12-09 22:20:44 | INFO | train_inner | epoch 199:    118 / 1102 loss=6.666, nll_loss=2.795, ppl=6.94, wps=32889.8, ups=9.29, wpb=3538.6, bsz=145.1, num_updates=218300, lr=6.7682e-05, gnorm=2.349, loss_scale=4, train_wall=11, gb_free=19.7, wall=8385
2022-12-09 22:20:55 | INFO | train_inner | epoch 199:    218 / 1102 loss=6.629, nll_loss=2.774, ppl=6.84, wps=32248.6, ups=9.1, wpb=3545.1, bsz=165, num_updates=218400, lr=6.76665e-05, gnorm=2.368, loss_scale=4, train_wall=11, gb_free=19.8, wall=8396
2022-12-09 22:21:06 | INFO | train_inner | epoch 199:    318 / 1102 loss=6.702, nll_loss=2.822, ppl=7.07, wps=32357, ups=9.09, wpb=3557.7, bsz=140.2, num_updates=218500, lr=6.7651e-05, gnorm=2.328, loss_scale=4, train_wall=11, gb_free=19.3, wall=8407
2022-12-09 22:21:17 | INFO | train_inner | epoch 199:    418 / 1102 loss=6.751, nll_loss=2.874, ppl=7.33, wps=32865.6, ups=9.13, wpb=3598.2, bsz=136.9, num_updates=218600, lr=6.76355e-05, gnorm=2.401, loss_scale=4, train_wall=11, gb_free=19.8, wall=8418
2022-12-09 22:21:28 | INFO | train_inner | epoch 199:    518 / 1102 loss=6.717, nll_loss=2.846, ppl=7.19, wps=32820.7, ups=9.07, wpb=3617.5, bsz=139.8, num_updates=218700, lr=6.76201e-05, gnorm=2.299, loss_scale=4, train_wall=11, gb_free=19.2, wall=8429
2022-12-09 22:21:39 | INFO | train_inner | epoch 199:    618 / 1102 loss=6.786, nll_loss=2.91, ppl=7.52, wps=32191.1, ups=9.09, wpb=3541.3, bsz=141.8, num_updates=218800, lr=6.76046e-05, gnorm=2.402, loss_scale=4, train_wall=11, gb_free=19.6, wall=8440
2022-12-09 22:21:50 | INFO | train_inner | epoch 199:    718 / 1102 loss=6.719, nll_loss=2.838, ppl=7.15, wps=32809.2, ups=9.24, wpb=3549.4, bsz=132.3, num_updates=218900, lr=6.75892e-05, gnorm=2.461, loss_scale=4, train_wall=11, gb_free=19.7, wall=8451
2022-12-09 22:22:01 | INFO | train_inner | epoch 199:    818 / 1102 loss=6.703, nll_loss=2.856, ppl=7.24, wps=32681.7, ups=8.99, wpb=3635.6, bsz=161.4, num_updates=219000, lr=6.75737e-05, gnorm=2.23, loss_scale=4, train_wall=11, gb_free=19.4, wall=8462
2022-12-09 22:22:12 | INFO | train_inner | epoch 199:    918 / 1102 loss=6.718, nll_loss=2.839, ppl=7.15, wps=32318.1, ups=9.11, wpb=3547, bsz=136.7, num_updates=219100, lr=6.75583e-05, gnorm=2.32, loss_scale=4, train_wall=11, gb_free=19.3, wall=8473
2022-12-09 22:22:23 | INFO | train_inner | epoch 199:   1018 / 1102 loss=6.796, nll_loss=2.933, ppl=7.63, wps=33161.2, ups=9.1, wpb=3645.7, bsz=146.3, num_updates=219200, lr=6.75429e-05, gnorm=2.301, loss_scale=4, train_wall=11, gb_free=19.4, wall=8484
2022-12-09 22:22:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:23:13 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 3.652 | nll_loss 2.118 | ppl 4.34 | bleu 37.52 | wps 4434.9 | wpb 2835.3 | bsz 115.6 | num_updates 219284 | best_bleu 37.58
2022-12-09 22:23:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 219284 updates
2022-12-09 22:23:14 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint199.pt
2022-12-09 22:23:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint199.pt (epoch 199 @ 219284 updates, score 37.52) (writing took 1.150572830811143 seconds)
2022-12-09 22:23:14 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-12-09 22:23:14 | INFO | train | epoch 199 | loss 6.72 | nll_loss 2.852 | ppl 7.22 | wps 24216.1 | ups 6.76 | wpb 3583.6 | bsz 145.4 | num_updates 219284 | lr 6.753e-05 | gnorm 2.336 | loss_scale 4 | train_wall 118 | gb_free 19.7 | wall 8535
2022-12-09 22:23:14 | INFO | fairseq.trainer | begin training epoch 200
2022-12-09 22:23:16 | INFO | train_inner | epoch 200:     16 / 1102 loss=6.753, nll_loss=2.895, ppl=7.44, wps=6833.9, ups=1.88, wpb=3638.9, bsz=147, num_updates=219300, lr=6.75275e-05, gnorm=2.256, loss_scale=4, train_wall=11, gb_free=19.6, wall=8537
2022-12-09 22:23:27 | INFO | train_inner | epoch 200:    116 / 1102 loss=6.701, nll_loss=2.822, ppl=7.07, wps=33168.4, ups=9.15, wpb=3626.7, bsz=134, num_updates=219400, lr=6.75121e-05, gnorm=2.264, loss_scale=4, train_wall=11, gb_free=19.5, wall=8548
2022-12-09 22:23:38 | INFO | train_inner | epoch 200:    216 / 1102 loss=6.662, nll_loss=2.799, ppl=6.96, wps=32444.1, ups=9.24, wpb=3509.9, bsz=150.3, num_updates=219500, lr=6.74967e-05, gnorm=2.316, loss_scale=4, train_wall=11, gb_free=19.4, wall=8559
2022-12-09 22:23:49 | INFO | train_inner | epoch 200:    316 / 1102 loss=6.717, nll_loss=2.862, ppl=7.27, wps=32915.7, ups=9.16, wpb=3595.1, bsz=155.3, num_updates=219600, lr=6.74814e-05, gnorm=2.266, loss_scale=4, train_wall=11, gb_free=19.7, wall=8570
2022-12-09 22:24:00 | INFO | train_inner | epoch 200:    416 / 1102 loss=6.709, nll_loss=2.858, ppl=7.25, wps=32863.3, ups=9.08, wpb=3620.2, bsz=152.6, num_updates=219700, lr=6.7466e-05, gnorm=2.29, loss_scale=4, train_wall=11, gb_free=20.1, wall=8581
2022-12-09 22:24:11 | INFO | train_inner | epoch 200:    516 / 1102 loss=6.754, nll_loss=2.856, ppl=7.24, wps=32078.8, ups=9.08, wpb=3533.4, bsz=129.1, num_updates=219800, lr=6.74507e-05, gnorm=2.504, loss_scale=4, train_wall=11, gb_free=19.3, wall=8592
2022-12-09 22:24:22 | INFO | train_inner | epoch 200:    616 / 1102 loss=6.738, nll_loss=2.856, ppl=7.24, wps=32433.3, ups=9.11, wpb=3559.1, bsz=133.8, num_updates=219900, lr=6.74353e-05, gnorm=2.322, loss_scale=4, train_wall=11, gb_free=19.3, wall=8603
2022-12-09 22:24:33 | INFO | train_inner | epoch 200:    716 / 1102 loss=6.719, nll_loss=2.841, ppl=7.17, wps=32741.4, ups=9.12, wpb=3588.1, bsz=140.2, num_updates=220000, lr=6.742e-05, gnorm=2.346, loss_scale=4, train_wall=11, gb_free=19.6, wall=8614
2022-12-09 22:24:44 | INFO | train_inner | epoch 200:    816 / 1102 loss=6.736, nll_loss=2.884, ppl=7.38, wps=32970, ups=9.1, wpb=3622.8, bsz=150.7, num_updates=220100, lr=6.74047e-05, gnorm=2.276, loss_scale=4, train_wall=11, gb_free=19.5, wall=8625
2022-12-09 22:24:55 | INFO | train_inner | epoch 200:    916 / 1102 loss=6.681, nll_loss=2.812, ppl=7.02, wps=32653.6, ups=9.17, wpb=3561.2, bsz=153.8, num_updates=220200, lr=6.73894e-05, gnorm=2.307, loss_scale=4, train_wall=11, gb_free=20, wall=8636
2022-12-09 22:25:06 | INFO | train_inner | epoch 200:   1016 / 1102 loss=6.733, nll_loss=2.85, ppl=7.21, wps=32716.9, ups=9.12, wpb=3586.8, bsz=134.7, num_updates=220300, lr=6.73741e-05, gnorm=2.457, loss_scale=4, train_wall=11, gb_free=19.4, wall=8646
2022-12-09 22:25:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:25:55 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 3.662 | nll_loss 2.119 | ppl 4.34 | bleu 37.45 | wps 4548 | wpb 2835.3 | bsz 115.6 | num_updates 220386 | best_bleu 37.58
2022-12-09 22:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 220386 updates
2022-12-09 22:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint200.pt
2022-12-09 22:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint200.pt (epoch 200 @ 220386 updates, score 37.45) (writing took 1.1936183953657746 seconds)
2022-12-09 22:25:56 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-12-09 22:25:56 | INFO | train | epoch 200 | loss 6.718 | nll_loss 2.85 | ppl 7.21 | wps 24352.6 | ups 6.8 | wpb 3583.6 | bsz 145.4 | num_updates 220386 | lr 6.73609e-05 | gnorm 2.33 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 8697
2022-12-09 22:25:57 | INFO | fairseq.trainer | begin training epoch 201
2022-12-09 22:25:58 | INFO | train_inner | epoch 201:     14 / 1102 loss=6.712, nll_loss=2.87, ppl=7.31, wps=6881.1, ups=1.9, wpb=3613.8, bsz=173.8, num_updates=220400, lr=6.73588e-05, gnorm=2.279, loss_scale=4, train_wall=11, gb_free=19.3, wall=8699
2022-12-09 22:26:09 | INFO | train_inner | epoch 201:    114 / 1102 loss=6.622, nll_loss=2.776, ppl=6.85, wps=32727.8, ups=9.03, wpb=3622.5, bsz=164.6, num_updates=220500, lr=6.73435e-05, gnorm=2.285, loss_scale=4, train_wall=11, gb_free=19.7, wall=8710
2022-12-09 22:26:20 | INFO | train_inner | epoch 201:    214 / 1102 loss=6.692, nll_loss=2.811, ppl=7.02, wps=32752.4, ups=9.31, wpb=3519.6, bsz=138.3, num_updates=220600, lr=6.73282e-05, gnorm=2.371, loss_scale=4, train_wall=11, gb_free=19.4, wall=8721
2022-12-09 22:26:31 | INFO | train_inner | epoch 201:    314 / 1102 loss=6.764, nll_loss=2.895, ppl=7.44, wps=33310, ups=9.09, wpb=3663.6, bsz=138.6, num_updates=220700, lr=6.7313e-05, gnorm=2.307, loss_scale=4, train_wall=11, gb_free=19.2, wall=8732
2022-12-09 22:26:42 | INFO | train_inner | epoch 201:    414 / 1102 loss=6.731, nll_loss=2.862, ppl=7.27, wps=32730.6, ups=9.13, wpb=3584.6, bsz=138.9, num_updates=220800, lr=6.72977e-05, gnorm=2.306, loss_scale=4, train_wall=11, gb_free=19.5, wall=8743
2022-12-09 22:26:53 | INFO | train_inner | epoch 201:    514 / 1102 loss=6.732, nll_loss=2.838, ppl=7.15, wps=32525.9, ups=9.22, wpb=3526.3, bsz=125.8, num_updates=220900, lr=6.72825e-05, gnorm=2.392, loss_scale=4, train_wall=11, gb_free=19.7, wall=8754
2022-12-09 22:27:04 | INFO | train_inner | epoch 201:    614 / 1102 loss=6.783, nll_loss=2.875, ppl=7.33, wps=32273.7, ups=9.15, wpb=3529, bsz=115.1, num_updates=221000, lr=6.72673e-05, gnorm=2.426, loss_scale=4, train_wall=11, gb_free=19.3, wall=8765
2022-12-09 22:27:15 | INFO | train_inner | epoch 201:    714 / 1102 loss=6.691, nll_loss=2.826, ppl=7.09, wps=32507.6, ups=9.17, wpb=3544.3, bsz=149.1, num_updates=221100, lr=6.72521e-05, gnorm=2.324, loss_scale=4, train_wall=11, gb_free=19.7, wall=8775
2022-12-09 22:27:26 | INFO | train_inner | epoch 201:    814 / 1102 loss=6.777, nll_loss=2.922, ppl=7.58, wps=32921.9, ups=9.09, wpb=3620.1, bsz=150.9, num_updates=221200, lr=6.72369e-05, gnorm=2.308, loss_scale=4, train_wall=11, gb_free=19.7, wall=8786
2022-12-09 22:27:37 | INFO | train_inner | epoch 201:    914 / 1102 loss=6.722, nll_loss=2.865, ppl=7.29, wps=32869.2, ups=9.05, wpb=3632.5, bsz=156.6, num_updates=221300, lr=6.72217e-05, gnorm=2.255, loss_scale=4, train_wall=11, gb_free=20, wall=8798
2022-12-09 22:27:48 | INFO | train_inner | epoch 201:   1014 / 1102 loss=6.634, nll_loss=2.77, ppl=6.82, wps=32011.2, ups=9.07, wpb=3528.6, bsz=166, num_updates=221400, lr=6.72065e-05, gnorm=2.355, loss_scale=4, train_wall=11, gb_free=19.6, wall=8809
2022-12-09 22:27:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:28:37 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 3.665 | nll_loss 2.123 | ppl 4.35 | bleu 37.45 | wps 4642.7 | wpb 2835.3 | bsz 115.6 | num_updates 221488 | best_bleu 37.58
2022-12-09 22:28:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 221488 updates
2022-12-09 22:28:38 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint201.pt
2022-12-09 22:28:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint201.pt (epoch 201 @ 221488 updates, score 37.45) (writing took 1.0887834075838327 seconds)
2022-12-09 22:28:38 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-12-09 22:28:38 | INFO | train | epoch 201 | loss 6.716 | nll_loss 2.847 | ppl 7.2 | wps 24460.2 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 221488 | lr 6.71931e-05 | gnorm 2.327 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 8858
2022-12-09 22:28:38 | INFO | fairseq.trainer | begin training epoch 202
2022-12-09 22:28:40 | INFO | train_inner | epoch 202:     12 / 1102 loss=6.756, nll_loss=2.894, ppl=7.43, wps=7053, ups=1.94, wpb=3640.5, bsz=148.2, num_updates=221500, lr=6.71913e-05, gnorm=2.28, loss_scale=4, train_wall=11, gb_free=19.7, wall=8860
2022-12-09 22:28:50 | INFO | train_inner | epoch 202:    112 / 1102 loss=6.626, nll_loss=2.763, ppl=6.79, wps=33036, ups=9.21, wpb=3587.3, bsz=144.7, num_updates=221600, lr=6.71762e-05, gnorm=2.251, loss_scale=4, train_wall=11, gb_free=19.3, wall=8871
2022-12-09 22:29:01 | INFO | train_inner | epoch 202:    212 / 1102 loss=6.658, nll_loss=2.769, ppl=6.82, wps=33568.8, ups=9.48, wpb=3542.2, bsz=137.1, num_updates=221700, lr=6.7161e-05, gnorm=2.388, loss_scale=4, train_wall=10, gb_free=19.6, wall=8882
2022-12-09 22:29:12 | INFO | train_inner | epoch 202:    312 / 1102 loss=6.694, nll_loss=2.821, ppl=7.07, wps=33601.7, ups=9.29, wpb=3617.6, bsz=147.5, num_updates=221800, lr=6.71459e-05, gnorm=2.281, loss_scale=4, train_wall=11, gb_free=19.6, wall=8892
2022-12-09 22:29:22 | INFO | train_inner | epoch 202:    412 / 1102 loss=6.739, nll_loss=2.882, ppl=7.37, wps=33984.1, ups=9.37, wpb=3627.5, bsz=150.2, num_updates=221900, lr=6.71307e-05, gnorm=2.274, loss_scale=4, train_wall=10, gb_free=19.4, wall=8903
2022-12-09 22:29:33 | INFO | train_inner | epoch 202:    512 / 1102 loss=6.643, nll_loss=2.779, ppl=6.86, wps=33678.1, ups=9.37, wpb=3593.5, bsz=152.2, num_updates=222000, lr=6.71156e-05, gnorm=2.358, loss_scale=4, train_wall=10, gb_free=19.4, wall=8914
2022-12-09 22:29:44 | INFO | train_inner | epoch 202:    612 / 1102 loss=6.653, nll_loss=2.774, ppl=6.84, wps=32980.6, ups=9.35, wpb=3526.8, bsz=153, num_updates=222100, lr=6.71005e-05, gnorm=2.416, loss_scale=4, train_wall=10, gb_free=19.6, wall=8924
2022-12-09 22:29:54 | INFO | train_inner | epoch 202:    712 / 1102 loss=6.783, nll_loss=2.913, ppl=7.53, wps=33723.4, ups=9.39, wpb=3591.6, bsz=146.4, num_updates=222200, lr=6.70854e-05, gnorm=2.44, loss_scale=4, train_wall=10, gb_free=19.5, wall=8935
2022-12-09 22:30:05 | INFO | train_inner | epoch 202:    812 / 1102 loss=6.75, nll_loss=2.872, ppl=7.32, wps=32968.4, ups=9.3, wpb=3545, bsz=134.3, num_updates=222300, lr=6.70703e-05, gnorm=2.439, loss_scale=4, train_wall=11, gb_free=19.4, wall=8946
2022-12-09 22:30:16 | INFO | train_inner | epoch 202:    912 / 1102 loss=6.746, nll_loss=2.885, ppl=7.39, wps=33076, ups=9.26, wpb=3570.9, bsz=150.5, num_updates=222400, lr=6.70552e-05, gnorm=2.428, loss_scale=4, train_wall=11, gb_free=19.5, wall=8957
2022-12-09 22:30:27 | INFO | train_inner | epoch 202:   1012 / 1102 loss=6.747, nll_loss=2.891, ppl=7.42, wps=33349.6, ups=9.2, wpb=3625.7, bsz=153.1, num_updates=222500, lr=6.70402e-05, gnorm=2.33, loss_scale=4, train_wall=11, gb_free=19.3, wall=8967
2022-12-09 22:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:31:17 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 3.659 | nll_loss 2.118 | ppl 4.34 | bleu 37.51 | wps 4493.7 | wpb 2835.3 | bsz 115.6 | num_updates 222590 | best_bleu 37.58
2022-12-09 22:31:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 222590 updates
2022-12-09 22:31:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint202.pt
2022-12-09 22:31:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint202.pt (epoch 202 @ 222590 updates, score 37.51) (writing took 1.157590664923191 seconds)
2022-12-09 22:31:18 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-12-09 22:31:18 | INFO | train | epoch 202 | loss 6.715 | nll_loss 2.845 | ppl 7.19 | wps 24685 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 222590 | lr 6.70266e-05 | gnorm 2.356 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 9018
2022-12-09 22:31:18 | INFO | fairseq.trainer | begin training epoch 203
2022-12-09 22:31:19 | INFO | train_inner | epoch 203:     10 / 1102 loss=6.837, nll_loss=2.965, ppl=7.81, wps=6876.3, ups=1.91, wpb=3609.6, bsz=129.1, num_updates=222600, lr=6.70251e-05, gnorm=2.304, loss_scale=4, train_wall=11, gb_free=19.3, wall=9020
2022-12-09 22:31:30 | INFO | train_inner | epoch 203:    110 / 1102 loss=6.652, nll_loss=2.798, ppl=6.95, wps=33653.5, ups=9.26, wpb=3635.7, bsz=154.8, num_updates=222700, lr=6.701e-05, gnorm=2.28, loss_scale=4, train_wall=11, gb_free=19.5, wall=9031
2022-12-09 22:31:41 | INFO | train_inner | epoch 203:    210 / 1102 loss=6.682, nll_loss=2.804, ppl=6.98, wps=32910.6, ups=9.16, wpb=3591.3, bsz=142.3, num_updates=222800, lr=6.6995e-05, gnorm=2.387, loss_scale=4, train_wall=11, gb_free=19.3, wall=9042
2022-12-09 22:31:52 | INFO | train_inner | epoch 203:    310 / 1102 loss=6.668, nll_loss=2.794, ppl=6.94, wps=32730.5, ups=9.32, wpb=3510.5, bsz=144.9, num_updates=222900, lr=6.698e-05, gnorm=2.6, loss_scale=4, train_wall=10, gb_free=19.4, wall=9052
2022-12-09 22:32:03 | INFO | train_inner | epoch 203:    410 / 1102 loss=6.715, nll_loss=2.844, ppl=7.18, wps=32774.9, ups=9.17, wpb=3575.8, bsz=148.6, num_updates=223000, lr=6.6965e-05, gnorm=2.489, loss_scale=4, train_wall=11, gb_free=19.7, wall=9063
2022-12-09 22:32:14 | INFO | train_inner | epoch 203:    510 / 1102 loss=6.691, nll_loss=2.829, ppl=7.11, wps=33418.5, ups=9.18, wpb=3640.3, bsz=151, num_updates=223100, lr=6.69499e-05, gnorm=2.277, loss_scale=4, train_wall=11, gb_free=19.5, wall=9074
2022-12-09 22:32:24 | INFO | train_inner | epoch 203:    610 / 1102 loss=6.763, nll_loss=2.883, ppl=7.38, wps=32903.1, ups=9.24, wpb=3561.9, bsz=133.4, num_updates=223200, lr=6.69349e-05, gnorm=2.385, loss_scale=4, train_wall=11, gb_free=19.4, wall=9085
2022-12-09 22:32:35 | INFO | train_inner | epoch 203:    710 / 1102 loss=6.778, nll_loss=2.89, ppl=7.41, wps=32670.2, ups=9.2, wpb=3551, bsz=129, num_updates=223300, lr=6.692e-05, gnorm=2.43, loss_scale=4, train_wall=11, gb_free=19.8, wall=9096
2022-12-09 22:32:46 | INFO | train_inner | epoch 203:    810 / 1102 loss=6.718, nll_loss=2.845, ppl=7.19, wps=32306.2, ups=9.05, wpb=3571.3, bsz=145.4, num_updates=223400, lr=6.6905e-05, gnorm=2.345, loss_scale=4, train_wall=11, gb_free=19.6, wall=9107
2022-12-09 22:32:57 | INFO | train_inner | epoch 203:    910 / 1102 loss=6.732, nll_loss=2.863, ppl=7.28, wps=32835.4, ups=9.11, wpb=3604.1, bsz=149.4, num_updates=223500, lr=6.689e-05, gnorm=2.377, loss_scale=4, train_wall=11, gb_free=19.5, wall=9118
2022-12-09 22:33:08 | INFO | train_inner | epoch 203:   1010 / 1102 loss=6.769, nll_loss=2.904, ppl=7.49, wps=32418.4, ups=8.99, wpb=3607.8, bsz=141.8, num_updates=223600, lr=6.6875e-05, gnorm=2.38, loss_scale=4, train_wall=11, gb_free=19.6, wall=9129
2022-12-09 22:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:33:57 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 3.665 | nll_loss 2.123 | ppl 4.36 | bleu 37.58 | wps 4646.7 | wpb 2835.3 | bsz 115.6 | num_updates 223692 | best_bleu 37.58
2022-12-09 22:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 223692 updates
2022-12-09 22:33:58 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint203.pt
2022-12-09 22:33:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint203.pt (epoch 203 @ 223692 updates, score 37.58) (writing took 1.4915456688031554 seconds)
2022-12-09 22:33:59 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-12-09 22:33:59 | INFO | train | epoch 203 | loss 6.714 | nll_loss 2.843 | ppl 7.18 | wps 24539.8 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 223692 | lr 6.68613e-05 | gnorm 2.39 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 9179
2022-12-09 22:33:59 | INFO | fairseq.trainer | begin training epoch 204
2022-12-09 22:34:00 | INFO | train_inner | epoch 204:      8 / 1102 loss=6.645, nll_loss=2.783, ppl=6.88, wps=6860.5, ups=1.94, wpb=3538.5, bsz=161.8, num_updates=223700, lr=6.68601e-05, gnorm=2.384, loss_scale=4, train_wall=11, gb_free=19.6, wall=9181
2022-12-09 22:34:11 | INFO | train_inner | epoch 204:    108 / 1102 loss=6.758, nll_loss=2.89, ppl=7.41, wps=33690, ups=9.39, wpb=3586.7, bsz=137, num_updates=223800, lr=6.68452e-05, gnorm=2.338, loss_scale=4, train_wall=10, gb_free=19.8, wall=9191
2022-12-09 22:34:21 | INFO | train_inner | epoch 204:    208 / 1102 loss=6.728, nll_loss=2.839, ppl=7.15, wps=32930.2, ups=9.33, wpb=3528.4, bsz=136.9, num_updates=223900, lr=6.68302e-05, gnorm=2.382, loss_scale=4, train_wall=10, gb_free=19.7, wall=9202
2022-12-09 22:34:32 | INFO | train_inner | epoch 204:    308 / 1102 loss=6.647, nll_loss=2.758, ppl=6.76, wps=32408.9, ups=9.29, wpb=3488.8, bsz=141.9, num_updates=224000, lr=6.68153e-05, gnorm=2.389, loss_scale=4, train_wall=11, gb_free=19.3, wall=9213
2022-12-09 22:34:43 | INFO | train_inner | epoch 204:    408 / 1102 loss=6.728, nll_loss=2.865, ppl=7.28, wps=33215.8, ups=9.19, wpb=3613.2, bsz=146.6, num_updates=224100, lr=6.68004e-05, gnorm=2.383, loss_scale=4, train_wall=11, gb_free=19.3, wall=9224
2022-12-09 22:34:54 | INFO | train_inner | epoch 204:    508 / 1102 loss=6.768, nll_loss=2.886, ppl=7.39, wps=33143.7, ups=9.21, wpb=3597.1, bsz=133.1, num_updates=224200, lr=6.67855e-05, gnorm=2.418, loss_scale=4, train_wall=11, gb_free=19.6, wall=9234
2022-12-09 22:35:05 | INFO | train_inner | epoch 204:    608 / 1102 loss=6.659, nll_loss=2.793, ppl=6.93, wps=32950.8, ups=9.19, wpb=3586.5, bsz=149.3, num_updates=224300, lr=6.67706e-05, gnorm=2.296, loss_scale=4, train_wall=11, gb_free=19.6, wall=9245
2022-12-09 22:35:16 | INFO | train_inner | epoch 204:    708 / 1102 loss=6.703, nll_loss=2.835, ppl=7.13, wps=33274.8, ups=9.18, wpb=3624.5, bsz=146.6, num_updates=224400, lr=6.67557e-05, gnorm=2.389, loss_scale=4, train_wall=11, gb_free=19.7, wall=9256
2022-12-09 22:35:27 | INFO | train_inner | epoch 204:    808 / 1102 loss=6.69, nll_loss=2.843, ppl=7.17, wps=32813.1, ups=9.05, wpb=3627.6, bsz=169, num_updates=224500, lr=6.67409e-05, gnorm=2.349, loss_scale=4, train_wall=11, gb_free=19.5, wall=9267
2022-12-09 22:35:37 | INFO | train_inner | epoch 204:    908 / 1102 loss=6.7, nll_loss=2.832, ppl=7.12, wps=33403.9, ups=9.33, wpb=3581.8, bsz=142.8, num_updates=224600, lr=6.6726e-05, gnorm=2.36, loss_scale=4, train_wall=10, gb_free=19.6, wall=9278
2022-12-09 22:35:48 | INFO | train_inner | epoch 204:   1008 / 1102 loss=6.685, nll_loss=2.831, ppl=7.11, wps=33123, ups=9.07, wpb=3651.3, bsz=157.8, num_updates=224700, lr=6.67112e-05, gnorm=2.337, loss_scale=4, train_wall=11, gb_free=19.5, wall=9289
2022-12-09 22:35:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:36:39 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 3.663 | nll_loss 2.121 | ppl 4.35 | bleu 37.49 | wps 4480.5 | wpb 2835.3 | bsz 115.6 | num_updates 224794 | best_bleu 37.58
2022-12-09 22:36:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 224794 updates
2022-12-09 22:36:40 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint204.pt
2022-12-09 22:36:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint204.pt (epoch 204 @ 224794 updates, score 37.49) (writing took 1.1722702514380217 seconds)
2022-12-09 22:36:40 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-12-09 22:36:40 | INFO | train | epoch 204 | loss 6.711 | nll_loss 2.841 | ppl 7.16 | wps 24481.1 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 224794 | lr 6.66972e-05 | gnorm 2.373 | loss_scale 4 | train_wall 117 | gb_free 19.6 | wall 9341
2022-12-09 22:36:40 | INFO | fairseq.trainer | begin training epoch 205
2022-12-09 22:36:41 | INFO | train_inner | epoch 205:      6 / 1102 loss=6.77, nll_loss=2.891, ppl=7.42, wps=6706.1, ups=1.9, wpb=3527.7, bsz=137.2, num_updates=224800, lr=6.66963e-05, gnorm=2.45, loss_scale=4, train_wall=11, gb_free=19.3, wall=9342
2022-12-09 22:36:52 | INFO | train_inner | epoch 205:    106 / 1102 loss=6.643, nll_loss=2.786, ppl=6.9, wps=33777.4, ups=9.27, wpb=3642.1, bsz=157.7, num_updates=224900, lr=6.66815e-05, gnorm=2.213, loss_scale=4, train_wall=11, gb_free=19.5, wall=9352
2022-12-09 22:37:03 | INFO | train_inner | epoch 205:    206 / 1102 loss=6.728, nll_loss=2.848, ppl=7.2, wps=33029.1, ups=9.3, wpb=3550.8, bsz=136.5, num_updates=225000, lr=6.66667e-05, gnorm=2.329, loss_scale=4, train_wall=11, gb_free=19.5, wall=9363
2022-12-09 22:37:14 | INFO | train_inner | epoch 205:    306 / 1102 loss=6.714, nll_loss=2.861, ppl=7.27, wps=33512.1, ups=9.17, wpb=3653, bsz=152.2, num_updates=225100, lr=6.66519e-05, gnorm=2.225, loss_scale=4, train_wall=11, gb_free=19.7, wall=9374
2022-12-09 22:37:24 | INFO | train_inner | epoch 205:    406 / 1102 loss=6.719, nll_loss=2.852, ppl=7.22, wps=33504.8, ups=9.3, wpb=3601.5, bsz=148.9, num_updates=225200, lr=6.66371e-05, gnorm=2.38, loss_scale=4, train_wall=11, gb_free=19.7, wall=9385
2022-12-09 22:37:35 | INFO | train_inner | epoch 205:    506 / 1102 loss=6.704, nll_loss=2.835, ppl=7.13, wps=33717.7, ups=9.25, wpb=3645.9, bsz=144.1, num_updates=225300, lr=6.66223e-05, gnorm=2.346, loss_scale=4, train_wall=11, gb_free=19.5, wall=9396
2022-12-09 22:37:46 | INFO | train_inner | epoch 205:    606 / 1102 loss=6.685, nll_loss=2.803, ppl=6.98, wps=32289.6, ups=9.25, wpb=3491.7, bsz=141.2, num_updates=225400, lr=6.66075e-05, gnorm=2.423, loss_scale=4, train_wall=11, gb_free=19.5, wall=9406
2022-12-09 22:37:57 | INFO | train_inner | epoch 205:    706 / 1102 loss=6.672, nll_loss=2.777, ppl=6.85, wps=32608.7, ups=9.36, wpb=3482.9, bsz=134.5, num_updates=225500, lr=6.65927e-05, gnorm=2.425, loss_scale=4, train_wall=10, gb_free=19.3, wall=9417
2022-12-09 22:38:08 | INFO | train_inner | epoch 205:    806 / 1102 loss=6.721, nll_loss=2.862, ppl=7.27, wps=33354.5, ups=9.12, wpb=3658.7, bsz=148.2, num_updates=225600, lr=6.6578e-05, gnorm=2.299, loss_scale=4, train_wall=11, gb_free=19.3, wall=9428
2022-12-09 22:38:18 | INFO | train_inner | epoch 205:    906 / 1102 loss=6.766, nll_loss=2.897, ppl=7.45, wps=32725.3, ups=9.18, wpb=3564.7, bsz=147.6, num_updates=225700, lr=6.65632e-05, gnorm=2.417, loss_scale=4, train_wall=11, gb_free=19.3, wall=9439
2022-12-09 22:38:29 | INFO | train_inner | epoch 205:   1006 / 1102 loss=6.66, nll_loss=2.801, ppl=6.97, wps=33092.8, ups=9.19, wpb=3599.8, bsz=159.5, num_updates=225800, lr=6.65485e-05, gnorm=2.337, loss_scale=4, train_wall=11, gb_free=19.3, wall=9450
2022-12-09 22:38:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:39:20 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 3.659 | nll_loss 2.119 | ppl 4.34 | bleu 37.53 | wps 4536 | wpb 2835.3 | bsz 115.6 | num_updates 225896 | best_bleu 37.58
2022-12-09 22:39:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 225896 updates
2022-12-09 22:39:20 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint205.pt
2022-12-09 22:39:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint205.pt (epoch 205 @ 225896 updates, score 37.53) (writing took 1.2341404985636473 seconds)
2022-12-09 22:39:21 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-12-09 22:39:21 | INFO | train | epoch 205 | loss 6.71 | nll_loss 2.839 | ppl 7.16 | wps 24577.4 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 225896 | lr 6.65343e-05 | gnorm 2.343 | loss_scale 4 | train_wall 117 | gb_free 19.6 | wall 9501
2022-12-09 22:39:21 | INFO | fairseq.trainer | begin training epoch 206
2022-12-09 22:39:22 | INFO | train_inner | epoch 206:      4 / 1102 loss=6.797, nll_loss=2.907, ppl=7.5, wps=6803.7, ups=1.91, wpb=3557.7, bsz=130.2, num_updates=225900, lr=6.65337e-05, gnorm=2.371, loss_scale=4, train_wall=11, gb_free=19.3, wall=9502
2022-12-09 22:39:32 | INFO | train_inner | epoch 206:    104 / 1102 loss=6.684, nll_loss=2.802, ppl=6.98, wps=33359.5, ups=9.34, wpb=3571, bsz=142.4, num_updates=226000, lr=6.6519e-05, gnorm=2.352, loss_scale=4, train_wall=10, gb_free=19.4, wall=9513
2022-12-09 22:39:43 | INFO | train_inner | epoch 206:    204 / 1102 loss=6.668, nll_loss=2.798, ppl=6.95, wps=33554.6, ups=9.31, wpb=3605.8, bsz=149.8, num_updates=226100, lr=6.65043e-05, gnorm=2.314, loss_scale=4, train_wall=11, gb_free=19.9, wall=9524
2022-12-09 22:39:54 | INFO | train_inner | epoch 206:    304 / 1102 loss=6.776, nll_loss=2.906, ppl=7.5, wps=33197.9, ups=9.19, wpb=3610.6, bsz=142, num_updates=226200, lr=6.64896e-05, gnorm=2.374, loss_scale=4, train_wall=11, gb_free=19.6, wall=9535
2022-12-09 22:40:05 | INFO | train_inner | epoch 206:    404 / 1102 loss=6.707, nll_loss=2.851, ppl=7.21, wps=32886.5, ups=9.17, wpb=3586.9, bsz=151.4, num_updates=226300, lr=6.64749e-05, gnorm=2.308, loss_scale=4, train_wall=11, gb_free=19.4, wall=9545
2022-12-09 22:40:16 | INFO | train_inner | epoch 206:    504 / 1102 loss=6.624, nll_loss=2.776, ppl=6.85, wps=32569.4, ups=9, wpb=3617.4, bsz=168.3, num_updates=226400, lr=6.64602e-05, gnorm=2.354, loss_scale=4, train_wall=11, gb_free=19.4, wall=9557
2022-12-09 22:40:27 | INFO | train_inner | epoch 206:    604 / 1102 loss=6.694, nll_loss=2.819, ppl=7.06, wps=33259.5, ups=9.15, wpb=3634.7, bsz=141.4, num_updates=226500, lr=6.64455e-05, gnorm=2.336, loss_scale=4, train_wall=11, gb_free=19.2, wall=9567
2022-12-09 22:40:38 | INFO | train_inner | epoch 206:    704 / 1102 loss=6.704, nll_loss=2.815, ppl=7.04, wps=32460.9, ups=9.28, wpb=3498.1, bsz=135.2, num_updates=226600, lr=6.64309e-05, gnorm=2.425, loss_scale=4, train_wall=11, gb_free=19.5, wall=9578
2022-12-09 22:40:49 | INFO | train_inner | epoch 206:    804 / 1102 loss=6.737, nll_loss=2.863, ppl=7.27, wps=32955.1, ups=9.18, wpb=3589, bsz=142.8, num_updates=226700, lr=6.64162e-05, gnorm=2.327, loss_scale=4, train_wall=11, gb_free=19.2, wall=9589
2022-12-09 22:40:59 | INFO | train_inner | epoch 206:    904 / 1102 loss=6.678, nll_loss=2.808, ppl=7, wps=32483, ups=9.16, wpb=3547.5, bsz=145.9, num_updates=226800, lr=6.64016e-05, gnorm=2.328, loss_scale=4, train_wall=11, gb_free=19.6, wall=9600
2022-12-09 22:41:11 | INFO | train_inner | epoch 206:   1004 / 1102 loss=6.744, nll_loss=2.881, ppl=7.37, wps=32338.2, ups=8.96, wpb=3610.1, bsz=144.2, num_updates=226900, lr=6.6387e-05, gnorm=2.342, loss_scale=4, train_wall=11, gb_free=19.7, wall=9611
2022-12-09 22:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:42:00 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 3.663 | nll_loss 2.12 | ppl 4.35 | bleu 37.4 | wps 4687.2 | wpb 2835.3 | bsz 115.6 | num_updates 226998 | best_bleu 37.58
2022-12-09 22:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 226998 updates
2022-12-09 22:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint206.pt
2022-12-09 22:42:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint206.pt (epoch 206 @ 226998 updates, score 37.4) (writing took 1.2071258341893554 seconds)
2022-12-09 22:42:01 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-12-09 22:42:01 | INFO | train | epoch 206 | loss 6.708 | nll_loss 2.837 | ppl 7.15 | wps 24632.1 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 226998 | lr 6.63726e-05 | gnorm 2.353 | loss_scale 4 | train_wall 117 | gb_free 19.5 | wall 9662
2022-12-09 22:42:01 | INFO | fairseq.trainer | begin training epoch 207
2022-12-09 22:42:02 | INFO | train_inner | epoch 207:      2 / 1102 loss=6.785, nll_loss=2.902, ppl=7.47, wps=6937.3, ups=1.96, wpb=3543, bsz=135.2, num_updates=227000, lr=6.63723e-05, gnorm=2.429, loss_scale=4, train_wall=11, gb_free=19.4, wall=9662
2022-12-09 22:42:12 | INFO | train_inner | epoch 207:    102 / 1102 loss=6.683, nll_loss=2.798, ppl=6.96, wps=32993.2, ups=9.28, wpb=3553.4, bsz=136.2, num_updates=227100, lr=6.63577e-05, gnorm=2.4, loss_scale=4, train_wall=11, gb_free=19.8, wall=9673
2022-12-09 22:42:23 | INFO | train_inner | epoch 207:    202 / 1102 loss=6.732, nll_loss=2.846, ppl=7.19, wps=33510.9, ups=9.42, wpb=3557.1, bsz=127.1, num_updates=227200, lr=6.63431e-05, gnorm=2.4, loss_scale=4, train_wall=10, gb_free=19.4, wall=9684
2022-12-09 22:42:34 | INFO | train_inner | epoch 207:    302 / 1102 loss=6.677, nll_loss=2.796, ppl=6.95, wps=33017.9, ups=9.19, wpb=3592.5, bsz=140.9, num_updates=227300, lr=6.63285e-05, gnorm=2.326, loss_scale=4, train_wall=11, gb_free=19.3, wall=9695
2022-12-09 22:42:45 | INFO | train_inner | epoch 207:    402 / 1102 loss=6.636, nll_loss=2.769, ppl=6.82, wps=33109.6, ups=9.21, wpb=3594.3, bsz=158, num_updates=227400, lr=6.63139e-05, gnorm=2.372, loss_scale=4, train_wall=11, gb_free=19.5, wall=9705
2022-12-09 22:42:56 | INFO | train_inner | epoch 207:    502 / 1102 loss=6.694, nll_loss=2.826, ppl=7.09, wps=32832.1, ups=9.19, wpb=3572.7, bsz=146.6, num_updates=227500, lr=6.62994e-05, gnorm=2.333, loss_scale=4, train_wall=11, gb_free=19.4, wall=9716
2022-12-09 22:43:07 | INFO | train_inner | epoch 207:    602 / 1102 loss=6.656, nll_loss=2.79, ppl=6.92, wps=32723.6, ups=9.25, wpb=3537.3, bsz=155.9, num_updates=227600, lr=6.62848e-05, gnorm=2.419, loss_scale=8, train_wall=11, gb_free=19.3, wall=9727
2022-12-09 22:43:17 | INFO | train_inner | epoch 207:    702 / 1102 loss=6.736, nll_loss=2.859, ppl=7.25, wps=33166.5, ups=9.2, wpb=3606.3, bsz=135.8, num_updates=227700, lr=6.62702e-05, gnorm=2.366, loss_scale=8, train_wall=11, gb_free=19.8, wall=9738
2022-12-09 22:43:28 | INFO | train_inner | epoch 207:    802 / 1102 loss=6.731, nll_loss=2.867, ppl=7.3, wps=33074.6, ups=9.09, wpb=3638.9, bsz=147, num_updates=227800, lr=6.62557e-05, gnorm=2.335, loss_scale=8, train_wall=11, gb_free=20.1, wall=9749
2022-12-09 22:43:39 | INFO | train_inner | epoch 207:    902 / 1102 loss=6.734, nll_loss=2.865, ppl=7.28, wps=32731.7, ups=9.16, wpb=3572.4, bsz=147.2, num_updates=227900, lr=6.62411e-05, gnorm=2.398, loss_scale=8, train_wall=11, gb_free=19.6, wall=9760
2022-12-09 22:43:50 | INFO | train_inner | epoch 207:   1002 / 1102 loss=6.741, nll_loss=2.869, ppl=7.31, wps=32823.7, ups=9.19, wpb=3572.6, bsz=142.3, num_updates=228000, lr=6.62266e-05, gnorm=2.469, loss_scale=8, train_wall=11, gb_free=19.4, wall=9771
2022-12-09 22:44:01 | INFO | train_inner | epoch 207:   1102 / 1102 loss=6.723, nll_loss=2.872, ppl=7.32, wps=33175.5, ups=9.17, wpb=3617.9, bsz=162.9, num_updates=228100, lr=6.62121e-05, gnorm=2.309, loss_scale=8, train_wall=11, gb_free=19.6, wall=9782
2022-12-09 22:44:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:44:38 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 3.662 | nll_loss 2.117 | ppl 4.34 | bleu 37.45 | wps 4904.6 | wpb 2835.3 | bsz 115.6 | num_updates 228100 | best_bleu 37.58
2022-12-09 22:44:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 228100 updates
2022-12-09 22:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint207.pt
2022-12-09 22:44:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint207.pt (epoch 207 @ 228100 updates, score 37.45) (writing took 1.1962335333228111 seconds)
2022-12-09 22:44:39 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-12-09 22:44:39 | INFO | train | epoch 207 | loss 6.704 | nll_loss 2.833 | ppl 7.13 | wps 24994.9 | ups 6.97 | wpb 3583.6 | bsz 145.4 | num_updates 228100 | lr 6.62121e-05 | gnorm 2.375 | loss_scale 8 | train_wall 117 | gb_free 19.6 | wall 9820
2022-12-09 22:44:39 | INFO | fairseq.trainer | begin training epoch 208
2022-12-09 22:44:50 | INFO | train_inner | epoch 208:    100 / 1102 loss=6.749, nll_loss=2.872, ppl=7.32, wps=7307.9, ups=2.04, wpb=3586.4, bsz=128.5, num_updates=228200, lr=6.61976e-05, gnorm=2.371, loss_scale=8, train_wall=10, gb_free=19.7, wall=9831
2022-12-09 22:45:01 | INFO | train_inner | epoch 208:    200 / 1102 loss=6.632, nll_loss=2.76, ppl=6.78, wps=33062.3, ups=9.19, wpb=3596.4, bsz=156.4, num_updates=228300, lr=6.61831e-05, gnorm=2.314, loss_scale=8, train_wall=11, gb_free=19.4, wall=9842
2022-12-09 22:45:12 | INFO | train_inner | epoch 208:    300 / 1102 loss=6.699, nll_loss=2.831, ppl=7.12, wps=33710.3, ups=9.24, wpb=3648.8, bsz=145.4, num_updates=228400, lr=6.61686e-05, gnorm=2.29, loss_scale=8, train_wall=11, gb_free=19.4, wall=9852
2022-12-09 22:45:23 | INFO | train_inner | epoch 208:    400 / 1102 loss=6.674, nll_loss=2.8, ppl=6.97, wps=33246.7, ups=9.22, wpb=3607.3, bsz=146.2, num_updates=228500, lr=6.61541e-05, gnorm=2.344, loss_scale=8, train_wall=11, gb_free=19.4, wall=9863
2022-12-09 22:45:34 | INFO | train_inner | epoch 208:    500 / 1102 loss=6.659, nll_loss=2.792, ppl=6.93, wps=33166.3, ups=9.22, wpb=3597.1, bsz=147.5, num_updates=228600, lr=6.61396e-05, gnorm=2.302, loss_scale=8, train_wall=11, gb_free=19.3, wall=9874
2022-12-09 22:45:44 | INFO | train_inner | epoch 208:    600 / 1102 loss=6.665, nll_loss=2.793, ppl=6.93, wps=32627, ups=9.25, wpb=3526.3, bsz=143.6, num_updates=228700, lr=6.61252e-05, gnorm=2.327, loss_scale=8, train_wall=11, gb_free=19.3, wall=9885
2022-12-09 22:45:55 | INFO | train_inner | epoch 208:    700 / 1102 loss=6.714, nll_loss=2.853, ppl=7.23, wps=33374.1, ups=9.21, wpb=3624.4, bsz=153.3, num_updates=228800, lr=6.61107e-05, gnorm=2.361, loss_scale=8, train_wall=11, gb_free=19.5, wall=9896
2022-12-09 22:46:06 | INFO | train_inner | epoch 208:    800 / 1102 loss=6.725, nll_loss=2.836, ppl=7.14, wps=31953.4, ups=9.2, wpb=3473.5, bsz=136.5, num_updates=228900, lr=6.60963e-05, gnorm=2.599, loss_scale=8, train_wall=11, gb_free=19.5, wall=9907
2022-12-09 22:46:17 | INFO | train_inner | epoch 208:    900 / 1102 loss=6.788, nll_loss=2.905, ppl=7.49, wps=33175.5, ups=9.21, wpb=3602.2, bsz=132.7, num_updates=229000, lr=6.60819e-05, gnorm=2.425, loss_scale=8, train_wall=11, gb_free=19.4, wall=9918
2022-12-09 22:46:28 | INFO | train_inner | epoch 208:   1000 / 1102 loss=6.693, nll_loss=2.824, ppl=7.08, wps=32965.3, ups=9.16, wpb=3599.6, bsz=150.7, num_updates=229100, lr=6.60674e-05, gnorm=2.454, loss_scale=8, train_wall=11, gb_free=19.5, wall=9928
2022-12-09 22:46:39 | INFO | train_inner | epoch 208:   1100 / 1102 loss=6.741, nll_loss=2.886, ppl=7.39, wps=32221.6, ups=9.08, wpb=3548.3, bsz=158.7, num_updates=229200, lr=6.6053e-05, gnorm=2.376, loss_scale=8, train_wall=11, gb_free=19.4, wall=9939
2022-12-09 22:46:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:47:18 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 3.671 | nll_loss 2.126 | ppl 4.36 | bleu 37.35 | wps 4632.2 | wpb 2835.3 | bsz 115.6 | num_updates 229202 | best_bleu 37.58
2022-12-09 22:47:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 229202 updates
2022-12-09 22:47:19 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint208.pt
2022-12-09 22:47:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint208.pt (epoch 208 @ 229202 updates, score 37.35) (writing took 1.1039291312918067 seconds)
2022-12-09 22:47:19 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-12-09 22:47:19 | INFO | train | epoch 208 | loss 6.704 | nll_loss 2.832 | ppl 7.12 | wps 24635.5 | ups 6.87 | wpb 3583.6 | bsz 145.4 | num_updates 229202 | lr 6.60527e-05 | gnorm 2.378 | loss_scale 8 | train_wall 117 | gb_free 19.2 | wall 9980
2022-12-09 22:47:19 | INFO | fairseq.trainer | begin training epoch 209
2022-12-09 22:47:30 | INFO | train_inner | epoch 209:     98 / 1102 loss=6.624, nll_loss=2.757, ppl=6.76, wps=6937.4, ups=1.94, wpb=3576.5, bsz=154.5, num_updates=229300, lr=6.60386e-05, gnorm=2.303, loss_scale=8, train_wall=11, gb_free=19.3, wall=9991
2022-12-09 22:47:41 | INFO | train_inner | epoch 209:    198 / 1102 loss=6.612, nll_loss=2.743, ppl=6.69, wps=32709.6, ups=9.19, wpb=3557.8, bsz=144.6, num_updates=229400, lr=6.60242e-05, gnorm=2.33, loss_scale=8, train_wall=11, gb_free=19.3, wall=10002
2022-12-09 22:47:52 | INFO | train_inner | epoch 209:    298 / 1102 loss=6.668, nll_loss=2.792, ppl=6.93, wps=32818.3, ups=9.17, wpb=3579.9, bsz=146.3, num_updates=229500, lr=6.60098e-05, gnorm=2.363, loss_scale=8, train_wall=11, gb_free=19.7, wall=10013
2022-12-09 22:48:03 | INFO | train_inner | epoch 209:    398 / 1102 loss=6.712, nll_loss=2.816, ppl=7.04, wps=32590.7, ups=9.22, wpb=3534.4, bsz=132.6, num_updates=229600, lr=6.59955e-05, gnorm=2.354, loss_scale=8, train_wall=11, gb_free=19.6, wall=10024
2022-12-09 22:48:14 | INFO | train_inner | epoch 209:    498 / 1102 loss=6.784, nll_loss=2.934, ppl=7.64, wps=34391.9, ups=9.25, wpb=3717.6, bsz=146.2, num_updates=229700, lr=6.59811e-05, gnorm=2.254, loss_scale=8, train_wall=11, gb_free=19.5, wall=10034
2022-12-09 22:48:25 | INFO | train_inner | epoch 209:    598 / 1102 loss=6.674, nll_loss=2.809, ppl=7.01, wps=33004.5, ups=9.16, wpb=3602.9, bsz=152.8, num_updates=229800, lr=6.59667e-05, gnorm=2.355, loss_scale=8, train_wall=11, gb_free=19.5, wall=10045
2022-12-09 22:48:36 | INFO | train_inner | epoch 209:    698 / 1102 loss=6.725, nll_loss=2.845, ppl=7.19, wps=32936.5, ups=9.27, wpb=3551.4, bsz=143.8, num_updates=229900, lr=6.59524e-05, gnorm=2.45, loss_scale=8, train_wall=11, gb_free=19.4, wall=10056
2022-12-09 22:48:46 | INFO | train_inner | epoch 209:    798 / 1102 loss=6.768, nll_loss=2.886, ppl=7.39, wps=32893.5, ups=9.37, wpb=3509.8, bsz=131.5, num_updates=230000, lr=6.5938e-05, gnorm=2.368, loss_scale=8, train_wall=10, gb_free=19.7, wall=10067
2022-12-09 22:48:57 | INFO | train_inner | epoch 209:    898 / 1102 loss=6.726, nll_loss=2.846, ppl=7.19, wps=33691.5, ups=9.34, wpb=3605.7, bsz=143, num_updates=230100, lr=6.59237e-05, gnorm=2.387, loss_scale=8, train_wall=10, gb_free=20.3, wall=10078
2022-12-09 22:49:08 | INFO | train_inner | epoch 209:    998 / 1102 loss=6.648, nll_loss=2.775, ppl=6.85, wps=32832.9, ups=9.28, wpb=3537.5, bsz=154.6, num_updates=230200, lr=6.59094e-05, gnorm=2.517, loss_scale=8, train_wall=11, gb_free=19.5, wall=10088
2022-12-09 22:49:19 | INFO | train_inner | epoch 209:   1098 / 1102 loss=6.78, nll_loss=2.924, ppl=7.59, wps=33530.9, ups=9.18, wpb=3652, bsz=149.9, num_updates=230300, lr=6.58951e-05, gnorm=2.492, loss_scale=8, train_wall=11, gb_free=19.4, wall=10099
2022-12-09 22:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:49:58 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 3.658 | nll_loss 2.117 | ppl 4.34 | bleu 37.37 | wps 4650.5 | wpb 2835.3 | bsz 115.6 | num_updates 230304 | best_bleu 37.58
2022-12-09 22:49:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 230304 updates
2022-12-09 22:49:59 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint209.pt
2022-12-09 22:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint209.pt (epoch 209 @ 230304 updates, score 37.37) (writing took 1.1418990716338158 seconds)
2022-12-09 22:49:59 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-12-09 22:49:59 | INFO | train | epoch 209 | loss 6.703 | nll_loss 2.831 | ppl 7.11 | wps 24741.4 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 230304 | lr 6.58945e-05 | gnorm 2.379 | loss_scale 8 | train_wall 117 | gb_free 19.4 | wall 10140
2022-12-09 22:49:59 | INFO | fairseq.trainer | begin training epoch 210
2022-12-09 22:50:10 | INFO | train_inner | epoch 210:     96 / 1102 loss=6.638, nll_loss=2.764, ppl=6.79, wps=6872.5, ups=1.96, wpb=3508.9, bsz=145.4, num_updates=230400, lr=6.58808e-05, gnorm=2.371, loss_scale=8, train_wall=11, gb_free=19.5, wall=10150
2022-12-09 22:50:21 | INFO | train_inner | epoch 210:    196 / 1102 loss=6.65, nll_loss=2.79, ppl=6.92, wps=33209.8, ups=9.25, wpb=3589.3, bsz=157.9, num_updates=230500, lr=6.58665e-05, gnorm=2.295, loss_scale=8, train_wall=11, gb_free=19.5, wall=10161
2022-12-09 22:50:31 | INFO | train_inner | epoch 210:    296 / 1102 loss=6.65, nll_loss=2.768, ppl=6.81, wps=33516.1, ups=9.27, wpb=3616.6, bsz=144.3, num_updates=230600, lr=6.58522e-05, gnorm=2.314, loss_scale=8, train_wall=11, gb_free=19.7, wall=10172
2022-12-09 22:50:42 | INFO | train_inner | epoch 210:    396 / 1102 loss=6.665, nll_loss=2.797, ppl=6.95, wps=32815.9, ups=9.18, wpb=3575.9, bsz=149, num_updates=230700, lr=6.58379e-05, gnorm=2.324, loss_scale=8, train_wall=11, gb_free=19.5, wall=10183
2022-12-09 22:50:53 | INFO | train_inner | epoch 210:    496 / 1102 loss=6.761, nll_loss=2.867, ppl=7.29, wps=33520.9, ups=9.37, wpb=3576.8, bsz=128.7, num_updates=230800, lr=6.58237e-05, gnorm=2.469, loss_scale=8, train_wall=10, gb_free=19.9, wall=10193
2022-12-09 22:51:04 | INFO | train_inner | epoch 210:    596 / 1102 loss=6.7, nll_loss=2.84, ppl=7.16, wps=33053.7, ups=9.18, wpb=3599.7, bsz=154.3, num_updates=230900, lr=6.58094e-05, gnorm=2.299, loss_scale=8, train_wall=11, gb_free=19.5, wall=10204
2022-12-09 22:51:15 | INFO | train_inner | epoch 210:    696 / 1102 loss=6.733, nll_loss=2.854, ppl=7.23, wps=33453, ups=9.22, wpb=3629, bsz=139.4, num_updates=231000, lr=6.57952e-05, gnorm=2.333, loss_scale=8, train_wall=11, gb_free=19.2, wall=10215
2022-12-09 22:51:26 | INFO | train_inner | epoch 210:    796 / 1102 loss=6.701, nll_loss=2.84, ppl=7.16, wps=33287.1, ups=9.15, wpb=3636.9, bsz=153.2, num_updates=231100, lr=6.57809e-05, gnorm=2.271, loss_scale=8, train_wall=11, gb_free=19.8, wall=10226
2022-12-09 22:51:36 | INFO | train_inner | epoch 210:    896 / 1102 loss=6.708, nll_loss=2.834, ppl=7.13, wps=32685.3, ups=9.32, wpb=3507, bsz=148.6, num_updates=231200, lr=6.57667e-05, gnorm=2.382, loss_scale=8, train_wall=10, gb_free=19.7, wall=10237
2022-12-09 22:51:47 | INFO | train_inner | epoch 210:    996 / 1102 loss=6.713, nll_loss=2.85, ppl=7.21, wps=33297.7, ups=9.3, wpb=3581.9, bsz=146.1, num_updates=231300, lr=6.57525e-05, gnorm=2.347, loss_scale=8, train_wall=11, gb_free=19.5, wall=10248
2022-12-09 22:51:58 | INFO | train_inner | epoch 210:   1096 / 1102 loss=6.786, nll_loss=2.912, ppl=7.53, wps=33557.7, ups=9.32, wpb=3599.7, bsz=135.2, num_updates=231400, lr=6.57383e-05, gnorm=2.446, loss_scale=8, train_wall=10, gb_free=19.5, wall=10258
2022-12-09 22:51:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:52:37 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 3.665 | nll_loss 2.122 | ppl 4.35 | bleu 37.52 | wps 4674.8 | wpb 2835.3 | bsz 115.6 | num_updates 231406 | best_bleu 37.58
2022-12-09 22:52:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 231406 updates
2022-12-09 22:52:38 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint210.pt
2022-12-09 22:52:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint210.pt (epoch 210 @ 231406 updates, score 37.52) (writing took 1.1176875559613109 seconds)
2022-12-09 22:52:38 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-12-09 22:52:38 | INFO | train | epoch 210 | loss 6.701 | nll_loss 2.829 | ppl 7.1 | wps 24827.4 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 231406 | lr 6.57374e-05 | gnorm 2.351 | loss_scale 8 | train_wall 116 | gb_free 19.2 | wall 10299
2022-12-09 22:52:38 | INFO | fairseq.trainer | begin training epoch 211
2022-12-09 22:52:48 | INFO | train_inner | epoch 211:     94 / 1102 loss=6.666, nll_loss=2.779, ppl=6.86, wps=7016.8, ups=1.97, wpb=3558, bsz=125.8, num_updates=231500, lr=6.57241e-05, gnorm=2.396, loss_scale=8, train_wall=10, gb_free=19.7, wall=10309
2022-12-09 22:52:59 | INFO | train_inner | epoch 211:    194 / 1102 loss=6.638, nll_loss=2.781, ppl=6.87, wps=33095, ups=9.2, wpb=3597.2, bsz=159, num_updates=231600, lr=6.57099e-05, gnorm=2.348, loss_scale=8, train_wall=11, gb_free=19.2, wall=10320
2022-12-09 22:53:10 | INFO | train_inner | epoch 211:    294 / 1102 loss=6.697, nll_loss=2.822, ppl=7.07, wps=33322.2, ups=9.31, wpb=3579.8, bsz=144.8, num_updates=231700, lr=6.56957e-05, gnorm=2.407, loss_scale=8, train_wall=11, gb_free=19.9, wall=10331
2022-12-09 22:53:21 | INFO | train_inner | epoch 211:    394 / 1102 loss=6.729, nll_loss=2.859, ppl=7.25, wps=32816.6, ups=9.2, wpb=3566, bsz=143, num_updates=231800, lr=6.56815e-05, gnorm=2.366, loss_scale=8, train_wall=11, gb_free=19.4, wall=10342
2022-12-09 22:53:32 | INFO | train_inner | epoch 211:    494 / 1102 loss=6.661, nll_loss=2.791, ppl=6.92, wps=33332.4, ups=9.2, wpb=3623.1, bsz=153.4, num_updates=231900, lr=6.56674e-05, gnorm=2.365, loss_scale=8, train_wall=11, gb_free=19.5, wall=10352
2022-12-09 22:53:43 | INFO | train_inner | epoch 211:    594 / 1102 loss=6.632, nll_loss=2.761, ppl=6.78, wps=32832.3, ups=9.18, wpb=3578.1, bsz=153.5, num_updates=232000, lr=6.56532e-05, gnorm=2.373, loss_scale=8, train_wall=11, gb_free=19.5, wall=10363
2022-12-09 22:53:54 | INFO | train_inner | epoch 211:    694 / 1102 loss=6.691, nll_loss=2.816, ppl=7.04, wps=32679, ups=9.16, wpb=3567.8, bsz=144.2, num_updates=232100, lr=6.56391e-05, gnorm=2.33, loss_scale=8, train_wall=11, gb_free=19.2, wall=10374
2022-12-09 22:54:04 | INFO | train_inner | epoch 211:    794 / 1102 loss=6.731, nll_loss=2.857, ppl=7.25, wps=32900.3, ups=9.21, wpb=3573.8, bsz=141.1, num_updates=232200, lr=6.56249e-05, gnorm=2.427, loss_scale=8, train_wall=11, gb_free=19.4, wall=10385
2022-12-09 22:54:15 | INFO | train_inner | epoch 211:    894 / 1102 loss=6.71, nll_loss=2.853, ppl=7.22, wps=32717.5, ups=9.13, wpb=3583.4, bsz=154.4, num_updates=232300, lr=6.56108e-05, gnorm=2.33, loss_scale=8, train_wall=11, gb_free=19.7, wall=10396
2022-12-09 22:54:26 | INFO | train_inner | epoch 211:    994 / 1102 loss=6.791, nll_loss=2.915, ppl=7.54, wps=33532.5, ups=9.22, wpb=3638.3, bsz=137.4, num_updates=232400, lr=6.55967e-05, gnorm=2.37, loss_scale=8, train_wall=11, gb_free=19.4, wall=10407
2022-12-09 22:54:37 | INFO | train_inner | epoch 211:   1094 / 1102 loss=6.744, nll_loss=2.864, ppl=7.28, wps=33380.8, ups=9.37, wpb=3562.5, bsz=143.8, num_updates=232500, lr=6.55826e-05, gnorm=2.572, loss_scale=8, train_wall=10, gb_free=19.3, wall=10418
2022-12-09 22:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:55:17 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 3.666 | nll_loss 2.124 | ppl 4.36 | bleu 37.39 | wps 4660.3 | wpb 2835.3 | bsz 115.6 | num_updates 232508 | best_bleu 37.58
2022-12-09 22:55:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 232508 updates
2022-12-09 22:55:17 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint211.pt
2022-12-09 22:55:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint211.pt (epoch 211 @ 232508 updates, score 37.39) (writing took 1.1098838336765766 seconds)
2022-12-09 22:55:18 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-12-09 22:55:18 | INFO | train | epoch 211 | loss 6.699 | nll_loss 2.827 | ppl 7.09 | wps 24742.1 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 232508 | lr 6.55815e-05 | gnorm 2.393 | loss_scale 8 | train_wall 117 | gb_free 19.7 | wall 10458
2022-12-09 22:55:18 | INFO | fairseq.trainer | begin training epoch 212
2022-12-09 22:55:28 | INFO | train_inner | epoch 212:     92 / 1102 loss=6.625, nll_loss=2.767, ppl=6.81, wps=7146.5, ups=1.95, wpb=3658.3, bsz=153.1, num_updates=232600, lr=6.55685e-05, gnorm=2.23, loss_scale=8, train_wall=11, gb_free=19.4, wall=10469
2022-12-09 22:55:39 | INFO | train_inner | epoch 212:    192 / 1102 loss=6.655, nll_loss=2.781, ppl=6.87, wps=33273.1, ups=9.2, wpb=3614.8, bsz=148.9, num_updates=232700, lr=6.55544e-05, gnorm=2.374, loss_scale=8, train_wall=11, gb_free=19.4, wall=10480
2022-12-09 22:55:50 | INFO | train_inner | epoch 212:    292 / 1102 loss=6.656, nll_loss=2.77, ppl=6.82, wps=32582.5, ups=9.21, wpb=3535.9, bsz=137.8, num_updates=232800, lr=6.55403e-05, gnorm=2.408, loss_scale=8, train_wall=11, gb_free=19.3, wall=10490
2022-12-09 22:56:01 | INFO | train_inner | epoch 212:    392 / 1102 loss=6.751, nll_loss=2.854, ppl=7.23, wps=32353.2, ups=9.31, wpb=3475.5, bsz=133.7, num_updates=232900, lr=6.55262e-05, gnorm=2.493, loss_scale=8, train_wall=10, gb_free=19.5, wall=10501
2022-12-09 22:56:12 | INFO | train_inner | epoch 212:    492 / 1102 loss=6.715, nll_loss=2.848, ppl=7.2, wps=32764.2, ups=9.17, wpb=3574.4, bsz=146.3, num_updates=233000, lr=6.55122e-05, gnorm=2.357, loss_scale=8, train_wall=11, gb_free=19.4, wall=10512
2022-12-09 22:56:23 | INFO | train_inner | epoch 212:    592 / 1102 loss=6.688, nll_loss=2.808, ppl=7, wps=32502.3, ups=9.08, wpb=3578.9, bsz=147.6, num_updates=233100, lr=6.54981e-05, gnorm=2.456, loss_scale=8, train_wall=11, gb_free=19.6, wall=10523
2022-12-09 22:56:34 | INFO | train_inner | epoch 212:    692 / 1102 loss=6.729, nll_loss=2.863, ppl=7.28, wps=32960.2, ups=9.11, wpb=3616.9, bsz=149, num_updates=233200, lr=6.54841e-05, gnorm=2.418, loss_scale=8, train_wall=11, gb_free=19.5, wall=10534
2022-12-09 22:56:44 | INFO | train_inner | epoch 212:    792 / 1102 loss=6.725, nll_loss=2.863, ppl=7.28, wps=32934.8, ups=9.12, wpb=3609.4, bsz=148.5, num_updates=233300, lr=6.547e-05, gnorm=2.509, loss_scale=8, train_wall=11, gb_free=19.6, wall=10545
2022-12-09 22:56:55 | INFO | train_inner | epoch 212:    892 / 1102 loss=6.688, nll_loss=2.814, ppl=7.03, wps=32421, ups=9.07, wpb=3573.3, bsz=147.4, num_updates=233400, lr=6.5456e-05, gnorm=2.406, loss_scale=8, train_wall=11, gb_free=19.6, wall=10556
2022-12-09 22:57:07 | INFO | train_inner | epoch 212:    992 / 1102 loss=6.711, nll_loss=2.836, ppl=7.14, wps=32234.6, ups=9, wpb=3583, bsz=143.1, num_updates=233500, lr=6.5442e-05, gnorm=2.416, loss_scale=8, train_wall=11, gb_free=19.4, wall=10567
2022-12-09 22:57:18 | INFO | train_inner | epoch 212:   1092 / 1102 loss=6.74, nll_loss=2.867, ppl=7.3, wps=32402.4, ups=9, wpb=3598.4, bsz=143, num_updates=233600, lr=6.5428e-05, gnorm=2.325, loss_scale=8, train_wall=11, gb_free=19.5, wall=10578
2022-12-09 22:57:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 22:57:58 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 3.666 | nll_loss 2.123 | ppl 4.36 | bleu 37.35 | wps 4646.4 | wpb 2835.3 | bsz 115.6 | num_updates 233610 | best_bleu 37.58
2022-12-09 22:57:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 233610 updates
2022-12-09 22:57:59 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint212.pt
2022-12-09 22:57:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint212.pt (epoch 212 @ 233610 updates, score 37.35) (writing took 1.3245934322476387 seconds)
2022-12-09 22:57:59 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-12-09 22:57:59 | INFO | train | epoch 212 | loss 6.698 | nll_loss 2.824 | ppl 7.08 | wps 24474.9 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 233610 | lr 6.54266e-05 | gnorm 2.398 | loss_scale 8 | train_wall 118 | gb_free 19.4 | wall 10620
2022-12-09 22:57:59 | INFO | fairseq.trainer | begin training epoch 213
2022-12-09 22:58:09 | INFO | train_inner | epoch 213:     90 / 1102 loss=6.659, nll_loss=2.786, ppl=6.9, wps=6890.8, ups=1.95, wpb=3539.6, bsz=139.1, num_updates=233700, lr=6.5414e-05, gnorm=2.284, loss_scale=8, train_wall=11, gb_free=19.5, wall=10630
2022-12-09 22:58:20 | INFO | train_inner | epoch 213:    190 / 1102 loss=6.689, nll_loss=2.817, ppl=7.05, wps=33552.8, ups=9.37, wpb=3579.1, bsz=140.4, num_updates=233800, lr=6.54e-05, gnorm=2.328, loss_scale=8, train_wall=10, gb_free=19.6, wall=10640
2022-12-09 22:58:30 | INFO | train_inner | epoch 213:    290 / 1102 loss=6.659, nll_loss=2.778, ppl=6.86, wps=33718.9, ups=9.33, wpb=3612.8, bsz=141.1, num_updates=233900, lr=6.5386e-05, gnorm=2.392, loss_scale=8, train_wall=10, gb_free=19.7, wall=10651
2022-12-09 22:58:41 | INFO | train_inner | epoch 213:    390 / 1102 loss=6.691, nll_loss=2.825, ppl=7.09, wps=33537.8, ups=9.32, wpb=3597.1, bsz=149.8, num_updates=234000, lr=6.5372e-05, gnorm=2.376, loss_scale=8, train_wall=10, gb_free=19.4, wall=10662
2022-12-09 22:58:52 | INFO | train_inner | epoch 213:    490 / 1102 loss=6.665, nll_loss=2.802, ppl=6.97, wps=33437.1, ups=9.25, wpb=3614.8, bsz=157.8, num_updates=234100, lr=6.53581e-05, gnorm=2.31, loss_scale=8, train_wall=11, gb_free=20, wall=10673
2022-12-09 22:59:03 | INFO | train_inner | epoch 213:    590 / 1102 loss=6.697, nll_loss=2.816, ppl=7.04, wps=33524.5, ups=9.32, wpb=3597, bsz=139.8, num_updates=234200, lr=6.53441e-05, gnorm=2.374, loss_scale=8, train_wall=10, gb_free=19.5, wall=10683
2022-12-09 22:59:14 | INFO | train_inner | epoch 213:    690 / 1102 loss=6.72, nll_loss=2.841, ppl=7.16, wps=33161.2, ups=9.26, wpb=3581.8, bsz=139.2, num_updates=234300, lr=6.53302e-05, gnorm=2.365, loss_scale=8, train_wall=11, gb_free=19.4, wall=10694
2022-12-09 22:59:24 | INFO | train_inner | epoch 213:    790 / 1102 loss=6.785, nll_loss=2.898, ppl=7.45, wps=33164.2, ups=9.38, wpb=3535.4, bsz=130.4, num_updates=234400, lr=6.53162e-05, gnorm=2.554, loss_scale=8, train_wall=10, gb_free=19.4, wall=10705
2022-12-09 22:59:35 | INFO | train_inner | epoch 213:    890 / 1102 loss=6.672, nll_loss=2.799, ppl=6.96, wps=33078.2, ups=9.4, wpb=3517.7, bsz=152.7, num_updates=234500, lr=6.53023e-05, gnorm=2.427, loss_scale=8, train_wall=10, gb_free=19.6, wall=10715
2022-12-09 22:59:46 | INFO | train_inner | epoch 213:    990 / 1102 loss=6.721, nll_loss=2.851, ppl=7.22, wps=33442.9, ups=9.28, wpb=3604.2, bsz=150.5, num_updates=234600, lr=6.52884e-05, gnorm=2.391, loss_scale=8, train_wall=11, gb_free=19.6, wall=10726
2022-12-09 22:59:56 | INFO | train_inner | epoch 213:   1090 / 1102 loss=6.713, nll_loss=2.844, ppl=7.18, wps=33303.7, ups=9.22, wpb=3610.5, bsz=150.2, num_updates=234700, lr=6.52745e-05, gnorm=2.427, loss_scale=8, train_wall=11, gb_free=19.4, wall=10737
2022-12-09 22:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:00:35 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 3.661 | nll_loss 2.121 | ppl 4.35 | bleu 37.4 | wps 4813.9 | wpb 2835.3 | bsz 115.6 | num_updates 234712 | best_bleu 37.58
2022-12-09 23:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 234712 updates
2022-12-09 23:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint213.pt
2022-12-09 23:00:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint213.pt (epoch 213 @ 234712 updates, score 37.4) (writing took 1.098841273225844 seconds)
2022-12-09 23:00:36 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-12-09 23:00:36 | INFO | train | epoch 213 | loss 6.695 | nll_loss 2.822 | ppl 7.07 | wps 25097.3 | ups 7 | wpb 3583.6 | bsz 145.4 | num_updates 234712 | lr 6.52728e-05 | gnorm 2.38 | loss_scale 8 | train_wall 116 | gb_free 19.4 | wall 10777
2022-12-09 23:00:37 | INFO | fairseq.trainer | begin training epoch 214
2022-12-09 23:00:46 | INFO | train_inner | epoch 214:     88 / 1102 loss=6.615, nll_loss=2.745, ppl=6.7, wps=7193.4, ups=2.01, wpb=3580.8, bsz=148.8, num_updates=234800, lr=6.52606e-05, gnorm=2.438, loss_scale=8, train_wall=11, gb_free=19.3, wall=10787
2022-12-09 23:00:57 | INFO | train_inner | epoch 214:    188 / 1102 loss=6.665, nll_loss=2.79, ppl=6.92, wps=33407.2, ups=9.25, wpb=3613.2, bsz=147, num_updates=234900, lr=6.52467e-05, gnorm=2.37, loss_scale=8, train_wall=11, gb_free=19.3, wall=10798
2022-12-09 23:01:08 | INFO | train_inner | epoch 214:    288 / 1102 loss=6.64, nll_loss=2.76, ppl=6.77, wps=31963, ups=9.04, wpb=3537.6, bsz=143.8, num_updates=235000, lr=6.52328e-05, gnorm=2.361, loss_scale=8, train_wall=11, gb_free=19.4, wall=10809
2022-12-09 23:01:19 | INFO | train_inner | epoch 214:    388 / 1102 loss=6.653, nll_loss=2.792, ppl=6.93, wps=32452.4, ups=8.98, wpb=3613.3, bsz=158.2, num_updates=235100, lr=6.52189e-05, gnorm=2.326, loss_scale=8, train_wall=11, gb_free=19.7, wall=10820
2022-12-09 23:01:30 | INFO | train_inner | epoch 214:    488 / 1102 loss=6.701, nll_loss=2.839, ppl=7.15, wps=33094.9, ups=9.12, wpb=3627, bsz=150.7, num_updates=235200, lr=6.52051e-05, gnorm=2.331, loss_scale=8, train_wall=11, gb_free=19.4, wall=10831
2022-12-09 23:01:41 | INFO | train_inner | epoch 214:    588 / 1102 loss=6.697, nll_loss=2.824, ppl=7.08, wps=32011.9, ups=9.11, wpb=3513.9, bsz=151.9, num_updates=235300, lr=6.51912e-05, gnorm=2.474, loss_scale=8, train_wall=11, gb_free=19.8, wall=10842
2022-12-09 23:01:52 | INFO | train_inner | epoch 214:    688 / 1102 loss=6.707, nll_loss=2.832, ppl=7.12, wps=33107.6, ups=9.08, wpb=3646.6, bsz=143, num_updates=235400, lr=6.51774e-05, gnorm=2.277, loss_scale=8, train_wall=11, gb_free=19.3, wall=10853
2022-12-09 23:02:03 | INFO | train_inner | epoch 214:    788 / 1102 loss=6.718, nll_loss=2.841, ppl=7.17, wps=32633.1, ups=9.06, wpb=3601.5, bsz=140.6, num_updates=235500, lr=6.51635e-05, gnorm=2.403, loss_scale=8, train_wall=11, gb_free=19.4, wall=10864
2022-12-09 23:02:14 | INFO | train_inner | epoch 214:    888 / 1102 loss=6.724, nll_loss=2.837, ppl=7.15, wps=32367, ups=9.08, wpb=3564, bsz=139, num_updates=235600, lr=6.51497e-05, gnorm=2.442, loss_scale=8, train_wall=11, gb_free=19.4, wall=10875
2022-12-09 23:02:25 | INFO | train_inner | epoch 214:    988 / 1102 loss=6.718, nll_loss=2.836, ppl=7.14, wps=32496.4, ups=9.09, wpb=3576.4, bsz=139, num_updates=235700, lr=6.51359e-05, gnorm=2.379, loss_scale=8, train_wall=11, gb_free=19.3, wall=10886
2022-12-09 23:02:36 | INFO | train_inner | epoch 214:   1088 / 1102 loss=6.76, nll_loss=2.891, ppl=7.42, wps=32484.8, ups=9.05, wpb=3588.3, bsz=142.8, num_updates=235800, lr=6.51221e-05, gnorm=2.406, loss_scale=8, train_wall=11, gb_free=19.3, wall=10897
2022-12-09 23:02:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:03:16 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 3.659 | nll_loss 2.121 | ppl 4.35 | bleu 37.45 | wps 4704.6 | wpb 2835.3 | bsz 115.6 | num_updates 235814 | best_bleu 37.58
2022-12-09 23:03:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 235814 updates
2022-12-09 23:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint214.pt
2022-12-09 23:03:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint214.pt (epoch 214 @ 235814 updates, score 37.45) (writing took 1.2780492855235934 seconds)
2022-12-09 23:03:18 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-12-09 23:03:18 | INFO | train | epoch 214 | loss 6.694 | nll_loss 2.82 | ppl 7.06 | wps 24504.5 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 235814 | lr 6.51201e-05 | gnorm 2.387 | loss_scale 8 | train_wall 118 | gb_free 19.5 | wall 10938
2022-12-09 23:03:18 | INFO | fairseq.trainer | begin training epoch 215
2022-12-09 23:03:27 | INFO | train_inner | epoch 215:     86 / 1102 loss=6.646, nll_loss=2.787, ppl=6.9, wps=7062, ups=1.96, wpb=3603.3, bsz=154.4, num_updates=235900, lr=6.51083e-05, gnorm=2.317, loss_scale=8, train_wall=11, gb_free=19.7, wall=10948
2022-12-09 23:03:38 | INFO | train_inner | epoch 215:    186 / 1102 loss=6.682, nll_loss=2.795, ppl=6.94, wps=32881.1, ups=9.18, wpb=3582.7, bsz=135.3, num_updates=236000, lr=6.50945e-05, gnorm=2.438, loss_scale=8, train_wall=11, gb_free=19.8, wall=10959
2022-12-09 23:03:49 | INFO | train_inner | epoch 215:    286 / 1102 loss=6.698, nll_loss=2.817, ppl=7.05, wps=32822.2, ups=9.15, wpb=3588.7, bsz=143, num_updates=236100, lr=6.50807e-05, gnorm=2.361, loss_scale=8, train_wall=11, gb_free=19.7, wall=10970
2022-12-09 23:04:00 | INFO | train_inner | epoch 215:    386 / 1102 loss=6.656, nll_loss=2.784, ppl=6.89, wps=32800.4, ups=9.07, wpb=3617.1, bsz=144.2, num_updates=236200, lr=6.50669e-05, gnorm=2.365, loss_scale=8, train_wall=11, gb_free=19.4, wall=10981
2022-12-09 23:04:11 | INFO | train_inner | epoch 215:    486 / 1102 loss=6.733, nll_loss=2.854, ppl=7.23, wps=32561.6, ups=9.13, wpb=3565.8, bsz=133.9, num_updates=236300, lr=6.50531e-05, gnorm=2.334, loss_scale=8, train_wall=11, gb_free=19.4, wall=10992
2022-12-09 23:04:22 | INFO | train_inner | epoch 215:    586 / 1102 loss=6.661, nll_loss=2.769, ppl=6.82, wps=32772.2, ups=9.21, wpb=3556.8, bsz=137.6, num_updates=236400, lr=6.50394e-05, gnorm=2.439, loss_scale=8, train_wall=11, gb_free=19.8, wall=11003
2022-12-09 23:04:33 | INFO | train_inner | epoch 215:    686 / 1102 loss=6.705, nll_loss=2.829, ppl=7.11, wps=32632.9, ups=9.19, wpb=3549.7, bsz=141.6, num_updates=236500, lr=6.50256e-05, gnorm=2.4, loss_scale=8, train_wall=11, gb_free=19.7, wall=11013
2022-12-09 23:04:44 | INFO | train_inner | epoch 215:    786 / 1102 loss=6.636, nll_loss=2.776, ppl=6.85, wps=32126.4, ups=9.11, wpb=3525.9, bsz=163.3, num_updates=236600, lr=6.50119e-05, gnorm=2.395, loss_scale=8, train_wall=11, gb_free=19.9, wall=11024
2022-12-09 23:04:55 | INFO | train_inner | epoch 215:    886 / 1102 loss=6.685, nll_loss=2.82, ppl=7.06, wps=32633.1, ups=9.06, wpb=3603.5, bsz=151.4, num_updates=236700, lr=6.49981e-05, gnorm=2.343, loss_scale=8, train_wall=11, gb_free=19.4, wall=11035
2022-12-09 23:05:06 | INFO | train_inner | epoch 215:    986 / 1102 loss=6.787, nll_loss=2.915, ppl=7.54, wps=32980.7, ups=9.08, wpb=3631.8, bsz=141, num_updates=236800, lr=6.49844e-05, gnorm=2.364, loss_scale=8, train_wall=11, gb_free=19.4, wall=11046
2022-12-09 23:05:17 | INFO | train_inner | epoch 215:   1086 / 1102 loss=6.718, nll_loss=2.852, ppl=7.22, wps=32655.5, ups=9.14, wpb=3572.5, bsz=156.2, num_updates=236900, lr=6.49707e-05, gnorm=2.461, loss_scale=8, train_wall=11, gb_free=19.2, wall=11057
2022-12-09 23:05:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:06:00 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 3.661 | nll_loss 2.12 | ppl 4.35 | bleu 37.38 | wps 4373.9 | wpb 2835.3 | bsz 115.6 | num_updates 236916 | best_bleu 37.58
2022-12-09 23:06:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 236916 updates
2022-12-09 23:06:01 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint215.pt
2022-12-09 23:06:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint215.pt (epoch 215 @ 236916 updates, score 37.38) (writing took 1.2712614918127656 seconds)
2022-12-09 23:06:01 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-12-09 23:06:01 | INFO | train | epoch 215 | loss 6.691 | nll_loss 2.817 | ppl 7.05 | wps 24151.9 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 236916 | lr 6.49685e-05 | gnorm 2.382 | loss_scale 8 | train_wall 118 | gb_free 19.6 | wall 11102
2022-12-09 23:06:01 | INFO | fairseq.trainer | begin training epoch 216
2022-12-09 23:06:11 | INFO | train_inner | epoch 216:     84 / 1102 loss=6.702, nll_loss=2.81, ppl=7.01, wps=6587.9, ups=1.86, wpb=3539.6, bsz=129.2, num_updates=237000, lr=6.4957e-05, gnorm=2.392, loss_scale=8, train_wall=11, gb_free=19.5, wall=11111
2022-12-09 23:06:21 | INFO | train_inner | epoch 216:    184 / 1102 loss=6.643, nll_loss=2.769, ppl=6.81, wps=32868.4, ups=9.17, wpb=3584.9, bsz=142.6, num_updates=237100, lr=6.49433e-05, gnorm=2.339, loss_scale=8, train_wall=11, gb_free=19.6, wall=11122
2022-12-09 23:06:32 | INFO | train_inner | epoch 216:    284 / 1102 loss=6.719, nll_loss=2.839, ppl=7.16, wps=32623.3, ups=9.15, wpb=3566.5, bsz=137.7, num_updates=237200, lr=6.49296e-05, gnorm=2.393, loss_scale=8, train_wall=11, gb_free=19.7, wall=11133
2022-12-09 23:06:43 | INFO | train_inner | epoch 216:    384 / 1102 loss=6.654, nll_loss=2.778, ppl=6.86, wps=32428.5, ups=9.13, wpb=3553.3, bsz=147.1, num_updates=237300, lr=6.49159e-05, gnorm=2.401, loss_scale=8, train_wall=11, gb_free=19.7, wall=11144
2022-12-09 23:06:54 | INFO | train_inner | epoch 216:    484 / 1102 loss=6.764, nll_loss=2.875, ppl=7.34, wps=32819.4, ups=9.14, wpb=3592.4, bsz=129.5, num_updates=237400, lr=6.49022e-05, gnorm=2.398, loss_scale=8, train_wall=11, gb_free=19.6, wall=11155
2022-12-09 23:07:05 | INFO | train_inner | epoch 216:    584 / 1102 loss=6.68, nll_loss=2.812, ppl=7.02, wps=32389.5, ups=9.06, wpb=3574.2, bsz=157.3, num_updates=237500, lr=6.48886e-05, gnorm=2.469, loss_scale=8, train_wall=11, gb_free=19.3, wall=11166
2022-12-09 23:07:16 | INFO | train_inner | epoch 216:    684 / 1102 loss=6.664, nll_loss=2.785, ppl=6.89, wps=32814.7, ups=9.14, wpb=3591.1, bsz=146.8, num_updates=237600, lr=6.48749e-05, gnorm=2.384, loss_scale=8, train_wall=11, gb_free=19.4, wall=11177
2022-12-09 23:07:27 | INFO | train_inner | epoch 216:    784 / 1102 loss=6.702, nll_loss=2.826, ppl=7.09, wps=32895.5, ups=9.14, wpb=3598.4, bsz=143.5, num_updates=237700, lr=6.48613e-05, gnorm=2.335, loss_scale=8, train_wall=11, gb_free=19.5, wall=11188
2022-12-09 23:07:38 | INFO | train_inner | epoch 216:    884 / 1102 loss=6.652, nll_loss=2.796, ppl=6.94, wps=32998.1, ups=9.12, wpb=3618.2, bsz=156.4, num_updates=237800, lr=6.48476e-05, gnorm=2.326, loss_scale=8, train_wall=11, gb_free=19.3, wall=11199
2022-12-09 23:07:49 | INFO | train_inner | epoch 216:    984 / 1102 loss=6.693, nll_loss=2.84, ppl=7.16, wps=32889.1, ups=9.07, wpb=3626.7, bsz=164, num_updates=237900, lr=6.4834e-05, gnorm=2.388, loss_scale=8, train_wall=11, gb_free=19.5, wall=11210
2022-12-09 23:08:00 | INFO | train_inner | epoch 216:   1084 / 1102 loss=6.721, nll_loss=2.845, ppl=7.18, wps=32707.4, ups=9.15, wpb=3576.1, bsz=144, num_updates=238000, lr=6.48204e-05, gnorm=2.406, loss_scale=8, train_wall=11, gb_free=19.4, wall=11221
2022-12-09 23:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:08:41 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 3.665 | nll_loss 2.123 | ppl 4.36 | bleu 37.23 | wps 4625.6 | wpb 2835.3 | bsz 115.6 | num_updates 238018 | best_bleu 37.58
2022-12-09 23:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 238018 updates
2022-12-09 23:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint216.pt
2022-12-09 23:08:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint216.pt (epoch 216 @ 238018 updates, score 37.23) (writing took 1.3232254907488823 seconds)
2022-12-09 23:08:43 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-12-09 23:08:43 | INFO | train | epoch 216 | loss 6.69 | nll_loss 2.815 | ppl 7.04 | wps 24455.4 | ups 6.82 | wpb 3583.6 | bsz 145.4 | num_updates 238018 | lr 6.48179e-05 | gnorm 2.387 | loss_scale 8 | train_wall 118 | gb_free 19.7 | wall 11263
2022-12-09 23:08:43 | INFO | fairseq.trainer | begin training epoch 217
2022-12-09 23:08:52 | INFO | train_inner | epoch 217:     82 / 1102 loss=6.651, nll_loss=2.774, ppl=6.84, wps=6942.3, ups=1.93, wpb=3592.7, bsz=150.1, num_updates=238100, lr=6.48068e-05, gnorm=2.367, loss_scale=8, train_wall=11, gb_free=19.2, wall=11272
2022-12-09 23:09:03 | INFO | train_inner | epoch 217:    182 / 1102 loss=6.615, nll_loss=2.747, ppl=6.71, wps=32693.2, ups=9.06, wpb=3606.7, bsz=149.6, num_updates=238200, lr=6.47932e-05, gnorm=2.301, loss_scale=8, train_wall=11, gb_free=19.4, wall=11284
2022-12-09 23:09:14 | INFO | train_inner | epoch 217:    282 / 1102 loss=6.653, nll_loss=2.788, ppl=6.91, wps=32433.3, ups=9.14, wpb=3549.6, bsz=150.5, num_updates=238300, lr=6.47796e-05, gnorm=2.392, loss_scale=8, train_wall=11, gb_free=19.9, wall=11294
2022-12-09 23:09:25 | INFO | train_inner | epoch 217:    382 / 1102 loss=6.695, nll_loss=2.823, ppl=7.07, wps=32718.6, ups=9.09, wpb=3600.2, bsz=147.5, num_updates=238400, lr=6.4766e-05, gnorm=2.399, loss_scale=8, train_wall=11, gb_free=19.7, wall=11305
2022-12-09 23:09:36 | INFO | train_inner | epoch 217:    482 / 1102 loss=6.74, nll_loss=2.867, ppl=7.3, wps=32735.5, ups=9.01, wpb=3632.6, bsz=138.6, num_updates=238500, lr=6.47524e-05, gnorm=2.344, loss_scale=8, train_wall=11, gb_free=19.6, wall=11317
2022-12-09 23:09:47 | INFO | train_inner | epoch 217:    582 / 1102 loss=6.655, nll_loss=2.776, ppl=6.85, wps=32367, ups=9.17, wpb=3529.9, bsz=147.6, num_updates=238600, lr=6.47388e-05, gnorm=2.408, loss_scale=8, train_wall=11, gb_free=19.6, wall=11327
2022-12-09 23:09:58 | INFO | train_inner | epoch 217:    682 / 1102 loss=6.704, nll_loss=2.832, ppl=7.12, wps=32471.5, ups=8.99, wpb=3612.8, bsz=144.9, num_updates=238700, lr=6.47253e-05, gnorm=2.399, loss_scale=8, train_wall=11, gb_free=20, wall=11339
2022-12-09 23:10:09 | INFO | train_inner | epoch 217:    782 / 1102 loss=6.696, nll_loss=2.816, ppl=7.04, wps=32226.1, ups=9.17, wpb=3513.7, bsz=143.4, num_updates=238800, lr=6.47117e-05, gnorm=2.419, loss_scale=8, train_wall=11, gb_free=19.2, wall=11350
2022-12-09 23:10:20 | INFO | train_inner | epoch 217:    882 / 1102 loss=6.707, nll_loss=2.816, ppl=7.04, wps=32179.5, ups=9.1, wpb=3534.8, bsz=134.1, num_updates=238900, lr=6.46982e-05, gnorm=2.393, loss_scale=8, train_wall=11, gb_free=19.4, wall=11360
2022-12-09 23:10:31 | INFO | train_inner | epoch 217:    982 / 1102 loss=6.696, nll_loss=2.835, ppl=7.13, wps=32641.4, ups=9.04, wpb=3611.8, bsz=158.7, num_updates=239000, lr=6.46846e-05, gnorm=2.448, loss_scale=8, train_wall=11, gb_free=19.5, wall=11372
2022-12-09 23:10:42 | INFO | train_inner | epoch 217:   1082 / 1102 loss=6.751, nll_loss=2.874, ppl=7.33, wps=32881.7, ups=9.03, wpb=3639.8, bsz=134.7, num_updates=239100, lr=6.46711e-05, gnorm=2.446, loss_scale=8, train_wall=11, gb_free=19.3, wall=11383
2022-12-09 23:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:11:22 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 3.66 | nll_loss 2.121 | ppl 4.35 | bleu 37.55 | wps 4795.5 | wpb 2835.3 | bsz 115.6 | num_updates 239120 | best_bleu 37.58
2022-12-09 23:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 239120 updates
2022-12-09 23:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint217.pt
2022-12-09 23:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint217.pt (epoch 217 @ 239120 updates, score 37.55) (writing took 1.5356795098632574 seconds)
2022-12-09 23:11:24 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-12-09 23:11:24 | INFO | train | epoch 217 | loss 6.687 | nll_loss 2.813 | ppl 7.03 | wps 24531.9 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 239120 | lr 6.46684e-05 | gnorm 2.39 | loss_scale 8 | train_wall 118 | gb_free 19.7 | wall 11424
2022-12-09 23:11:24 | INFO | fairseq.trainer | begin training epoch 218
2022-12-09 23:11:33 | INFO | train_inner | epoch 218:     80 / 1102 loss=6.659, nll_loss=2.789, ppl=6.91, wps=7009.5, ups=1.98, wpb=3547.4, bsz=149.3, num_updates=239200, lr=6.46576e-05, gnorm=2.4, loss_scale=8, train_wall=11, gb_free=19.5, wall=11433
2022-12-09 23:11:44 | INFO | train_inner | epoch 218:    180 / 1102 loss=6.662, nll_loss=2.771, ppl=6.83, wps=32526.6, ups=9.16, wpb=3550.7, bsz=133.9, num_updates=239300, lr=6.46441e-05, gnorm=2.601, loss_scale=8, train_wall=11, gb_free=19.8, wall=11444
2022-12-09 23:11:54 | INFO | train_inner | epoch 218:    280 / 1102 loss=6.722, nll_loss=2.84, ppl=7.16, wps=32682.8, ups=9.2, wpb=3553.9, bsz=135.6, num_updates=239400, lr=6.46306e-05, gnorm=2.35, loss_scale=8, train_wall=11, gb_free=19.5, wall=11455
2022-12-09 23:12:05 | INFO | train_inner | epoch 218:    380 / 1102 loss=6.62, nll_loss=2.758, ppl=6.77, wps=33140.4, ups=9.29, wpb=3566.7, bsz=155.8, num_updates=239500, lr=6.46171e-05, gnorm=2.383, loss_scale=8, train_wall=11, gb_free=19.7, wall=11466
2022-12-09 23:12:16 | INFO | train_inner | epoch 218:    480 / 1102 loss=6.656, nll_loss=2.773, ppl=6.84, wps=32875.2, ups=9.22, wpb=3565.4, bsz=149.4, num_updates=239600, lr=6.46036e-05, gnorm=2.413, loss_scale=8, train_wall=11, gb_free=19.7, wall=11477
2022-12-09 23:12:27 | INFO | train_inner | epoch 218:    580 / 1102 loss=6.774, nll_loss=2.909, ppl=7.51, wps=33593.4, ups=9.21, wpb=3647.2, bsz=142.8, num_updates=239700, lr=6.45901e-05, gnorm=2.385, loss_scale=8, train_wall=11, gb_free=19.6, wall=11487
2022-12-09 23:12:38 | INFO | train_inner | epoch 218:    680 / 1102 loss=6.605, nll_loss=2.735, ppl=6.66, wps=32981.5, ups=9.23, wpb=3573.4, bsz=163.7, num_updates=239800, lr=6.45766e-05, gnorm=2.357, loss_scale=8, train_wall=11, gb_free=20.1, wall=11498
2022-12-09 23:12:49 | INFO | train_inner | epoch 218:    780 / 1102 loss=6.717, nll_loss=2.849, ppl=7.21, wps=33592, ups=9.1, wpb=3693.2, bsz=141.8, num_updates=239900, lr=6.45632e-05, gnorm=2.325, loss_scale=8, train_wall=11, gb_free=19.7, wall=11509
2022-12-09 23:13:00 | INFO | train_inner | epoch 218:    880 / 1102 loss=6.741, nll_loss=2.849, ppl=7.21, wps=32934.8, ups=9.17, wpb=3592.2, bsz=135.3, num_updates=240000, lr=6.45497e-05, gnorm=2.442, loss_scale=8, train_wall=11, gb_free=19.6, wall=11520
2022-12-09 23:13:11 | INFO | train_inner | epoch 218:    980 / 1102 loss=6.734, nll_loss=2.845, ppl=7.19, wps=32234.5, ups=9.01, wpb=3577, bsz=133.2, num_updates=240100, lr=6.45363e-05, gnorm=2.419, loss_scale=8, train_wall=11, gb_free=19.4, wall=11531
2022-12-09 23:13:22 | INFO | train_inner | epoch 218:   1080 / 1102 loss=6.689, nll_loss=2.817, ppl=7.05, wps=32703.9, ups=9.18, wpb=3561.3, bsz=152.6, num_updates=240200, lr=6.45228e-05, gnorm=2.449, loss_scale=8, train_wall=11, gb_free=19.9, wall=11542
2022-12-09 23:13:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:14:03 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 3.666 | nll_loss 2.123 | ppl 4.36 | bleu 37.49 | wps 4575.5 | wpb 2835.3 | bsz 115.6 | num_updates 240222 | best_bleu 37.58
2022-12-09 23:14:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 240222 updates
2022-12-09 23:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint218.pt
2022-12-09 23:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint218.pt (epoch 218 @ 240222 updates, score 37.49) (writing took 1.5562002006918192 seconds)
2022-12-09 23:14:05 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-12-09 23:14:05 | INFO | train | epoch 218 | loss 6.687 | nll_loss 2.811 | ppl 7.02 | wps 24462.3 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 240222 | lr 6.45199e-05 | gnorm 2.411 | loss_scale 8 | train_wall 117 | gb_free 19.5 | wall 11586
2022-12-09 23:14:05 | INFO | fairseq.trainer | begin training epoch 219
2022-12-09 23:14:14 | INFO | train_inner | epoch 219:     78 / 1102 loss=6.599, nll_loss=2.733, ppl=6.65, wps=6776.4, ups=1.92, wpb=3535.1, bsz=151.6, num_updates=240300, lr=6.45094e-05, gnorm=2.358, loss_scale=8, train_wall=11, gb_free=19.7, wall=11594
2022-12-09 23:14:25 | INFO | train_inner | epoch 219:    178 / 1102 loss=6.701, nll_loss=2.811, ppl=7.02, wps=33109.4, ups=9.09, wpb=3641.1, bsz=129.9, num_updates=240400, lr=6.4496e-05, gnorm=2.332, loss_scale=8, train_wall=11, gb_free=19.3, wall=11605
2022-12-09 23:14:36 | INFO | train_inner | epoch 219:    278 / 1102 loss=6.675, nll_loss=2.803, ppl=6.98, wps=33278.6, ups=9.14, wpb=3639.6, bsz=148.7, num_updates=240500, lr=6.44826e-05, gnorm=2.301, loss_scale=8, train_wall=11, gb_free=19.5, wall=11616
2022-12-09 23:14:47 | INFO | train_inner | epoch 219:    378 / 1102 loss=6.591, nll_loss=2.724, ppl=6.61, wps=31846.9, ups=9.02, wpb=3530.2, bsz=163.3, num_updates=240600, lr=6.44692e-05, gnorm=2.382, loss_scale=8, train_wall=11, gb_free=19.6, wall=11627
2022-12-09 23:14:58 | INFO | train_inner | epoch 219:    478 / 1102 loss=6.669, nll_loss=2.806, ppl=6.99, wps=33287.2, ups=9.16, wpb=3634.4, bsz=154.5, num_updates=240700, lr=6.44558e-05, gnorm=2.346, loss_scale=8, train_wall=11, gb_free=19.3, wall=11638
2022-12-09 23:15:08 | INFO | train_inner | epoch 219:    578 / 1102 loss=6.713, nll_loss=2.815, ppl=7.04, wps=32710.7, ups=9.43, wpb=3467, bsz=134.3, num_updates=240800, lr=6.44424e-05, gnorm=2.607, loss_scale=8, train_wall=10, gb_free=19.4, wall=11649
2022-12-09 23:15:19 | INFO | train_inner | epoch 219:    678 / 1102 loss=6.74, nll_loss=2.846, ppl=7.19, wps=33752.5, ups=9.46, wpb=3568.5, bsz=133.3, num_updates=240900, lr=6.4429e-05, gnorm=2.431, loss_scale=8, train_wall=10, gb_free=19.4, wall=11659
2022-12-09 23:15:30 | INFO | train_inner | epoch 219:    778 / 1102 loss=6.682, nll_loss=2.814, ppl=7.03, wps=33287, ups=9.24, wpb=3604.1, bsz=147, num_updates=241000, lr=6.44157e-05, gnorm=2.358, loss_scale=8, train_wall=11, gb_free=19.7, wall=11670
2022-12-09 23:15:41 | INFO | train_inner | epoch 219:    878 / 1102 loss=6.721, nll_loss=2.856, ppl=7.24, wps=32843.6, ups=9.27, wpb=3544.8, bsz=148.5, num_updates=241100, lr=6.44023e-05, gnorm=2.438, loss_scale=8, train_wall=11, gb_free=19.6, wall=11681
2022-12-09 23:15:51 | INFO | train_inner | epoch 219:    978 / 1102 loss=6.727, nll_loss=2.847, ppl=7.19, wps=33166.4, ups=9.23, wpb=3594.3, bsz=143.9, num_updates=241200, lr=6.4389e-05, gnorm=2.419, loss_scale=8, train_wall=11, gb_free=19.3, wall=11692
2022-12-09 23:16:02 | INFO | train_inner | epoch 219:   1078 / 1102 loss=6.694, nll_loss=2.828, ppl=7.1, wps=33170.7, ups=9.2, wpb=3606.2, bsz=150.7, num_updates=241300, lr=6.43756e-05, gnorm=2.4, loss_scale=8, train_wall=11, gb_free=19.4, wall=11703
2022-12-09 23:16:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:16:42 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 3.667 | nll_loss 2.123 | ppl 4.35 | bleu 37.49 | wps 4918.7 | wpb 2835.3 | bsz 115.6 | num_updates 241324 | best_bleu 37.58
2022-12-09 23:16:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 241324 updates
2022-12-09 23:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint219.pt
2022-12-09 23:16:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint219.pt (epoch 219 @ 241324 updates, score 37.49) (writing took 1.1470644129440188 seconds)
2022-12-09 23:16:43 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-12-09 23:16:43 | INFO | train | epoch 219 | loss 6.685 | nll_loss 2.809 | ppl 7.01 | wps 25036.9 | ups 6.99 | wpb 3583.6 | bsz 145.4 | num_updates 241324 | lr 6.43724e-05 | gnorm 2.399 | loss_scale 8 | train_wall 117 | gb_free 19.4 | wall 11743
2022-12-09 23:16:43 | INFO | fairseq.trainer | begin training epoch 220
2022-12-09 23:16:51 | INFO | train_inner | epoch 220:     76 / 1102 loss=6.664, nll_loss=2.783, ppl=6.88, wps=7351.5, ups=2.03, wpb=3615.1, bsz=137.6, num_updates=241400, lr=6.43623e-05, gnorm=2.513, loss_scale=8, train_wall=11, gb_free=19.4, wall=11752
2022-12-09 23:17:02 | INFO | train_inner | epoch 220:    176 / 1102 loss=6.677, nll_loss=2.792, ppl=6.93, wps=32929.4, ups=9.26, wpb=3554.6, bsz=140.6, num_updates=241500, lr=6.43489e-05, gnorm=2.394, loss_scale=8, train_wall=11, gb_free=19.5, wall=11763
2022-12-09 23:17:13 | INFO | train_inner | epoch 220:    276 / 1102 loss=6.69, nll_loss=2.806, ppl=6.99, wps=33469.7, ups=9.34, wpb=3584.4, bsz=143.8, num_updates=241600, lr=6.43356e-05, gnorm=2.453, loss_scale=8, train_wall=10, gb_free=19.5, wall=11774
2022-12-09 23:17:24 | INFO | train_inner | epoch 220:    376 / 1102 loss=6.663, nll_loss=2.78, ppl=6.87, wps=33086.4, ups=9.28, wpb=3564.8, bsz=139.2, num_updates=241700, lr=6.43223e-05, gnorm=2.362, loss_scale=8, train_wall=11, gb_free=19.3, wall=11784
2022-12-09 23:17:34 | INFO | train_inner | epoch 220:    476 / 1102 loss=6.71, nll_loss=2.825, ppl=7.09, wps=33199.4, ups=9.32, wpb=3560.4, bsz=136.1, num_updates=241800, lr=6.4309e-05, gnorm=2.434, loss_scale=8, train_wall=10, gb_free=19.6, wall=11795
2022-12-09 23:17:45 | INFO | train_inner | epoch 220:    576 / 1102 loss=6.627, nll_loss=2.762, ppl=6.79, wps=32695.8, ups=9.22, wpb=3545.1, bsz=159.3, num_updates=241900, lr=6.42957e-05, gnorm=2.369, loss_scale=8, train_wall=11, gb_free=19.4, wall=11806
2022-12-09 23:17:56 | INFO | train_inner | epoch 220:    676 / 1102 loss=6.692, nll_loss=2.825, ppl=7.09, wps=33561.4, ups=9.23, wpb=3637.1, bsz=143, num_updates=242000, lr=6.42824e-05, gnorm=2.404, loss_scale=8, train_wall=11, gb_free=19.2, wall=11817
2022-12-09 23:18:07 | INFO | train_inner | epoch 220:    776 / 1102 loss=6.658, nll_loss=2.792, ppl=6.92, wps=32535.6, ups=9.14, wpb=3561.3, bsz=149.1, num_updates=242100, lr=6.42692e-05, gnorm=2.39, loss_scale=8, train_wall=11, gb_free=19.5, wall=11828
2022-12-09 23:18:18 | INFO | train_inner | epoch 220:    876 / 1102 loss=6.747, nll_loss=2.873, ppl=7.33, wps=33227.3, ups=9.22, wpb=3604.5, bsz=142.7, num_updates=242200, lr=6.42559e-05, gnorm=2.392, loss_scale=8, train_wall=11, gb_free=19.5, wall=11838
2022-12-09 23:18:29 | INFO | train_inner | epoch 220:    976 / 1102 loss=6.614, nll_loss=2.76, ppl=6.78, wps=33378.3, ups=9.21, wpb=3622.8, bsz=169.6, num_updates=242300, lr=6.42426e-05, gnorm=2.35, loss_scale=8, train_wall=11, gb_free=19.3, wall=11849
2022-12-09 23:18:40 | INFO | train_inner | epoch 220:   1076 / 1102 loss=6.743, nll_loss=2.863, ppl=7.28, wps=33010.2, ups=9.19, wpb=3590.1, bsz=140.3, num_updates=242400, lr=6.42294e-05, gnorm=2.401, loss_scale=8, train_wall=11, gb_free=19.8, wall=11860
2022-12-09 23:18:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:19:21 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 3.66 | nll_loss 2.122 | ppl 4.35 | bleu 37.61 | wps 4747.2 | wpb 2835.3 | bsz 115.6 | num_updates 242426 | best_bleu 37.61
2022-12-09 23:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 242426 updates
2022-12-09 23:19:22 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint220.pt
2022-12-09 23:19:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint220.pt (epoch 220 @ 242426 updates, score 37.61) (writing took 1.7455690847709775 seconds)
2022-12-09 23:19:22 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-12-09 23:19:22 | INFO | train | epoch 220 | loss 6.683 | nll_loss 2.808 | ppl 7 | wps 24737.1 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 242426 | lr 6.42259e-05 | gnorm 2.405 | loss_scale 8 | train_wall 117 | gb_free 19.5 | wall 11903
2022-12-09 23:19:22 | INFO | fairseq.trainer | begin training epoch 221
2022-12-09 23:19:31 | INFO | train_inner | epoch 221:     74 / 1102 loss=6.64, nll_loss=2.773, ppl=6.84, wps=7047.6, ups=1.96, wpb=3602.5, bsz=151.9, num_updates=242500, lr=6.42161e-05, gnorm=2.399, loss_scale=8, train_wall=11, gb_free=19.5, wall=11911
2022-12-09 23:19:41 | INFO | train_inner | epoch 221:    174 / 1102 loss=6.729, nll_loss=2.842, ppl=7.17, wps=33092.3, ups=9.32, wpb=3549.8, bsz=130.2, num_updates=242600, lr=6.42029e-05, gnorm=2.393, loss_scale=8, train_wall=10, gb_free=19.3, wall=11922
2022-12-09 23:19:52 | INFO | train_inner | epoch 221:    274 / 1102 loss=6.64, nll_loss=2.76, ppl=6.78, wps=32430.5, ups=9.18, wpb=3533.6, bsz=145.9, num_updates=242700, lr=6.41897e-05, gnorm=2.412, loss_scale=8, train_wall=11, gb_free=19.7, wall=11933
2022-12-09 23:20:03 | INFO | train_inner | epoch 221:    374 / 1102 loss=6.711, nll_loss=2.83, ppl=7.11, wps=33288.6, ups=9.18, wpb=3626.8, bsz=143, num_updates=242800, lr=6.41764e-05, gnorm=2.444, loss_scale=8, train_wall=11, gb_free=19.7, wall=11944
2022-12-09 23:20:14 | INFO | train_inner | epoch 221:    474 / 1102 loss=6.673, nll_loss=2.793, ppl=6.93, wps=33376.9, ups=9.16, wpb=3642.1, bsz=138.6, num_updates=242900, lr=6.41632e-05, gnorm=2.376, loss_scale=8, train_wall=11, gb_free=19.5, wall=11955
2022-12-09 23:20:25 | INFO | train_inner | epoch 221:    574 / 1102 loss=6.648, nll_loss=2.777, ppl=6.85, wps=32192.9, ups=9.03, wpb=3564.3, bsz=155.9, num_updates=243000, lr=6.415e-05, gnorm=2.478, loss_scale=8, train_wall=11, gb_free=19.5, wall=11966
2022-12-09 23:20:36 | INFO | train_inner | epoch 221:    674 / 1102 loss=6.666, nll_loss=2.795, ppl=6.94, wps=32726.1, ups=9.21, wpb=3554.2, bsz=146.5, num_updates=243100, lr=6.41368e-05, gnorm=2.388, loss_scale=8, train_wall=11, gb_free=19.4, wall=11977
2022-12-09 23:20:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-09 23:20:47 | INFO | train_inner | epoch 221:    775 / 1102 loss=6.716, nll_loss=2.858, ppl=7.25, wps=32448.3, ups=8.93, wpb=3635.1, bsz=155, num_updates=243200, lr=6.41236e-05, gnorm=2.33, loss_scale=4, train_wall=11, gb_free=19.3, wall=11988
2022-12-09 23:20:58 | INFO | train_inner | epoch 221:    875 / 1102 loss=6.683, nll_loss=2.8, ppl=6.97, wps=32624.4, ups=9.03, wpb=3611.2, bsz=140.1, num_updates=243300, lr=6.41105e-05, gnorm=2.382, loss_scale=4, train_wall=11, gb_free=19.4, wall=11999
2022-12-09 23:21:10 | INFO | train_inner | epoch 221:    975 / 1102 loss=6.671, nll_loss=2.806, ppl=6.99, wps=31737.2, ups=8.96, wpb=3541.8, bsz=156.9, num_updates=243400, lr=6.40973e-05, gnorm=2.459, loss_scale=4, train_wall=11, gb_free=19.6, wall=12010
2022-12-09 23:21:21 | INFO | train_inner | epoch 221:   1075 / 1102 loss=6.767, nll_loss=2.867, ppl=7.29, wps=32527.9, ups=9.04, wpb=3598.4, bsz=127.1, num_updates=243500, lr=6.40841e-05, gnorm=2.484, loss_scale=4, train_wall=11, gb_free=19.7, wall=12021
2022-12-09 23:21:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:22:04 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 3.665 | nll_loss 2.122 | ppl 4.35 | bleu 37.38 | wps 4482.7 | wpb 2835.3 | bsz 115.6 | num_updates 243527 | best_bleu 37.61
2022-12-09 23:22:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 243527 updates
2022-12-09 23:22:05 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint221.pt
2022-12-09 23:22:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint221.pt (epoch 221 @ 243527 updates, score 37.38) (writing took 1.359718119725585 seconds)
2022-12-09 23:22:05 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-12-09 23:22:05 | INFO | train | epoch 221 | loss 6.682 | nll_loss 2.806 | ppl 6.99 | wps 24192.5 | ups 6.75 | wpb 3583.6 | bsz 145.5 | num_updates 243527 | lr 6.40806e-05 | gnorm 2.412 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 12066
2022-12-09 23:22:06 | INFO | fairseq.trainer | begin training epoch 222
2022-12-09 23:22:14 | INFO | train_inner | epoch 222:     73 / 1102 loss=6.615, nll_loss=2.745, ppl=6.7, wps=6662.5, ups=1.88, wpb=3543.4, bsz=153.1, num_updates=243600, lr=6.4071e-05, gnorm=2.458, loss_scale=4, train_wall=11, gb_free=19.3, wall=12074
2022-12-09 23:22:25 | INFO | train_inner | epoch 222:    173 / 1102 loss=6.707, nll_loss=2.833, ppl=7.12, wps=32528.6, ups=9.03, wpb=3603.4, bsz=145.4, num_updates=243700, lr=6.40578e-05, gnorm=2.368, loss_scale=4, train_wall=11, gb_free=19.4, wall=12085
2022-12-09 23:22:36 | INFO | train_inner | epoch 222:    273 / 1102 loss=6.687, nll_loss=2.79, ppl=6.92, wps=32046.5, ups=8.96, wpb=3576.2, bsz=131.1, num_updates=243800, lr=6.40447e-05, gnorm=2.44, loss_scale=4, train_wall=11, gb_free=19.6, wall=12097
2022-12-09 23:22:47 | INFO | train_inner | epoch 222:    373 / 1102 loss=6.635, nll_loss=2.756, ppl=6.75, wps=32620.4, ups=9.05, wpb=3606.2, bsz=146, num_updates=243900, lr=6.40316e-05, gnorm=2.323, loss_scale=4, train_wall=11, gb_free=19.4, wall=12108
2022-12-09 23:22:58 | INFO | train_inner | epoch 222:    473 / 1102 loss=6.706, nll_loss=2.82, ppl=7.06, wps=32152.2, ups=9.03, wpb=3561.1, bsz=136, num_updates=244000, lr=6.40184e-05, gnorm=2.507, loss_scale=4, train_wall=11, gb_free=19.2, wall=12119
2022-12-09 23:23:09 | INFO | train_inner | epoch 222:    573 / 1102 loss=6.651, nll_loss=2.775, ppl=6.85, wps=32461, ups=8.98, wpb=3616.2, bsz=146.3, num_updates=244100, lr=6.40053e-05, gnorm=2.363, loss_scale=4, train_wall=11, gb_free=19.3, wall=12130
2022-12-09 23:23:20 | INFO | train_inner | epoch 222:    673 / 1102 loss=6.715, nll_loss=2.816, ppl=7.04, wps=31701.2, ups=9.07, wpb=3494.1, bsz=128.5, num_updates=244200, lr=6.39922e-05, gnorm=2.59, loss_scale=4, train_wall=11, gb_free=19.4, wall=12141
2022-12-09 23:23:31 | INFO | train_inner | epoch 222:    773 / 1102 loss=6.67, nll_loss=2.808, ppl=7, wps=32170.9, ups=8.96, wpb=3589.3, bsz=157.1, num_updates=244300, lr=6.39791e-05, gnorm=2.405, loss_scale=4, train_wall=11, gb_free=19.5, wall=12152
2022-12-09 23:23:43 | INFO | train_inner | epoch 222:    873 / 1102 loss=6.674, nll_loss=2.792, ppl=6.92, wps=31521.9, ups=8.93, wpb=3529.3, bsz=153.9, num_updates=244400, lr=6.3966e-05, gnorm=2.449, loss_scale=4, train_wall=11, gb_free=19.5, wall=12163
2022-12-09 23:23:54 | INFO | train_inner | epoch 222:    973 / 1102 loss=6.703, nll_loss=2.829, ppl=7.11, wps=32400.7, ups=9.09, wpb=3565.3, bsz=146.6, num_updates=244500, lr=6.39529e-05, gnorm=2.401, loss_scale=4, train_wall=11, gb_free=19.4, wall=12174
2022-12-09 23:24:05 | INFO | train_inner | epoch 222:   1073 / 1102 loss=6.729, nll_loss=2.863, ppl=7.28, wps=32516.6, ups=8.86, wpb=3670.4, bsz=150.6, num_updates=244600, lr=6.39399e-05, gnorm=2.4, loss_scale=4, train_wall=11, gb_free=19.3, wall=12186
2022-12-09 23:24:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:24:49 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 3.664 | nll_loss 2.121 | ppl 4.35 | bleu 37.47 | wps 4466.4 | wpb 2835.3 | bsz 115.6 | num_updates 244629 | best_bleu 37.61
2022-12-09 23:24:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 244629 updates
2022-12-09 23:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint222.pt
2022-12-09 23:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint222.pt (epoch 222 @ 244629 updates, score 37.47) (writing took 1.431819273158908 seconds)
2022-12-09 23:24:50 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-12-09 23:24:50 | INFO | train | epoch 222 | loss 6.68 | nll_loss 2.803 | ppl 6.98 | wps 23941.9 | ups 6.68 | wpb 3583.6 | bsz 145.4 | num_updates 244629 | lr 6.39361e-05 | gnorm 2.423 | loss_scale 4 | train_wall 120 | gb_free 19.6 | wall 12231
2022-12-09 23:24:50 | INFO | fairseq.trainer | begin training epoch 223
2022-12-09 23:24:59 | INFO | train_inner | epoch 223:     71 / 1102 loss=6.6, nll_loss=2.752, ppl=6.73, wps=6698.4, ups=1.87, wpb=3590.4, bsz=167.1, num_updates=244700, lr=6.39268e-05, gnorm=2.321, loss_scale=4, train_wall=11, gb_free=19.4, wall=12239
2022-12-09 23:25:10 | INFO | train_inner | epoch 223:    171 / 1102 loss=6.62, nll_loss=2.737, ppl=6.67, wps=32331.7, ups=9.09, wpb=3558, bsz=147.8, num_updates=244800, lr=6.39137e-05, gnorm=2.381, loss_scale=4, train_wall=11, gb_free=19.5, wall=12250
2022-12-09 23:25:21 | INFO | train_inner | epoch 223:    271 / 1102 loss=6.707, nll_loss=2.825, ppl=7.09, wps=32594.6, ups=9.15, wpb=3561.4, bsz=136.6, num_updates=244900, lr=6.39007e-05, gnorm=2.452, loss_scale=4, train_wall=11, gb_free=19.6, wall=12261
2022-12-09 23:25:31 | INFO | train_inner | epoch 223:    371 / 1102 loss=6.694, nll_loss=2.812, ppl=7.02, wps=32419.2, ups=9.12, wpb=3554.9, bsz=142.1, num_updates=245000, lr=6.38877e-05, gnorm=2.428, loss_scale=4, train_wall=11, gb_free=19.2, wall=12272
2022-12-09 23:25:42 | INFO | train_inner | epoch 223:    471 / 1102 loss=6.753, nll_loss=2.861, ppl=7.27, wps=33135.4, ups=9.18, wpb=3607.9, bsz=129.5, num_updates=245100, lr=6.38746e-05, gnorm=2.408, loss_scale=4, train_wall=11, gb_free=19.3, wall=12283
2022-12-09 23:25:53 | INFO | train_inner | epoch 223:    571 / 1102 loss=6.712, nll_loss=2.813, ppl=7.03, wps=32312.8, ups=9.09, wpb=3556.5, bsz=131.5, num_updates=245200, lr=6.38616e-05, gnorm=2.515, loss_scale=4, train_wall=11, gb_free=19.3, wall=12294
2022-12-09 23:26:04 | INFO | train_inner | epoch 223:    671 / 1102 loss=6.768, nll_loss=2.898, ppl=7.45, wps=33374.7, ups=9.1, wpb=3668.3, bsz=138, num_updates=245300, lr=6.38486e-05, gnorm=2.35, loss_scale=4, train_wall=11, gb_free=19.4, wall=12305
2022-12-09 23:26:15 | INFO | train_inner | epoch 223:    771 / 1102 loss=6.71, nll_loss=2.819, ppl=7.05, wps=32400.4, ups=9.17, wpb=3532.8, bsz=132.4, num_updates=245400, lr=6.38356e-05, gnorm=2.458, loss_scale=4, train_wall=11, gb_free=19.6, wall=12316
2022-12-09 23:26:26 | INFO | train_inner | epoch 223:    871 / 1102 loss=6.634, nll_loss=2.762, ppl=6.78, wps=32778.8, ups=9.16, wpb=3578.1, bsz=162, num_updates=245500, lr=6.38226e-05, gnorm=2.429, loss_scale=4, train_wall=11, gb_free=19.5, wall=12327
2022-12-09 23:26:37 | INFO | train_inner | epoch 223:    971 / 1102 loss=6.627, nll_loss=2.775, ppl=6.85, wps=33425, ups=9.2, wpb=3634.7, bsz=164.2, num_updates=245600, lr=6.38096e-05, gnorm=2.403, loss_scale=4, train_wall=11, gb_free=19.3, wall=12338
2022-12-09 23:26:48 | INFO | train_inner | epoch 223:   1071 / 1102 loss=6.64, nll_loss=2.778, ppl=6.86, wps=33283.6, ups=9.16, wpb=3635.1, bsz=155.8, num_updates=245700, lr=6.37966e-05, gnorm=2.308, loss_scale=4, train_wall=11, gb_free=19.6, wall=12349
2022-12-09 23:26:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:27:33 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 3.663 | nll_loss 2.121 | ppl 4.35 | bleu 37.58 | wps 4318.5 | wpb 2835.3 | bsz 115.6 | num_updates 245731 | best_bleu 37.61
2022-12-09 23:27:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 245731 updates
2022-12-09 23:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint223.pt
2022-12-09 23:27:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint223.pt (epoch 223 @ 245731 updates, score 37.58) (writing took 1.385762113146484 seconds)
2022-12-09 23:27:35 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-12-09 23:27:35 | INFO | train | epoch 223 | loss 6.679 | nll_loss 2.802 | ppl 6.97 | wps 24045.9 | ups 6.71 | wpb 3583.6 | bsz 145.4 | num_updates 245731 | lr 6.37926e-05 | gnorm 2.411 | loss_scale 4 | train_wall 118 | gb_free 20 | wall 12395
2022-12-09 23:27:35 | INFO | fairseq.trainer | begin training epoch 224
2022-12-09 23:27:43 | INFO | train_inner | epoch 224:     69 / 1102 loss=6.579, nll_loss=2.726, ppl=6.62, wps=6728.4, ups=1.83, wpb=3681.2, bsz=163.2, num_updates=245800, lr=6.37836e-05, gnorm=2.279, loss_scale=4, train_wall=11, gb_free=19.6, wall=12403
2022-12-09 23:27:54 | INFO | train_inner | epoch 224:    169 / 1102 loss=6.633, nll_loss=2.755, ppl=6.75, wps=32616.3, ups=9.18, wpb=3552.8, bsz=139.2, num_updates=245900, lr=6.37706e-05, gnorm=2.38, loss_scale=4, train_wall=11, gb_free=19.2, wall=12414
2022-12-09 23:28:04 | INFO | train_inner | epoch 224:    269 / 1102 loss=6.748, nll_loss=2.851, ppl=7.21, wps=32834, ups=9.17, wpb=3579.1, bsz=121.8, num_updates=246000, lr=6.37577e-05, gnorm=2.441, loss_scale=4, train_wall=11, gb_free=19.8, wall=12425
2022-12-09 23:28:15 | INFO | train_inner | epoch 224:    369 / 1102 loss=6.656, nll_loss=2.77, ppl=6.82, wps=32412.9, ups=9.13, wpb=3548.8, bsz=147, num_updates=246100, lr=6.37447e-05, gnorm=2.411, loss_scale=4, train_wall=11, gb_free=19.5, wall=12436
2022-12-09 23:28:26 | INFO | train_inner | epoch 224:    469 / 1102 loss=6.632, nll_loss=2.733, ppl=6.65, wps=32196.5, ups=9.16, wpb=3514.6, bsz=137.8, num_updates=246200, lr=6.37318e-05, gnorm=2.513, loss_scale=4, train_wall=11, gb_free=19.6, wall=12447
2022-12-09 23:28:37 | INFO | train_inner | epoch 224:    569 / 1102 loss=6.687, nll_loss=2.816, ppl=7.04, wps=32990.8, ups=9.14, wpb=3609, bsz=145.9, num_updates=246300, lr=6.37188e-05, gnorm=2.354, loss_scale=4, train_wall=11, gb_free=19.4, wall=12458
2022-12-09 23:28:48 | INFO | train_inner | epoch 224:    669 / 1102 loss=6.733, nll_loss=2.845, ppl=7.19, wps=32973.7, ups=9.15, wpb=3604.7, bsz=138.7, num_updates=246400, lr=6.37059e-05, gnorm=2.456, loss_scale=4, train_wall=11, gb_free=19.3, wall=12469
2022-12-09 23:28:59 | INFO | train_inner | epoch 224:    769 / 1102 loss=6.699, nll_loss=2.81, ppl=7.02, wps=32453.7, ups=9.17, wpb=3538.5, bsz=133.9, num_updates=246500, lr=6.3693e-05, gnorm=2.492, loss_scale=4, train_wall=11, gb_free=19.3, wall=12480
2022-12-09 23:29:10 | INFO | train_inner | epoch 224:    869 / 1102 loss=6.745, nll_loss=2.871, ppl=7.32, wps=33183.7, ups=9.17, wpb=3618.5, bsz=144.2, num_updates=246600, lr=6.36801e-05, gnorm=2.387, loss_scale=4, train_wall=11, gb_free=19.8, wall=12491
2022-12-09 23:29:21 | INFO | train_inner | epoch 224:    969 / 1102 loss=6.684, nll_loss=2.822, ppl=7.07, wps=32910.8, ups=9.09, wpb=3620.6, bsz=155.8, num_updates=246700, lr=6.36672e-05, gnorm=2.42, loss_scale=4, train_wall=11, gb_free=19.5, wall=12502
2022-12-09 23:29:32 | INFO | train_inner | epoch 224:   1069 / 1102 loss=6.617, nll_loss=2.773, ppl=6.84, wps=32395.3, ups=9.08, wpb=3567.9, bsz=180.5, num_updates=246800, lr=6.36543e-05, gnorm=2.413, loss_scale=4, train_wall=11, gb_free=19.4, wall=12513
2022-12-09 23:29:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:30:14 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 3.667 | nll_loss 2.124 | ppl 4.36 | bleu 37.38 | wps 4695.7 | wpb 2835.3 | bsz 115.6 | num_updates 246833 | best_bleu 37.61
2022-12-09 23:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 246833 updates
2022-12-09 23:30:15 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint224.pt
2022-12-09 23:30:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint224.pt (epoch 224 @ 246833 updates, score 37.38) (writing took 1.3971890397369862 seconds)
2022-12-09 23:30:15 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-12-09 23:30:15 | INFO | train | epoch 224 | loss 6.676 | nll_loss 2.799 | ppl 6.96 | wps 24553.5 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 246833 | lr 6.365e-05 | gnorm 2.423 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 12556
2022-12-09 23:30:16 | INFO | fairseq.trainer | begin training epoch 225
2022-12-09 23:30:23 | INFO | train_inner | epoch 225:     67 / 1102 loss=6.656, nll_loss=2.772, ppl=6.83, wps=7041.2, ups=1.96, wpb=3591.5, bsz=140.5, num_updates=246900, lr=6.36414e-05, gnorm=2.498, loss_scale=4, train_wall=11, gb_free=20, wall=12564
2022-12-09 23:30:34 | INFO | train_inner | epoch 225:    167 / 1102 loss=6.591, nll_loss=2.729, ppl=6.63, wps=33147.2, ups=9.15, wpb=3621.3, bsz=162.1, num_updates=247000, lr=6.36285e-05, gnorm=2.336, loss_scale=4, train_wall=11, gb_free=19.9, wall=12575
2022-12-09 23:30:45 | INFO | train_inner | epoch 225:    267 / 1102 loss=6.609, nll_loss=2.744, ppl=6.7, wps=32911.9, ups=9.1, wpb=3617.8, bsz=155.8, num_updates=247100, lr=6.36156e-05, gnorm=2.355, loss_scale=4, train_wall=11, gb_free=19.8, wall=12586
2022-12-09 23:30:56 | INFO | train_inner | epoch 225:    367 / 1102 loss=6.652, nll_loss=2.761, ppl=6.78, wps=32197.5, ups=9.17, wpb=3510.1, bsz=139.3, num_updates=247200, lr=6.36027e-05, gnorm=2.508, loss_scale=4, train_wall=11, gb_free=19.5, wall=12596
2022-12-09 23:31:07 | INFO | train_inner | epoch 225:    467 / 1102 loss=6.603, nll_loss=2.72, ppl=6.59, wps=32655.5, ups=9.11, wpb=3583.9, bsz=150.3, num_updates=247300, lr=6.35899e-05, gnorm=2.399, loss_scale=4, train_wall=11, gb_free=19.2, wall=12607
2022-12-09 23:31:18 | INFO | train_inner | epoch 225:    567 / 1102 loss=6.698, nll_loss=2.812, ppl=7.02, wps=32641.5, ups=9.1, wpb=3588.2, bsz=135.4, num_updates=247400, lr=6.3577e-05, gnorm=2.471, loss_scale=4, train_wall=11, gb_free=19.7, wall=12618
2022-12-09 23:31:29 | INFO | train_inner | epoch 225:    667 / 1102 loss=6.754, nll_loss=2.852, ppl=7.22, wps=32378.3, ups=9.12, wpb=3551, bsz=128.5, num_updates=247500, lr=6.35642e-05, gnorm=2.471, loss_scale=4, train_wall=11, gb_free=19.7, wall=12629
2022-12-09 23:31:40 | INFO | train_inner | epoch 225:    767 / 1102 loss=6.688, nll_loss=2.799, ppl=6.96, wps=32654.3, ups=9.2, wpb=3549.3, bsz=139.7, num_updates=247600, lr=6.35513e-05, gnorm=2.46, loss_scale=4, train_wall=11, gb_free=19.6, wall=12640
2022-12-09 23:31:51 | INFO | train_inner | epoch 225:    867 / 1102 loss=6.772, nll_loss=2.893, ppl=7.43, wps=32632.7, ups=9.06, wpb=3602.2, bsz=139, num_updates=247700, lr=6.35385e-05, gnorm=2.526, loss_scale=4, train_wall=11, gb_free=19.8, wall=12651
2022-12-09 23:32:02 | INFO | train_inner | epoch 225:    967 / 1102 loss=6.654, nll_loss=2.795, ppl=6.94, wps=32891.8, ups=9.15, wpb=3595.3, bsz=153.9, num_updates=247800, lr=6.35257e-05, gnorm=2.324, loss_scale=4, train_wall=11, gb_free=19.5, wall=12662
2022-12-09 23:32:13 | INFO | train_inner | epoch 225:   1067 / 1102 loss=6.734, nll_loss=2.871, ppl=7.32, wps=32436, ups=9.09, wpb=3567.2, bsz=151, num_updates=247900, lr=6.35129e-05, gnorm=2.374, loss_scale=4, train_wall=11, gb_free=19.5, wall=12673
2022-12-09 23:32:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:32:56 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 3.662 | nll_loss 2.122 | ppl 4.35 | bleu 37.56 | wps 4549.7 | wpb 2835.3 | bsz 115.6 | num_updates 247935 | best_bleu 37.61
2022-12-09 23:32:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 247935 updates
2022-12-09 23:32:57 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint225.pt
2022-12-09 23:32:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint225.pt (epoch 225 @ 247935 updates, score 37.56) (writing took 1.3299076203256845 seconds)
2022-12-09 23:32:58 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-12-09 23:32:58 | INFO | train | epoch 225 | loss 6.676 | nll_loss 2.798 | ppl 6.96 | wps 24358.4 | ups 6.8 | wpb 3583.6 | bsz 145.4 | num_updates 247935 | lr 6.35084e-05 | gnorm 2.419 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 12718
2022-12-09 23:32:58 | INFO | fairseq.trainer | begin training epoch 226
2022-12-09 23:33:05 | INFO | train_inner | epoch 226:     65 / 1102 loss=6.706, nll_loss=2.82, ppl=7.06, wps=6827.9, ups=1.91, wpb=3575.6, bsz=133.7, num_updates=248000, lr=6.35001e-05, gnorm=2.396, loss_scale=4, train_wall=11, gb_free=19.4, wall=12726
2022-12-09 23:33:16 | INFO | train_inner | epoch 226:    165 / 1102 loss=6.638, nll_loss=2.744, ppl=6.7, wps=32809.4, ups=9.25, wpb=3546.4, bsz=139.1, num_updates=248100, lr=6.34873e-05, gnorm=2.422, loss_scale=4, train_wall=11, gb_free=19.3, wall=12736
2022-12-09 23:33:27 | INFO | train_inner | epoch 226:    265 / 1102 loss=6.558, nll_loss=2.708, ppl=6.53, wps=32710.8, ups=9.13, wpb=3581.4, bsz=171.3, num_updates=248200, lr=6.34745e-05, gnorm=2.229, loss_scale=4, train_wall=11, gb_free=19.4, wall=12747
2022-12-09 23:33:38 | INFO | train_inner | epoch 226:    365 / 1102 loss=6.617, nll_loss=2.743, ppl=6.69, wps=32831.9, ups=9.21, wpb=3564.1, bsz=156.4, num_updates=248300, lr=6.34617e-05, gnorm=2.415, loss_scale=4, train_wall=11, gb_free=19.8, wall=12758
2022-12-09 23:33:48 | INFO | train_inner | epoch 226:    465 / 1102 loss=6.612, nll_loss=2.748, ppl=6.72, wps=33198.4, ups=9.35, wpb=3552, bsz=164.1, num_updates=248400, lr=6.34489e-05, gnorm=2.517, loss_scale=4, train_wall=10, gb_free=19.4, wall=12769
2022-12-09 23:33:59 | INFO | train_inner | epoch 226:    565 / 1102 loss=6.696, nll_loss=2.815, ppl=7.04, wps=33820.9, ups=9.24, wpb=3659.1, bsz=133.8, num_updates=248500, lr=6.34361e-05, gnorm=2.368, loss_scale=4, train_wall=11, gb_free=19.4, wall=12780
2022-12-09 23:34:10 | INFO | train_inner | epoch 226:    665 / 1102 loss=6.724, nll_loss=2.824, ppl=7.08, wps=33123.9, ups=9.35, wpb=3541.1, bsz=128, num_updates=248600, lr=6.34234e-05, gnorm=2.498, loss_scale=4, train_wall=10, gb_free=19.6, wall=12790
2022-12-09 23:34:21 | INFO | train_inner | epoch 226:    765 / 1102 loss=6.72, nll_loss=2.826, ppl=7.09, wps=33042.3, ups=9.23, wpb=3580.6, bsz=131, num_updates=248700, lr=6.34106e-05, gnorm=2.453, loss_scale=4, train_wall=11, gb_free=19.7, wall=12801
2022-12-09 23:34:31 | INFO | train_inner | epoch 226:    865 / 1102 loss=6.63, nll_loss=2.762, ppl=6.79, wps=33214.4, ups=9.25, wpb=3592.6, bsz=158.1, num_updates=248800, lr=6.33979e-05, gnorm=2.419, loss_scale=4, train_wall=11, gb_free=19.4, wall=12812
2022-12-09 23:34:42 | INFO | train_inner | epoch 226:    965 / 1102 loss=6.754, nll_loss=2.885, ppl=7.39, wps=33754.2, ups=9.35, wpb=3610.6, bsz=144.6, num_updates=248900, lr=6.33852e-05, gnorm=2.361, loss_scale=4, train_wall=10, gb_free=19.7, wall=12823
2022-12-09 23:34:53 | INFO | train_inner | epoch 226:   1065 / 1102 loss=6.772, nll_loss=2.894, ppl=7.43, wps=33863.4, ups=9.34, wpb=3623.8, bsz=136.2, num_updates=249000, lr=6.33724e-05, gnorm=2.433, loss_scale=4, train_wall=10, gb_free=19.6, wall=12833
2022-12-09 23:34:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:35:35 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 3.662 | nll_loss 2.12 | ppl 4.35 | bleu 37.41 | wps 4710.7 | wpb 2835.3 | bsz 115.6 | num_updates 249037 | best_bleu 37.61
2022-12-09 23:35:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 249037 updates
2022-12-09 23:35:36 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint226.pt
2022-12-09 23:35:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint226.pt (epoch 226 @ 249037 updates, score 37.41) (writing took 1.3529014363884926 seconds)
2022-12-09 23:35:36 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-12-09 23:35:36 | INFO | train | epoch 226 | loss 6.674 | nll_loss 2.796 | ppl 6.95 | wps 24870.8 | ups 6.94 | wpb 3583.6 | bsz 145.4 | num_updates 249037 | lr 6.33677e-05 | gnorm 2.413 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 12877
2022-12-09 23:35:36 | INFO | fairseq.trainer | begin training epoch 227
2022-12-09 23:35:44 | INFO | train_inner | epoch 227:     63 / 1102 loss=6.59, nll_loss=2.737, ppl=6.67, wps=7147, ups=1.97, wpb=3619.8, bsz=169, num_updates=249100, lr=6.33597e-05, gnorm=2.307, loss_scale=4, train_wall=11, gb_free=19.3, wall=12884
2022-12-09 23:35:54 | INFO | train_inner | epoch 227:    163 / 1102 loss=6.663, nll_loss=2.773, ppl=6.84, wps=33547.4, ups=9.46, wpb=3547.6, bsz=137.2, num_updates=249200, lr=6.3347e-05, gnorm=2.399, loss_scale=4, train_wall=10, gb_free=19.4, wall=12895
2022-12-09 23:36:05 | INFO | train_inner | epoch 227:    263 / 1102 loss=6.608, nll_loss=2.715, ppl=6.57, wps=33199.1, ups=9.33, wpb=3559.9, bsz=140.4, num_updates=249300, lr=6.33343e-05, gnorm=2.376, loss_scale=4, train_wall=10, gb_free=19.4, wall=12905
2022-12-09 23:36:16 | INFO | train_inner | epoch 227:    363 / 1102 loss=6.604, nll_loss=2.738, ppl=6.67, wps=32861.1, ups=9.23, wpb=3560.7, bsz=159.4, num_updates=249400, lr=6.33216e-05, gnorm=2.377, loss_scale=4, train_wall=11, gb_free=19.7, wall=12916
2022-12-09 23:36:27 | INFO | train_inner | epoch 227:    463 / 1102 loss=6.662, nll_loss=2.76, ppl=6.77, wps=32192.5, ups=9.2, wpb=3499.1, bsz=137.7, num_updates=249500, lr=6.33089e-05, gnorm=2.482, loss_scale=4, train_wall=11, gb_free=19.5, wall=12927
2022-12-09 23:36:37 | INFO | train_inner | epoch 227:    563 / 1102 loss=6.722, nll_loss=2.846, ppl=7.19, wps=32658.4, ups=9.13, wpb=3575.2, bsz=141.9, num_updates=249600, lr=6.32962e-05, gnorm=2.44, loss_scale=4, train_wall=11, gb_free=19.4, wall=12938
2022-12-09 23:36:48 | INFO | train_inner | epoch 227:    663 / 1102 loss=6.698, nll_loss=2.817, ppl=7.05, wps=32557.6, ups=9.12, wpb=3571.2, bsz=142.4, num_updates=249700, lr=6.32835e-05, gnorm=2.458, loss_scale=4, train_wall=11, gb_free=19.8, wall=12949
2022-12-09 23:36:59 | INFO | train_inner | epoch 227:    763 / 1102 loss=6.688, nll_loss=2.804, ppl=6.99, wps=32697.3, ups=9.2, wpb=3553.7, bsz=142.9, num_updates=249800, lr=6.32709e-05, gnorm=2.544, loss_scale=4, train_wall=11, gb_free=19.8, wall=12960
2022-12-09 23:37:10 | INFO | train_inner | epoch 227:    863 / 1102 loss=6.686, nll_loss=2.815, ppl=7.04, wps=33542.4, ups=9.21, wpb=3642.4, bsz=145.8, num_updates=249900, lr=6.32582e-05, gnorm=2.347, loss_scale=4, train_wall=11, gb_free=19.6, wall=12971
2022-12-09 23:37:21 | INFO | train_inner | epoch 227:    963 / 1102 loss=6.714, nll_loss=2.855, ppl=7.24, wps=33479.1, ups=9.08, wpb=3687.5, bsz=150.2, num_updates=250000, lr=6.32456e-05, gnorm=2.321, loss_scale=4, train_wall=11, gb_free=19.4, wall=12982
2022-12-09 23:37:32 | INFO | train_inner | epoch 227:   1063 / 1102 loss=6.769, nll_loss=2.882, ppl=7.37, wps=33293.1, ups=9.23, wpb=3605.9, bsz=134.3, num_updates=250100, lr=6.32329e-05, gnorm=2.471, loss_scale=4, train_wall=11, gb_free=19.3, wall=12993
2022-12-09 23:37:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:38:17 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 3.659 | nll_loss 2.121 | ppl 4.35 | bleu 37.55 | wps 4484 | wpb 2835.3 | bsz 115.6 | num_updates 250139 | best_bleu 37.61
2022-12-09 23:38:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 250139 updates
2022-12-09 23:38:18 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint227.pt
2022-12-09 23:38:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint227.pt (epoch 227 @ 250139 updates, score 37.55) (writing took 1.3503242079168558 seconds)
2022-12-09 23:38:18 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-12-09 23:38:18 | INFO | train | epoch 227 | loss 6.673 | nll_loss 2.795 | ppl 6.94 | wps 24453.5 | ups 6.82 | wpb 3583.6 | bsz 145.4 | num_updates 250139 | lr 6.3228e-05 | gnorm 2.41 | loss_scale 4 | train_wall 117 | gb_free 19.4 | wall 13038
2022-12-09 23:38:18 | INFO | fairseq.trainer | begin training epoch 228
2022-12-09 23:38:25 | INFO | train_inner | epoch 228:     61 / 1102 loss=6.624, nll_loss=2.745, ppl=6.7, wps=6813.2, ups=1.9, wpb=3593.1, bsz=144.6, num_updates=250200, lr=6.32203e-05, gnorm=2.359, loss_scale=4, train_wall=11, gb_free=19.4, wall=13045
2022-12-09 23:38:36 | INFO | train_inner | epoch 228:    161 / 1102 loss=6.637, nll_loss=2.773, ppl=6.83, wps=33688.4, ups=9.18, wpb=3670.7, bsz=151.6, num_updates=250300, lr=6.32076e-05, gnorm=2.287, loss_scale=4, train_wall=11, gb_free=20, wall=13056
2022-12-09 23:38:47 | INFO | train_inner | epoch 228:    261 / 1102 loss=6.585, nll_loss=2.716, ppl=6.57, wps=32434.6, ups=9.21, wpb=3520.8, bsz=159.2, num_updates=250400, lr=6.3195e-05, gnorm=2.422, loss_scale=4, train_wall=11, gb_free=19.3, wall=13067
2022-12-09 23:38:57 | INFO | train_inner | epoch 228:    361 / 1102 loss=6.632, nll_loss=2.766, ppl=6.8, wps=32984.3, ups=9.21, wpb=3580.3, bsz=153.9, num_updates=250500, lr=6.31824e-05, gnorm=2.408, loss_scale=4, train_wall=11, gb_free=19.4, wall=13078
2022-12-09 23:39:08 | INFO | train_inner | epoch 228:    461 / 1102 loss=6.658, nll_loss=2.757, ppl=6.76, wps=32489.1, ups=9.41, wpb=3453.5, bsz=136.3, num_updates=250600, lr=6.31698e-05, gnorm=2.537, loss_scale=4, train_wall=10, gb_free=19.6, wall=13089
2022-12-09 23:39:19 | INFO | train_inner | epoch 228:    561 / 1102 loss=6.696, nll_loss=2.812, ppl=7.02, wps=33220.1, ups=9.27, wpb=3585.2, bsz=138.2, num_updates=250700, lr=6.31572e-05, gnorm=2.436, loss_scale=4, train_wall=11, gb_free=19.6, wall=13099
2022-12-09 23:39:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-12-09 23:39:30 | INFO | train_inner | epoch 228:    662 / 1102 loss=6.707, nll_loss=2.822, ppl=7.07, wps=32316.6, ups=9.24, wpb=3496.4, bsz=142.5, num_updates=250800, lr=6.31446e-05, gnorm=2.566, loss_scale=2, train_wall=11, gb_free=19.4, wall=13110
2022-12-09 23:39:40 | INFO | train_inner | epoch 228:    762 / 1102 loss=6.778, nll_loss=2.892, ppl=7.42, wps=33065.6, ups=9.19, wpb=3596.2, bsz=134.6, num_updates=250900, lr=6.3132e-05, gnorm=2.4, loss_scale=2, train_wall=11, gb_free=19.5, wall=13121
2022-12-09 23:39:51 | INFO | train_inner | epoch 228:    862 / 1102 loss=6.712, nll_loss=2.83, ppl=7.11, wps=33195.1, ups=9.16, wpb=3624.6, bsz=141.2, num_updates=251000, lr=6.31194e-05, gnorm=2.565, loss_scale=2, train_wall=11, gb_free=19.6, wall=13132
2022-12-09 23:40:02 | INFO | train_inner | epoch 228:    962 / 1102 loss=6.662, nll_loss=2.786, ppl=6.9, wps=33173.9, ups=9.18, wpb=3614.4, bsz=148.4, num_updates=251100, lr=6.31069e-05, gnorm=2.456, loss_scale=2, train_wall=11, gb_free=19.4, wall=13143
2022-12-09 23:40:13 | INFO | train_inner | epoch 228:   1062 / 1102 loss=6.667, nll_loss=2.791, ppl=6.92, wps=33140.5, ups=9.09, wpb=3647.7, bsz=149.7, num_updates=251200, lr=6.30943e-05, gnorm=2.379, loss_scale=2, train_wall=11, gb_free=19.3, wall=13154
2022-12-09 23:40:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:40:59 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 3.663 | nll_loss 2.122 | ppl 4.35 | bleu 37.61 | wps 4322.1 | wpb 2835.3 | bsz 115.6 | num_updates 251240 | best_bleu 37.61
2022-12-09 23:40:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 251240 updates
2022-12-09 23:41:00 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint228.pt
2022-12-09 23:41:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint228.pt (epoch 228 @ 251240 updates, score 37.61) (writing took 1.768017834983766 seconds)
2022-12-09 23:41:01 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-12-09 23:41:01 | INFO | train | epoch 228 | loss 6.67 | nll_loss 2.791 | ppl 6.92 | wps 24158.6 | ups 6.74 | wpb 3583.8 | bsz 145.5 | num_updates 251240 | lr 6.30893e-05 | gnorm 2.436 | loss_scale 2 | train_wall 117 | gb_free 19.6 | wall 13202
2022-12-09 23:41:01 | INFO | fairseq.trainer | begin training epoch 229
2022-12-09 23:41:08 | INFO | train_inner | epoch 229:     60 / 1102 loss=6.66, nll_loss=2.782, ppl=6.88, wps=6569.4, ups=1.83, wpb=3592.6, bsz=144.3, num_updates=251300, lr=6.30818e-05, gnorm=2.41, loss_scale=2, train_wall=11, gb_free=19.6, wall=13209
2022-12-09 23:41:19 | INFO | train_inner | epoch 229:    160 / 1102 loss=6.624, nll_loss=2.746, ppl=6.71, wps=32979.9, ups=9.22, wpb=3578.6, bsz=140.5, num_updates=251400, lr=6.30692e-05, gnorm=2.543, loss_scale=2, train_wall=11, gb_free=19.5, wall=13219
2022-12-09 23:41:30 | INFO | train_inner | epoch 229:    260 / 1102 loss=6.61, nll_loss=2.716, ppl=6.57, wps=32856.2, ups=9.26, wpb=3549.7, bsz=136.8, num_updates=251500, lr=6.30567e-05, gnorm=2.488, loss_scale=2, train_wall=11, gb_free=19.7, wall=13230
2022-12-09 23:41:41 | INFO | train_inner | epoch 229:    360 / 1102 loss=6.612, nll_loss=2.725, ppl=6.61, wps=32224.6, ups=9.03, wpb=3567.8, bsz=144.7, num_updates=251600, lr=6.30441e-05, gnorm=2.447, loss_scale=2, train_wall=11, gb_free=19.3, wall=13241
2022-12-09 23:41:52 | INFO | train_inner | epoch 229:    460 / 1102 loss=6.706, nll_loss=2.832, ppl=7.12, wps=33215.4, ups=9.19, wpb=3615.1, bsz=143.5, num_updates=251700, lr=6.30316e-05, gnorm=2.375, loss_scale=2, train_wall=11, gb_free=19.5, wall=13252
2022-12-09 23:42:03 | INFO | train_inner | epoch 229:    560 / 1102 loss=6.658, nll_loss=2.764, ppl=6.79, wps=32220, ups=9.06, wpb=3556.9, bsz=134.6, num_updates=251800, lr=6.30191e-05, gnorm=2.443, loss_scale=2, train_wall=11, gb_free=19.5, wall=13263
2022-12-09 23:42:14 | INFO | train_inner | epoch 229:    660 / 1102 loss=6.654, nll_loss=2.777, ppl=6.85, wps=32779, ups=9.17, wpb=3573.6, bsz=146.4, num_updates=251900, lr=6.30066e-05, gnorm=2.486, loss_scale=2, train_wall=11, gb_free=19.3, wall=13274
2022-12-09 23:42:24 | INFO | train_inner | epoch 229:    760 / 1102 loss=6.691, nll_loss=2.805, ppl=6.99, wps=32558.6, ups=9.24, wpb=3525.3, bsz=139.6, num_updates=252000, lr=6.29941e-05, gnorm=2.46, loss_scale=2, train_wall=11, gb_free=19.5, wall=13285
2022-12-09 23:42:35 | INFO | train_inner | epoch 229:    860 / 1102 loss=6.755, nll_loss=2.895, ppl=7.44, wps=33109, ups=9.03, wpb=3667.6, bsz=149.2, num_updates=252100, lr=6.29816e-05, gnorm=2.353, loss_scale=2, train_wall=11, gb_free=19.4, wall=13296
2022-12-09 23:42:47 | INFO | train_inner | epoch 229:    960 / 1102 loss=6.748, nll_loss=2.869, ppl=7.31, wps=31679, ups=8.9, wpb=3557.8, bsz=141.6, num_updates=252200, lr=6.29691e-05, gnorm=2.527, loss_scale=2, train_wall=11, gb_free=19.8, wall=13307
2022-12-09 23:42:58 | INFO | train_inner | epoch 229:   1060 / 1102 loss=6.647, nll_loss=2.791, ppl=6.92, wps=32205.7, ups=8.84, wpb=3641.3, bsz=170.7, num_updates=252300, lr=6.29566e-05, gnorm=2.432, loss_scale=2, train_wall=11, gb_free=19.2, wall=13319
2022-12-09 23:43:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:43:43 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 3.661 | nll_loss 2.122 | ppl 4.35 | bleu 37.63 | wps 4521.8 | wpb 2835.3 | bsz 115.6 | num_updates 252342 | best_bleu 37.63
2022-12-09 23:43:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 252342 updates
2022-12-09 23:43:44 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint229.pt
2022-12-09 23:43:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint229.pt (epoch 229 @ 252342 updates, score 37.63) (writing took 1.817002703435719 seconds)
2022-12-09 23:43:45 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-12-09 23:43:45 | INFO | train | epoch 229 | loss 6.668 | nll_loss 2.791 | ppl 6.92 | wps 24158.2 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 252342 | lr 6.29514e-05 | gnorm 2.446 | loss_scale 2 | train_wall 118 | gb_free 19.5 | wall 13365
2022-12-09 23:43:45 | INFO | fairseq.trainer | begin training epoch 230
2022-12-09 23:43:51 | INFO | train_inner | epoch 230:     58 / 1102 loss=6.625, nll_loss=2.769, ppl=6.81, wps=6815.4, ups=1.87, wpb=3644, bsz=157.2, num_updates=252400, lr=6.29441e-05, gnorm=2.278, loss_scale=2, train_wall=11, gb_free=19.9, wall=13372
2022-12-09 23:44:03 | INFO | train_inner | epoch 230:    158 / 1102 loss=6.615, nll_loss=2.725, ppl=6.61, wps=32214.7, ups=9.06, wpb=3557.4, bsz=139.3, num_updates=252500, lr=6.29317e-05, gnorm=2.47, loss_scale=2, train_wall=11, gb_free=19.8, wall=13383
2022-12-09 23:44:14 | INFO | train_inner | epoch 230:    258 / 1102 loss=6.673, nll_loss=2.793, ppl=6.93, wps=32309.6, ups=9, wpb=3591.9, bsz=144.2, num_updates=252600, lr=6.29192e-05, gnorm=2.478, loss_scale=2, train_wall=11, gb_free=19.4, wall=13394
2022-12-09 23:44:25 | INFO | train_inner | epoch 230:    358 / 1102 loss=6.635, nll_loss=2.758, ppl=6.76, wps=31868.1, ups=8.9, wpb=3579.4, bsz=153.8, num_updates=252700, lr=6.29068e-05, gnorm=2.461, loss_scale=2, train_wall=11, gb_free=19.9, wall=13405
2022-12-09 23:44:35 | INFO | train_inner | epoch 230:    458 / 1102 loss=6.702, nll_loss=2.808, ppl=7, wps=33624.4, ups=9.44, wpb=3561.9, bsz=130.2, num_updates=252800, lr=6.28943e-05, gnorm=2.441, loss_scale=2, train_wall=10, gb_free=19.6, wall=13416
2022-12-09 23:44:46 | INFO | train_inner | epoch 230:    558 / 1102 loss=6.697, nll_loss=2.808, ppl=7, wps=33264.7, ups=9.3, wpb=3575.3, bsz=142.8, num_updates=252900, lr=6.28819e-05, gnorm=2.444, loss_scale=2, train_wall=11, gb_free=20, wall=13427
2022-12-09 23:44:57 | INFO | train_inner | epoch 230:    658 / 1102 loss=6.726, nll_loss=2.825, ppl=7.08, wps=33461.4, ups=9.35, wpb=3577.2, bsz=130.3, num_updates=253000, lr=6.28695e-05, gnorm=2.501, loss_scale=2, train_wall=10, gb_free=19.4, wall=13437
2022-12-09 23:45:08 | INFO | train_inner | epoch 230:    758 / 1102 loss=6.604, nll_loss=2.733, ppl=6.65, wps=32752.3, ups=9.22, wpb=3550.7, bsz=158.2, num_updates=253100, lr=6.2857e-05, gnorm=2.671, loss_scale=2, train_wall=11, gb_free=19.6, wall=13448
2022-12-09 23:45:19 | INFO | train_inner | epoch 230:    858 / 1102 loss=6.646, nll_loss=2.776, ppl=6.85, wps=32791.9, ups=9.2, wpb=3563.2, bsz=156.7, num_updates=253200, lr=6.28446e-05, gnorm=2.475, loss_scale=2, train_wall=11, gb_free=19.3, wall=13459
2022-12-09 23:45:30 | INFO | train_inner | epoch 230:    958 / 1102 loss=6.706, nll_loss=2.851, ppl=7.22, wps=33372.7, ups=9.05, wpb=3688, bsz=155.1, num_updates=253300, lr=6.28322e-05, gnorm=2.347, loss_scale=2, train_wall=11, gb_free=19.4, wall=13470
2022-12-09 23:45:41 | INFO | train_inner | epoch 230:   1058 / 1102 loss=6.7, nll_loss=2.824, ppl=7.08, wps=32717.9, ups=9.2, wpb=3557.9, bsz=148.5, num_updates=253400, lr=6.28198e-05, gnorm=2.413, loss_scale=2, train_wall=11, gb_free=19.3, wall=13481
2022-12-09 23:45:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:46:26 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 3.661 | nll_loss 2.119 | ppl 4.34 | bleu 37.6 | wps 4439.6 | wpb 2835.3 | bsz 115.6 | num_updates 253444 | best_bleu 37.63
2022-12-09 23:46:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 253444 updates
2022-12-09 23:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint230.pt
2022-12-09 23:46:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint230.pt (epoch 230 @ 253444 updates, score 37.6) (writing took 1.4090611850842834 seconds)
2022-12-09 23:46:27 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-12-09 23:46:27 | INFO | train | epoch 230 | loss 6.668 | nll_loss 2.789 | ppl 6.91 | wps 24294.7 | ups 6.78 | wpb 3583.6 | bsz 145.4 | num_updates 253444 | lr 6.28144e-05 | gnorm 2.458 | loss_scale 2 | train_wall 117 | gb_free 19.7 | wall 13528
2022-12-09 23:46:27 | INFO | fairseq.trainer | begin training epoch 231
2022-12-09 23:46:34 | INFO | train_inner | epoch 231:     56 / 1102 loss=6.645, nll_loss=2.758, ppl=6.76, wps=6652.4, ups=1.88, wpb=3538.3, bsz=139.7, num_updates=253500, lr=6.28074e-05, gnorm=2.469, loss_scale=2, train_wall=11, gb_free=19.3, wall=13534
2022-12-09 23:46:45 | INFO | train_inner | epoch 231:    156 / 1102 loss=6.638, nll_loss=2.76, ppl=6.77, wps=32912.1, ups=9.17, wpb=3589.8, bsz=143, num_updates=253600, lr=6.2795e-05, gnorm=2.342, loss_scale=2, train_wall=11, gb_free=19.8, wall=13545
2022-12-09 23:46:55 | INFO | train_inner | epoch 231:    256 / 1102 loss=6.662, nll_loss=2.778, ppl=6.86, wps=32588.9, ups=9.22, wpb=3535.3, bsz=141, num_updates=253700, lr=6.27827e-05, gnorm=2.404, loss_scale=2, train_wall=11, gb_free=19.5, wall=13556
2022-12-09 23:47:06 | INFO | train_inner | epoch 231:    356 / 1102 loss=6.729, nll_loss=2.831, ppl=7.11, wps=32473.7, ups=9.33, wpb=3482.4, bsz=128, num_updates=253800, lr=6.27703e-05, gnorm=2.561, loss_scale=2, train_wall=10, gb_free=20, wall=13567
2022-12-09 23:47:17 | INFO | train_inner | epoch 231:    456 / 1102 loss=6.632, nll_loss=2.749, ppl=6.72, wps=32990.1, ups=9.22, wpb=3579.1, bsz=144, num_updates=253900, lr=6.27579e-05, gnorm=2.378, loss_scale=2, train_wall=11, gb_free=19.5, wall=13578
2022-12-09 23:47:28 | INFO | train_inner | epoch 231:    556 / 1102 loss=6.725, nll_loss=2.846, ppl=7.19, wps=33404.8, ups=9.17, wpb=3642.2, bsz=136.6, num_updates=254000, lr=6.27456e-05, gnorm=2.359, loss_scale=2, train_wall=11, gb_free=19.4, wall=13589
2022-12-09 23:47:39 | INFO | train_inner | epoch 231:    656 / 1102 loss=6.635, nll_loss=2.749, ppl=6.72, wps=32895.2, ups=9.16, wpb=3590.2, bsz=149.1, num_updates=254100, lr=6.27332e-05, gnorm=2.51, loss_scale=2, train_wall=11, gb_free=19.7, wall=13599
2022-12-09 23:47:50 | INFO | train_inner | epoch 231:    756 / 1102 loss=6.674, nll_loss=2.793, ppl=6.93, wps=33055.3, ups=9.19, wpb=3597.1, bsz=145.8, num_updates=254200, lr=6.27209e-05, gnorm=2.383, loss_scale=2, train_wall=11, gb_free=19.9, wall=13610
2022-12-09 23:48:01 | INFO | train_inner | epoch 231:    856 / 1102 loss=6.706, nll_loss=2.828, ppl=7.1, wps=32979.3, ups=9.23, wpb=3572.8, bsz=146.4, num_updates=254300, lr=6.27086e-05, gnorm=2.387, loss_scale=2, train_wall=11, gb_free=19.6, wall=13621
2022-12-09 23:48:11 | INFO | train_inner | epoch 231:    956 / 1102 loss=6.686, nll_loss=2.825, ppl=7.08, wps=33546.3, ups=9.22, wpb=3638.2, bsz=159.8, num_updates=254400, lr=6.26962e-05, gnorm=2.394, loss_scale=2, train_wall=11, gb_free=19.4, wall=13632
2022-12-09 23:48:22 | INFO | train_inner | epoch 231:   1056 / 1102 loss=6.632, nll_loss=2.761, ppl=6.78, wps=33477.3, ups=9.19, wpb=3641.4, bsz=154.1, num_updates=254500, lr=6.26839e-05, gnorm=2.42, loss_scale=2, train_wall=11, gb_free=19.4, wall=13643
2022-12-09 23:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:49:06 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 3.669 | nll_loss 2.126 | ppl 4.37 | bleu 37.57 | wps 4721.4 | wpb 2835.3 | bsz 115.6 | num_updates 254546 | best_bleu 37.63
2022-12-09 23:49:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 254546 updates
2022-12-09 23:49:07 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint231.pt
2022-12-09 23:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint231.pt (epoch 231 @ 254546 updates, score 37.57) (writing took 1.362762431614101 seconds)
2022-12-09 23:49:07 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-12-09 23:49:07 | INFO | train | epoch 231 | loss 6.666 | nll_loss 2.786 | ppl 6.9 | wps 24730.5 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 254546 | lr 6.26782e-05 | gnorm 2.418 | loss_scale 2 | train_wall 117 | gb_free 19.3 | wall 13688
2022-12-09 23:49:07 | INFO | fairseq.trainer | begin training epoch 232
2022-12-09 23:49:13 | INFO | train_inner | epoch 232:     54 / 1102 loss=6.671, nll_loss=2.802, ppl=6.97, wps=7113.1, ups=1.97, wpb=3606.9, bsz=143.1, num_updates=254600, lr=6.26716e-05, gnorm=2.354, loss_scale=2, train_wall=10, gb_free=19.4, wall=13694
2022-12-09 23:49:24 | INFO | train_inner | epoch 232:    154 / 1102 loss=6.613, nll_loss=2.726, ppl=6.62, wps=32663.3, ups=9.24, wpb=3536.6, bsz=143.8, num_updates=254700, lr=6.26593e-05, gnorm=2.425, loss_scale=2, train_wall=11, gb_free=19.8, wall=13704
2022-12-09 23:49:35 | INFO | train_inner | epoch 232:    254 / 1102 loss=6.629, nll_loss=2.748, ppl=6.72, wps=33298, ups=9.3, wpb=3580.4, bsz=145, num_updates=254800, lr=6.2647e-05, gnorm=2.399, loss_scale=2, train_wall=11, gb_free=19.8, wall=13715
2022-12-09 23:49:45 | INFO | train_inner | epoch 232:    354 / 1102 loss=6.672, nll_loss=2.774, ppl=6.84, wps=33517.8, ups=9.3, wpb=3602.8, bsz=134.4, num_updates=254900, lr=6.26347e-05, gnorm=2.49, loss_scale=2, train_wall=11, gb_free=19.3, wall=13726
2022-12-09 23:49:56 | INFO | train_inner | epoch 232:    454 / 1102 loss=6.719, nll_loss=2.805, ppl=6.99, wps=32709.9, ups=9.35, wpb=3499.7, bsz=118.8, num_updates=255000, lr=6.26224e-05, gnorm=2.573, loss_scale=2, train_wall=10, gb_free=19.3, wall=13737
2022-12-09 23:50:07 | INFO | train_inner | epoch 232:    554 / 1102 loss=6.588, nll_loss=2.725, ppl=6.61, wps=33099.7, ups=9.2, wpb=3599.7, bsz=157.7, num_updates=255100, lr=6.26102e-05, gnorm=2.467, loss_scale=2, train_wall=11, gb_free=19.8, wall=13748
2022-12-09 23:50:18 | INFO | train_inner | epoch 232:    654 / 1102 loss=6.643, nll_loss=2.786, ppl=6.9, wps=33354.1, ups=9.13, wpb=3652.3, bsz=168.6, num_updates=255200, lr=6.25979e-05, gnorm=2.333, loss_scale=2, train_wall=11, gb_free=19.6, wall=13758
2022-12-09 23:50:29 | INFO | train_inner | epoch 232:    754 / 1102 loss=6.671, nll_loss=2.795, ppl=6.94, wps=33006.2, ups=9.15, wpb=3606.9, bsz=141.4, num_updates=255300, lr=6.25856e-05, gnorm=2.356, loss_scale=2, train_wall=11, gb_free=19.3, wall=13769
2022-12-09 23:50:40 | INFO | train_inner | epoch 232:    854 / 1102 loss=6.719, nll_loss=2.827, ppl=7.1, wps=32941.9, ups=9.17, wpb=3591.7, bsz=136.2, num_updates=255400, lr=6.25734e-05, gnorm=2.51, loss_scale=2, train_wall=11, gb_free=19.8, wall=13780
2022-12-09 23:50:51 | INFO | train_inner | epoch 232:    954 / 1102 loss=6.684, nll_loss=2.83, ppl=7.11, wps=33029.8, ups=9.16, wpb=3605.9, bsz=163, num_updates=255500, lr=6.25611e-05, gnorm=2.322, loss_scale=2, train_wall=11, gb_free=19.5, wall=13791
2022-12-09 23:51:02 | INFO | train_inner | epoch 232:   1054 / 1102 loss=6.669, nll_loss=2.802, ppl=6.98, wps=32571.1, ups=9.04, wpb=3603.4, bsz=158.1, num_updates=255600, lr=6.25489e-05, gnorm=2.379, loss_scale=2, train_wall=11, gb_free=19.3, wall=13802
2022-12-09 23:51:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:51:45 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 3.666 | nll_loss 2.122 | ppl 4.35 | bleu 37.55 | wps 4793.8 | wpb 2835.3 | bsz 115.6 | num_updates 255648 | best_bleu 37.63
2022-12-09 23:51:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 255648 updates
2022-12-09 23:51:46 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint232.pt
2022-12-09 23:51:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint232.pt (epoch 232 @ 255648 updates, score 37.55) (writing took 1.473480125889182 seconds)
2022-12-09 23:51:46 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-12-09 23:51:46 | INFO | train | epoch 232 | loss 6.664 | nll_loss 2.785 | ppl 6.89 | wps 24809.7 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 255648 | lr 6.2543e-05 | gnorm 2.423 | loss_scale 2 | train_wall 117 | gb_free 19.3 | wall 13847
2022-12-09 23:51:46 | INFO | fairseq.trainer | begin training epoch 233
2022-12-09 23:51:52 | INFO | train_inner | epoch 233:     52 / 1102 loss=6.688, nll_loss=2.802, ppl=6.97, wps=6992.7, ups=1.99, wpb=3517.3, bsz=137, num_updates=255700, lr=6.25367e-05, gnorm=2.419, loss_scale=2, train_wall=11, gb_free=19.9, wall=13853
2022-12-09 23:52:03 | INFO | train_inner | epoch 233:    152 / 1102 loss=6.616, nll_loss=2.74, ppl=6.68, wps=32612.6, ups=9.14, wpb=3566.3, bsz=151.5, num_updates=255800, lr=6.25244e-05, gnorm=2.409, loss_scale=2, train_wall=11, gb_free=19.3, wall=13864
2022-12-09 23:52:14 | INFO | train_inner | epoch 233:    252 / 1102 loss=6.583, nll_loss=2.701, ppl=6.5, wps=32207.1, ups=9.1, wpb=3540.9, bsz=152.2, num_updates=255900, lr=6.25122e-05, gnorm=2.496, loss_scale=2, train_wall=11, gb_free=19.6, wall=13875
2022-12-09 23:52:25 | INFO | train_inner | epoch 233:    352 / 1102 loss=6.656, nll_loss=2.781, ppl=6.88, wps=32814.8, ups=9.07, wpb=3616, bsz=143.7, num_updates=256000, lr=6.25e-05, gnorm=2.403, loss_scale=2, train_wall=11, gb_free=19.7, wall=13886
2022-12-09 23:52:36 | INFO | train_inner | epoch 233:    452 / 1102 loss=6.642, nll_loss=2.758, ppl=6.76, wps=32529.6, ups=9.14, wpb=3559.2, bsz=145.9, num_updates=256100, lr=6.24878e-05, gnorm=2.454, loss_scale=2, train_wall=11, gb_free=19.3, wall=13896
2022-12-09 23:52:47 | INFO | train_inner | epoch 233:    552 / 1102 loss=6.725, nll_loss=2.851, ppl=7.22, wps=33053.4, ups=9.13, wpb=3621.3, bsz=140.8, num_updates=256200, lr=6.24756e-05, gnorm=2.394, loss_scale=2, train_wall=11, gb_free=19.5, wall=13907
2022-12-09 23:52:58 | INFO | train_inner | epoch 233:    652 / 1102 loss=6.651, nll_loss=2.779, ppl=6.87, wps=32707.5, ups=9.11, wpb=3589.2, bsz=153.4, num_updates=256300, lr=6.24634e-05, gnorm=2.362, loss_scale=2, train_wall=11, gb_free=19.3, wall=13918
2022-12-09 23:53:09 | INFO | train_inner | epoch 233:    752 / 1102 loss=6.671, nll_loss=2.768, ppl=6.81, wps=32710.6, ups=9.2, wpb=3553.9, bsz=130.2, num_updates=256400, lr=6.24512e-05, gnorm=2.504, loss_scale=2, train_wall=11, gb_free=19.4, wall=13929
2022-12-09 23:53:20 | INFO | train_inner | epoch 233:    852 / 1102 loss=6.708, nll_loss=2.827, ppl=7.09, wps=32586.8, ups=9.05, wpb=3602.2, bsz=141.8, num_updates=256500, lr=6.24391e-05, gnorm=2.474, loss_scale=2, train_wall=11, gb_free=19.4, wall=13940
2022-12-09 23:53:31 | INFO | train_inner | epoch 233:    952 / 1102 loss=6.609, nll_loss=2.746, ppl=6.71, wps=32673.4, ups=9.07, wpb=3601.1, bsz=166.7, num_updates=256600, lr=6.24269e-05, gnorm=2.376, loss_scale=2, train_wall=11, gb_free=19.5, wall=13951
2022-12-09 23:53:42 | INFO | train_inner | epoch 233:   1052 / 1102 loss=6.721, nll_loss=2.828, ppl=7.1, wps=32730.6, ups=9.2, wpb=3556.4, bsz=131.5, num_updates=256700, lr=6.24147e-05, gnorm=2.508, loss_scale=2, train_wall=11, gb_free=19.4, wall=13962
2022-12-09 23:53:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:54:25 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 3.663 | nll_loss 2.123 | ppl 4.36 | bleu 37.47 | wps 4803 | wpb 2835.3 | bsz 115.6 | num_updates 256750 | best_bleu 37.63
2022-12-09 23:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 256750 updates
2022-12-09 23:54:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint233.pt
2022-12-09 23:54:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint233.pt (epoch 233 @ 256750 updates, score 37.47) (writing took 1.4069195799529552 seconds)
2022-12-09 23:54:26 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-12-09 23:54:26 | INFO | train | epoch 233 | loss 6.664 | nll_loss 2.783 | ppl 6.88 | wps 24681.4 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 256750 | lr 6.24086e-05 | gnorm 2.434 | loss_scale 2 | train_wall 118 | gb_free 19.4 | wall 14007
2022-12-09 23:54:26 | INFO | fairseq.trainer | begin training epoch 234
2022-12-09 23:54:32 | INFO | train_inner | epoch 234:     50 / 1102 loss=6.704, nll_loss=2.823, ppl=7.08, wps=7202.5, ups=1.99, wpb=3615.1, bsz=142.4, num_updates=256800, lr=6.24026e-05, gnorm=2.435, loss_scale=2, train_wall=10, gb_free=19.4, wall=14012
2022-12-09 23:54:43 | INFO | train_inner | epoch 234:    150 / 1102 loss=6.557, nll_loss=2.674, ppl=6.38, wps=32342.5, ups=9.17, wpb=3526.9, bsz=151.4, num_updates=256900, lr=6.23904e-05, gnorm=2.485, loss_scale=2, train_wall=11, gb_free=19.6, wall=14023
2022-12-09 23:54:54 | INFO | train_inner | epoch 234:    250 / 1102 loss=6.632, nll_loss=2.758, ppl=6.77, wps=32682.4, ups=9.05, wpb=3611, bsz=142.9, num_updates=257000, lr=6.23783e-05, gnorm=2.327, loss_scale=2, train_wall=11, gb_free=19.7, wall=14034
2022-12-09 23:55:05 | INFO | train_inner | epoch 234:    350 / 1102 loss=6.655, nll_loss=2.77, ppl=6.82, wps=32412.1, ups=9.03, wpb=3587.9, bsz=145.3, num_updates=257100, lr=6.23662e-05, gnorm=2.403, loss_scale=2, train_wall=11, gb_free=19.3, wall=14045
2022-12-09 23:55:16 | INFO | train_inner | epoch 234:    450 / 1102 loss=6.72, nll_loss=2.821, ppl=7.06, wps=32935.2, ups=9.25, wpb=3560.8, bsz=129.8, num_updates=257200, lr=6.2354e-05, gnorm=2.487, loss_scale=2, train_wall=11, gb_free=19.4, wall=14056
2022-12-09 23:55:27 | INFO | train_inner | epoch 234:    550 / 1102 loss=6.736, nll_loss=2.844, ppl=7.18, wps=33244.5, ups=9.16, wpb=3630.1, bsz=128.5, num_updates=257300, lr=6.23419e-05, gnorm=2.456, loss_scale=2, train_wall=11, gb_free=19.6, wall=14067
2022-12-09 23:55:38 | INFO | train_inner | epoch 234:    650 / 1102 loss=6.712, nll_loss=2.83, ppl=7.11, wps=32544.2, ups=9.14, wpb=3562.4, bsz=145, num_updates=257400, lr=6.23298e-05, gnorm=2.55, loss_scale=2, train_wall=11, gb_free=19.5, wall=14078
2022-12-09 23:55:48 | INFO | train_inner | epoch 234:    750 / 1102 loss=6.668, nll_loss=2.773, ppl=6.84, wps=32348.7, ups=9.2, wpb=3516.3, bsz=138.5, num_updates=257500, lr=6.23177e-05, gnorm=2.512, loss_scale=2, train_wall=11, gb_free=19.4, wall=14089
2022-12-09 23:55:59 | INFO | train_inner | epoch 234:    850 / 1102 loss=6.671, nll_loss=2.789, ppl=6.91, wps=32729.4, ups=9.22, wpb=3550.2, bsz=143.9, num_updates=257600, lr=6.23056e-05, gnorm=2.42, loss_scale=2, train_wall=11, gb_free=19.5, wall=14100
2022-12-09 23:56:10 | INFO | train_inner | epoch 234:    950 / 1102 loss=6.642, nll_loss=2.77, ppl=6.82, wps=33144.2, ups=9.1, wpb=3642.8, bsz=159.4, num_updates=257700, lr=6.22935e-05, gnorm=2.408, loss_scale=2, train_wall=11, gb_free=19.3, wall=14111
2022-12-09 23:56:21 | INFO | train_inner | epoch 234:   1050 / 1102 loss=6.639, nll_loss=2.793, ppl=6.93, wps=33084.4, ups=9.04, wpb=3660.4, bsz=171.4, num_updates=257800, lr=6.22814e-05, gnorm=2.41, loss_scale=2, train_wall=11, gb_free=19.8, wall=14122
2022-12-09 23:56:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:57:04 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 3.663 | nll_loss 2.124 | ppl 4.36 | bleu 37.59 | wps 4913.5 | wpb 2835.3 | bsz 115.6 | num_updates 257852 | best_bleu 37.63
2022-12-09 23:57:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 257852 updates
2022-12-09 23:57:05 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint234.pt
2022-12-09 23:57:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint234.pt (epoch 234 @ 257852 updates, score 37.59) (writing took 1.4379074359312654 seconds)
2022-12-09 23:57:05 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-12-09 23:57:05 | INFO | train | epoch 234 | loss 6.661 | nll_loss 2.781 | ppl 6.87 | wps 24818.9 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 257852 | lr 6.22751e-05 | gnorm 2.45 | loss_scale 2 | train_wall 118 | gb_free 19.4 | wall 14166
2022-12-09 23:57:05 | INFO | fairseq.trainer | begin training epoch 235
2022-12-09 23:57:11 | INFO | train_inner | epoch 235:     48 / 1102 loss=6.637, nll_loss=2.749, ppl=6.72, wps=7267.7, ups=2.02, wpb=3601.5, bsz=136.3, num_updates=257900, lr=6.22693e-05, gnorm=2.479, loss_scale=2, train_wall=11, gb_free=19.8, wall=14171
2022-12-09 23:57:22 | INFO | train_inner | epoch 235:    148 / 1102 loss=6.602, nll_loss=2.717, ppl=6.57, wps=32439.8, ups=9.11, wpb=3559.9, bsz=147.1, num_updates=258000, lr=6.22573e-05, gnorm=2.391, loss_scale=2, train_wall=11, gb_free=19.6, wall=14182
2022-12-09 23:57:33 | INFO | train_inner | epoch 235:    248 / 1102 loss=6.583, nll_loss=2.718, ppl=6.58, wps=32987.9, ups=9.15, wpb=3603.4, bsz=161, num_updates=258100, lr=6.22452e-05, gnorm=2.414, loss_scale=2, train_wall=11, gb_free=19.4, wall=14193
2022-12-09 23:57:44 | INFO | train_inner | epoch 235:    348 / 1102 loss=6.665, nll_loss=2.783, ppl=6.88, wps=32793, ups=9.16, wpb=3578.2, bsz=145.6, num_updates=258200, lr=6.22332e-05, gnorm=2.457, loss_scale=2, train_wall=11, gb_free=19.4, wall=14204
2022-12-09 23:57:55 | INFO | train_inner | epoch 235:    448 / 1102 loss=6.65, nll_loss=2.769, ppl=6.82, wps=32849.7, ups=9.16, wpb=3585.3, bsz=142.6, num_updates=258300, lr=6.22211e-05, gnorm=2.371, loss_scale=2, train_wall=11, gb_free=19.6, wall=14215
2022-12-09 23:58:05 | INFO | train_inner | epoch 235:    548 / 1102 loss=6.697, nll_loss=2.806, ppl=7, wps=32738.5, ups=9.17, wpb=3571.7, bsz=137.7, num_updates=258400, lr=6.22091e-05, gnorm=2.458, loss_scale=2, train_wall=11, gb_free=19.6, wall=14226
2022-12-09 23:58:16 | INFO | train_inner | epoch 235:    648 / 1102 loss=6.642, nll_loss=2.773, ppl=6.84, wps=32908.5, ups=9.23, wpb=3563.8, bsz=154.6, num_updates=258500, lr=6.2197e-05, gnorm=2.325, loss_scale=2, train_wall=11, gb_free=19.9, wall=14237
2022-12-09 23:58:27 | INFO | train_inner | epoch 235:    748 / 1102 loss=6.645, nll_loss=2.758, ppl=6.76, wps=32498.1, ups=9.01, wpb=3608.4, bsz=145.4, num_updates=258600, lr=6.2185e-05, gnorm=2.447, loss_scale=2, train_wall=11, gb_free=19.7, wall=14248
2022-12-09 23:58:38 | INFO | train_inner | epoch 235:    848 / 1102 loss=6.667, nll_loss=2.776, ppl=6.85, wps=32682.2, ups=9.23, wpb=3540, bsz=139.3, num_updates=258700, lr=6.2173e-05, gnorm=2.51, loss_scale=2, train_wall=11, gb_free=19.4, wall=14259
2022-12-09 23:58:49 | INFO | train_inner | epoch 235:    948 / 1102 loss=6.749, nll_loss=2.852, ppl=7.22, wps=32812, ups=9.16, wpb=3582.1, bsz=127.7, num_updates=258800, lr=6.2161e-05, gnorm=2.458, loss_scale=2, train_wall=11, gb_free=19.3, wall=14270
2022-12-09 23:59:00 | INFO | train_inner | epoch 235:   1048 / 1102 loss=6.756, nll_loss=2.87, ppl=7.31, wps=33028.8, ups=9.12, wpb=3620.5, bsz=137.6, num_updates=258900, lr=6.2149e-05, gnorm=2.51, loss_scale=2, train_wall=11, gb_free=19.7, wall=14281
2022-12-09 23:59:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-09 23:59:45 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 3.664 | nll_loss 2.127 | ppl 4.37 | bleu 37.52 | wps 4599.9 | wpb 2835.3 | bsz 115.6 | num_updates 258954 | best_bleu 37.63
2022-12-09 23:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 258954 updates
2022-12-09 23:59:46 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint235.pt
2022-12-09 23:59:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint235.pt (epoch 235 @ 258954 updates, score 37.52) (writing took 1.3749317908659577 seconds)
2022-12-09 23:59:47 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-12-09 23:59:47 | INFO | train | epoch 235 | loss 6.661 | nll_loss 2.78 | ppl 6.87 | wps 24448 | ups 6.82 | wpb 3583.6 | bsz 145.4 | num_updates 258954 | lr 6.21425e-05 | gnorm 2.431 | loss_scale 2 | train_wall 118 | gb_free 19.7 | wall 14327
2022-12-09 23:59:47 | INFO | fairseq.trainer | begin training epoch 236
2022-12-09 23:59:52 | INFO | train_inner | epoch 236:     46 / 1102 loss=6.577, nll_loss=2.737, ppl=6.67, wps=6972.8, ups=1.92, wpb=3625.7, bsz=180.6, num_updates=259000, lr=6.2137e-05, gnorm=2.399, loss_scale=2, train_wall=11, gb_free=19.6, wall=14333
2022-12-10 00:00:03 | INFO | train_inner | epoch 236:    146 / 1102 loss=6.638, nll_loss=2.78, ppl=6.87, wps=33244.3, ups=9.05, wpb=3671.4, bsz=157.3, num_updates=259100, lr=6.2125e-05, gnorm=2.35, loss_scale=2, train_wall=11, gb_free=19.3, wall=14344
2022-12-10 00:00:14 | INFO | train_inner | epoch 236:    246 / 1102 loss=6.61, nll_loss=2.715, ppl=6.57, wps=32505.3, ups=9.19, wpb=3536.8, bsz=137.5, num_updates=259200, lr=6.2113e-05, gnorm=2.489, loss_scale=2, train_wall=11, gb_free=19.7, wall=14355
2022-12-10 00:00:25 | INFO | train_inner | epoch 236:    346 / 1102 loss=6.681, nll_loss=2.804, ppl=6.98, wps=32911, ups=9.19, wpb=3581.7, bsz=143.7, num_updates=259300, lr=6.2101e-05, gnorm=2.378, loss_scale=2, train_wall=11, gb_free=19.6, wall=14366
2022-12-10 00:00:36 | INFO | train_inner | epoch 236:    446 / 1102 loss=6.644, nll_loss=2.743, ppl=6.7, wps=32693.6, ups=9.18, wpb=3561.3, bsz=134.2, num_updates=259400, lr=6.2089e-05, gnorm=2.472, loss_scale=2, train_wall=11, gb_free=19.6, wall=14376
2022-12-10 00:00:47 | INFO | train_inner | epoch 236:    546 / 1102 loss=6.692, nll_loss=2.796, ppl=6.94, wps=32690.4, ups=9.21, wpb=3549.2, bsz=133.4, num_updates=259500, lr=6.20771e-05, gnorm=2.466, loss_scale=2, train_wall=11, gb_free=19.5, wall=14387
2022-12-10 00:00:58 | INFO | train_inner | epoch 236:    646 / 1102 loss=6.65, nll_loss=2.76, ppl=6.77, wps=32793.2, ups=9.23, wpb=3551.9, bsz=145.2, num_updates=259600, lr=6.20651e-05, gnorm=2.456, loss_scale=2, train_wall=11, gb_free=19.3, wall=14398
2022-12-10 00:01:09 | INFO | train_inner | epoch 236:    746 / 1102 loss=6.672, nll_loss=2.809, ppl=7.01, wps=32553.3, ups=9.09, wpb=3579.4, bsz=157.5, num_updates=259700, lr=6.20532e-05, gnorm=2.483, loss_scale=2, train_wall=11, gb_free=19.4, wall=14409
2022-12-10 00:01:20 | INFO | train_inner | epoch 236:    846 / 1102 loss=6.649, nll_loss=2.773, ppl=6.84, wps=32550.2, ups=9.1, wpb=3578.5, bsz=155.6, num_updates=259800, lr=6.20412e-05, gnorm=2.426, loss_scale=2, train_wall=11, gb_free=19.3, wall=14420
2022-12-10 00:01:30 | INFO | train_inner | epoch 236:    946 / 1102 loss=6.723, nll_loss=2.828, ppl=7.1, wps=32664.1, ups=9.1, wpb=3588.3, bsz=132.8, num_updates=259900, lr=6.20293e-05, gnorm=2.427, loss_scale=2, train_wall=11, gb_free=19.4, wall=14431
2022-12-10 00:01:42 | INFO | train_inner | epoch 236:   1046 / 1102 loss=6.63, nll_loss=2.767, ppl=6.81, wps=32514.3, ups=9.07, wpb=3584.1, bsz=153.5, num_updates=260000, lr=6.20174e-05, gnorm=2.384, loss_scale=2, train_wall=11, gb_free=19.6, wall=14442
2022-12-10 00:01:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:02:28 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 3.662 | nll_loss 2.122 | ppl 4.35 | bleu 37.63 | wps 4413.2 | wpb 2835.3 | bsz 115.6 | num_updates 260056 | best_bleu 37.63
2022-12-10 00:02:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 260056 updates
2022-12-10 00:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint236.pt
2022-12-10 00:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint236.pt (epoch 236 @ 260056 updates, score 37.63) (writing took 1.6456970293074846 seconds)
2022-12-10 00:02:30 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-12-10 00:02:30 | INFO | train | epoch 236 | loss 6.658 | nll_loss 2.777 | ppl 6.85 | wps 24172.9 | ups 6.75 | wpb 3583.6 | bsz 145.4 | num_updates 260056 | lr 6.20107e-05 | gnorm 2.432 | loss_scale 2 | train_wall 118 | gb_free 19.8 | wall 14491
2022-12-10 00:02:30 | INFO | fairseq.trainer | begin training epoch 237
2022-12-10 00:02:35 | INFO | train_inner | epoch 237:     44 / 1102 loss=6.718, nll_loss=2.823, ppl=7.08, wps=6656.6, ups=1.86, wpb=3577.1, bsz=126.5, num_updates=260100, lr=6.20054e-05, gnorm=2.437, loss_scale=2, train_wall=11, gb_free=20, wall=14496
2022-12-10 00:02:46 | INFO | train_inner | epoch 237:    144 / 1102 loss=6.634, nll_loss=2.738, ppl=6.67, wps=32158.3, ups=9.06, wpb=3550.8, bsz=138.9, num_updates=260200, lr=6.19935e-05, gnorm=2.507, loss_scale=2, train_wall=11, gb_free=19.7, wall=14507
2022-12-10 00:02:57 | INFO | train_inner | epoch 237:    244 / 1102 loss=6.576, nll_loss=2.708, ppl=6.53, wps=32641.4, ups=9.12, wpb=3578.2, bsz=157.1, num_updates=260300, lr=6.19816e-05, gnorm=2.396, loss_scale=2, train_wall=11, gb_free=19.4, wall=14518
2022-12-10 00:03:08 | INFO | train_inner | epoch 237:    344 / 1102 loss=6.707, nll_loss=2.808, ppl=7, wps=32718.5, ups=9.2, wpb=3556.5, bsz=133, num_updates=260400, lr=6.19697e-05, gnorm=2.505, loss_scale=2, train_wall=11, gb_free=19.7, wall=14529
2022-12-10 00:03:19 | INFO | train_inner | epoch 237:    444 / 1102 loss=6.72, nll_loss=2.839, ppl=7.16, wps=33885.5, ups=9.22, wpb=3673.7, bsz=140.5, num_updates=260500, lr=6.19578e-05, gnorm=2.425, loss_scale=2, train_wall=11, gb_free=19.5, wall=14540
2022-12-10 00:03:30 | INFO | train_inner | epoch 237:    544 / 1102 loss=6.594, nll_loss=2.707, ppl=6.53, wps=32616, ups=9.18, wpb=3551.4, bsz=146.4, num_updates=260600, lr=6.19459e-05, gnorm=2.469, loss_scale=2, train_wall=11, gb_free=19.8, wall=14550
2022-12-10 00:03:41 | INFO | train_inner | epoch 237:    644 / 1102 loss=6.629, nll_loss=2.741, ppl=6.69, wps=32074.6, ups=9.09, wpb=3526.8, bsz=149.7, num_updates=260700, lr=6.19341e-05, gnorm=2.548, loss_scale=2, train_wall=11, gb_free=19.4, wall=14561
2022-12-10 00:03:52 | INFO | train_inner | epoch 237:    744 / 1102 loss=6.675, nll_loss=2.793, ppl=6.93, wps=32905.6, ups=9.06, wpb=3633.4, bsz=142.2, num_updates=260800, lr=6.19222e-05, gnorm=2.43, loss_scale=2, train_wall=11, gb_free=19.4, wall=14572
2022-12-10 00:04:03 | INFO | train_inner | epoch 237:    844 / 1102 loss=6.658, nll_loss=2.779, ppl=6.86, wps=32380, ups=9.08, wpb=3564.2, bsz=147.1, num_updates=260900, lr=6.19103e-05, gnorm=2.423, loss_scale=2, train_wall=11, gb_free=19.6, wall=14583
2022-12-10 00:04:14 | INFO | train_inner | epoch 237:    944 / 1102 loss=6.652, nll_loss=2.789, ppl=6.91, wps=32881.4, ups=9.08, wpb=3621.1, bsz=163.8, num_updates=261000, lr=6.18984e-05, gnorm=2.395, loss_scale=2, train_wall=11, gb_free=19.4, wall=14595
2022-12-10 00:04:25 | INFO | train_inner | epoch 237:   1044 / 1102 loss=6.689, nll_loss=2.815, ppl=7.04, wps=32871.3, ups=9.12, wpb=3603.6, bsz=146.2, num_updates=261100, lr=6.18866e-05, gnorm=2.417, loss_scale=2, train_wall=11, gb_free=19.5, wall=14605
2022-12-10 00:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:05:12 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 3.658 | nll_loss 2.121 | ppl 4.35 | bleu 37.67 | wps 4380.7 | wpb 2835.3 | bsz 115.6 | num_updates 261158 | best_bleu 37.67
2022-12-10 00:05:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 261158 updates
2022-12-10 00:05:14 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint237.pt
2022-12-10 00:05:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint237.pt (epoch 237 @ 261158 updates, score 37.67) (writing took 2.2243135552853346 seconds)
2022-12-10 00:05:15 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-12-10 00:05:15 | INFO | train | epoch 237 | loss 6.658 | nll_loss 2.776 | ppl 6.85 | wps 24012.6 | ups 6.7 | wpb 3583.6 | bsz 145.4 | num_updates 261158 | lr 6.18797e-05 | gnorm 2.447 | loss_scale 2 | train_wall 118 | gb_free 19.3 | wall 14655
2022-12-10 00:05:15 | INFO | fairseq.trainer | begin training epoch 238
2022-12-10 00:05:20 | INFO | train_inner | epoch 238:     42 / 1102 loss=6.617, nll_loss=2.757, ppl=6.76, wps=6564.8, ups=1.83, wpb=3586.3, bsz=157.1, num_updates=261200, lr=6.18747e-05, gnorm=2.322, loss_scale=2, train_wall=11, gb_free=19.4, wall=14660
2022-12-10 00:05:30 | INFO | train_inner | epoch 238:    142 / 1102 loss=6.64, nll_loss=2.772, ppl=6.83, wps=33118.3, ups=9.15, wpb=3618.1, bsz=146.2, num_updates=261300, lr=6.18629e-05, gnorm=2.353, loss_scale=2, train_wall=11, gb_free=19.3, wall=14671
2022-12-10 00:05:41 | INFO | train_inner | epoch 238:    242 / 1102 loss=6.621, nll_loss=2.74, ppl=6.68, wps=33100.3, ups=9.07, wpb=3649.3, bsz=147.8, num_updates=261400, lr=6.18511e-05, gnorm=2.405, loss_scale=2, train_wall=11, gb_free=19.7, wall=14682
2022-12-10 00:05:52 | INFO | train_inner | epoch 238:    342 / 1102 loss=6.68, nll_loss=2.794, ppl=6.94, wps=32421.1, ups=9.17, wpb=3536.4, bsz=140.6, num_updates=261500, lr=6.18392e-05, gnorm=2.481, loss_scale=2, train_wall=11, gb_free=19.7, wall=14693
2022-12-10 00:06:03 | INFO | train_inner | epoch 238:    442 / 1102 loss=6.613, nll_loss=2.732, ppl=6.64, wps=33266.6, ups=9.23, wpb=3606.1, bsz=146.7, num_updates=261600, lr=6.18274e-05, gnorm=2.428, loss_scale=2, train_wall=11, gb_free=19.8, wall=14704
2022-12-10 00:06:14 | INFO | train_inner | epoch 238:    542 / 1102 loss=6.596, nll_loss=2.714, ppl=6.56, wps=32486.4, ups=9.13, wpb=3558.3, bsz=155.3, num_updates=261700, lr=6.18156e-05, gnorm=2.529, loss_scale=2, train_wall=11, gb_free=19.3, wall=14715
2022-12-10 00:06:25 | INFO | train_inner | epoch 238:    642 / 1102 loss=6.688, nll_loss=2.797, ppl=6.95, wps=32825.8, ups=9.08, wpb=3613.9, bsz=135, num_updates=261800, lr=6.18038e-05, gnorm=2.465, loss_scale=2, train_wall=11, gb_free=19.5, wall=14726
2022-12-10 00:06:36 | INFO | train_inner | epoch 238:    742 / 1102 loss=6.68, nll_loss=2.791, ppl=6.92, wps=32459.5, ups=9.13, wpb=3555.9, bsz=137.6, num_updates=261900, lr=6.1792e-05, gnorm=2.533, loss_scale=2, train_wall=11, gb_free=19.5, wall=14737
2022-12-10 00:06:47 | INFO | train_inner | epoch 238:    842 / 1102 loss=6.632, nll_loss=2.758, ppl=6.76, wps=32814.2, ups=9.04, wpb=3631.3, bsz=154, num_updates=262000, lr=6.17802e-05, gnorm=2.389, loss_scale=2, train_wall=11, gb_free=19.8, wall=14748
2022-12-10 00:06:58 | INFO | train_inner | epoch 238:    942 / 1102 loss=6.683, nll_loss=2.797, ppl=6.95, wps=32281.5, ups=9.1, wpb=3547.3, bsz=142.5, num_updates=262100, lr=6.17684e-05, gnorm=2.626, loss_scale=2, train_wall=11, gb_free=19.7, wall=14759
2022-12-10 00:07:09 | INFO | train_inner | epoch 238:   1042 / 1102 loss=6.704, nll_loss=2.812, ppl=7.02, wps=32333.3, ups=9.26, wpb=3493.2, bsz=145.8, num_updates=262200, lr=6.17566e-05, gnorm=2.568, loss_scale=2, train_wall=11, gb_free=19.2, wall=14770
2022-12-10 00:07:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:07:57 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 3.662 | nll_loss 2.122 | ppl 4.35 | bleu 37.53 | wps 4383.6 | wpb 2835.3 | bsz 115.6 | num_updates 262260 | best_bleu 37.67
2022-12-10 00:07:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 262260 updates
2022-12-10 00:07:58 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint238.pt
2022-12-10 00:07:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint238.pt (epoch 238 @ 262260 updates, score 37.53) (writing took 1.3801467772573233 seconds)
2022-12-10 00:07:58 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-12-10 00:07:58 | INFO | train | epoch 238 | loss 6.657 | nll_loss 2.775 | ppl 6.85 | wps 24137.5 | ups 6.74 | wpb 3583.6 | bsz 145.4 | num_updates 262260 | lr 6.17496e-05 | gnorm 2.467 | loss_scale 2 | train_wall 118 | gb_free 19.8 | wall 14819
2022-12-10 00:07:58 | INFO | fairseq.trainer | begin training epoch 239
2022-12-10 00:08:03 | INFO | train_inner | epoch 239:     40 / 1102 loss=6.703, nll_loss=2.832, ppl=7.12, wps=6758.7, ups=1.86, wpb=3640.8, bsz=144.3, num_updates=262300, lr=6.17449e-05, gnorm=2.489, loss_scale=2, train_wall=11, gb_free=19.4, wall=14823
2022-12-10 00:08:14 | INFO | train_inner | epoch 239:    140 / 1102 loss=6.593, nll_loss=2.714, ppl=6.56, wps=32907.7, ups=9.08, wpb=3624.8, bsz=152.6, num_updates=262400, lr=6.17331e-05, gnorm=2.414, loss_scale=2, train_wall=11, gb_free=19.2, wall=14834
2022-12-10 00:08:25 | INFO | train_inner | epoch 239:    240 / 1102 loss=6.62, nll_loss=2.755, ppl=6.75, wps=32719.4, ups=9.13, wpb=3584.3, bsz=154, num_updates=262500, lr=6.17213e-05, gnorm=2.361, loss_scale=2, train_wall=11, gb_free=19.5, wall=14845
2022-12-10 00:08:36 | INFO | train_inner | epoch 239:    340 / 1102 loss=6.614, nll_loss=2.731, ppl=6.64, wps=32909.3, ups=9.21, wpb=3572.4, bsz=146.8, num_updates=262600, lr=6.17096e-05, gnorm=2.464, loss_scale=2, train_wall=11, gb_free=19.7, wall=14856
2022-12-10 00:08:47 | INFO | train_inner | epoch 239:    440 / 1102 loss=6.643, nll_loss=2.754, ppl=6.74, wps=32471.9, ups=9.13, wpb=3556.2, bsz=139, num_updates=262700, lr=6.16978e-05, gnorm=2.418, loss_scale=2, train_wall=11, gb_free=19.3, wall=14867
2022-12-10 00:08:58 | INFO | train_inner | epoch 239:    540 / 1102 loss=6.662, nll_loss=2.778, ppl=6.86, wps=32928.3, ups=9.16, wpb=3594.5, bsz=144.2, num_updates=262800, lr=6.16861e-05, gnorm=2.434, loss_scale=2, train_wall=11, gb_free=19.5, wall=14878
2022-12-10 00:09:08 | INFO | train_inner | epoch 239:    640 / 1102 loss=6.688, nll_loss=2.789, ppl=6.91, wps=32557.8, ups=9.15, wpb=3559.9, bsz=130.6, num_updates=262900, lr=6.16744e-05, gnorm=2.484, loss_scale=2, train_wall=11, gb_free=19.3, wall=14889
2022-12-10 00:09:19 | INFO | train_inner | epoch 239:    740 / 1102 loss=6.684, nll_loss=2.807, ppl=7, wps=32665.6, ups=9.15, wpb=3568.7, bsz=145.3, num_updates=263000, lr=6.16626e-05, gnorm=2.444, loss_scale=2, train_wall=11, gb_free=19.5, wall=14900
2022-12-10 00:09:30 | INFO | train_inner | epoch 239:    840 / 1102 loss=6.735, nll_loss=2.852, ppl=7.22, wps=32729.1, ups=9.19, wpb=3561.6, bsz=136.9, num_updates=263100, lr=6.16509e-05, gnorm=2.513, loss_scale=2, train_wall=11, gb_free=19.5, wall=14911
2022-12-10 00:09:41 | INFO | train_inner | epoch 239:    940 / 1102 loss=6.672, nll_loss=2.81, ppl=7.01, wps=32740.6, ups=9, wpb=3638.1, bsz=163.1, num_updates=263200, lr=6.16392e-05, gnorm=2.51, loss_scale=2, train_wall=11, gb_free=19.6, wall=14922
2022-12-10 00:09:52 | INFO | train_inner | epoch 239:   1040 / 1102 loss=6.648, nll_loss=2.77, ppl=6.82, wps=32566.4, ups=9.16, wpb=3554.9, bsz=146.9, num_updates=263300, lr=6.16275e-05, gnorm=2.44, loss_scale=2, train_wall=11, gb_free=19.4, wall=14933
2022-12-10 00:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:10:39 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 3.666 | nll_loss 2.124 | ppl 4.36 | bleu 37.55 | wps 4497.6 | wpb 2835.3 | bsz 115.6 | num_updates 263362 | best_bleu 37.67
2022-12-10 00:10:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 263362 updates
2022-12-10 00:10:40 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint239.pt
2022-12-10 00:10:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint239.pt (epoch 239 @ 263362 updates, score 37.55) (writing took 1.3315530065447092 seconds)
2022-12-10 00:10:41 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-12-10 00:10:41 | INFO | train | epoch 239 | loss 6.653 | nll_loss 2.772 | ppl 6.83 | wps 24312.6 | ups 6.78 | wpb 3583.6 | bsz 145.4 | num_updates 263362 | lr 6.16202e-05 | gnorm 2.457 | loss_scale 2 | train_wall 118 | gb_free 19.6 | wall 14981
2022-12-10 00:10:41 | INFO | fairseq.trainer | begin training epoch 240
2022-12-10 00:10:45 | INFO | train_inner | epoch 240:     38 / 1102 loss=6.655, nll_loss=2.752, ppl=6.74, wps=6759.9, ups=1.9, wpb=3560.2, bsz=127.8, num_updates=263400, lr=6.16158e-05, gnorm=2.498, loss_scale=2, train_wall=11, gb_free=19.3, wall=14986
2022-12-10 00:10:56 | INFO | train_inner | epoch 240:    138 / 1102 loss=6.538, nll_loss=2.662, ppl=6.33, wps=32547, ups=9.05, wpb=3594.8, bsz=161.1, num_updates=263500, lr=6.16041e-05, gnorm=2.383, loss_scale=2, train_wall=11, gb_free=19.9, wall=14997
2022-12-10 00:11:07 | INFO | train_inner | epoch 240:    238 / 1102 loss=6.573, nll_loss=2.721, ppl=6.59, wps=32751.5, ups=9.13, wpb=3587.5, bsz=174.3, num_updates=263600, lr=6.15924e-05, gnorm=2.337, loss_scale=2, train_wall=11, gb_free=19.5, wall=15008
2022-12-10 00:11:18 | INFO | train_inner | epoch 240:    338 / 1102 loss=6.657, nll_loss=2.776, ppl=6.85, wps=33001.3, ups=9.12, wpb=3619.6, bsz=148.6, num_updates=263700, lr=6.15807e-05, gnorm=2.448, loss_scale=2, train_wall=11, gb_free=19.3, wall=15019
2022-12-10 00:11:29 | INFO | train_inner | epoch 240:    438 / 1102 loss=6.664, nll_loss=2.763, ppl=6.79, wps=32620.2, ups=9.25, wpb=3527, bsz=126.3, num_updates=263800, lr=6.15691e-05, gnorm=2.515, loss_scale=2, train_wall=11, gb_free=19.9, wall=15029
2022-12-10 00:11:40 | INFO | train_inner | epoch 240:    538 / 1102 loss=6.667, nll_loss=2.777, ppl=6.85, wps=32785.9, ups=9.15, wpb=3581.7, bsz=142.2, num_updates=263900, lr=6.15574e-05, gnorm=2.521, loss_scale=2, train_wall=11, gb_free=19.3, wall=15040
2022-12-10 00:11:51 | INFO | train_inner | epoch 240:    638 / 1102 loss=6.686, nll_loss=2.8, ppl=6.96, wps=33016, ups=9.09, wpb=3630.8, bsz=139.1, num_updates=264000, lr=6.15457e-05, gnorm=2.424, loss_scale=2, train_wall=11, gb_free=20.1, wall=15051
2022-12-10 00:12:02 | INFO | train_inner | epoch 240:    738 / 1102 loss=6.626, nll_loss=2.743, ppl=6.69, wps=32731.8, ups=9.16, wpb=3573.6, bsz=145, num_updates=264100, lr=6.15341e-05, gnorm=2.455, loss_scale=2, train_wall=11, gb_free=19.5, wall=15062
2022-12-10 00:12:12 | INFO | train_inner | epoch 240:    838 / 1102 loss=6.672, nll_loss=2.787, ppl=6.9, wps=33416.9, ups=9.2, wpb=3632.6, bsz=145.3, num_updates=264200, lr=6.15224e-05, gnorm=2.527, loss_scale=2, train_wall=11, gb_free=19.3, wall=15073
2022-12-10 00:12:23 | INFO | train_inner | epoch 240:    938 / 1102 loss=6.683, nll_loss=2.799, ppl=6.96, wps=32891.6, ups=9.26, wpb=3551.5, bsz=141.4, num_updates=264300, lr=6.15108e-05, gnorm=2.412, loss_scale=2, train_wall=11, gb_free=19.2, wall=15084
2022-12-10 00:12:34 | INFO | train_inner | epoch 240:   1038 / 1102 loss=6.722, nll_loss=2.831, ppl=7.12, wps=33077.9, ups=9.28, wpb=3564.1, bsz=133.2, num_updates=264400, lr=6.14992e-05, gnorm=2.462, loss_scale=2, train_wall=11, gb_free=19.6, wall=15095
2022-12-10 00:12:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:13:20 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 3.662 | nll_loss 2.121 | ppl 4.35 | bleu 37.45 | wps 4602.1 | wpb 2835.3 | bsz 115.6 | num_updates 264464 | best_bleu 37.67
2022-12-10 00:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 264464 updates
2022-12-10 00:13:21 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint240.pt
2022-12-10 00:13:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint240.pt (epoch 240 @ 264464 updates, score 37.45) (writing took 1.381957745179534 seconds)
2022-12-10 00:13:22 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-12-10 00:13:22 | INFO | train | epoch 240 | loss 6.652 | nll_loss 2.77 | ppl 6.82 | wps 24527.5 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 264464 | lr 6.14917e-05 | gnorm 2.449 | loss_scale 2 | train_wall 117 | gb_free 19.4 | wall 15142
2022-12-10 00:13:22 | INFO | fairseq.trainer | begin training epoch 241
2022-12-10 00:13:26 | INFO | train_inner | epoch 241:     36 / 1102 loss=6.69, nll_loss=2.822, ppl=7.07, wps=6865.4, ups=1.93, wpb=3555.6, bsz=147.4, num_updates=264500, lr=6.14875e-05, gnorm=2.432, loss_scale=2, train_wall=10, gb_free=19.4, wall=15146
2022-12-10 00:13:37 | INFO | train_inner | epoch 241:    136 / 1102 loss=6.613, nll_loss=2.73, ppl=6.63, wps=33212.3, ups=9.25, wpb=3589.5, bsz=143.8, num_updates=264600, lr=6.14759e-05, gnorm=2.385, loss_scale=2, train_wall=11, gb_free=19.7, wall=15157
2022-12-10 00:13:48 | INFO | train_inner | epoch 241:    236 / 1102 loss=6.629, nll_loss=2.756, ppl=6.75, wps=33020.7, ups=9.19, wpb=3591.6, bsz=148.4, num_updates=264700, lr=6.14643e-05, gnorm=2.36, loss_scale=2, train_wall=11, gb_free=19.3, wall=15168
2022-12-10 00:13:58 | INFO | train_inner | epoch 241:    336 / 1102 loss=6.568, nll_loss=2.674, ppl=6.38, wps=32738.2, ups=9.33, wpb=3507.2, bsz=151.8, num_updates=264800, lr=6.14527e-05, gnorm=2.521, loss_scale=2, train_wall=10, gb_free=20, wall=15179
2022-12-10 00:14:09 | INFO | train_inner | epoch 241:    436 / 1102 loss=6.71, nll_loss=2.815, ppl=7.04, wps=33439.8, ups=9.32, wpb=3586.4, bsz=140.2, num_updates=264900, lr=6.14411e-05, gnorm=2.547, loss_scale=2, train_wall=10, gb_free=19.3, wall=15190
2022-12-10 00:14:20 | INFO | train_inner | epoch 241:    536 / 1102 loss=6.641, nll_loss=2.756, ppl=6.76, wps=33671.6, ups=9.28, wpb=3627.4, bsz=141.7, num_updates=265000, lr=6.14295e-05, gnorm=2.47, loss_scale=2, train_wall=11, gb_free=20.1, wall=15200
2022-12-10 00:14:31 | INFO | train_inner | epoch 241:    636 / 1102 loss=6.66, nll_loss=2.784, ppl=6.89, wps=33228.6, ups=9.12, wpb=3641.5, bsz=151.9, num_updates=265100, lr=6.14179e-05, gnorm=2.398, loss_scale=2, train_wall=11, gb_free=19.4, wall=15211
2022-12-10 00:14:42 | INFO | train_inner | epoch 241:    736 / 1102 loss=6.673, nll_loss=2.803, ppl=6.98, wps=33134.6, ups=9.21, wpb=3597.9, bsz=145.8, num_updates=265200, lr=6.14063e-05, gnorm=2.381, loss_scale=2, train_wall=11, gb_free=19.8, wall=15222
2022-12-10 00:14:52 | INFO | train_inner | epoch 241:    836 / 1102 loss=6.707, nll_loss=2.825, ppl=7.09, wps=33495.9, ups=9.26, wpb=3617.7, bsz=137.8, num_updates=265300, lr=6.13948e-05, gnorm=2.503, loss_scale=2, train_wall=11, gb_free=19.5, wall=15233
2022-12-10 00:15:03 | INFO | train_inner | epoch 241:    936 / 1102 loss=6.652, nll_loss=2.758, ppl=6.76, wps=32839, ups=9.39, wpb=3497.1, bsz=136.6, num_updates=265400, lr=6.13832e-05, gnorm=2.528, loss_scale=2, train_wall=10, gb_free=19.6, wall=15244
2022-12-10 00:15:14 | INFO | train_inner | epoch 241:   1036 / 1102 loss=6.671, nll_loss=2.795, ppl=6.94, wps=33217.1, ups=9.25, wpb=3590.6, bsz=146, num_updates=265500, lr=6.13716e-05, gnorm=2.569, loss_scale=2, train_wall=11, gb_free=19.3, wall=15254
2022-12-10 00:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:16:02 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 3.662 | nll_loss 2.122 | ppl 4.35 | bleu 37.5 | wps 4357 | wpb 2835.3 | bsz 115.6 | num_updates 265566 | best_bleu 37.67
2022-12-10 00:16:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 265566 updates
2022-12-10 00:16:04 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint241.pt
2022-12-10 00:16:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint241.pt (epoch 241 @ 265566 updates, score 37.5) (writing took 1.5729325357824564 seconds)
2022-12-10 00:16:04 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-12-10 00:16:04 | INFO | train | epoch 241 | loss 6.651 | nll_loss 2.769 | ppl 6.82 | wps 24315.1 | ups 6.79 | wpb 3583.6 | bsz 145.4 | num_updates 265566 | lr 6.1364e-05 | gnorm 2.463 | loss_scale 2 | train_wall 116 | gb_free 19.4 | wall 15305
2022-12-10 00:16:04 | INFO | fairseq.trainer | begin training epoch 242
2022-12-10 00:16:08 | INFO | train_inner | epoch 242:     34 / 1102 loss=6.644, nll_loss=2.775, ppl=6.84, wps=6669.7, ups=1.84, wpb=3618.7, bsz=155.8, num_updates=265600, lr=6.13601e-05, gnorm=2.474, loss_scale=2, train_wall=11, gb_free=19.4, wall=15309
2022-12-10 00:16:19 | INFO | train_inner | epoch 242:    134 / 1102 loss=6.568, nll_loss=2.692, ppl=6.46, wps=33146.5, ups=9.28, wpb=3573.1, bsz=153, num_updates=265700, lr=6.13485e-05, gnorm=2.445, loss_scale=2, train_wall=11, gb_free=19.2, wall=15319
2022-12-10 00:16:30 | INFO | train_inner | epoch 242:    234 / 1102 loss=6.61, nll_loss=2.739, ppl=6.68, wps=32948.2, ups=9.24, wpb=3564.1, bsz=158.7, num_updates=265800, lr=6.1337e-05, gnorm=2.429, loss_scale=2, train_wall=11, gb_free=19.5, wall=15330
2022-12-10 00:16:40 | INFO | train_inner | epoch 242:    334 / 1102 loss=6.61, nll_loss=2.72, ppl=6.59, wps=32964.2, ups=9.35, wpb=3525.1, bsz=139, num_updates=265900, lr=6.13255e-05, gnorm=2.488, loss_scale=2, train_wall=10, gb_free=19.3, wall=15341
2022-12-10 00:16:51 | INFO | train_inner | epoch 242:    434 / 1102 loss=6.656, nll_loss=2.764, ppl=6.79, wps=33583.5, ups=9.43, wpb=3562.4, bsz=143.3, num_updates=266000, lr=6.13139e-05, gnorm=2.517, loss_scale=2, train_wall=10, gb_free=19.5, wall=15352
2022-12-10 00:17:02 | INFO | train_inner | epoch 242:    534 / 1102 loss=6.622, nll_loss=2.74, ppl=6.68, wps=33091.9, ups=9.24, wpb=3581.6, bsz=147.8, num_updates=266100, lr=6.13024e-05, gnorm=2.433, loss_scale=2, train_wall=11, gb_free=19.3, wall=15362
2022-12-10 00:17:13 | INFO | train_inner | epoch 242:    634 / 1102 loss=6.656, nll_loss=2.78, ppl=6.87, wps=33196.3, ups=9.18, wpb=3616.3, bsz=152.8, num_updates=266200, lr=6.12909e-05, gnorm=2.455, loss_scale=2, train_wall=11, gb_free=19.5, wall=15373
2022-12-10 00:17:23 | INFO | train_inner | epoch 242:    734 / 1102 loss=6.653, nll_loss=2.776, ppl=6.85, wps=33645.8, ups=9.3, wpb=3618.2, bsz=147.4, num_updates=266300, lr=6.12794e-05, gnorm=2.428, loss_scale=2, train_wall=11, gb_free=19.8, wall=15384
2022-12-10 00:17:34 | INFO | train_inner | epoch 242:    834 / 1102 loss=6.697, nll_loss=2.811, ppl=7.02, wps=33010.6, ups=9.25, wpb=3569.4, bsz=142.2, num_updates=266400, lr=6.12679e-05, gnorm=2.562, loss_scale=2, train_wall=11, gb_free=19.7, wall=15395
2022-12-10 00:17:45 | INFO | train_inner | epoch 242:    934 / 1102 loss=6.709, nll_loss=2.829, ppl=7.1, wps=33394.6, ups=9.14, wpb=3654.5, bsz=142, num_updates=266500, lr=6.12564e-05, gnorm=2.468, loss_scale=2, train_wall=11, gb_free=19.3, wall=15406
2022-12-10 00:17:56 | INFO | train_inner | epoch 242:   1034 / 1102 loss=6.684, nll_loss=2.798, ppl=6.96, wps=32797.2, ups=9.24, wpb=3548.5, bsz=144.4, num_updates=266600, lr=6.12449e-05, gnorm=2.676, loss_scale=2, train_wall=11, gb_free=19.4, wall=15417
2022-12-10 00:18:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:18:41 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 3.665 | nll_loss 2.122 | ppl 4.35 | bleu 37.51 | wps 4756.6 | wpb 2835.3 | bsz 115.6 | num_updates 266668 | best_bleu 37.67
2022-12-10 00:18:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 266668 updates
2022-12-10 00:18:42 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint242.pt
2022-12-10 00:18:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint242.pt (epoch 242 @ 266668 updates, score 37.51) (writing took 1.215520548634231 seconds)
2022-12-10 00:18:43 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-12-10 00:18:43 | INFO | train | epoch 242 | loss 6.651 | nll_loss 2.768 | ppl 6.81 | wps 24911.2 | ups 6.95 | wpb 3583.6 | bsz 145.4 | num_updates 266668 | lr 6.12371e-05 | gnorm 2.496 | loss_scale 2 | train_wall 116 | gb_free 19.4 | wall 15463
2022-12-10 00:18:43 | INFO | fairseq.trainer | begin training epoch 243
2022-12-10 00:18:46 | INFO | train_inner | epoch 243:     32 / 1102 loss=6.674, nll_loss=2.755, ppl=6.75, wps=7050.2, ups=1.99, wpb=3543.3, bsz=115.1, num_updates=266700, lr=6.12334e-05, gnorm=2.565, loss_scale=2, train_wall=10, gb_free=19.7, wall=15467
2022-12-10 00:18:57 | INFO | train_inner | epoch 243:    132 / 1102 loss=6.601, nll_loss=2.735, ppl=6.66, wps=33204, ups=9.17, wpb=3620.8, bsz=157.5, num_updates=266800, lr=6.12219e-05, gnorm=2.392, loss_scale=2, train_wall=11, gb_free=19.4, wall=15478
2022-12-10 00:19:08 | INFO | train_inner | epoch 243:    232 / 1102 loss=6.678, nll_loss=2.797, ppl=6.95, wps=33564.1, ups=9.19, wpb=3650.5, bsz=137.8, num_updates=266900, lr=6.12105e-05, gnorm=2.398, loss_scale=2, train_wall=11, gb_free=19.5, wall=15489
2022-12-10 00:19:19 | INFO | train_inner | epoch 243:    332 / 1102 loss=6.685, nll_loss=2.806, ppl=6.99, wps=33689, ups=9.27, wpb=3633.3, bsz=139.3, num_updates=267000, lr=6.1199e-05, gnorm=2.369, loss_scale=2, train_wall=11, gb_free=19.4, wall=15499
2022-12-10 00:19:30 | INFO | train_inner | epoch 243:    432 / 1102 loss=6.599, nll_loss=2.714, ppl=6.56, wps=33173.8, ups=9.33, wpb=3555.5, bsz=149, num_updates=267100, lr=6.11875e-05, gnorm=2.543, loss_scale=2, train_wall=10, gb_free=19.6, wall=15510
2022-12-10 00:19:40 | INFO | train_inner | epoch 243:    532 / 1102 loss=6.682, nll_loss=2.791, ppl=6.92, wps=32694.5, ups=9.18, wpb=3561.5, bsz=134.1, num_updates=267200, lr=6.11761e-05, gnorm=2.463, loss_scale=4, train_wall=11, gb_free=19.5, wall=15521
2022-12-10 00:19:51 | INFO | train_inner | epoch 243:    632 / 1102 loss=6.687, nll_loss=2.789, ppl=6.91, wps=32779, ups=9.23, wpb=3551.9, bsz=139.5, num_updates=267300, lr=6.11647e-05, gnorm=2.495, loss_scale=4, train_wall=11, gb_free=19.4, wall=15532
2022-12-10 00:20:02 | INFO | train_inner | epoch 243:    732 / 1102 loss=6.608, nll_loss=2.728, ppl=6.62, wps=33209.5, ups=9.21, wpb=3606.2, bsz=150.9, num_updates=267400, lr=6.11532e-05, gnorm=2.404, loss_scale=4, train_wall=11, gb_free=19.3, wall=15543
2022-12-10 00:20:13 | INFO | train_inner | epoch 243:    832 / 1102 loss=6.627, nll_loss=2.764, ppl=6.79, wps=32895.5, ups=9.08, wpb=3622.7, bsz=157.9, num_updates=267500, lr=6.11418e-05, gnorm=2.415, loss_scale=4, train_wall=11, gb_free=19.4, wall=15554
2022-12-10 00:20:24 | INFO | train_inner | epoch 243:    932 / 1102 loss=6.655, nll_loss=2.77, ppl=6.82, wps=32573.2, ups=9.19, wpb=3542.9, bsz=143.8, num_updates=267600, lr=6.11304e-05, gnorm=2.561, loss_scale=4, train_wall=11, gb_free=19.5, wall=15565
2022-12-10 00:20:35 | INFO | train_inner | epoch 243:   1032 / 1102 loss=6.669, nll_loss=2.792, ppl=6.92, wps=32646.4, ups=9.15, wpb=3569.5, bsz=152.6, num_updates=267700, lr=6.11189e-05, gnorm=2.455, loss_scale=4, train_wall=11, gb_free=19.4, wall=15576
2022-12-10 00:20:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:21:20 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 3.663 | nll_loss 2.123 | ppl 4.36 | bleu 37.55 | wps 4845.7 | wpb 2835.3 | bsz 115.6 | num_updates 267770 | best_bleu 37.67
2022-12-10 00:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 267770 updates
2022-12-10 00:21:21 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint243.pt
2022-12-10 00:21:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint243.pt (epoch 243 @ 267770 updates, score 37.55) (writing took 1.2073405645787716 seconds)
2022-12-10 00:21:21 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-12-10 00:21:21 | INFO | train | epoch 243 | loss 6.648 | nll_loss 2.766 | ppl 6.8 | wps 24915.6 | ups 6.95 | wpb 3583.6 | bsz 145.4 | num_updates 267770 | lr 6.1111e-05 | gnorm 2.452 | loss_scale 4 | train_wall 117 | gb_free 19.2 | wall 15622
2022-12-10 00:21:21 | INFO | fairseq.trainer | begin training epoch 244
2022-12-10 00:21:25 | INFO | train_inner | epoch 244:     30 / 1102 loss=6.632, nll_loss=2.744, ppl=6.7, wps=7053.5, ups=2.02, wpb=3500, bsz=145.3, num_updates=267800, lr=6.11075e-05, gnorm=2.465, loss_scale=4, train_wall=11, gb_free=19.2, wall=15625
2022-12-10 00:21:35 | INFO | train_inner | epoch 244:    130 / 1102 loss=6.578, nll_loss=2.703, ppl=6.51, wps=32539.1, ups=9.21, wpb=3532.4, bsz=157.4, num_updates=267900, lr=6.10961e-05, gnorm=2.409, loss_scale=4, train_wall=11, gb_free=19.6, wall=15636
2022-12-10 00:21:46 | INFO | train_inner | epoch 244:    230 / 1102 loss=6.629, nll_loss=2.739, ppl=6.67, wps=33144.9, ups=9.15, wpb=3621.1, bsz=138.7, num_updates=268000, lr=6.10847e-05, gnorm=2.413, loss_scale=4, train_wall=11, gb_free=19.5, wall=15647
2022-12-10 00:21:57 | INFO | train_inner | epoch 244:    330 / 1102 loss=6.627, nll_loss=2.747, ppl=6.71, wps=32799.9, ups=9, wpb=3642.4, bsz=154.4, num_updates=268100, lr=6.10733e-05, gnorm=2.394, loss_scale=4, train_wall=11, gb_free=19.4, wall=15658
2022-12-10 00:22:08 | INFO | train_inner | epoch 244:    430 / 1102 loss=6.645, nll_loss=2.753, ppl=6.74, wps=32714.3, ups=9.2, wpb=3555.4, bsz=141.8, num_updates=268200, lr=6.10619e-05, gnorm=2.45, loss_scale=4, train_wall=11, gb_free=19.3, wall=15669
2022-12-10 00:22:19 | INFO | train_inner | epoch 244:    530 / 1102 loss=6.603, nll_loss=2.72, ppl=6.59, wps=32810.7, ups=9.25, wpb=3545.7, bsz=144.1, num_updates=268300, lr=6.10506e-05, gnorm=2.415, loss_scale=4, train_wall=11, gb_free=19.5, wall=15680
2022-12-10 00:22:30 | INFO | train_inner | epoch 244:    630 / 1102 loss=6.723, nll_loss=2.84, ppl=7.16, wps=33207.8, ups=9.16, wpb=3623.8, bsz=140.6, num_updates=268400, lr=6.10392e-05, gnorm=2.428, loss_scale=4, train_wall=11, gb_free=19.8, wall=15691
2022-12-10 00:22:41 | INFO | train_inner | epoch 244:    730 / 1102 loss=6.7, nll_loss=2.812, ppl=7.02, wps=32378.4, ups=9.08, wpb=3567.5, bsz=137, num_updates=268500, lr=6.10278e-05, gnorm=2.505, loss_scale=4, train_wall=11, gb_free=19.4, wall=15702
2022-12-10 00:22:52 | INFO | train_inner | epoch 244:    830 / 1102 loss=6.602, nll_loss=2.71, ppl=6.54, wps=32537.7, ups=9.19, wpb=3539.2, bsz=142.9, num_updates=268600, lr=6.10165e-05, gnorm=2.58, loss_scale=4, train_wall=11, gb_free=19.7, wall=15713
2022-12-10 00:23:03 | INFO | train_inner | epoch 244:    930 / 1102 loss=6.62, nll_loss=2.745, ppl=6.7, wps=32761.1, ups=9.15, wpb=3580.3, bsz=152.8, num_updates=268700, lr=6.10051e-05, gnorm=2.388, loss_scale=4, train_wall=11, gb_free=19.3, wall=15723
2022-12-10 00:23:14 | INFO | train_inner | epoch 244:   1030 / 1102 loss=6.693, nll_loss=2.814, ppl=7.03, wps=32937.9, ups=9.08, wpb=3627.7, bsz=148.9, num_updates=268800, lr=6.09938e-05, gnorm=2.414, loss_scale=4, train_wall=11, gb_free=19.5, wall=15735
2022-12-10 00:23:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:24:05 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 3.668 | nll_loss 2.126 | ppl 4.36 | bleu 37.58 | wps 4239.6 | wpb 2835.3 | bsz 115.6 | num_updates 268872 | best_bleu 37.67
2022-12-10 00:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 268872 updates
2022-12-10 00:24:06 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint244.pt
2022-12-10 00:24:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint244.pt (epoch 244 @ 268872 updates, score 37.58) (writing took 1.5292340703308582 seconds)
2022-12-10 00:24:06 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-12-10 00:24:06 | INFO | train | epoch 244 | loss 6.647 | nll_loss 2.763 | ppl 6.79 | wps 23936.5 | ups 6.68 | wpb 3583.6 | bsz 145.4 | num_updates 268872 | lr 6.09856e-05 | gnorm 2.445 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 15787
2022-12-10 00:24:06 | INFO | fairseq.trainer | begin training epoch 245
2022-12-10 00:24:09 | INFO | train_inner | epoch 245:     28 / 1102 loss=6.7, nll_loss=2.827, ppl=7.1, wps=6436.7, ups=1.8, wpb=3570.5, bsz=144.9, num_updates=268900, lr=6.09824e-05, gnorm=2.481, loss_scale=4, train_wall=11, gb_free=19.6, wall=15790
2022-12-10 00:24:20 | INFO | train_inner | epoch 245:    128 / 1102 loss=6.597, nll_loss=2.718, ppl=6.58, wps=33026.3, ups=9.12, wpb=3621.9, bsz=151.1, num_updates=269000, lr=6.09711e-05, gnorm=2.38, loss_scale=4, train_wall=11, gb_free=19.6, wall=15801
2022-12-10 00:24:31 | INFO | train_inner | epoch 245:    228 / 1102 loss=6.595, nll_loss=2.695, ppl=6.47, wps=32762.8, ups=9.12, wpb=3591.3, bsz=141.2, num_updates=269100, lr=6.09597e-05, gnorm=2.503, loss_scale=4, train_wall=11, gb_free=19.5, wall=15812
2022-12-10 00:24:42 | INFO | train_inner | epoch 245:    328 / 1102 loss=6.619, nll_loss=2.731, ppl=6.64, wps=33235.2, ups=9.26, wpb=3587.6, bsz=139, num_updates=269200, lr=6.09484e-05, gnorm=2.459, loss_scale=4, train_wall=11, gb_free=19.5, wall=15823
2022-12-10 00:24:53 | INFO | train_inner | epoch 245:    428 / 1102 loss=6.644, nll_loss=2.748, ppl=6.72, wps=32978.3, ups=9.15, wpb=3602.5, bsz=139.8, num_updates=269300, lr=6.09371e-05, gnorm=2.537, loss_scale=4, train_wall=11, gb_free=19.3, wall=15834
2022-12-10 00:25:04 | INFO | train_inner | epoch 245:    528 / 1102 loss=6.699, nll_loss=2.82, ppl=7.06, wps=32998.6, ups=9.17, wpb=3597.3, bsz=142.6, num_updates=269400, lr=6.09258e-05, gnorm=2.426, loss_scale=4, train_wall=11, gb_free=19.5, wall=15845
2022-12-10 00:25:15 | INFO | train_inner | epoch 245:    628 / 1102 loss=6.682, nll_loss=2.8, ppl=6.96, wps=32860.8, ups=9.11, wpb=3607.2, bsz=138.5, num_updates=269500, lr=6.09145e-05, gnorm=2.43, loss_scale=4, train_wall=11, gb_free=19.3, wall=15856
2022-12-10 00:25:26 | INFO | train_inner | epoch 245:    728 / 1102 loss=6.632, nll_loss=2.759, ppl=6.77, wps=31910.6, ups=9, wpb=3544.1, bsz=154.1, num_updates=269600, lr=6.09032e-05, gnorm=2.493, loss_scale=4, train_wall=11, gb_free=19.9, wall=15867
2022-12-10 00:25:37 | INFO | train_inner | epoch 245:    828 / 1102 loss=6.624, nll_loss=2.759, ppl=6.77, wps=33294.3, ups=9.22, wpb=3612.2, bsz=160.6, num_updates=269700, lr=6.08919e-05, gnorm=2.437, loss_scale=4, train_wall=11, gb_free=19.6, wall=15877
2022-12-10 00:25:48 | INFO | train_inner | epoch 245:    928 / 1102 loss=6.693, nll_loss=2.802, ppl=6.97, wps=32075.7, ups=9.1, wpb=3525.8, bsz=139.8, num_updates=269800, lr=6.08806e-05, gnorm=2.481, loss_scale=4, train_wall=11, gb_free=19.6, wall=15888
2022-12-10 00:25:59 | INFO | train_inner | epoch 245:   1028 / 1102 loss=6.715, nll_loss=2.826, ppl=7.09, wps=32634.1, ups=9.14, wpb=3571.6, bsz=131.4, num_updates=269900, lr=6.08693e-05, gnorm=2.48, loss_scale=4, train_wall=11, gb_free=19.4, wall=15899
2022-12-10 00:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:26:48 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 3.663 | nll_loss 2.124 | ppl 4.36 | bleu 37.55 | wps 4407.1 | wpb 2835.3 | bsz 115.6 | num_updates 269974 | best_bleu 37.67
2022-12-10 00:26:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 269974 updates
2022-12-10 00:26:49 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint245.pt
2022-12-10 00:26:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint245.pt (epoch 245 @ 269974 updates, score 37.55) (writing took 1.2198419803753495 seconds)
2022-12-10 00:26:49 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-12-10 00:26:49 | INFO | train | epoch 245 | loss 6.644 | nll_loss 2.761 | ppl 6.78 | wps 24191.5 | ups 6.75 | wpb 3583.6 | bsz 145.4 | num_updates 269974 | lr 6.0861e-05 | gnorm 2.463 | loss_scale 4 | train_wall 118 | gb_free 19.9 | wall 15950
2022-12-10 00:26:49 | INFO | fairseq.trainer | begin training epoch 246
2022-12-10 00:26:53 | INFO | train_inner | epoch 246:     26 / 1102 loss=6.575, nll_loss=2.708, ppl=6.54, wps=6668.3, ups=1.86, wpb=3581.2, bsz=164.6, num_updates=270000, lr=6.08581e-05, gnorm=2.46, loss_scale=4, train_wall=11, gb_free=19.9, wall=15953
2022-12-10 00:27:03 | INFO | train_inner | epoch 246:    126 / 1102 loss=6.561, nll_loss=2.683, ppl=6.42, wps=32597.5, ups=9.2, wpb=3543.5, bsz=145.8, num_updates=270100, lr=6.08468e-05, gnorm=2.414, loss_scale=4, train_wall=11, gb_free=19.4, wall=15964
2022-12-10 00:27:14 | INFO | train_inner | epoch 246:    226 / 1102 loss=6.632, nll_loss=2.755, ppl=6.75, wps=33199.5, ups=9.08, wpb=3655.9, bsz=147.8, num_updates=270200, lr=6.08355e-05, gnorm=2.442, loss_scale=4, train_wall=11, gb_free=19.5, wall=15975
2022-12-10 00:27:25 | INFO | train_inner | epoch 246:    326 / 1102 loss=6.58, nll_loss=2.711, ppl=6.55, wps=33183.4, ups=9.18, wpb=3613.3, bsz=156.6, num_updates=270300, lr=6.08243e-05, gnorm=2.395, loss_scale=4, train_wall=11, gb_free=19.5, wall=15986
2022-12-10 00:27:36 | INFO | train_inner | epoch 246:    426 / 1102 loss=6.608, nll_loss=2.736, ppl=6.66, wps=33180.9, ups=9.14, wpb=3629, bsz=151.3, num_updates=270400, lr=6.0813e-05, gnorm=2.365, loss_scale=4, train_wall=11, gb_free=19.4, wall=15997
2022-12-10 00:27:47 | INFO | train_inner | epoch 246:    526 / 1102 loss=6.676, nll_loss=2.773, ppl=6.84, wps=33115.3, ups=9.3, wpb=3559.2, bsz=132.4, num_updates=270500, lr=6.08018e-05, gnorm=2.538, loss_scale=4, train_wall=10, gb_free=21.3, wall=16008
2022-12-10 00:27:58 | INFO | train_inner | epoch 246:    626 / 1102 loss=6.649, nll_loss=2.763, ppl=6.79, wps=32461.1, ups=9.09, wpb=3570, bsz=148.6, num_updates=270600, lr=6.07906e-05, gnorm=2.468, loss_scale=4, train_wall=11, gb_free=20, wall=16019
2022-12-10 00:28:09 | INFO | train_inner | epoch 246:    726 / 1102 loss=6.674, nll_loss=2.789, ppl=6.91, wps=32998.4, ups=9.19, wpb=3590.2, bsz=150.8, num_updates=270700, lr=6.07793e-05, gnorm=2.507, loss_scale=4, train_wall=11, gb_free=19.3, wall=16029
2022-12-10 00:28:20 | INFO | train_inner | epoch 246:    826 / 1102 loss=6.723, nll_loss=2.814, ppl=7.03, wps=32931.7, ups=9.29, wpb=3544.1, bsz=132.7, num_updates=270800, lr=6.07681e-05, gnorm=2.627, loss_scale=4, train_wall=11, gb_free=19.9, wall=16040
2022-12-10 00:28:31 | INFO | train_inner | epoch 246:    926 / 1102 loss=6.68, nll_loss=2.786, ppl=6.9, wps=32469.1, ups=9.11, wpb=3564.5, bsz=134.9, num_updates=270900, lr=6.07569e-05, gnorm=2.451, loss_scale=4, train_wall=11, gb_free=19.4, wall=16051
2022-12-10 00:28:41 | INFO | train_inner | epoch 246:   1026 / 1102 loss=6.709, nll_loss=2.83, ppl=7.11, wps=33250.6, ups=9.17, wpb=3624.5, bsz=146.2, num_updates=271000, lr=6.07457e-05, gnorm=2.442, loss_scale=4, train_wall=11, gb_free=19.4, wall=16062
2022-12-10 00:28:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:29:27 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 3.668 | nll_loss 2.125 | ppl 4.36 | bleu 37.52 | wps 4857.5 | wpb 2835.3 | bsz 115.6 | num_updates 271076 | best_bleu 37.67
2022-12-10 00:29:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 271076 updates
2022-12-10 00:29:28 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint246.pt
2022-12-10 00:29:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint246.pt (epoch 246 @ 271076 updates, score 37.52) (writing took 1.2728601973503828 seconds)
2022-12-10 00:29:28 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-12-10 00:29:28 | INFO | train | epoch 246 | loss 6.644 | nll_loss 2.76 | ppl 6.77 | wps 24826 | ups 6.93 | wpb 3583.6 | bsz 145.4 | num_updates 271076 | lr 6.07372e-05 | gnorm 2.463 | loss_scale 4 | train_wall 117 | gb_free 20.4 | wall 16109
2022-12-10 00:29:28 | INFO | fairseq.trainer | begin training epoch 247
2022-12-10 00:29:31 | INFO | train_inner | epoch 247:     24 / 1102 loss=6.607, nll_loss=2.73, ppl=6.64, wps=7106.3, ups=2, wpb=3546.1, bsz=156.5, num_updates=271100, lr=6.07345e-05, gnorm=2.42, loss_scale=4, train_wall=11, gb_free=19.6, wall=16112
2022-12-10 00:29:42 | INFO | train_inner | epoch 247:    124 / 1102 loss=6.594, nll_loss=2.705, ppl=6.52, wps=32511.5, ups=9.14, wpb=3557.7, bsz=150.6, num_updates=271200, lr=6.07233e-05, gnorm=2.553, loss_scale=4, train_wall=11, gb_free=19.4, wall=16123
2022-12-10 00:29:53 | INFO | train_inner | epoch 247:    224 / 1102 loss=6.63, nll_loss=2.742, ppl=6.69, wps=33164.7, ups=9.26, wpb=3583.3, bsz=136.8, num_updates=271300, lr=6.07121e-05, gnorm=2.562, loss_scale=4, train_wall=11, gb_free=19.8, wall=16134
2022-12-10 00:30:04 | INFO | train_inner | epoch 247:    324 / 1102 loss=6.667, nll_loss=2.766, ppl=6.8, wps=32987.2, ups=9.15, wpb=3606.2, bsz=131.5, num_updates=271400, lr=6.07009e-05, gnorm=2.449, loss_scale=4, train_wall=11, gb_free=19.7, wall=16145
2022-12-10 00:30:15 | INFO | train_inner | epoch 247:    424 / 1102 loss=6.597, nll_loss=2.725, ppl=6.61, wps=33068.3, ups=9.14, wpb=3616.6, bsz=159.3, num_updates=271500, lr=6.06897e-05, gnorm=2.443, loss_scale=4, train_wall=11, gb_free=19.5, wall=16156
2022-12-10 00:30:26 | INFO | train_inner | epoch 247:    524 / 1102 loss=6.648, nll_loss=2.768, ppl=6.81, wps=33371.7, ups=9.18, wpb=3633.8, bsz=143, num_updates=271600, lr=6.06785e-05, gnorm=2.434, loss_scale=4, train_wall=11, gb_free=19.9, wall=16166
2022-12-10 00:30:37 | INFO | train_inner | epoch 247:    624 / 1102 loss=6.606, nll_loss=2.733, ppl=6.65, wps=33151.8, ups=9.2, wpb=3604.6, bsz=150.3, num_updates=271700, lr=6.06674e-05, gnorm=2.391, loss_scale=4, train_wall=11, gb_free=19.7, wall=16177
2022-12-10 00:30:48 | INFO | train_inner | epoch 247:    724 / 1102 loss=6.637, nll_loss=2.761, ppl=6.78, wps=32955.1, ups=9.17, wpb=3594.7, bsz=149, num_updates=271800, lr=6.06562e-05, gnorm=2.398, loss_scale=4, train_wall=11, gb_free=19.5, wall=16188
2022-12-10 00:30:59 | INFO | train_inner | epoch 247:    824 / 1102 loss=6.665, nll_loss=2.774, ppl=6.84, wps=32073.5, ups=9.16, wpb=3501.1, bsz=140.9, num_updates=271900, lr=6.06451e-05, gnorm=2.55, loss_scale=4, train_wall=11, gb_free=19.2, wall=16199
2022-12-10 00:31:10 | INFO | train_inner | epoch 247:    924 / 1102 loss=6.651, nll_loss=2.77, ppl=6.82, wps=32836.6, ups=9.12, wpb=3602.2, bsz=145.8, num_updates=272000, lr=6.06339e-05, gnorm=2.514, loss_scale=4, train_wall=11, gb_free=19.3, wall=16210
2022-12-10 00:31:21 | INFO | train_inner | epoch 247:   1024 / 1102 loss=6.701, nll_loss=2.81, ppl=7.01, wps=32456.8, ups=9.14, wpb=3551.5, bsz=143, num_updates=272100, lr=6.06228e-05, gnorm=2.506, loss_scale=4, train_wall=11, gb_free=19.7, wall=16221
2022-12-10 00:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:32:14 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 3.659 | nll_loss 2.124 | ppl 4.36 | bleu 37.59 | wps 4017.2 | wpb 2835.3 | bsz 115.6 | num_updates 272178 | best_bleu 37.67
2022-12-10 00:32:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 272178 updates
2022-12-10 00:32:15 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint247.pt
2022-12-10 00:32:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint247.pt (epoch 247 @ 272178 updates, score 37.59) (writing took 1.27276695612818 seconds)
2022-12-10 00:32:15 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-12-10 00:32:15 | INFO | train | epoch 247 | loss 6.643 | nll_loss 2.758 | ppl 6.77 | wps 23692.7 | ups 6.61 | wpb 3583.6 | bsz 145.4 | num_updates 272178 | lr 6.06141e-05 | gnorm 2.488 | loss_scale 4 | train_wall 117 | gb_free 19.6 | wall 16276
2022-12-10 00:32:15 | INFO | fairseq.trainer | begin training epoch 248
2022-12-10 00:32:18 | INFO | train_inner | epoch 248:     22 / 1102 loss=6.678, nll_loss=2.796, ppl=6.94, wps=6275.6, ups=1.74, wpb=3598, bsz=150.4, num_updates=272200, lr=6.06116e-05, gnorm=2.537, loss_scale=4, train_wall=11, gb_free=19.3, wall=16278
2022-12-10 00:32:29 | INFO | train_inner | epoch 248:    122 / 1102 loss=6.573, nll_loss=2.69, ppl=6.45, wps=32750.7, ups=9.29, wpb=3525.1, bsz=150.9, num_updates=272300, lr=6.06005e-05, gnorm=2.523, loss_scale=4, train_wall=11, gb_free=20, wall=16289
2022-12-10 00:32:39 | INFO | train_inner | epoch 248:    222 / 1102 loss=6.615, nll_loss=2.731, ppl=6.64, wps=32687.5, ups=9.22, wpb=3547.1, bsz=149.4, num_updates=272400, lr=6.05894e-05, gnorm=2.484, loss_scale=4, train_wall=11, gb_free=19.6, wall=16300
2022-12-10 00:32:50 | INFO | train_inner | epoch 248:    322 / 1102 loss=6.623, nll_loss=2.716, ppl=6.57, wps=32740.5, ups=9.3, wpb=3519.7, bsz=129.6, num_updates=272500, lr=6.05783e-05, gnorm=2.516, loss_scale=4, train_wall=11, gb_free=19.3, wall=16311
2022-12-10 00:33:01 | INFO | train_inner | epoch 248:    422 / 1102 loss=6.632, nll_loss=2.739, ppl=6.68, wps=32752.2, ups=9.18, wpb=3567.4, bsz=141.4, num_updates=272600, lr=6.05671e-05, gnorm=2.475, loss_scale=4, train_wall=11, gb_free=19.5, wall=16322
2022-12-10 00:33:12 | INFO | train_inner | epoch 248:    522 / 1102 loss=6.653, nll_loss=2.782, ppl=6.88, wps=33094, ups=9.07, wpb=3649.3, bsz=155.1, num_updates=272700, lr=6.0556e-05, gnorm=2.416, loss_scale=4, train_wall=11, gb_free=19.8, wall=16333
2022-12-10 00:33:23 | INFO | train_inner | epoch 248:    622 / 1102 loss=6.68, nll_loss=2.804, ppl=6.98, wps=33667, ups=9.27, wpb=3632.7, bsz=150.7, num_updates=272800, lr=6.05449e-05, gnorm=2.411, loss_scale=4, train_wall=11, gb_free=19.3, wall=16344
2022-12-10 00:33:34 | INFO | train_inner | epoch 248:    722 / 1102 loss=6.596, nll_loss=2.718, ppl=6.58, wps=33254, ups=9.18, wpb=3623.3, bsz=152, num_updates=272900, lr=6.05338e-05, gnorm=2.485, loss_scale=4, train_wall=11, gb_free=19.6, wall=16354
2022-12-10 00:33:45 | INFO | train_inner | epoch 248:    822 / 1102 loss=6.685, nll_loss=2.791, ppl=6.92, wps=32987.1, ups=9.18, wpb=3592.8, bsz=138.8, num_updates=273000, lr=6.05228e-05, gnorm=2.548, loss_scale=4, train_wall=11, gb_free=19.3, wall=16365
2022-12-10 00:33:56 | INFO | train_inner | epoch 248:    922 / 1102 loss=6.679, nll_loss=2.778, ppl=6.86, wps=32893.7, ups=9.27, wpb=3548.2, bsz=132.3, num_updates=273100, lr=6.05117e-05, gnorm=2.525, loss_scale=4, train_wall=11, gb_free=19.4, wall=16376
2022-12-10 00:34:06 | INFO | train_inner | epoch 248:   1022 / 1102 loss=6.695, nll_loss=2.807, ppl=7, wps=33052.7, ups=9.22, wpb=3586.5, bsz=135.4, num_updates=273200, lr=6.05006e-05, gnorm=2.51, loss_scale=4, train_wall=11, gb_free=19.5, wall=16387
2022-12-10 00:34:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:34:55 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 3.662 | nll_loss 2.121 | ppl 4.35 | bleu 37.5 | wps 4544.8 | wpb 2835.3 | bsz 115.6 | num_updates 273280 | best_bleu 37.67
2022-12-10 00:34:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 273280 updates
2022-12-10 00:34:56 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint248.pt
2022-12-10 00:34:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint248.pt (epoch 248 @ 273280 updates, score 37.5) (writing took 1.2459444282576442 seconds)
2022-12-10 00:34:56 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-12-10 00:34:56 | INFO | train | epoch 248 | loss 6.641 | nll_loss 2.756 | ppl 6.76 | wps 24507.6 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 273280 | lr 6.04917e-05 | gnorm 2.48 | loss_scale 4 | train_wall 117 | gb_free 19.6 | wall 16437
2022-12-10 00:34:56 | INFO | fairseq.trainer | begin training epoch 249
2022-12-10 00:34:59 | INFO | train_inner | epoch 249:     20 / 1102 loss=6.594, nll_loss=2.722, ppl=6.6, wps=6909.6, ups=1.91, wpb=3617.8, bsz=155.4, num_updates=273300, lr=6.04895e-05, gnorm=2.419, loss_scale=4, train_wall=11, gb_free=19.4, wall=16439
2022-12-10 00:35:10 | INFO | train_inner | epoch 249:    120 / 1102 loss=6.587, nll_loss=2.711, ppl=6.55, wps=33291.7, ups=9.2, wpb=3617, bsz=146.7, num_updates=273400, lr=6.04785e-05, gnorm=2.309, loss_scale=4, train_wall=11, gb_free=19.6, wall=16450
2022-12-10 00:35:20 | INFO | train_inner | epoch 249:    220 / 1102 loss=6.597, nll_loss=2.703, ppl=6.51, wps=32813.1, ups=9.25, wpb=3548, bsz=140.9, num_updates=273500, lr=6.04674e-05, gnorm=2.424, loss_scale=4, train_wall=11, gb_free=19.5, wall=16461
2022-12-10 00:35:31 | INFO | train_inner | epoch 249:    320 / 1102 loss=6.644, nll_loss=2.762, ppl=6.78, wps=33050.4, ups=9.3, wpb=3554.5, bsz=151, num_updates=273600, lr=6.04564e-05, gnorm=2.432, loss_scale=4, train_wall=11, gb_free=19.5, wall=16472
2022-12-10 00:35:42 | INFO | train_inner | epoch 249:    420 / 1102 loss=6.577, nll_loss=2.687, ppl=6.44, wps=32392, ups=9.1, wpb=3558.3, bsz=147.6, num_updates=273700, lr=6.04453e-05, gnorm=2.537, loss_scale=4, train_wall=11, gb_free=19.5, wall=16483
2022-12-10 00:35:53 | INFO | train_inner | epoch 249:    520 / 1102 loss=6.709, nll_loss=2.814, ppl=7.03, wps=33381, ups=9.25, wpb=3607.1, bsz=129, num_updates=273800, lr=6.04343e-05, gnorm=2.466, loss_scale=4, train_wall=11, gb_free=19.4, wall=16494
2022-12-10 00:36:04 | INFO | train_inner | epoch 249:    620 / 1102 loss=6.63, nll_loss=2.741, ppl=6.69, wps=32970.4, ups=9.22, wpb=3574.4, bsz=145.3, num_updates=273900, lr=6.04232e-05, gnorm=2.605, loss_scale=4, train_wall=11, gb_free=19.8, wall=16504
2022-12-10 00:36:15 | INFO | train_inner | epoch 249:    720 / 1102 loss=6.643, nll_loss=2.753, ppl=6.74, wps=32472.3, ups=9.11, wpb=3565.6, bsz=145.3, num_updates=274000, lr=6.04122e-05, gnorm=2.54, loss_scale=4, train_wall=11, gb_free=19.3, wall=16515
2022-12-10 00:36:26 | INFO | train_inner | epoch 249:    820 / 1102 loss=6.644, nll_loss=2.758, ppl=6.77, wps=32717.6, ups=9.13, wpb=3582, bsz=145.9, num_updates=274100, lr=6.04012e-05, gnorm=2.454, loss_scale=4, train_wall=11, gb_free=19.8, wall=16526
2022-12-10 00:36:37 | INFO | train_inner | epoch 249:    920 / 1102 loss=6.667, nll_loss=2.782, ppl=6.88, wps=32317.3, ups=9.06, wpb=3568, bsz=144.8, num_updates=274200, lr=6.03902e-05, gnorm=2.497, loss_scale=4, train_wall=11, gb_free=19.6, wall=16537
2022-12-10 00:36:48 | INFO | train_inner | epoch 249:   1020 / 1102 loss=6.703, nll_loss=2.828, ppl=7.1, wps=32469.8, ups=8.99, wpb=3611.2, bsz=146.5, num_updates=274300, lr=6.03792e-05, gnorm=2.418, loss_scale=4, train_wall=11, gb_free=19.5, wall=16548
2022-12-10 00:36:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:37:37 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 3.665 | nll_loss 2.126 | ppl 4.37 | bleu 37.55 | wps 4568.1 | wpb 2835.3 | bsz 115.6 | num_updates 274382 | best_bleu 37.67
2022-12-10 00:37:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 274382 updates
2022-12-10 00:37:38 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint249.pt
2022-12-10 00:37:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint249.pt (epoch 249 @ 274382 updates, score 37.55) (writing took 1.2701862575486302 seconds)
2022-12-10 00:37:38 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-12-10 00:37:38 | INFO | train | epoch 249 | loss 6.64 | nll_loss 2.755 | ppl 6.75 | wps 24415.8 | ups 6.81 | wpb 3583.6 | bsz 145.4 | num_updates 274382 | lr 6.03701e-05 | gnorm 2.464 | loss_scale 4 | train_wall 118 | gb_free 19.3 | wall 16599
2022-12-10 00:37:38 | INFO | fairseq.trainer | begin training epoch 250
2022-12-10 00:37:40 | INFO | train_inner | epoch 250:     18 / 1102 loss=6.662, nll_loss=2.788, ppl=6.91, wps=6877.3, ups=1.91, wpb=3604.8, bsz=153.9, num_updates=274400, lr=6.03682e-05, gnorm=2.439, loss_scale=4, train_wall=11, gb_free=19.7, wall=16601
2022-12-10 00:37:51 | INFO | train_inner | epoch 250:    118 / 1102 loss=6.589, nll_loss=2.698, ppl=6.49, wps=33126, ups=9.19, wpb=3603.2, bsz=139.8, num_updates=274500, lr=6.03572e-05, gnorm=2.412, loss_scale=4, train_wall=11, gb_free=19.7, wall=16612
2022-12-10 00:38:02 | INFO | train_inner | epoch 250:    218 / 1102 loss=6.571, nll_loss=2.681, ppl=6.41, wps=32735.8, ups=9.23, wpb=3545.6, bsz=152.2, num_updates=274600, lr=6.03462e-05, gnorm=2.43, loss_scale=4, train_wall=11, gb_free=19.5, wall=16623
2022-12-10 00:38:13 | INFO | train_inner | epoch 250:    318 / 1102 loss=6.608, nll_loss=2.73, ppl=6.64, wps=32870.5, ups=9.18, wpb=3580.3, bsz=146.2, num_updates=274700, lr=6.03352e-05, gnorm=2.425, loss_scale=4, train_wall=11, gb_free=19.4, wall=16633
2022-12-10 00:38:24 | INFO | train_inner | epoch 250:    418 / 1102 loss=6.618, nll_loss=2.743, ppl=6.7, wps=32857.7, ups=9.13, wpb=3597.1, bsz=153.6, num_updates=274800, lr=6.03242e-05, gnorm=2.46, loss_scale=4, train_wall=11, gb_free=19.5, wall=16644
2022-12-10 00:38:34 | INFO | train_inner | epoch 250:    518 / 1102 loss=6.709, nll_loss=2.808, ppl=7, wps=34310, ups=9.56, wpb=3590.3, bsz=131, num_updates=274900, lr=6.03132e-05, gnorm=2.693, loss_scale=4, train_wall=10, gb_free=19.7, wall=16655
2022-12-10 00:38:45 | INFO | train_inner | epoch 250:    618 / 1102 loss=6.631, nll_loss=2.732, ppl=6.64, wps=33094, ups=9.36, wpb=3535.8, bsz=148.6, num_updates=275000, lr=6.03023e-05, gnorm=2.629, loss_scale=4, train_wall=10, gb_free=19.4, wall=16666
2022-12-10 00:38:56 | INFO | train_inner | epoch 250:    718 / 1102 loss=6.732, nll_loss=2.848, ppl=7.2, wps=33923.2, ups=9.28, wpb=3656.6, bsz=137.7, num_updates=275100, lr=6.02913e-05, gnorm=2.405, loss_scale=4, train_wall=11, gb_free=19.3, wall=16676
2022-12-10 00:39:07 | INFO | train_inner | epoch 250:    818 / 1102 loss=6.67, nll_loss=2.777, ppl=6.86, wps=33100.9, ups=9.31, wpb=3555.4, bsz=131.4, num_updates=275200, lr=6.02804e-05, gnorm=2.489, loss_scale=4, train_wall=11, gb_free=20.1, wall=16687
2022-12-10 00:39:17 | INFO | train_inner | epoch 250:    918 / 1102 loss=6.614, nll_loss=2.734, ppl=6.65, wps=33172.3, ups=9.28, wpb=3575.8, bsz=154.9, num_updates=275300, lr=6.02694e-05, gnorm=2.509, loss_scale=4, train_wall=11, gb_free=19.3, wall=16698
2022-12-10 00:39:28 | INFO | train_inner | epoch 250:   1018 / 1102 loss=6.625, nll_loss=2.761, ppl=6.78, wps=34112.8, ups=9.41, wpb=3625.1, bsz=162.2, num_updates=275400, lr=6.02585e-05, gnorm=2.448, loss_scale=4, train_wall=10, gb_free=19.9, wall=16709
2022-12-10 00:39:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:40:16 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 3.663 | nll_loss 2.122 | ppl 4.35 | bleu 37.5 | wps 4627.2 | wpb 2835.3 | bsz 115.6 | num_updates 275484 | best_bleu 37.67
2022-12-10 00:40:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 275484 updates
2022-12-10 00:40:17 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint250.pt
2022-12-10 00:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint250.pt (epoch 250 @ 275484 updates, score 37.5) (writing took 1.2030427269637585 seconds)
2022-12-10 00:40:17 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-12-10 00:40:17 | INFO | train | epoch 250 | loss 6.639 | nll_loss 2.754 | ppl 6.74 | wps 24798.6 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 275484 | lr 6.02493e-05 | gnorm 2.491 | loss_scale 4 | train_wall 116 | gb_free 19.5 | wall 16758
2022-12-10 00:40:17 | INFO | fairseq.trainer | begin training epoch 251
2022-12-10 00:40:19 | INFO | train_inner | epoch 251:     16 / 1102 loss=6.657, nll_loss=2.776, ppl=6.85, wps=6969.4, ups=1.95, wpb=3581.2, bsz=148.5, num_updates=275500, lr=6.02475e-05, gnorm=2.473, loss_scale=4, train_wall=11, gb_free=19.4, wall=16760
2022-12-10 00:40:30 | INFO | train_inner | epoch 251:    116 / 1102 loss=6.594, nll_loss=2.699, ppl=6.49, wps=32698, ups=9.41, wpb=3476, bsz=141, num_updates=275600, lr=6.02366e-05, gnorm=2.535, loss_scale=4, train_wall=10, gb_free=19.4, wall=16771
2022-12-10 00:40:41 | INFO | train_inner | epoch 251:    216 / 1102 loss=6.619, nll_loss=2.739, ppl=6.68, wps=33333.9, ups=9.33, wpb=3574.2, bsz=151.8, num_updates=275700, lr=6.02257e-05, gnorm=2.444, loss_scale=4, train_wall=10, gb_free=19.6, wall=16781
2022-12-10 00:40:51 | INFO | train_inner | epoch 251:    316 / 1102 loss=6.662, nll_loss=2.773, ppl=6.84, wps=33295.5, ups=9.38, wpb=3548.5, bsz=141.3, num_updates=275800, lr=6.02147e-05, gnorm=2.525, loss_scale=4, train_wall=10, gb_free=19.3, wall=16792
2022-12-10 00:41:02 | INFO | train_inner | epoch 251:    416 / 1102 loss=6.574, nll_loss=2.686, ppl=6.43, wps=33176, ups=9.28, wpb=3573.9, bsz=148.9, num_updates=275900, lr=6.02038e-05, gnorm=2.491, loss_scale=4, train_wall=11, gb_free=19.4, wall=16803
2022-12-10 00:41:13 | INFO | train_inner | epoch 251:    516 / 1102 loss=6.661, nll_loss=2.774, ppl=6.84, wps=33019.4, ups=9.21, wpb=3584.9, bsz=141.6, num_updates=276000, lr=6.01929e-05, gnorm=2.508, loss_scale=4, train_wall=11, gb_free=19.6, wall=16814
2022-12-10 00:41:24 | INFO | train_inner | epoch 251:    616 / 1102 loss=6.649, nll_loss=2.773, ppl=6.84, wps=33678.3, ups=9.29, wpb=3625.3, bsz=148, num_updates=276100, lr=6.0182e-05, gnorm=2.416, loss_scale=4, train_wall=11, gb_free=19.3, wall=16824
2022-12-10 00:41:35 | INFO | train_inner | epoch 251:    716 / 1102 loss=6.701, nll_loss=2.791, ppl=6.92, wps=33035.6, ups=9.18, wpb=3597.8, bsz=134.9, num_updates=276200, lr=6.01711e-05, gnorm=2.554, loss_scale=4, train_wall=11, gb_free=19.4, wall=16835
2022-12-10 00:41:46 | INFO | train_inner | epoch 251:    816 / 1102 loss=6.632, nll_loss=2.754, ppl=6.75, wps=33332.7, ups=9.15, wpb=3643.7, bsz=149.6, num_updates=276300, lr=6.01602e-05, gnorm=2.432, loss_scale=4, train_wall=11, gb_free=19.4, wall=16846
2022-12-10 00:41:56 | INFO | train_inner | epoch 251:    916 / 1102 loss=6.637, nll_loss=2.76, ppl=6.77, wps=33373.2, ups=9.26, wpb=3602.3, bsz=144.5, num_updates=276400, lr=6.01494e-05, gnorm=2.436, loss_scale=4, train_wall=11, gb_free=19.6, wall=16857
2022-12-10 00:42:07 | INFO | train_inner | epoch 251:   1016 / 1102 loss=6.673, nll_loss=2.785, ppl=6.89, wps=32892.1, ups=9.16, wpb=3590, bsz=139.5, num_updates=276500, lr=6.01385e-05, gnorm=2.434, loss_scale=4, train_wall=11, gb_free=19.5, wall=16868
2022-12-10 00:42:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:42:57 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 3.666 | nll_loss 2.125 | ppl 4.36 | bleu 37.35 | wps 4507.3 | wpb 2835.3 | bsz 115.6 | num_updates 276586 | best_bleu 37.67
2022-12-10 00:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 276586 updates
2022-12-10 00:42:58 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint251.pt
2022-12-10 00:42:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint251.pt (epoch 251 @ 276586 updates, score 37.35) (writing took 1.2790444670245051 seconds)
2022-12-10 00:42:58 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-12-10 00:42:58 | INFO | train | epoch 251 | loss 6.637 | nll_loss 2.752 | ppl 6.73 | wps 24559.1 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 276586 | lr 6.01291e-05 | gnorm 2.47 | loss_scale 4 | train_wall 117 | gb_free 19.6 | wall 16919
2022-12-10 00:42:58 | INFO | fairseq.trainer | begin training epoch 252
2022-12-10 00:43:00 | INFO | train_inner | epoch 252:     14 / 1102 loss=6.639, nll_loss=2.762, ppl=6.78, wps=6840.3, ups=1.9, wpb=3602.5, bsz=148.2, num_updates=276600, lr=6.01276e-05, gnorm=2.435, loss_scale=4, train_wall=11, gb_free=19.4, wall=16921
2022-12-10 00:43:11 | INFO | train_inner | epoch 252:    114 / 1102 loss=6.594, nll_loss=2.718, ppl=6.58, wps=33190, ups=9.32, wpb=3562.7, bsz=152.6, num_updates=276700, lr=6.01167e-05, gnorm=2.586, loss_scale=4, train_wall=11, gb_free=19.6, wall=16931
2022-12-10 00:43:22 | INFO | train_inner | epoch 252:    214 / 1102 loss=6.613, nll_loss=2.727, ppl=6.62, wps=33080.5, ups=9.21, wpb=3590.6, bsz=149.9, num_updates=276800, lr=6.01059e-05, gnorm=2.549, loss_scale=4, train_wall=11, gb_free=19.6, wall=16942
2022-12-10 00:43:32 | INFO | train_inner | epoch 252:    314 / 1102 loss=6.602, nll_loss=2.703, ppl=6.51, wps=33245.7, ups=9.35, wpb=3556.7, bsz=136.2, num_updates=276900, lr=6.0095e-05, gnorm=2.451, loss_scale=4, train_wall=10, gb_free=19.3, wall=16953
2022-12-10 00:43:43 | INFO | train_inner | epoch 252:    414 / 1102 loss=6.65, nll_loss=2.77, ppl=6.82, wps=33433.8, ups=9.27, wpb=3606.8, bsz=148.4, num_updates=277000, lr=6.00842e-05, gnorm=2.454, loss_scale=4, train_wall=11, gb_free=19.6, wall=16964
2022-12-10 00:43:54 | INFO | train_inner | epoch 252:    514 / 1102 loss=6.597, nll_loss=2.706, ppl=6.53, wps=32447.8, ups=9.12, wpb=3557.1, bsz=147.8, num_updates=277100, lr=6.00733e-05, gnorm=2.456, loss_scale=4, train_wall=11, gb_free=19.7, wall=16975
2022-12-10 00:44:05 | INFO | train_inner | epoch 252:    614 / 1102 loss=6.64, nll_loss=2.747, ppl=6.72, wps=32782.4, ups=9.18, wpb=3572.6, bsz=140, num_updates=277200, lr=6.00625e-05, gnorm=2.544, loss_scale=4, train_wall=11, gb_free=19.8, wall=16985
2022-12-10 00:44:16 | INFO | train_inner | epoch 252:    714 / 1102 loss=6.61, nll_loss=2.737, ppl=6.66, wps=33123, ups=9.08, wpb=3648.3, bsz=157.6, num_updates=277300, lr=6.00517e-05, gnorm=2.374, loss_scale=4, train_wall=11, gb_free=19.3, wall=16996
2022-12-10 00:44:27 | INFO | train_inner | epoch 252:    814 / 1102 loss=6.652, nll_loss=2.747, ppl=6.71, wps=32435, ups=9.32, wpb=3480.7, bsz=137.3, num_updates=277400, lr=6.00408e-05, gnorm=2.601, loss_scale=4, train_wall=10, gb_free=20, wall=17007
2022-12-10 00:44:38 | INFO | train_inner | epoch 252:    914 / 1102 loss=6.695, nll_loss=2.809, ppl=7.01, wps=32560.3, ups=9.05, wpb=3598.2, bsz=141, num_updates=277500, lr=6.003e-05, gnorm=2.455, loss_scale=4, train_wall=11, gb_free=19.4, wall=17018
2022-12-10 00:44:49 | INFO | train_inner | epoch 252:   1014 / 1102 loss=6.681, nll_loss=2.787, ppl=6.9, wps=32805.9, ups=9.14, wpb=3589.6, bsz=135.4, num_updates=277600, lr=6.00192e-05, gnorm=2.602, loss_scale=4, train_wall=11, gb_free=20.1, wall=17029
2022-12-10 00:44:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:45:38 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 3.664 | nll_loss 2.124 | ppl 4.36 | bleu 37.58 | wps 4588.9 | wpb 2835.3 | bsz 115.6 | num_updates 277688 | best_bleu 37.67
2022-12-10 00:45:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 277688 updates
2022-12-10 00:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint252.pt
2022-12-10 00:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint252.pt (epoch 252 @ 277688 updates, score 37.58) (writing took 1.2494177585467696 seconds)
2022-12-10 00:45:39 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-12-10 00:45:39 | INFO | train | epoch 252 | loss 6.635 | nll_loss 2.75 | ppl 6.72 | wps 24534.4 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 277688 | lr 6.00097e-05 | gnorm 2.501 | loss_scale 4 | train_wall 117 | gb_free 19.6 | wall 17080
2022-12-10 00:45:39 | INFO | fairseq.trainer | begin training epoch 253
2022-12-10 00:45:41 | INFO | train_inner | epoch 253:     12 / 1102 loss=6.642, nll_loss=2.781, ppl=6.87, wps=7010.6, ups=1.93, wpb=3641, bsz=156, num_updates=277700, lr=6.00084e-05, gnorm=2.476, loss_scale=4, train_wall=11, gb_free=19.7, wall=17081
2022-12-10 00:45:51 | INFO | train_inner | epoch 253:    112 / 1102 loss=6.625, nll_loss=2.739, ppl=6.68, wps=32963.9, ups=9.18, wpb=3589.2, bsz=142.7, num_updates=277800, lr=5.99976e-05, gnorm=2.495, loss_scale=4, train_wall=11, gb_free=19.4, wall=17092
2022-12-10 00:46:02 | INFO | train_inner | epoch 253:    212 / 1102 loss=6.561, nll_loss=2.697, ppl=6.48, wps=32864.9, ups=9.13, wpb=3600, bsz=163.4, num_updates=277900, lr=5.99868e-05, gnorm=2.357, loss_scale=4, train_wall=11, gb_free=19.5, wall=17103
2022-12-10 00:46:13 | INFO | train_inner | epoch 253:    312 / 1102 loss=6.606, nll_loss=2.719, ppl=6.59, wps=32950.6, ups=9.21, wpb=3575.9, bsz=144.6, num_updates=278000, lr=5.9976e-05, gnorm=2.47, loss_scale=4, train_wall=11, gb_free=19.5, wall=17114
2022-12-10 00:46:24 | INFO | train_inner | epoch 253:    412 / 1102 loss=6.598, nll_loss=2.688, ppl=6.45, wps=32631.5, ups=9.21, wpb=3542.7, bsz=136.6, num_updates=278100, lr=5.99652e-05, gnorm=2.636, loss_scale=4, train_wall=11, gb_free=19.3, wall=17125
2022-12-10 00:46:35 | INFO | train_inner | epoch 253:    512 / 1102 loss=6.661, nll_loss=2.772, ppl=6.83, wps=32987.4, ups=9.17, wpb=3595.8, bsz=137.4, num_updates=278200, lr=5.99545e-05, gnorm=2.539, loss_scale=4, train_wall=11, gb_free=19.6, wall=17136
2022-12-10 00:46:46 | INFO | train_inner | epoch 253:    612 / 1102 loss=6.633, nll_loss=2.759, ppl=6.77, wps=32934.9, ups=9.11, wpb=3616.8, bsz=151.3, num_updates=278300, lr=5.99437e-05, gnorm=2.43, loss_scale=4, train_wall=11, gb_free=19.6, wall=17147
2022-12-10 00:46:57 | INFO | train_inner | epoch 253:    712 / 1102 loss=6.667, nll_loss=2.783, ppl=6.88, wps=32962, ups=9.22, wpb=3576.8, bsz=144.6, num_updates=278400, lr=5.99329e-05, gnorm=2.539, loss_scale=4, train_wall=11, gb_free=19.3, wall=17157
2022-12-10 00:47:08 | INFO | train_inner | epoch 253:    812 / 1102 loss=6.646, nll_loss=2.755, ppl=6.75, wps=32927, ups=9.25, wpb=3560.1, bsz=146.2, num_updates=278500, lr=5.99222e-05, gnorm=2.527, loss_scale=4, train_wall=11, gb_free=19.6, wall=17168
2022-12-10 00:47:19 | INFO | train_inner | epoch 253:    912 / 1102 loss=6.629, nll_loss=2.737, ppl=6.67, wps=32438.5, ups=9.09, wpb=3570.2, bsz=139.3, num_updates=278600, lr=5.99114e-05, gnorm=2.592, loss_scale=4, train_wall=11, gb_free=19.8, wall=17179
2022-12-10 00:47:30 | INFO | train_inner | epoch 253:   1012 / 1102 loss=6.651, nll_loss=2.761, ppl=6.78, wps=32727.7, ups=9.14, wpb=3578.8, bsz=144.9, num_updates=278700, lr=5.99006e-05, gnorm=2.554, loss_scale=4, train_wall=11, gb_free=19.3, wall=17190
2022-12-10 00:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:48:20 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 3.658 | nll_loss 2.121 | ppl 4.35 | bleu 37.59 | wps 4470 | wpb 2835.3 | bsz 115.6 | num_updates 278790 | best_bleu 37.67
2022-12-10 00:48:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 278790 updates
2022-12-10 00:48:21 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint253.pt
2022-12-10 00:48:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint253.pt (epoch 253 @ 278790 updates, score 37.59) (writing took 1.4201559163630009 seconds)
2022-12-10 00:48:21 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-12-10 00:48:21 | INFO | train | epoch 253 | loss 6.634 | nll_loss 2.748 | ppl 6.72 | wps 24326.8 | ups 6.79 | wpb 3583.6 | bsz 145.4 | num_updates 278790 | lr 5.9891e-05 | gnorm 2.512 | loss_scale 4 | train_wall 118 | gb_free 19.4 | wall 17242
2022-12-10 00:48:21 | INFO | fairseq.trainer | begin training epoch 254
2022-12-10 00:48:23 | INFO | train_inner | epoch 254:     10 / 1102 loss=6.659, nll_loss=2.79, ppl=6.92, wps=6778.6, ups=1.88, wpb=3604.2, bsz=153.4, num_updates=278800, lr=5.98899e-05, gnorm=2.439, loss_scale=4, train_wall=11, gb_free=19.3, wall=17243
2022-12-10 00:48:34 | INFO | train_inner | epoch 254:    110 / 1102 loss=6.532, nll_loss=2.651, ppl=6.28, wps=33083.6, ups=9.16, wpb=3612.6, bsz=152.9, num_updates=278900, lr=5.98792e-05, gnorm=2.382, loss_scale=4, train_wall=11, gb_free=19.8, wall=17254
2022-12-10 00:48:45 | INFO | train_inner | epoch 254:    210 / 1102 loss=6.627, nll_loss=2.733, ppl=6.65, wps=32758.2, ups=9.17, wpb=3572.9, bsz=141.9, num_updates=279000, lr=5.98684e-05, gnorm=2.531, loss_scale=4, train_wall=11, gb_free=19.5, wall=17265
2022-12-10 00:48:56 | INFO | train_inner | epoch 254:    310 / 1102 loss=6.642, nll_loss=2.769, ppl=6.82, wps=32624, ups=9.1, wpb=3583.4, bsz=150.5, num_updates=279100, lr=5.98577e-05, gnorm=2.411, loss_scale=4, train_wall=11, gb_free=19.3, wall=17276
2022-12-10 00:49:07 | INFO | train_inner | epoch 254:    410 / 1102 loss=6.683, nll_loss=2.795, ppl=6.94, wps=32165.5, ups=9.09, wpb=3539.6, bsz=135.1, num_updates=279200, lr=5.9847e-05, gnorm=2.649, loss_scale=4, train_wall=11, gb_free=19.4, wall=17287
2022-12-10 00:49:17 | INFO | train_inner | epoch 254:    510 / 1102 loss=6.581, nll_loss=2.7, ppl=6.5, wps=33046.3, ups=9.18, wpb=3601.1, bsz=152.4, num_updates=279300, lr=5.98363e-05, gnorm=2.43, loss_scale=4, train_wall=11, gb_free=19.7, wall=17298
2022-12-10 00:49:28 | INFO | train_inner | epoch 254:    610 / 1102 loss=6.657, nll_loss=2.754, ppl=6.74, wps=32643.8, ups=9.36, wpb=3488.1, bsz=138.9, num_updates=279400, lr=5.98256e-05, gnorm=2.633, loss_scale=4, train_wall=10, gb_free=19.5, wall=17309
2022-12-10 00:49:39 | INFO | train_inner | epoch 254:    710 / 1102 loss=6.67, nll_loss=2.799, ppl=6.96, wps=33429.6, ups=9.15, wpb=3653.8, bsz=152.6, num_updates=279500, lr=5.98149e-05, gnorm=2.444, loss_scale=4, train_wall=11, gb_free=19.4, wall=17320
2022-12-10 00:49:50 | INFO | train_inner | epoch 254:    810 / 1102 loss=6.684, nll_loss=2.779, ppl=6.86, wps=33021.1, ups=9.26, wpb=3567.2, bsz=131.2, num_updates=279600, lr=5.98042e-05, gnorm=2.54, loss_scale=4, train_wall=11, gb_free=19.6, wall=17330
2022-12-10 00:50:01 | INFO | train_inner | epoch 254:    910 / 1102 loss=6.671, nll_loss=2.775, ppl=6.85, wps=33176, ups=9.23, wpb=3594.5, bsz=137.1, num_updates=279700, lr=5.97935e-05, gnorm=2.469, loss_scale=4, train_wall=11, gb_free=19.6, wall=17341
2022-12-10 00:50:12 | INFO | train_inner | epoch 254:   1010 / 1102 loss=6.648, nll_loss=2.751, ppl=6.73, wps=32467.5, ups=9.09, wpb=3570.4, bsz=141.3, num_updates=279800, lr=5.97828e-05, gnorm=2.547, loss_scale=4, train_wall=11, gb_free=19.3, wall=17352
2022-12-10 00:50:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:51:02 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 3.667 | nll_loss 2.128 | ppl 4.37 | bleu 37.33 | wps 4484.7 | wpb 2835.3 | bsz 115.6 | num_updates 279892 | best_bleu 37.67
2022-12-10 00:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 279892 updates
2022-12-10 00:51:03 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint254.pt
2022-12-10 00:51:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint254.pt (epoch 254 @ 279892 updates, score 37.33) (writing took 1.2396762920543551 seconds)
2022-12-10 00:51:04 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-12-10 00:51:04 | INFO | train | epoch 254 | loss 6.633 | nll_loss 2.747 | ppl 6.71 | wps 24341.8 | ups 6.79 | wpb 3583.6 | bsz 145.4 | num_updates 279892 | lr 5.9773e-05 | gnorm 2.495 | loss_scale 4 | train_wall 117 | gb_free 19.5 | wall 17404
2022-12-10 00:51:04 | INFO | fairseq.trainer | begin training epoch 255
2022-12-10 00:51:05 | INFO | train_inner | epoch 255:      8 / 1102 loss=6.589, nll_loss=2.729, ppl=6.63, wps=6871.1, ups=1.89, wpb=3642.4, bsz=163.3, num_updates=279900, lr=5.97721e-05, gnorm=2.432, loss_scale=4, train_wall=11, gb_free=19.3, wall=17405
2022-12-10 00:51:16 | INFO | train_inner | epoch 255:    108 / 1102 loss=6.607, nll_loss=2.718, ppl=6.58, wps=33070.6, ups=9.2, wpb=3595.9, bsz=141.8, num_updates=280000, lr=5.97614e-05, gnorm=2.474, loss_scale=4, train_wall=11, gb_free=19.3, wall=17416
2022-12-10 00:51:27 | INFO | train_inner | epoch 255:    208 / 1102 loss=6.644, nll_loss=2.773, ppl=6.83, wps=33346.7, ups=9.09, wpb=3669.4, bsz=151.4, num_updates=280100, lr=5.97508e-05, gnorm=2.436, loss_scale=4, train_wall=11, gb_free=19.7, wall=17427
2022-12-10 00:51:38 | INFO | train_inner | epoch 255:    308 / 1102 loss=6.547, nll_loss=2.675, ppl=6.39, wps=33059.4, ups=9.14, wpb=3618.9, bsz=165.4, num_updates=280200, lr=5.97401e-05, gnorm=2.429, loss_scale=4, train_wall=11, gb_free=19.3, wall=17438
2022-12-10 00:51:48 | INFO | train_inner | epoch 255:    408 / 1102 loss=6.629, nll_loss=2.728, ppl=6.62, wps=33269.8, ups=9.22, wpb=3609.2, bsz=132.8, num_updates=280300, lr=5.97294e-05, gnorm=2.488, loss_scale=4, train_wall=11, gb_free=19.6, wall=17449
2022-12-10 00:51:59 | INFO | train_inner | epoch 255:    508 / 1102 loss=6.578, nll_loss=2.67, ppl=6.36, wps=31847.4, ups=9.26, wpb=3438.3, bsz=148.5, num_updates=280400, lr=5.97188e-05, gnorm=2.623, loss_scale=4, train_wall=11, gb_free=19.9, wall=17460
2022-12-10 00:52:10 | INFO | train_inner | epoch 255:    608 / 1102 loss=6.674, nll_loss=2.771, ppl=6.83, wps=32652.3, ups=9.25, wpb=3528.7, bsz=126.6, num_updates=280500, lr=5.97081e-05, gnorm=2.612, loss_scale=4, train_wall=11, gb_free=19.5, wall=17471
2022-12-10 00:52:21 | INFO | train_inner | epoch 255:    708 / 1102 loss=6.681, nll_loss=2.801, ppl=6.97, wps=33310.6, ups=9.16, wpb=3638.2, bsz=140.7, num_updates=280600, lr=5.96975e-05, gnorm=2.407, loss_scale=4, train_wall=11, gb_free=19.2, wall=17482
2022-12-10 00:52:32 | INFO | train_inner | epoch 255:    808 / 1102 loss=6.643, nll_loss=2.766, ppl=6.8, wps=32872.1, ups=9.11, wpb=3607.7, bsz=157, num_updates=280700, lr=5.96869e-05, gnorm=2.467, loss_scale=4, train_wall=11, gb_free=19.6, wall=17492
2022-12-10 00:52:43 | INFO | train_inner | epoch 255:    908 / 1102 loss=6.642, nll_loss=2.771, ppl=6.83, wps=33091, ups=9.12, wpb=3626.5, bsz=156.1, num_updates=280800, lr=5.96762e-05, gnorm=2.483, loss_scale=4, train_wall=11, gb_free=19.4, wall=17503
2022-12-10 00:52:54 | INFO | train_inner | epoch 255:   1008 / 1102 loss=6.68, nll_loss=2.762, ppl=6.78, wps=31772.2, ups=9.06, wpb=3505.9, bsz=125.4, num_updates=280900, lr=5.96656e-05, gnorm=2.68, loss_scale=4, train_wall=11, gb_free=19.8, wall=17514
2022-12-10 00:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:53:41 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 3.662 | nll_loss 2.125 | ppl 4.36 | bleu 37.62 | wps 4894.6 | wpb 2835.3 | bsz 115.6 | num_updates 280994 | best_bleu 37.67
2022-12-10 00:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 280994 updates
2022-12-10 00:53:42 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint255.pt
2022-12-10 00:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint255.pt (epoch 255 @ 280994 updates, score 37.62) (writing took 1.3590137520805001 seconds)
2022-12-10 00:53:43 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-12-10 00:53:43 | INFO | train | epoch 255 | loss 6.632 | nll_loss 2.745 | ppl 6.7 | wps 24807.7 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 280994 | lr 5.96556e-05 | gnorm 2.509 | loss_scale 4 | train_wall 118 | gb_free 19.5 | wall 17563
2022-12-10 00:53:43 | INFO | fairseq.trainer | begin training epoch 256
2022-12-10 00:53:44 | INFO | train_inner | epoch 256:      6 / 1102 loss=6.617, nll_loss=2.751, ppl=6.73, wps=7195.2, ups=2.01, wpb=3588.1, bsz=159.2, num_updates=281000, lr=5.9655e-05, gnorm=2.482, loss_scale=4, train_wall=11, gb_free=19.4, wall=17564
2022-12-10 00:53:55 | INFO | train_inner | epoch 256:    106 / 1102 loss=6.559, nll_loss=2.683, ppl=6.42, wps=33244.2, ups=9.15, wpb=3632.1, bsz=156.8, num_updates=281100, lr=5.96444e-05, gnorm=2.358, loss_scale=4, train_wall=11, gb_free=19.7, wall=17575
2022-12-10 00:54:06 | INFO | train_inner | epoch 256:    206 / 1102 loss=6.569, nll_loss=2.687, ppl=6.44, wps=32483.1, ups=9.17, wpb=3542.7, bsz=150.9, num_updates=281200, lr=5.96338e-05, gnorm=2.485, loss_scale=4, train_wall=11, gb_free=19.4, wall=17586
2022-12-10 00:54:17 | INFO | train_inner | epoch 256:    306 / 1102 loss=6.637, nll_loss=2.751, ppl=6.73, wps=32984.9, ups=9.13, wpb=3612.1, bsz=140.6, num_updates=281300, lr=5.96232e-05, gnorm=2.381, loss_scale=4, train_wall=11, gb_free=19.3, wall=17597
2022-12-10 00:54:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-12-10 00:54:28 | INFO | train_inner | epoch 256:    407 / 1102 loss=6.555, nll_loss=2.682, ppl=6.42, wps=32283.6, ups=9.03, wpb=3575.1, bsz=164.5, num_updates=281400, lr=5.96126e-05, gnorm=2.464, loss_scale=2, train_wall=11, gb_free=19.3, wall=17608
2022-12-10 00:54:39 | INFO | train_inner | epoch 256:    507 / 1102 loss=6.628, nll_loss=2.733, ppl=6.65, wps=32613.4, ups=9.15, wpb=3565.8, bsz=142.2, num_updates=281500, lr=5.9602e-05, gnorm=2.57, loss_scale=2, train_wall=11, gb_free=19.4, wall=17619
2022-12-10 00:54:49 | INFO | train_inner | epoch 256:    607 / 1102 loss=6.62, nll_loss=2.73, ppl=6.63, wps=32354, ups=9.14, wpb=3539.9, bsz=153, num_updates=281600, lr=5.95914e-05, gnorm=2.603, loss_scale=2, train_wall=11, gb_free=19.4, wall=17630
2022-12-10 00:55:00 | INFO | train_inner | epoch 256:    707 / 1102 loss=6.626, nll_loss=2.735, ppl=6.66, wps=32773.2, ups=9.14, wpb=3585, bsz=142, num_updates=281700, lr=5.95808e-05, gnorm=2.459, loss_scale=2, train_wall=11, gb_free=19.7, wall=17641
2022-12-10 00:55:11 | INFO | train_inner | epoch 256:    807 / 1102 loss=6.667, nll_loss=2.777, ppl=6.85, wps=33028.9, ups=9.17, wpb=3603.7, bsz=139, num_updates=281800, lr=5.95703e-05, gnorm=2.525, loss_scale=2, train_wall=11, gb_free=19.3, wall=17652
2022-12-10 00:55:22 | INFO | train_inner | epoch 256:    907 / 1102 loss=6.738, nll_loss=2.83, ppl=7.11, wps=32748.5, ups=9.21, wpb=3556.4, bsz=124.6, num_updates=281900, lr=5.95597e-05, gnorm=2.567, loss_scale=2, train_wall=11, gb_free=19.5, wall=17663
2022-12-10 00:55:33 | INFO | train_inner | epoch 256:   1007 / 1102 loss=6.714, nll_loss=2.827, ppl=7.1, wps=33608.2, ups=9.22, wpb=3643.7, bsz=133.8, num_updates=282000, lr=5.95491e-05, gnorm=2.528, loss_scale=2, train_wall=11, gb_free=19.4, wall=17674
2022-12-10 00:55:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:56:23 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 3.664 | nll_loss 2.122 | ppl 4.35 | bleu 37.53 | wps 4610.5 | wpb 2835.3 | bsz 115.6 | num_updates 282095 | best_bleu 37.67
2022-12-10 00:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 282095 updates
2022-12-10 00:56:24 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint256.pt
2022-12-10 00:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint256.pt (epoch 256 @ 282095 updates, score 37.53) (writing took 1.1099566090852022 seconds)
2022-12-10 00:56:24 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-12-10 00:56:24 | INFO | train | epoch 256 | loss 6.63 | nll_loss 2.743 | ppl 6.7 | wps 24491.7 | ups 6.83 | wpb 3583.6 | bsz 145.5 | num_updates 282095 | lr 5.95391e-05 | gnorm 2.498 | loss_scale 2 | train_wall 118 | gb_free 19.5 | wall 17724
2022-12-10 00:56:24 | INFO | fairseq.trainer | begin training epoch 257
2022-12-10 00:56:25 | INFO | train_inner | epoch 257:      5 / 1102 loss=6.615, nll_loss=2.736, ppl=6.66, wps=6898.4, ups=1.94, wpb=3556.4, bsz=154.6, num_updates=282100, lr=5.95386e-05, gnorm=2.535, loss_scale=2, train_wall=11, gb_free=19.5, wall=17725
2022-12-10 00:56:36 | INFO | train_inner | epoch 257:    105 / 1102 loss=6.551, nll_loss=2.659, ppl=6.32, wps=33222.9, ups=9.16, wpb=3627.2, bsz=146.6, num_updates=282200, lr=5.9528e-05, gnorm=2.523, loss_scale=2, train_wall=11, gb_free=19.4, wall=17736
2022-12-10 00:56:46 | INFO | train_inner | epoch 257:    205 / 1102 loss=6.59, nll_loss=2.704, ppl=6.52, wps=32974.6, ups=9.27, wpb=3558.4, bsz=149, num_updates=282300, lr=5.95175e-05, gnorm=2.464, loss_scale=2, train_wall=11, gb_free=19.3, wall=17747
2022-12-10 00:56:57 | INFO | train_inner | epoch 257:    305 / 1102 loss=6.65, nll_loss=2.76, ppl=6.77, wps=32865, ups=9.14, wpb=3596.3, bsz=140.6, num_updates=282400, lr=5.95069e-05, gnorm=2.522, loss_scale=2, train_wall=11, gb_free=19.4, wall=17758
2022-12-10 00:57:08 | INFO | train_inner | epoch 257:    405 / 1102 loss=6.631, nll_loss=2.749, ppl=6.72, wps=33373.9, ups=9.15, wpb=3649.3, bsz=147, num_updates=282500, lr=5.94964e-05, gnorm=2.474, loss_scale=2, train_wall=11, gb_free=19.7, wall=17769
2022-12-10 00:57:19 | INFO | train_inner | epoch 257:    505 / 1102 loss=6.649, nll_loss=2.766, ppl=6.8, wps=33269.2, ups=9.24, wpb=3601.8, bsz=142, num_updates=282600, lr=5.94859e-05, gnorm=2.426, loss_scale=2, train_wall=11, gb_free=19.3, wall=17780
2022-12-10 00:57:30 | INFO | train_inner | epoch 257:    605 / 1102 loss=6.621, nll_loss=2.73, ppl=6.63, wps=32736.7, ups=9.27, wpb=3531.7, bsz=143.5, num_updates=282700, lr=5.94754e-05, gnorm=2.491, loss_scale=2, train_wall=11, gb_free=19.2, wall=17790
2022-12-10 00:57:41 | INFO | train_inner | epoch 257:    705 / 1102 loss=6.62, nll_loss=2.715, ppl=6.57, wps=32843.5, ups=9.2, wpb=3571.6, bsz=136.5, num_updates=282800, lr=5.94648e-05, gnorm=2.526, loss_scale=2, train_wall=11, gb_free=19.5, wall=17801
2022-12-10 00:57:52 | INFO | train_inner | epoch 257:    805 / 1102 loss=6.655, nll_loss=2.76, ppl=6.77, wps=32947.4, ups=9.24, wpb=3566.1, bsz=145, num_updates=282900, lr=5.94543e-05, gnorm=2.515, loss_scale=2, train_wall=11, gb_free=19.5, wall=17812
2022-12-10 00:58:02 | INFO | train_inner | epoch 257:    905 / 1102 loss=6.708, nll_loss=2.821, ppl=7.07, wps=32420.7, ups=9.1, wpb=3562.9, bsz=142, num_updates=283000, lr=5.94438e-05, gnorm=2.526, loss_scale=2, train_wall=11, gb_free=19.5, wall=17823
2022-12-10 00:58:14 | INFO | train_inner | epoch 257:   1005 / 1102 loss=6.651, nll_loss=2.771, ppl=6.82, wps=32961, ups=9.07, wpb=3634.3, bsz=149, num_updates=283100, lr=5.94333e-05, gnorm=2.48, loss_scale=2, train_wall=11, gb_free=19.5, wall=17834
2022-12-10 00:58:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 00:59:03 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 3.667 | nll_loss 2.125 | ppl 4.36 | bleu 37.64 | wps 4659.8 | wpb 2835.3 | bsz 115.6 | num_updates 283197 | best_bleu 37.67
2022-12-10 00:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 283197 updates
2022-12-10 00:59:04 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint257.pt
2022-12-10 00:59:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint257.pt (epoch 257 @ 283197 updates, score 37.64) (writing took 1.2476376434788108 seconds)
2022-12-10 00:59:04 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-12-10 00:59:04 | INFO | train | epoch 257 | loss 6.63 | nll_loss 2.742 | ppl 6.69 | wps 24654 | ups 6.88 | wpb 3583.6 | bsz 145.4 | num_updates 283197 | lr 5.94232e-05 | gnorm 2.507 | loss_scale 2 | train_wall 117 | gb_free 19.7 | wall 17885
2022-12-10 00:59:04 | INFO | fairseq.trainer | begin training epoch 258
2022-12-10 00:59:05 | INFO | train_inner | epoch 258:      3 / 1102 loss=6.608, nll_loss=2.721, ppl=6.59, wps=6901.8, ups=1.96, wpb=3524.6, bsz=151.7, num_updates=283200, lr=5.94228e-05, gnorm=2.644, loss_scale=2, train_wall=11, gb_free=19.3, wall=17885
2022-12-10 00:59:15 | INFO | train_inner | epoch 258:    103 / 1102 loss=6.568, nll_loss=2.687, ppl=6.44, wps=33071.3, ups=9.19, wpb=3599.8, bsz=151.6, num_updates=283300, lr=5.94123e-05, gnorm=2.428, loss_scale=2, train_wall=11, gb_free=19.4, wall=17896
2022-12-10 00:59:26 | INFO | train_inner | epoch 258:    203 / 1102 loss=6.558, nll_loss=2.681, ppl=6.41, wps=32776, ups=9.18, wpb=3571.2, bsz=151.4, num_updates=283400, lr=5.94019e-05, gnorm=2.346, loss_scale=2, train_wall=11, gb_free=19.7, wall=17907
2022-12-10 00:59:37 | INFO | train_inner | epoch 258:    303 / 1102 loss=6.629, nll_loss=2.738, ppl=6.67, wps=33095.3, ups=9.23, wpb=3587.1, bsz=140.8, num_updates=283500, lr=5.93914e-05, gnorm=2.506, loss_scale=2, train_wall=11, gb_free=19.5, wall=17918
2022-12-10 00:59:48 | INFO | train_inner | epoch 258:    403 / 1102 loss=6.615, nll_loss=2.738, ppl=6.67, wps=32968.8, ups=9.24, wpb=3566.6, bsz=151.2, num_updates=283600, lr=5.93809e-05, gnorm=2.468, loss_scale=2, train_wall=11, gb_free=19.4, wall=17929
2022-12-10 00:59:59 | INFO | train_inner | epoch 258:    503 / 1102 loss=6.612, nll_loss=2.715, ppl=6.56, wps=33052.4, ups=9.31, wpb=3551.7, bsz=149, num_updates=283700, lr=5.93704e-05, gnorm=2.51, loss_scale=2, train_wall=10, gb_free=19.5, wall=17939
2022-12-10 01:00:10 | INFO | train_inner | epoch 258:    603 / 1102 loss=6.66, nll_loss=2.771, ppl=6.83, wps=33047.3, ups=9.22, wpb=3583.9, bsz=134.2, num_updates=283800, lr=5.936e-05, gnorm=2.454, loss_scale=2, train_wall=11, gb_free=19.6, wall=17950
2022-12-10 01:00:20 | INFO | train_inner | epoch 258:    703 / 1102 loss=6.66, nll_loss=2.757, ppl=6.76, wps=33212.6, ups=9.33, wpb=3559.2, bsz=131.4, num_updates=283900, lr=5.93495e-05, gnorm=2.569, loss_scale=2, train_wall=10, gb_free=19.5, wall=17961
2022-12-10 01:00:31 | INFO | train_inner | epoch 258:    803 / 1102 loss=6.666, nll_loss=2.776, ppl=6.85, wps=32915.8, ups=9.11, wpb=3613.8, bsz=142.2, num_updates=284000, lr=5.93391e-05, gnorm=2.518, loss_scale=2, train_wall=11, gb_free=19.7, wall=17972
2022-12-10 01:00:42 | INFO | train_inner | epoch 258:    903 / 1102 loss=6.638, nll_loss=2.756, ppl=6.75, wps=32815.9, ups=9.17, wpb=3577.6, bsz=150.2, num_updates=284100, lr=5.93286e-05, gnorm=2.53, loss_scale=2, train_wall=11, gb_free=19.6, wall=17983
2022-12-10 01:00:53 | INFO | train_inner | epoch 258:   1003 / 1102 loss=6.674, nll_loss=2.776, ppl=6.85, wps=32534.8, ups=8.93, wpb=3644.8, bsz=138.4, num_updates=284200, lr=5.93182e-05, gnorm=2.543, loss_scale=2, train_wall=11, gb_free=19.8, wall=17994
2022-12-10 01:01:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:01:44 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 3.665 | nll_loss 2.127 | ppl 4.37 | bleu 37.53 | wps 4525.6 | wpb 2835.3 | bsz 115.6 | num_updates 284299 | best_bleu 37.67
2022-12-10 01:01:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 284299 updates
2022-12-10 01:01:45 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint258.pt
2022-12-10 01:01:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint258.pt (epoch 258 @ 284299 updates, score 37.53) (writing took 1.2299743629992008 seconds)
2022-12-10 01:01:45 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-12-10 01:01:45 | INFO | train | epoch 258 | loss 6.628 | nll_loss 2.741 | ppl 6.68 | wps 24469.4 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 284299 | lr 5.93079e-05 | gnorm 2.485 | loss_scale 2 | train_wall 117 | gb_free 19.6 | wall 18046
2022-12-10 01:01:45 | INFO | fairseq.trainer | begin training epoch 259
2022-12-10 01:01:46 | INFO | train_inner | epoch 259:      1 / 1102 loss=6.631, nll_loss=2.76, ppl=6.77, wps=6795, ups=1.91, wpb=3555.2, bsz=158.8, num_updates=284300, lr=5.93078e-05, gnorm=2.468, loss_scale=2, train_wall=11, gb_free=19.5, wall=18046
2022-12-10 01:01:56 | INFO | train_inner | epoch 259:    101 / 1102 loss=6.684, nll_loss=2.778, ppl=6.86, wps=33177.9, ups=9.35, wpb=3548.8, bsz=124.1, num_updates=284400, lr=5.92973e-05, gnorm=2.613, loss_scale=2, train_wall=10, gb_free=19.6, wall=18057
2022-12-10 01:02:07 | INFO | train_inner | epoch 259:    201 / 1102 loss=6.566, nll_loss=2.684, ppl=6.43, wps=32467.6, ups=9.11, wpb=3564.2, bsz=150.8, num_updates=284500, lr=5.92869e-05, gnorm=2.482, loss_scale=2, train_wall=11, gb_free=19.6, wall=18068
2022-12-10 01:02:18 | INFO | train_inner | epoch 259:    301 / 1102 loss=6.623, nll_loss=2.727, ppl=6.62, wps=33224.2, ups=9.19, wpb=3616.4, bsz=143.4, num_updates=284600, lr=5.92765e-05, gnorm=2.459, loss_scale=2, train_wall=11, gb_free=19.4, wall=18079
2022-12-10 01:02:29 | INFO | train_inner | epoch 259:    401 / 1102 loss=6.604, nll_loss=2.721, ppl=6.59, wps=33155.5, ups=9.12, wpb=3636.2, bsz=151.7, num_updates=284700, lr=5.92661e-05, gnorm=2.521, loss_scale=2, train_wall=11, gb_free=19.6, wall=18090
2022-12-10 01:02:40 | INFO | train_inner | epoch 259:    501 / 1102 loss=6.646, nll_loss=2.762, ppl=6.78, wps=32830.2, ups=9.17, wpb=3578.6, bsz=146.6, num_updates=284800, lr=5.92557e-05, gnorm=2.516, loss_scale=2, train_wall=11, gb_free=19.9, wall=18101
2022-12-10 01:02:51 | INFO | train_inner | epoch 259:    601 / 1102 loss=6.607, nll_loss=2.719, ppl=6.58, wps=32615.9, ups=9.07, wpb=3595.6, bsz=151.8, num_updates=284900, lr=5.92453e-05, gnorm=2.502, loss_scale=2, train_wall=11, gb_free=19.6, wall=18112
2022-12-10 01:03:02 | INFO | train_inner | epoch 259:    701 / 1102 loss=6.648, nll_loss=2.755, ppl=6.75, wps=32486.5, ups=9.07, wpb=3580.3, bsz=137.1, num_updates=285000, lr=5.92349e-05, gnorm=2.524, loss_scale=2, train_wall=11, gb_free=19.5, wall=18123
2022-12-10 01:03:13 | INFO | train_inner | epoch 259:    801 / 1102 loss=6.607, nll_loss=2.73, ppl=6.63, wps=32758.6, ups=9.11, wpb=3594.5, bsz=150.5, num_updates=285100, lr=5.92245e-05, gnorm=2.448, loss_scale=2, train_wall=11, gb_free=19.3, wall=18134
2022-12-10 01:03:24 | INFO | train_inner | epoch 259:    901 / 1102 loss=6.595, nll_loss=2.722, ppl=6.6, wps=32820.2, ups=9.14, wpb=3590.2, bsz=162.4, num_updates=285200, lr=5.92141e-05, gnorm=2.413, loss_scale=2, train_wall=11, gb_free=19.6, wall=18145
2022-12-10 01:03:35 | INFO | train_inner | epoch 259:   1001 / 1102 loss=6.662, nll_loss=2.768, ppl=6.81, wps=32570, ups=9.23, wpb=3527.7, bsz=140.2, num_updates=285300, lr=5.92037e-05, gnorm=2.573, loss_scale=2, train_wall=11, gb_free=19.7, wall=18156
2022-12-10 01:03:46 | INFO | train_inner | epoch 259:   1101 / 1102 loss=6.645, nll_loss=2.753, ppl=6.74, wps=32737.5, ups=9.13, wpb=3584.7, bsz=139.6, num_updates=285400, lr=5.91934e-05, gnorm=2.548, loss_scale=2, train_wall=11, gb_free=19.3, wall=18166
2022-12-10 01:03:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:04:25 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 3.664 | nll_loss 2.127 | ppl 4.37 | bleu 37.57 | wps 4677.4 | wpb 2835.3 | bsz 115.6 | num_updates 285401 | best_bleu 37.67
2022-12-10 01:04:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 285401 updates
2022-12-10 01:04:26 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint259.pt
2022-12-10 01:04:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint259.pt (epoch 259 @ 285401 updates, score 37.57) (writing took 1.2065769648179412 seconds)
2022-12-10 01:04:26 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-12-10 01:04:26 | INFO | train | epoch 259 | loss 6.626 | nll_loss 2.738 | ppl 6.67 | wps 24595.8 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 285401 | lr 5.91933e-05 | gnorm 2.508 | loss_scale 2 | train_wall 118 | gb_free 19.4 | wall 18207
2022-12-10 01:04:26 | INFO | fairseq.trainer | begin training epoch 260
2022-12-10 01:04:37 | INFO | train_inner | epoch 260:     99 / 1102 loss=6.542, nll_loss=2.668, ppl=6.36, wps=7082.9, ups=1.95, wpb=3629.2, bsz=158.2, num_updates=285500, lr=5.9183e-05, gnorm=2.437, loss_scale=2, train_wall=11, gb_free=20.1, wall=18218
2022-12-10 01:04:48 | INFO | train_inner | epoch 260:    199 / 1102 loss=6.607, nll_loss=2.707, ppl=6.53, wps=32991.8, ups=9.17, wpb=3596, bsz=130.2, num_updates=285600, lr=5.91726e-05, gnorm=2.46, loss_scale=2, train_wall=11, gb_free=19.5, wall=18229
2022-12-10 01:04:59 | INFO | train_inner | epoch 260:    299 / 1102 loss=6.573, nll_loss=2.692, ppl=6.46, wps=32968.8, ups=9.16, wpb=3599.8, bsz=150.4, num_updates=285700, lr=5.91623e-05, gnorm=2.494, loss_scale=2, train_wall=11, gb_free=19.4, wall=18240
2022-12-10 01:05:10 | INFO | train_inner | epoch 260:    399 / 1102 loss=6.644, nll_loss=2.745, ppl=6.7, wps=32527.4, ups=9.25, wpb=3516.6, bsz=142.1, num_updates=285800, lr=5.91519e-05, gnorm=2.641, loss_scale=2, train_wall=11, gb_free=19.8, wall=18250
2022-12-10 01:05:21 | INFO | train_inner | epoch 260:    499 / 1102 loss=6.542, nll_loss=2.656, ppl=6.3, wps=32452.4, ups=9.2, wpb=3529.3, bsz=155.8, num_updates=285900, lr=5.91416e-05, gnorm=2.478, loss_scale=2, train_wall=11, gb_free=19.3, wall=18261
2022-12-10 01:05:32 | INFO | train_inner | epoch 260:    599 / 1102 loss=6.671, nll_loss=2.773, ppl=6.84, wps=32881.2, ups=9.2, wpb=3573, bsz=134.3, num_updates=286000, lr=5.91312e-05, gnorm=2.558, loss_scale=2, train_wall=11, gb_free=19.4, wall=18272
2022-12-10 01:05:42 | INFO | train_inner | epoch 260:    699 / 1102 loss=6.554, nll_loss=2.675, ppl=6.38, wps=32688.6, ups=9.15, wpb=3572.2, bsz=161.9, num_updates=286100, lr=5.91209e-05, gnorm=2.608, loss_scale=2, train_wall=11, gb_free=19.4, wall=18283
2022-12-10 01:05:53 | INFO | train_inner | epoch 260:    799 / 1102 loss=6.696, nll_loss=2.813, ppl=7.03, wps=33199.4, ups=9.16, wpb=3625.9, bsz=138.5, num_updates=286200, lr=5.91106e-05, gnorm=2.45, loss_scale=2, train_wall=11, gb_free=20, wall=18294
2022-12-10 01:06:04 | INFO | train_inner | epoch 260:    899 / 1102 loss=6.686, nll_loss=2.811, ppl=7.02, wps=33285.9, ups=9.05, wpb=3676.1, bsz=150.3, num_updates=286300, lr=5.91003e-05, gnorm=2.562, loss_scale=2, train_wall=11, gb_free=19.6, wall=18305
2022-12-10 01:06:15 | INFO | train_inner | epoch 260:    999 / 1102 loss=6.676, nll_loss=2.789, ppl=6.91, wps=32734.3, ups=9.15, wpb=3578.2, bsz=140.5, num_updates=286400, lr=5.90899e-05, gnorm=2.458, loss_scale=2, train_wall=11, gb_free=20, wall=18316
2022-12-10 01:06:26 | INFO | train_inner | epoch 260:   1099 / 1102 loss=6.661, nll_loss=2.771, ppl=6.83, wps=32258, ups=9.14, wpb=3528.8, bsz=141.3, num_updates=286500, lr=5.90796e-05, gnorm=2.528, loss_scale=2, train_wall=11, gb_free=19.3, wall=18327
2022-12-10 01:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:07:04 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 3.669 | nll_loss 2.125 | ppl 4.36 | bleu 37.65 | wps 4847.1 | wpb 2835.3 | bsz 115.6 | num_updates 286503 | best_bleu 37.67
2022-12-10 01:07:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 286503 updates
2022-12-10 01:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint260.pt
2022-12-10 01:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint260.pt (epoch 260 @ 286503 updates, score 37.65) (writing took 1.2372976681217551 seconds)
2022-12-10 01:07:05 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-12-10 01:07:05 | INFO | train | epoch 260 | loss 6.624 | nll_loss 2.737 | ppl 6.67 | wps 24792.3 | ups 6.92 | wpb 3583.6 | bsz 145.4 | num_updates 286503 | lr 5.90793e-05 | gnorm 2.517 | loss_scale 2 | train_wall 118 | gb_free 19.7 | wall 18366
2022-12-10 01:07:05 | INFO | fairseq.trainer | begin training epoch 261
2022-12-10 01:07:16 | INFO | train_inner | epoch 261:     97 / 1102 loss=6.57, nll_loss=2.695, ppl=6.48, wps=7245.3, ups=2.01, wpb=3608.2, bsz=152.6, num_updates=286600, lr=5.90693e-05, gnorm=2.427, loss_scale=2, train_wall=11, gb_free=19.8, wall=18377
2022-12-10 01:07:27 | INFO | train_inner | epoch 261:    197 / 1102 loss=6.584, nll_loss=2.702, ppl=6.51, wps=32570, ups=9.13, wpb=3565.7, bsz=149.2, num_updates=286700, lr=5.9059e-05, gnorm=2.471, loss_scale=2, train_wall=11, gb_free=19.3, wall=18388
2022-12-10 01:07:38 | INFO | train_inner | epoch 261:    297 / 1102 loss=6.668, nll_loss=2.759, ppl=6.77, wps=32481.3, ups=9.19, wpb=3532.6, bsz=130.6, num_updates=286800, lr=5.90487e-05, gnorm=2.626, loss_scale=2, train_wall=11, gb_free=19.4, wall=18399
2022-12-10 01:07:49 | INFO | train_inner | epoch 261:    397 / 1102 loss=6.587, nll_loss=2.695, ppl=6.48, wps=32385.1, ups=9.2, wpb=3521.9, bsz=146.6, num_updates=286900, lr=5.90384e-05, gnorm=2.458, loss_scale=2, train_wall=11, gb_free=19.3, wall=18409
2022-12-10 01:08:00 | INFO | train_inner | epoch 261:    497 / 1102 loss=6.63, nll_loss=2.743, ppl=6.69, wps=33246.6, ups=9.32, wpb=3566.9, bsz=144.3, num_updates=287000, lr=5.90281e-05, gnorm=2.492, loss_scale=2, train_wall=10, gb_free=19.9, wall=18420
2022-12-10 01:08:10 | INFO | train_inner | epoch 261:    597 / 1102 loss=6.64, nll_loss=2.736, ppl=6.66, wps=32937.4, ups=9.17, wpb=3593.4, bsz=131, num_updates=287100, lr=5.90179e-05, gnorm=2.686, loss_scale=2, train_wall=11, gb_free=19.5, wall=18431
2022-12-10 01:08:21 | INFO | train_inner | epoch 261:    697 / 1102 loss=6.62, nll_loss=2.739, ppl=6.67, wps=32827.2, ups=9.07, wpb=3617.9, bsz=149, num_updates=287200, lr=5.90076e-05, gnorm=2.457, loss_scale=2, train_wall=11, gb_free=19.8, wall=18442
2022-12-10 01:08:32 | INFO | train_inner | epoch 261:    797 / 1102 loss=6.662, nll_loss=2.789, ppl=6.91, wps=33702.7, ups=9.17, wpb=3673.8, bsz=154.1, num_updates=287300, lr=5.89973e-05, gnorm=2.652, loss_scale=2, train_wall=11, gb_free=19.8, wall=18453
2022-12-10 01:08:43 | INFO | train_inner | epoch 261:    897 / 1102 loss=6.639, nll_loss=2.751, ppl=6.73, wps=32862.1, ups=9.22, wpb=3562.4, bsz=147, num_updates=287400, lr=5.8987e-05, gnorm=2.482, loss_scale=2, train_wall=11, gb_free=19.5, wall=18464
2022-12-10 01:08:54 | INFO | train_inner | epoch 261:    997 / 1102 loss=6.582, nll_loss=2.697, ppl=6.49, wps=33196.9, ups=9.2, wpb=3609.8, bsz=158.8, num_updates=287500, lr=5.89768e-05, gnorm=2.497, loss_scale=2, train_wall=11, gb_free=19.5, wall=18475
2022-12-10 01:09:05 | INFO | train_inner | epoch 261:   1097 / 1102 loss=6.681, nll_loss=2.771, ppl=6.83, wps=32682, ups=9.16, wpb=3568.5, bsz=131.8, num_updates=287600, lr=5.89665e-05, gnorm=2.738, loss_scale=2, train_wall=11, gb_free=19.4, wall=18486
2022-12-10 01:09:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:09:45 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 3.663 | nll_loss 2.126 | ppl 4.37 | bleu 37.72 | wps 4619 | wpb 2835.3 | bsz 115.6 | num_updates 287605 | best_bleu 37.72
2022-12-10 01:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 287605 updates
2022-12-10 01:09:46 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint261.pt
2022-12-10 01:09:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint261.pt (epoch 261 @ 287605 updates, score 37.72) (writing took 1.6970226243138313 seconds)
2022-12-10 01:09:46 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-12-10 01:09:46 | INFO | train | epoch 261 | loss 6.624 | nll_loss 2.735 | ppl 6.66 | wps 24495.2 | ups 6.84 | wpb 3583.6 | bsz 145.4 | num_updates 287605 | lr 5.8966e-05 | gnorm 2.543 | loss_scale 2 | train_wall 117 | gb_free 20 | wall 18527
2022-12-10 01:09:47 | INFO | fairseq.trainer | begin training epoch 262
2022-12-10 01:09:57 | INFO | train_inner | epoch 262:     95 / 1102 loss=6.615, nll_loss=2.734, ppl=6.65, wps=6915.2, ups=1.92, wpb=3595.5, bsz=144.3, num_updates=287700, lr=5.89563e-05, gnorm=2.453, loss_scale=2, train_wall=11, gb_free=19.5, wall=18538
2022-12-10 01:10:08 | INFO | train_inner | epoch 262:    195 / 1102 loss=6.548, nll_loss=2.663, ppl=6.33, wps=32266.6, ups=8.99, wpb=3590.7, bsz=152.2, num_updates=287800, lr=5.8946e-05, gnorm=2.445, loss_scale=2, train_wall=11, gb_free=19.6, wall=18549
2022-12-10 01:10:19 | INFO | train_inner | epoch 262:    295 / 1102 loss=6.582, nll_loss=2.689, ppl=6.45, wps=32844.3, ups=9.17, wpb=3582.8, bsz=148.6, num_updates=287900, lr=5.89358e-05, gnorm=2.509, loss_scale=2, train_wall=11, gb_free=19.5, wall=18560
2022-12-10 01:10:30 | INFO | train_inner | epoch 262:    395 / 1102 loss=6.637, nll_loss=2.745, ppl=6.7, wps=33243.3, ups=9.17, wpb=3626.2, bsz=143.2, num_updates=288000, lr=5.89256e-05, gnorm=2.472, loss_scale=2, train_wall=11, gb_free=19.5, wall=18571
2022-12-10 01:10:41 | INFO | train_inner | epoch 262:    495 / 1102 loss=6.666, nll_loss=2.761, ppl=6.78, wps=32943.6, ups=9.13, wpb=3610.1, bsz=129.1, num_updates=288100, lr=5.89153e-05, gnorm=2.555, loss_scale=2, train_wall=11, gb_free=19.4, wall=18581
2022-12-10 01:10:52 | INFO | train_inner | epoch 262:    595 / 1102 loss=6.607, nll_loss=2.698, ppl=6.49, wps=32752.2, ups=9.27, wpb=3534.2, bsz=133.7, num_updates=288200, lr=5.89051e-05, gnorm=2.54, loss_scale=2, train_wall=11, gb_free=19.4, wall=18592
2022-12-10 01:11:03 | INFO | train_inner | epoch 262:    695 / 1102 loss=6.605, nll_loss=2.73, ppl=6.64, wps=32637.3, ups=9.1, wpb=3587.2, bsz=156.2, num_updates=288300, lr=5.88949e-05, gnorm=2.468, loss_scale=2, train_wall=11, gb_free=19.7, wall=18603
2022-12-10 01:11:14 | INFO | train_inner | epoch 262:    795 / 1102 loss=6.658, nll_loss=2.779, ppl=6.86, wps=32630.4, ups=9.1, wpb=3584.2, bsz=150.6, num_updates=288400, lr=5.88847e-05, gnorm=2.53, loss_scale=2, train_wall=11, gb_free=19.6, wall=18614
2022-12-10 01:11:25 | INFO | train_inner | epoch 262:    895 / 1102 loss=6.579, nll_loss=2.683, ppl=6.42, wps=32131.5, ups=9.14, wpb=3516.8, bsz=144.7, num_updates=288500, lr=5.88745e-05, gnorm=2.551, loss_scale=2, train_wall=11, gb_free=19.8, wall=18625
2022-12-10 01:11:36 | INFO | train_inner | epoch 262:    995 / 1102 loss=6.644, nll_loss=2.776, ppl=6.85, wps=33030.4, ups=9.08, wpb=3638.4, bsz=155.3, num_updates=288600, lr=5.88643e-05, gnorm=2.42, loss_scale=2, train_wall=11, gb_free=19.5, wall=18636
2022-12-10 01:11:47 | INFO | train_inner | epoch 262:   1095 / 1102 loss=6.697, nll_loss=2.821, ppl=7.07, wps=32715.3, ups=9.17, wpb=3568.7, bsz=148.6, num_updates=288700, lr=5.88541e-05, gnorm=2.602, loss_scale=2, train_wall=11, gb_free=19.7, wall=18647
2022-12-10 01:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:12:26 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 3.669 | nll_loss 2.127 | ppl 4.37 | bleu 37.44 | wps 4710.9 | wpb 2835.3 | bsz 115.6 | num_updates 288707 | best_bleu 37.72
2022-12-10 01:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 288707 updates
2022-12-10 01:12:27 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint262.pt
2022-12-10 01:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint262.pt (epoch 262 @ 288707 updates, score 37.44) (writing took 1.5596728743985295 seconds)
2022-12-10 01:12:27 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-12-10 01:12:27 | INFO | train | epoch 262 | loss 6.623 | nll_loss 2.735 | ppl 6.66 | wps 24564 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 288707 | lr 5.88534e-05 | gnorm 2.509 | loss_scale 2 | train_wall 118 | gb_free 19.7 | wall 18688
2022-12-10 01:12:27 | INFO | fairseq.trainer | begin training epoch 263
2022-12-10 01:12:38 | INFO | train_inner | epoch 263:     93 / 1102 loss=6.615, nll_loss=2.711, ppl=6.55, wps=6972, ups=1.95, wpb=3567.8, bsz=132.9, num_updates=288800, lr=5.88439e-05, gnorm=2.551, loss_scale=2, train_wall=11, gb_free=19.5, wall=18698
2022-12-10 01:12:49 | INFO | train_inner | epoch 263:    193 / 1102 loss=6.614, nll_loss=2.712, ppl=6.55, wps=32500.3, ups=9.22, wpb=3525.8, bsz=133.6, num_updates=288900, lr=5.88337e-05, gnorm=2.585, loss_scale=2, train_wall=11, gb_free=19.2, wall=18709
2022-12-10 01:12:59 | INFO | train_inner | epoch 263:    293 / 1102 loss=6.542, nll_loss=2.677, ppl=6.4, wps=32797.2, ups=9.16, wpb=3580.8, bsz=173, num_updates=289000, lr=5.88235e-05, gnorm=2.563, loss_scale=2, train_wall=11, gb_free=19.3, wall=18720
2022-12-10 01:13:10 | INFO | train_inner | epoch 263:    393 / 1102 loss=6.6, nll_loss=2.719, ppl=6.59, wps=33180.1, ups=9.18, wpb=3612.4, bsz=147.6, num_updates=289100, lr=5.88134e-05, gnorm=2.477, loss_scale=2, train_wall=11, gb_free=19.7, wall=18731
2022-12-10 01:13:21 | INFO | train_inner | epoch 263:    493 / 1102 loss=6.612, nll_loss=2.723, ppl=6.6, wps=32879, ups=9.16, wpb=3589.4, bsz=152.5, num_updates=289200, lr=5.88032e-05, gnorm=2.487, loss_scale=2, train_wall=11, gb_free=19.3, wall=18742
2022-12-10 01:13:32 | INFO | train_inner | epoch 263:    593 / 1102 loss=6.683, nll_loss=2.79, ppl=6.91, wps=32726, ups=9.17, wpb=3569.8, bsz=138.3, num_updates=289300, lr=5.8793e-05, gnorm=2.544, loss_scale=2, train_wall=11, gb_free=19.3, wall=18753
2022-12-10 01:13:43 | INFO | train_inner | epoch 263:    693 / 1102 loss=6.647, nll_loss=2.758, ppl=6.77, wps=33185.8, ups=9.25, wpb=3587, bsz=141.6, num_updates=289400, lr=5.87829e-05, gnorm=2.576, loss_scale=2, train_wall=11, gb_free=19.6, wall=18764
2022-12-10 01:13:54 | INFO | train_inner | epoch 263:    793 / 1102 loss=6.607, nll_loss=2.716, ppl=6.57, wps=32611.4, ups=9.09, wpb=3586.8, bsz=146.3, num_updates=289500, lr=5.87727e-05, gnorm=2.468, loss_scale=2, train_wall=11, gb_free=19.3, wall=18775
2022-12-10 01:14:05 | INFO | train_inner | epoch 263:    893 / 1102 loss=6.59, nll_loss=2.707, ppl=6.53, wps=32675.5, ups=9.06, wpb=3605.1, bsz=147.4, num_updates=289600, lr=5.87626e-05, gnorm=2.508, loss_scale=2, train_wall=11, gb_free=19.4, wall=18786
2022-12-10 01:14:16 | INFO | train_inner | epoch 263:    993 / 1102 loss=6.665, nll_loss=2.778, ppl=6.86, wps=33026.5, ups=9.19, wpb=3591.8, bsz=145.8, num_updates=289700, lr=5.87524e-05, gnorm=2.599, loss_scale=2, train_wall=11, gb_free=19.7, wall=18796
2022-12-10 01:14:27 | INFO | train_inner | epoch 263:   1093 / 1102 loss=6.664, nll_loss=2.764, ppl=6.79, wps=32826.7, ups=9.19, wpb=3571.5, bsz=137.4, num_updates=289800, lr=5.87423e-05, gnorm=2.509, loss_scale=2, train_wall=11, gb_free=19.3, wall=18807
2022-12-10 01:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:15:06 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 3.663 | nll_loss 2.125 | ppl 4.36 | bleu 37.68 | wps 4778.4 | wpb 2835.3 | bsz 115.6 | num_updates 289809 | best_bleu 37.72
2022-12-10 01:15:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 289809 updates
2022-12-10 01:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint263.pt
2022-12-10 01:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint263.pt (epoch 263 @ 289809 updates, score 37.68) (writing took 1.737229966558516 seconds)
2022-12-10 01:15:07 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-12-10 01:15:07 | INFO | train | epoch 263 | loss 6.621 | nll_loss 2.732 | ppl 6.65 | wps 24676.3 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 289809 | lr 5.87414e-05 | gnorm 2.528 | loss_scale 2 | train_wall 117 | gb_free 19.3 | wall 18848
2022-12-10 01:15:07 | INFO | fairseq.trainer | begin training epoch 264
2022-12-10 01:15:17 | INFO | train_inner | epoch 264:     91 / 1102 loss=6.575, nll_loss=2.678, ppl=6.4, wps=7051.5, ups=1.98, wpb=3569.8, bsz=140.4, num_updates=289900, lr=5.87321e-05, gnorm=2.464, loss_scale=2, train_wall=11, gb_free=19.6, wall=18858
2022-12-10 01:15:28 | INFO | train_inner | epoch 264:    191 / 1102 loss=6.568, nll_loss=2.696, ppl=6.48, wps=33418.9, ups=9.05, wpb=3694.6, bsz=156.6, num_updates=290000, lr=5.8722e-05, gnorm=2.416, loss_scale=2, train_wall=11, gb_free=19.3, wall=18869
2022-12-10 01:15:39 | INFO | train_inner | epoch 264:    291 / 1102 loss=6.583, nll_loss=2.691, ppl=6.46, wps=32957.8, ups=9.23, wpb=3571.2, bsz=143, num_updates=290100, lr=5.87119e-05, gnorm=2.576, loss_scale=2, train_wall=11, gb_free=19.4, wall=18880
2022-12-10 01:15:50 | INFO | train_inner | epoch 264:    391 / 1102 loss=6.647, nll_loss=2.759, ppl=6.77, wps=32828.9, ups=9.16, wpb=3583.5, bsz=145.7, num_updates=290200, lr=5.87018e-05, gnorm=2.513, loss_scale=2, train_wall=11, gb_free=19.4, wall=18891
2022-12-10 01:16:01 | INFO | train_inner | epoch 264:    491 / 1102 loss=6.634, nll_loss=2.739, ppl=6.67, wps=32530.7, ups=9.16, wpb=3553.3, bsz=138.1, num_updates=290300, lr=5.86917e-05, gnorm=2.497, loss_scale=2, train_wall=11, gb_free=19.6, wall=18902
2022-12-10 01:16:12 | INFO | train_inner | epoch 264:    591 / 1102 loss=6.643, nll_loss=2.755, ppl=6.75, wps=33099.4, ups=9.22, wpb=3590.8, bsz=143.8, num_updates=290400, lr=5.86816e-05, gnorm=2.654, loss_scale=2, train_wall=11, gb_free=19.3, wall=18913
2022-12-10 01:16:23 | INFO | train_inner | epoch 264:    691 / 1102 loss=6.574, nll_loss=2.71, ppl=6.54, wps=32872, ups=9.22, wpb=3563.8, bsz=166.9, num_updates=290500, lr=5.86715e-05, gnorm=2.389, loss_scale=2, train_wall=11, gb_free=19.3, wall=18923
2022-12-10 01:16:34 | INFO | train_inner | epoch 264:    791 / 1102 loss=6.702, nll_loss=2.801, ppl=6.97, wps=32940.4, ups=9.15, wpb=3600.6, bsz=127.7, num_updates=290600, lr=5.86614e-05, gnorm=2.522, loss_scale=2, train_wall=11, gb_free=19.8, wall=18934
2022-12-10 01:16:45 | INFO | train_inner | epoch 264:    891 / 1102 loss=6.663, nll_loss=2.778, ppl=6.86, wps=33120, ups=9.15, wpb=3619.8, bsz=144.5, num_updates=290700, lr=5.86513e-05, gnorm=2.519, loss_scale=2, train_wall=11, gb_free=19.6, wall=18945
2022-12-10 01:16:56 | INFO | train_inner | epoch 264:    991 / 1102 loss=6.624, nll_loss=2.722, ppl=6.6, wps=32495.8, ups=9.13, wpb=3557.9, bsz=141.9, num_updates=290800, lr=5.86412e-05, gnorm=2.603, loss_scale=2, train_wall=11, gb_free=19.3, wall=18956
2022-12-10 01:17:07 | INFO | train_inner | epoch 264:   1091 / 1102 loss=6.635, nll_loss=2.745, ppl=6.7, wps=32537.1, ups=9.15, wpb=3557.7, bsz=148.9, num_updates=290900, lr=5.86311e-05, gnorm=2.631, loss_scale=2, train_wall=11, gb_free=19.4, wall=18967
2022-12-10 01:17:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:17:46 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 3.671 | nll_loss 2.128 | ppl 4.37 | bleu 37.48 | wps 4735.9 | wpb 2835.3 | bsz 115.6 | num_updates 290911 | best_bleu 37.72
2022-12-10 01:17:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 290911 updates
2022-12-10 01:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint264.pt
2022-12-10 01:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint264.pt (epoch 264 @ 290911 updates, score 37.48) (writing took 1.3592248372733593 seconds)
2022-12-10 01:17:47 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-12-10 01:17:47 | INFO | train | epoch 264 | loss 6.62 | nll_loss 2.731 | ppl 6.64 | wps 24688.5 | ups 6.89 | wpb 3583.6 | bsz 145.4 | num_updates 290911 | lr 5.863e-05 | gnorm 2.527 | loss_scale 2 | train_wall 117 | gb_free 19.4 | wall 19008
2022-12-10 01:17:47 | INFO | fairseq.trainer | begin training epoch 265
2022-12-10 01:17:57 | INFO | train_inner | epoch 265:     89 / 1102 loss=6.57, nll_loss=2.691, ppl=6.46, wps=7023.4, ups=1.97, wpb=3559.8, bsz=158.1, num_updates=291000, lr=5.8621e-05, gnorm=2.499, loss_scale=2, train_wall=11, gb_free=19.9, wall=19018
2022-12-10 01:18:08 | INFO | train_inner | epoch 265:    189 / 1102 loss=6.65, nll_loss=2.742, ppl=6.69, wps=32714.6, ups=9.14, wpb=3578.7, bsz=126.1, num_updates=291100, lr=5.8611e-05, gnorm=2.505, loss_scale=2, train_wall=11, gb_free=19.4, wall=19029
2022-12-10 01:18:19 | INFO | train_inner | epoch 265:    289 / 1102 loss=6.553, nll_loss=2.677, ppl=6.4, wps=32478.7, ups=9.13, wpb=3555.7, bsz=155.1, num_updates=291200, lr=5.86009e-05, gnorm=2.436, loss_scale=2, train_wall=11, gb_free=19.6, wall=19040
2022-12-10 01:18:30 | INFO | train_inner | epoch 265:    389 / 1102 loss=6.583, nll_loss=2.682, ppl=6.42, wps=32924.7, ups=9.18, wpb=3586.2, bsz=133.9, num_updates=291300, lr=5.85908e-05, gnorm=2.463, loss_scale=2, train_wall=11, gb_free=19.8, wall=19051
2022-12-10 01:18:41 | INFO | train_inner | epoch 265:    489 / 1102 loss=6.677, nll_loss=2.777, ppl=6.86, wps=33673.4, ups=9.23, wpb=3649.7, bsz=129.2, num_updates=291400, lr=5.85808e-05, gnorm=2.491, loss_scale=2, train_wall=11, gb_free=19.4, wall=19061
2022-12-10 01:18:52 | INFO | train_inner | epoch 265:    589 / 1102 loss=6.582, nll_loss=2.691, ppl=6.46, wps=32854.8, ups=9.21, wpb=3567, bsz=145.4, num_updates=291500, lr=5.85707e-05, gnorm=2.472, loss_scale=2, train_wall=11, gb_free=19.6, wall=19072
2022-12-10 01:19:03 | INFO | train_inner | epoch 265:    689 / 1102 loss=6.598, nll_loss=2.705, ppl=6.52, wps=33068.4, ups=9.25, wpb=3574.1, bsz=150.8, num_updates=291600, lr=5.85607e-05, gnorm=2.544, loss_scale=2, train_wall=11, gb_free=19.3, wall=19083
2022-12-10 01:19:13 | INFO | train_inner | epoch 265:    789 / 1102 loss=6.552, nll_loss=2.704, ppl=6.52, wps=33381.7, ups=9.13, wpb=3657.6, bsz=184.5, num_updates=291700, lr=5.85507e-05, gnorm=2.383, loss_scale=2, train_wall=11, gb_free=19.5, wall=19094
2022-12-10 01:19:24 | INFO | train_inner | epoch 265:    889 / 1102 loss=6.691, nll_loss=2.793, ppl=6.93, wps=32745.3, ups=9.23, wpb=3548.1, bsz=135.3, num_updates=291800, lr=5.85406e-05, gnorm=2.689, loss_scale=2, train_wall=11, gb_free=19.5, wall=19105
2022-12-10 01:19:35 | INFO | train_inner | epoch 265:    989 / 1102 loss=6.661, nll_loss=2.763, ppl=6.79, wps=32610.1, ups=9.25, wpb=3526.8, bsz=145.8, num_updates=291900, lr=5.85306e-05, gnorm=2.576, loss_scale=2, train_wall=11, gb_free=19.5, wall=19116
2022-12-10 01:19:46 | INFO | train_inner | epoch 265:   1089 / 1102 loss=6.666, nll_loss=2.769, ppl=6.82, wps=32592.5, ups=9.07, wpb=3591.5, bsz=138.6, num_updates=292000, lr=5.85206e-05, gnorm=2.644, loss_scale=2, train_wall=11, gb_free=19.5, wall=19127
2022-12-10 01:19:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:20:27 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 3.665 | nll_loss 2.125 | ppl 4.36 | bleu 37.69 | wps 4561.1 | wpb 2835.3 | bsz 115.6 | num_updates 292013 | best_bleu 37.72
2022-12-10 01:20:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 292013 updates
2022-12-10 01:20:28 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint265.pt
2022-12-10 01:20:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint265.pt (epoch 265 @ 292013 updates, score 37.69) (writing took 1.3473416902124882 seconds)
2022-12-10 01:20:29 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-12-10 01:20:29 | INFO | train | epoch 265 | loss 6.619 | nll_loss 2.729 | ppl 6.63 | wps 24471.3 | ups 6.83 | wpb 3583.6 | bsz 145.4 | num_updates 292013 | lr 5.85193e-05 | gnorm 2.52 | loss_scale 2 | train_wall 117 | gb_free 19.8 | wall 19169
2022-12-10 01:20:29 | INFO | fairseq.trainer | begin training epoch 266
2022-12-10 01:20:39 | INFO | train_inner | epoch 266:     87 / 1102 loss=6.54, nll_loss=2.671, ppl=6.37, wps=6965.8, ups=1.91, wpb=3649.5, bsz=155.9, num_updates=292100, lr=5.85106e-05, gnorm=2.41, loss_scale=2, train_wall=11, gb_free=19.2, wall=19179
2022-12-10 01:20:49 | INFO | train_inner | epoch 266:    187 / 1102 loss=6.629, nll_loss=2.732, ppl=6.64, wps=33214.9, ups=9.15, wpb=3628.2, bsz=140.9, num_updates=292200, lr=5.85005e-05, gnorm=2.496, loss_scale=2, train_wall=11, gb_free=19.3, wall=19190
2022-12-10 01:21:00 | INFO | train_inner | epoch 266:    287 / 1102 loss=6.558, nll_loss=2.666, ppl=6.35, wps=32923.9, ups=9.17, wpb=3589.1, bsz=151.9, num_updates=292300, lr=5.84905e-05, gnorm=2.574, loss_scale=2, train_wall=11, gb_free=19.7, wall=19201
2022-12-10 01:21:11 | INFO | train_inner | epoch 266:    387 / 1102 loss=6.644, nll_loss=2.748, ppl=6.72, wps=32989, ups=9.22, wpb=3577.8, bsz=139.2, num_updates=292400, lr=5.84805e-05, gnorm=2.68, loss_scale=2, train_wall=11, gb_free=19.4, wall=19212
2022-12-10 01:21:22 | INFO | train_inner | epoch 266:    487 / 1102 loss=6.642, nll_loss=2.736, ppl=6.66, wps=32726.9, ups=9.19, wpb=3559.4, bsz=134.9, num_updates=292500, lr=5.84705e-05, gnorm=2.561, loss_scale=2, train_wall=11, gb_free=19.5, wall=19223
2022-12-10 01:21:33 | INFO | train_inner | epoch 266:    587 / 1102 loss=6.62, nll_loss=2.737, ppl=6.67, wps=32850.7, ups=9.12, wpb=3603.6, bsz=155.7, num_updates=292600, lr=5.84605e-05, gnorm=2.505, loss_scale=2, train_wall=11, gb_free=19.7, wall=19234
2022-12-10 01:21:44 | INFO | train_inner | epoch 266:    687 / 1102 loss=6.625, nll_loss=2.74, ppl=6.68, wps=32995.3, ups=9.35, wpb=3529.2, bsz=145, num_updates=292700, lr=5.84506e-05, gnorm=2.522, loss_scale=2, train_wall=10, gb_free=19.6, wall=19244
2022-12-10 01:21:55 | INFO | train_inner | epoch 266:    787 / 1102 loss=6.575, nll_loss=2.705, ppl=6.52, wps=32931.4, ups=9.07, wpb=3630.3, bsz=164.9, num_updates=292800, lr=5.84406e-05, gnorm=2.422, loss_scale=2, train_wall=11, gb_free=19.7, wall=19255
2022-12-10 01:22:06 | INFO | train_inner | epoch 266:    887 / 1102 loss=6.706, nll_loss=2.79, ppl=6.92, wps=32013.8, ups=9.21, wpb=3476.8, bsz=118.1, num_updates=292900, lr=5.84306e-05, gnorm=2.723, loss_scale=2, train_wall=11, gb_free=19.6, wall=19266
2022-12-10 01:22:17 | INFO | train_inner | epoch 266:    987 / 1102 loss=6.632, nll_loss=2.739, ppl=6.68, wps=32850.5, ups=9.15, wpb=3588.9, bsz=143.1, num_updates=293000, lr=5.84206e-05, gnorm=2.507, loss_scale=2, train_wall=11, gb_free=19.4, wall=19277
2022-12-10 01:22:28 | INFO | train_inner | epoch 266:   1087 / 1102 loss=6.627, nll_loss=2.749, ppl=6.72, wps=32505, ups=9.11, wpb=3569.9, bsz=148.5, num_updates=293100, lr=5.84107e-05, gnorm=2.533, loss_scale=2, train_wall=11, gb_free=19.8, wall=19288
2022-12-10 01:22:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:23:08 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 3.668 | nll_loss 2.127 | ppl 4.37 | bleu 37.59 | wps 4655.3 | wpb 2835.3 | bsz 115.6 | num_updates 293115 | best_bleu 37.72
2022-12-10 01:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 293115 updates
2022-12-10 01:23:09 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint266.pt
2022-12-10 01:23:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint266.pt (epoch 266 @ 293115 updates, score 37.59) (writing took 1.3220619168132544 seconds)
2022-12-10 01:23:09 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-12-10 01:23:09 | INFO | train | epoch 266 | loss 6.616 | nll_loss 2.727 | ppl 6.62 | wps 24573.2 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 293115 | lr 5.84092e-05 | gnorm 2.536 | loss_scale 2 | train_wall 118 | gb_free 19.3 | wall 19330
2022-12-10 01:23:09 | INFO | fairseq.trainer | begin training epoch 267
2022-12-10 01:23:19 | INFO | train_inner | epoch 267:     85 / 1102 loss=6.616, nll_loss=2.717, ppl=6.57, wps=7002.6, ups=1.96, wpb=3579, bsz=129.9, num_updates=293200, lr=5.84007e-05, gnorm=2.447, loss_scale=2, train_wall=11, gb_free=19.4, wall=19339
2022-12-10 01:23:29 | INFO | train_inner | epoch 267:    185 / 1102 loss=6.519, nll_loss=2.638, ppl=6.22, wps=32971.1, ups=9.3, wpb=3545.9, bsz=147.7, num_updates=293300, lr=5.83907e-05, gnorm=2.468, loss_scale=2, train_wall=11, gb_free=19.5, wall=19350
2022-12-10 01:23:40 | INFO | train_inner | epoch 267:    285 / 1102 loss=6.661, nll_loss=2.762, ppl=6.78, wps=32775.1, ups=9.13, wpb=3590, bsz=137.4, num_updates=293400, lr=5.83808e-05, gnorm=2.547, loss_scale=2, train_wall=11, gb_free=19.3, wall=19361
2022-12-10 01:23:51 | INFO | train_inner | epoch 267:    385 / 1102 loss=6.622, nll_loss=2.732, ppl=6.64, wps=32875.6, ups=9.21, wpb=3568.5, bsz=147.6, num_updates=293500, lr=5.83708e-05, gnorm=2.545, loss_scale=2, train_wall=11, gb_free=19.7, wall=19372
2022-12-10 01:24:02 | INFO | train_inner | epoch 267:    485 / 1102 loss=6.658, nll_loss=2.749, ppl=6.72, wps=32847.1, ups=9.21, wpb=3565, bsz=131.5, num_updates=293600, lr=5.83609e-05, gnorm=2.561, loss_scale=2, train_wall=11, gb_free=19.5, wall=19383
2022-12-10 01:24:13 | INFO | train_inner | epoch 267:    585 / 1102 loss=6.625, nll_loss=2.742, ppl=6.69, wps=32822.9, ups=9.1, wpb=3606.7, bsz=151.8, num_updates=293700, lr=5.8351e-05, gnorm=2.456, loss_scale=2, train_wall=11, gb_free=19.3, wall=19394
2022-12-10 01:24:24 | INFO | train_inner | epoch 267:    685 / 1102 loss=6.628, nll_loss=2.739, ppl=6.68, wps=33201, ups=9.13, wpb=3634.8, bsz=143.3, num_updates=293800, lr=5.8341e-05, gnorm=2.504, loss_scale=2, train_wall=11, gb_free=19.7, wall=19405
2022-12-10 01:24:35 | INFO | train_inner | epoch 267:    785 / 1102 loss=6.618, nll_loss=2.724, ppl=6.61, wps=32175.2, ups=9.14, wpb=3520.9, bsz=146.7, num_updates=293900, lr=5.83311e-05, gnorm=2.558, loss_scale=2, train_wall=11, gb_free=19.5, wall=19416
2022-12-10 01:24:46 | INFO | train_inner | epoch 267:    885 / 1102 loss=6.642, nll_loss=2.754, ppl=6.75, wps=33139.7, ups=9.12, wpb=3632.1, bsz=142.5, num_updates=294000, lr=5.83212e-05, gnorm=2.504, loss_scale=2, train_wall=11, gb_free=19.3, wall=19427
2022-12-10 01:24:57 | INFO | train_inner | epoch 267:    985 / 1102 loss=6.594, nll_loss=2.716, ppl=6.57, wps=32819.7, ups=9.09, wpb=3611.8, bsz=162.3, num_updates=294100, lr=5.83113e-05, gnorm=2.517, loss_scale=2, train_wall=11, gb_free=19.2, wall=19438
2022-12-10 01:25:08 | INFO | train_inner | epoch 267:   1085 / 1102 loss=6.622, nll_loss=2.74, ppl=6.68, wps=32510, ups=9.1, wpb=3571.1, bsz=147.3, num_updates=294200, lr=5.83014e-05, gnorm=2.465, loss_scale=2, train_wall=11, gb_free=19.6, wall=19449
2022-12-10 01:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:25:49 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 3.671 | nll_loss 2.129 | ppl 4.37 | bleu 37.53 | wps 4552.3 | wpb 2835.3 | bsz 115.6 | num_updates 294217 | best_bleu 37.72
2022-12-10 01:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 294217 updates
2022-12-10 01:25:50 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint267.pt
2022-12-10 01:25:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint267.pt (epoch 267 @ 294217 updates, score 37.53) (writing took 1.358052747324109 seconds)
2022-12-10 01:25:51 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-12-10 01:25:51 | INFO | train | epoch 267 | loss 6.615 | nll_loss 2.726 | ppl 6.62 | wps 24457.4 | ups 6.82 | wpb 3583.6 | bsz 145.4 | num_updates 294217 | lr 5.82997e-05 | gnorm 2.504 | loss_scale 2 | train_wall 118 | gb_free 19.5 | wall 19491
2022-12-10 01:25:51 | INFO | fairseq.trainer | begin training epoch 268
2022-12-10 01:26:00 | INFO | train_inner | epoch 268:     83 / 1102 loss=6.538, nll_loss=2.655, ppl=6.3, wps=6744, ups=1.92, wpb=3513.9, bsz=156.9, num_updates=294300, lr=5.82915e-05, gnorm=2.485, loss_scale=2, train_wall=11, gb_free=19.3, wall=19501
2022-12-10 01:26:11 | INFO | train_inner | epoch 268:    183 / 1102 loss=6.601, nll_loss=2.715, ppl=6.57, wps=33339.4, ups=9.21, wpb=3619, bsz=141.4, num_updates=294400, lr=5.82816e-05, gnorm=2.438, loss_scale=2, train_wall=11, gb_free=19.3, wall=19511
2022-12-10 01:26:22 | INFO | train_inner | epoch 268:    283 / 1102 loss=6.557, nll_loss=2.68, ppl=6.41, wps=33106.9, ups=9.09, wpb=3643.5, bsz=154, num_updates=294500, lr=5.82717e-05, gnorm=2.418, loss_scale=2, train_wall=11, gb_free=19.5, wall=19522
2022-12-10 01:26:33 | INFO | train_inner | epoch 268:    383 / 1102 loss=6.67, nll_loss=2.769, ppl=6.81, wps=32909.3, ups=9.2, wpb=3578.3, bsz=133.9, num_updates=294600, lr=5.82618e-05, gnorm=2.565, loss_scale=2, train_wall=11, gb_free=19.7, wall=19533
2022-12-10 01:26:44 | INFO | train_inner | epoch 268:    483 / 1102 loss=6.612, nll_loss=2.726, ppl=6.62, wps=32612.4, ups=9.16, wpb=3560.7, bsz=150.4, num_updates=294700, lr=5.82519e-05, gnorm=2.57, loss_scale=2, train_wall=11, gb_free=19.7, wall=19544
2022-12-10 01:26:55 | INFO | train_inner | epoch 268:    583 / 1102 loss=6.639, nll_loss=2.745, ppl=6.7, wps=33076.6, ups=9.19, wpb=3598.3, bsz=143.2, num_updates=294800, lr=5.8242e-05, gnorm=2.571, loss_scale=2, train_wall=11, gb_free=19.5, wall=19555
2022-12-10 01:27:06 | INFO | train_inner | epoch 268:    683 / 1102 loss=6.591, nll_loss=2.697, ppl=6.49, wps=32676.3, ups=9.14, wpb=3575.8, bsz=142.6, num_updates=294900, lr=5.82321e-05, gnorm=2.577, loss_scale=2, train_wall=11, gb_free=19.3, wall=19566
2022-12-10 01:27:16 | INFO | train_inner | epoch 268:    783 / 1102 loss=6.562, nll_loss=2.683, ppl=6.42, wps=33044.7, ups=9.18, wpb=3601.5, bsz=162.9, num_updates=295000, lr=5.82223e-05, gnorm=2.511, loss_scale=2, train_wall=11, gb_free=19.4, wall=19577
2022-12-10 01:27:27 | INFO | train_inner | epoch 268:    883 / 1102 loss=6.72, nll_loss=2.823, ppl=7.07, wps=32509.3, ups=9.15, wpb=3553.8, bsz=137, num_updates=295100, lr=5.82124e-05, gnorm=2.597, loss_scale=2, train_wall=11, gb_free=19.5, wall=19588
2022-12-10 01:27:38 | INFO | train_inner | epoch 268:    983 / 1102 loss=6.654, nll_loss=2.756, ppl=6.76, wps=33031.2, ups=9.22, wpb=3582.2, bsz=137.6, num_updates=295200, lr=5.82025e-05, gnorm=2.65, loss_scale=2, train_wall=11, gb_free=19.2, wall=19599
2022-12-10 01:27:49 | INFO | train_inner | epoch 268:   1083 / 1102 loss=6.565, nll_loss=2.688, ppl=6.44, wps=32878.7, ups=9.11, wpb=3608.4, bsz=152.4, num_updates=295300, lr=5.81927e-05, gnorm=2.397, loss_scale=2, train_wall=11, gb_free=19.5, wall=19610
2022-12-10 01:27:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:28:29 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 3.661 | nll_loss 2.123 | ppl 4.36 | bleu 37.51 | wps 4814 | wpb 2835.3 | bsz 115.6 | num_updates 295319 | best_bleu 37.72
2022-12-10 01:28:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 295319 updates
2022-12-10 01:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint268.pt
2022-12-10 01:28:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint268.pt (epoch 268 @ 295319 updates, score 37.51) (writing took 1.3603585716336966 seconds)
2022-12-10 01:28:30 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-12-10 01:28:30 | INFO | train | epoch 268 | loss 6.615 | nll_loss 2.725 | ppl 6.61 | wps 24738.7 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 295319 | lr 5.81908e-05 | gnorm 2.53 | loss_scale 2 | train_wall 117 | gb_free 20.1 | wall 19651
2022-12-10 01:28:30 | INFO | fairseq.trainer | begin training epoch 269
2022-12-10 01:28:40 | INFO | train_inner | epoch 269:     81 / 1102 loss=6.576, nll_loss=2.697, ppl=6.48, wps=7119.8, ups=1.98, wpb=3591.1, bsz=150.2, num_updates=295400, lr=5.81828e-05, gnorm=2.473, loss_scale=2, train_wall=11, gb_free=19.3, wall=19660
2022-12-10 01:28:50 | INFO | train_inner | epoch 269:    181 / 1102 loss=6.599, nll_loss=2.711, ppl=6.55, wps=33290, ups=9.29, wpb=3584.1, bsz=148.6, num_updates=295500, lr=5.8173e-05, gnorm=2.553, loss_scale=2, train_wall=11, gb_free=19.8, wall=19671
2022-12-10 01:29:01 | INFO | train_inner | epoch 269:    281 / 1102 loss=6.525, nll_loss=2.654, ppl=6.29, wps=32644.1, ups=9.19, wpb=3553.8, bsz=170.7, num_updates=295600, lr=5.81631e-05, gnorm=2.456, loss_scale=2, train_wall=11, gb_free=21.3, wall=19682
2022-12-10 01:29:12 | INFO | train_inner | epoch 269:    381 / 1102 loss=6.594, nll_loss=2.71, ppl=6.54, wps=33212.4, ups=9.1, wpb=3649.9, bsz=147.8, num_updates=295700, lr=5.81533e-05, gnorm=2.555, loss_scale=2, train_wall=11, gb_free=19.3, wall=19693
2022-12-10 01:29:23 | INFO | train_inner | epoch 269:    481 / 1102 loss=6.62, nll_loss=2.718, ppl=6.58, wps=32610.9, ups=9.14, wpb=3568.1, bsz=139, num_updates=295800, lr=5.81435e-05, gnorm=2.535, loss_scale=2, train_wall=11, gb_free=19.6, wall=19704
2022-12-10 01:29:34 | INFO | train_inner | epoch 269:    581 / 1102 loss=6.685, nll_loss=2.766, ppl=6.8, wps=32639.2, ups=9.21, wpb=3542.4, bsz=116.2, num_updates=295900, lr=5.81336e-05, gnorm=2.623, loss_scale=2, train_wall=11, gb_free=19.5, wall=19715
2022-12-10 01:29:45 | INFO | train_inner | epoch 269:    681 / 1102 loss=6.653, nll_loss=2.763, ppl=6.79, wps=33309.1, ups=9.16, wpb=3637.1, bsz=146.2, num_updates=296000, lr=5.81238e-05, gnorm=2.56, loss_scale=2, train_wall=11, gb_free=19.7, wall=19726
2022-12-10 01:29:56 | INFO | train_inner | epoch 269:    781 / 1102 loss=6.585, nll_loss=2.709, ppl=6.54, wps=32499, ups=9.18, wpb=3541.5, bsz=161.6, num_updates=296100, lr=5.8114e-05, gnorm=2.465, loss_scale=2, train_wall=11, gb_free=19.4, wall=19736
2022-12-10 01:30:07 | INFO | train_inner | epoch 269:    881 / 1102 loss=6.695, nll_loss=2.783, ppl=6.88, wps=32914.4, ups=9.17, wpb=3588, bsz=119.6, num_updates=296200, lr=5.81042e-05, gnorm=2.643, loss_scale=2, train_wall=11, gb_free=19.7, wall=19747
2022-12-10 01:30:18 | INFO | train_inner | epoch 269:    981 / 1102 loss=6.649, nll_loss=2.746, ppl=6.71, wps=32409.2, ups=9.15, wpb=3540.6, bsz=133.1, num_updates=296300, lr=5.80944e-05, gnorm=2.675, loss_scale=2, train_wall=11, gb_free=19.5, wall=19758
2022-12-10 01:30:29 | INFO | train_inner | epoch 269:   1081 / 1102 loss=6.575, nll_loss=2.708, ppl=6.53, wps=33219.8, ups=9.17, wpb=3621.2, bsz=162.2, num_updates=296400, lr=5.80846e-05, gnorm=2.421, loss_scale=2, train_wall=11, gb_free=19.4, wall=19769
2022-12-10 01:30:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:31:10 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 3.669 | nll_loss 2.128 | ppl 4.37 | bleu 37.55 | wps 4640.5 | wpb 2835.3 | bsz 115.6 | num_updates 296421 | best_bleu 37.72
2022-12-10 01:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 296421 updates
2022-12-10 01:31:11 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint269.pt
2022-12-10 01:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint269.pt (epoch 269 @ 296421 updates, score 37.55) (writing took 1.3260224014520645 seconds)
2022-12-10 01:31:11 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-12-10 01:31:11 | INFO | train | epoch 269 | loss 6.613 | nll_loss 2.723 | ppl 6.6 | wps 24565.2 | ups 6.85 | wpb 3583.6 | bsz 145.4 | num_updates 296421 | lr 5.80825e-05 | gnorm 2.539 | loss_scale 2 | train_wall 117 | gb_free 19.6 | wall 19812
2022-12-10 01:31:11 | INFO | fairseq.trainer | begin training epoch 270
2022-12-10 01:31:20 | INFO | train_inner | epoch 270:     79 / 1102 loss=6.638, nll_loss=2.754, ppl=6.75, wps=7020.2, ups=1.94, wpb=3610.3, bsz=141.4, num_updates=296500, lr=5.80748e-05, gnorm=2.464, loss_scale=2, train_wall=11, gb_free=19.5, wall=19821
2022-12-10 01:31:31 | INFO | train_inner | epoch 270:    179 / 1102 loss=6.567, nll_loss=2.684, ppl=6.43, wps=33013.3, ups=9.21, wpb=3583.2, bsz=151.1, num_updates=296600, lr=5.8065e-05, gnorm=2.511, loss_scale=2, train_wall=11, gb_free=19.4, wall=19831
2022-12-10 01:31:42 | INFO | train_inner | epoch 270:    279 / 1102 loss=6.494, nll_loss=2.621, ppl=6.15, wps=32807, ups=9.15, wpb=3585.6, bsz=169.4, num_updates=296700, lr=5.80552e-05, gnorm=2.409, loss_scale=2, train_wall=11, gb_free=19.8, wall=19842
2022-12-10 01:31:52 | INFO | train_inner | epoch 270:    379 / 1102 loss=6.605, nll_loss=2.697, ppl=6.49, wps=33124.2, ups=9.36, wpb=3538.5, bsz=132.4, num_updates=296800, lr=5.80454e-05, gnorm=2.536, loss_scale=2, train_wall=10, gb_free=19.8, wall=19853
2022-12-10 01:32:03 | INFO | train_inner | epoch 270:    479 / 1102 loss=6.658, nll_loss=2.737, ppl=6.67, wps=33016, ups=9.25, wpb=3569.1, bsz=129.7, num_updates=296900, lr=5.80357e-05, gnorm=2.651, loss_scale=2, train_wall=11, gb_free=19.3, wall=19864
2022-12-10 01:32:14 | INFO | train_inner | epoch 270:    579 / 1102 loss=6.629, nll_loss=2.724, ppl=6.61, wps=32924.8, ups=9.29, wpb=3545.9, bsz=139.3, num_updates=297000, lr=5.80259e-05, gnorm=2.611, loss_scale=2, train_wall=11, gb_free=19.6, wall=19875
2022-12-10 01:32:25 | INFO | train_inner | epoch 270:    679 / 1102 loss=6.626, nll_loss=2.723, ppl=6.6, wps=32653.6, ups=9.16, wpb=3563.2, bsz=138.4, num_updates=297100, lr=5.80161e-05, gnorm=2.614, loss_scale=2, train_wall=11, gb_free=19.9, wall=19886
2022-12-10 01:32:36 | INFO | train_inner | epoch 270:    779 / 1102 loss=6.615, nll_loss=2.734, ppl=6.65, wps=33379, ups=9.2, wpb=3628.1, bsz=144.2, num_updates=297200, lr=5.80064e-05, gnorm=2.471, loss_scale=2, train_wall=11, gb_free=19.3, wall=19896
2022-12-10 01:32:47 | INFO | train_inner | epoch 270:    879 / 1102 loss=6.613, nll_loss=2.736, ppl=6.66, wps=32952, ups=9.04, wpb=3644.3, bsz=157.2, num_updates=297300, lr=5.79966e-05, gnorm=2.53, loss_scale=2, train_wall=11, gb_free=19.5, wall=19907
2022-12-10 01:32:58 | INFO | train_inner | epoch 270:    979 / 1102 loss=6.606, nll_loss=2.719, ppl=6.58, wps=32786.6, ups=9.25, wpb=3543.4, bsz=149.5, num_updates=297400, lr=5.79869e-05, gnorm=2.473, loss_scale=2, train_wall=11, gb_free=19.7, wall=19918
2022-12-10 01:33:09 | INFO | train_inner | epoch 270:   1079 / 1102 loss=6.668, nll_loss=2.783, ppl=6.88, wps=33079.1, ups=9.25, wpb=3576.3, bsz=149.7, num_updates=297500, lr=5.79771e-05, gnorm=2.68, loss_scale=2, train_wall=11, gb_free=19.8, wall=19929
2022-12-10 01:33:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:33:50 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 3.665 | nll_loss 2.126 | ppl 4.37 | bleu 37.71 | wps 4574.1 | wpb 2835.3 | bsz 115.6 | num_updates 297523 | best_bleu 37.72
2022-12-10 01:33:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 297523 updates
2022-12-10 01:33:51 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint270.pt
2022-12-10 01:33:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint270.pt (epoch 270 @ 297523 updates, score 37.71) (writing took 1.32714700140059 seconds)
2022-12-10 01:33:52 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-12-10 01:33:52 | INFO | train | epoch 270 | loss 6.613 | nll_loss 2.722 | ppl 6.6 | wps 24582.6 | ups 6.86 | wpb 3583.6 | bsz 145.4 | num_updates 297523 | lr 5.79749e-05 | gnorm 2.545 | loss_scale 2 | train_wall 117 | gb_free 19.4 | wall 19972
2022-12-10 01:33:52 | INFO | fairseq.trainer | begin training epoch 271
2022-12-10 01:34:01 | INFO | train_inner | epoch 271:     77 / 1102 loss=6.583, nll_loss=2.704, ppl=6.51, wps=6998.4, ups=1.92, wpb=3641.7, bsz=148.5, num_updates=297600, lr=5.79674e-05, gnorm=2.469, loss_scale=2, train_wall=11, gb_free=19.8, wall=19981
2022-12-10 01:34:11 | INFO | train_inner | epoch 271:    177 / 1102 loss=6.596, nll_loss=2.711, ppl=6.55, wps=33099.6, ups=9.26, wpb=3572.8, bsz=141.3, num_updates=297700, lr=5.79576e-05, gnorm=2.455, loss_scale=2, train_wall=11, gb_free=19.5, wall=19992
2022-12-10 01:34:22 | INFO | train_inner | epoch 271:    277 / 1102 loss=6.534, nll_loss=2.633, ppl=6.2, wps=32546.4, ups=9.32, wpb=3490.9, bsz=148.3, num_updates=297800, lr=5.79479e-05, gnorm=2.649, loss_scale=4, train_wall=10, gb_free=19.5, wall=20003
2022-12-10 01:34:33 | INFO | train_inner | epoch 271:    377 / 1102 loss=6.609, nll_loss=2.704, ppl=6.51, wps=33032.8, ups=9.34, wpb=3537, bsz=138.9, num_updates=297900, lr=5.79382e-05, gnorm=2.56, loss_scale=4, train_wall=10, gb_free=19.8, wall=20013
2022-12-10 01:34:44 | INFO | train_inner | epoch 271:    477 / 1102 loss=6.559, nll_loss=2.678, ppl=6.4, wps=32960.3, ups=9.24, wpb=3568.9, bsz=157.4, num_updates=298000, lr=5.79284e-05, gnorm=2.46, loss_scale=4, train_wall=11, gb_free=19.5, wall=20024
2022-12-10 01:34:55 | INFO | train_inner | epoch 271:    577 / 1102 loss=6.644, nll_loss=2.766, ppl=6.8, wps=33994.1, ups=9.18, wpb=3701.4, bsz=144.4, num_updates=298100, lr=5.79187e-05, gnorm=2.424, loss_scale=4, train_wall=11, gb_free=19.4, wall=20035
2022-12-10 01:35:05 | INFO | train_inner | epoch 271:    677 / 1102 loss=6.612, nll_loss=2.717, ppl=6.57, wps=32923.3, ups=9.2, wpb=3577, bsz=149.4, num_updates=298200, lr=5.7909e-05, gnorm=2.557, loss_scale=4, train_wall=11, gb_free=19.8, wall=20046
2022-12-10 01:35:16 | INFO | train_inner | epoch 271:    777 / 1102 loss=6.655, nll_loss=2.748, ppl=6.72, wps=32996.1, ups=9.23, wpb=3576.2, bsz=133.1, num_updates=298300, lr=5.78993e-05, gnorm=2.585, loss_scale=4, train_wall=11, gb_free=19.4, wall=20057
2022-12-10 01:35:27 | INFO | train_inner | epoch 271:    877 / 1102 loss=6.638, nll_loss=2.743, ppl=6.7, wps=33278.4, ups=9.31, wpb=3573.9, bsz=140.6, num_updates=298400, lr=5.78896e-05, gnorm=2.592, loss_scale=4, train_wall=10, gb_free=19.8, wall=20068
2022-12-10 01:35:38 | INFO | train_inner | epoch 271:    977 / 1102 loss=6.663, nll_loss=2.769, ppl=6.82, wps=33183.9, ups=9.31, wpb=3564.3, bsz=141.7, num_updates=298500, lr=5.78799e-05, gnorm=2.54, loss_scale=4, train_wall=11, gb_free=19.8, wall=20078
2022-12-10 01:35:49 | INFO | train_inner | epoch 271:   1077 / 1102 loss=6.604, nll_loss=2.729, ppl=6.63, wps=33109, ups=9.17, wpb=3610.9, bsz=158.4, num_updates=298600, lr=5.78702e-05, gnorm=2.428, loss_scale=4, train_wall=11, gb_free=19.4, wall=20089
2022-12-10 01:35:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:36:30 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 3.668 | nll_loss 2.127 | ppl 4.37 | bleu 37.58 | wps 4634.6 | wpb 2835.3 | bsz 115.6 | num_updates 298625 | best_bleu 37.72
2022-12-10 01:36:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 298625 updates
2022-12-10 01:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint271.pt
2022-12-10 01:36:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint271.pt (epoch 271 @ 298625 updates, score 37.58) (writing took 1.3221691818907857 seconds)
2022-12-10 01:36:32 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-12-10 01:36:32 | INFO | train | epoch 271 | loss 6.61 | nll_loss 2.72 | ppl 6.59 | wps 24726.5 | ups 6.9 | wpb 3583.6 | bsz 145.4 | num_updates 298625 | lr 5.78678e-05 | gnorm 2.516 | loss_scale 4 | train_wall 116 | gb_free 19.4 | wall 20132
2022-12-10 01:36:32 | INFO | fairseq.trainer | begin training epoch 272
2022-12-10 01:36:40 | INFO | train_inner | epoch 272:     75 / 1102 loss=6.6, nll_loss=2.715, ppl=6.56, wps=6985.2, ups=1.94, wpb=3592.8, bsz=141.7, num_updates=298700, lr=5.78605e-05, gnorm=2.487, loss_scale=4, train_wall=11, gb_free=19.4, wall=20141
2022-12-10 01:36:51 | INFO | train_inner | epoch 272:    175 / 1102 loss=6.567, nll_loss=2.673, ppl=6.38, wps=33219.2, ups=9.26, wpb=3588.8, bsz=146.2, num_updates=298800, lr=5.78508e-05, gnorm=2.643, loss_scale=4, train_wall=11, gb_free=19.7, wall=20151
2022-12-10 01:37:02 | INFO | train_inner | epoch 272:    275 / 1102 loss=6.593, nll_loss=2.689, ppl=6.45, wps=33135.3, ups=9.3, wpb=3561.7, bsz=135.7, num_updates=298900, lr=5.78412e-05, gnorm=2.617, loss_scale=4, train_wall=11, gb_free=19.7, wall=20162
2022-12-10 01:37:12 | INFO | train_inner | epoch 272:    375 / 1102 loss=6.668, nll_loss=2.782, ppl=6.88, wps=33354.8, ups=9.28, wpb=3593.4, bsz=139.6, num_updates=299000, lr=5.78315e-05, gnorm=2.526, loss_scale=4, train_wall=11, gb_free=19.6, wall=20173
2022-12-10 01:37:23 | INFO | train_inner | epoch 272:    475 / 1102 loss=6.608, nll_loss=2.723, ppl=6.6, wps=33519.9, ups=9.31, wpb=3601.3, bsz=147, num_updates=299100, lr=5.78218e-05, gnorm=2.515, loss_scale=4, train_wall=11, gb_free=19.3, wall=20184
2022-12-10 01:37:34 | INFO | train_inner | epoch 272:    575 / 1102 loss=6.58, nll_loss=2.683, ppl=6.42, wps=32728.4, ups=9.26, wpb=3535.6, bsz=151.9, num_updates=299200, lr=5.78122e-05, gnorm=2.536, loss_scale=4, train_wall=11, gb_free=19.3, wall=20195
2022-12-10 01:37:45 | INFO | train_inner | epoch 272:    675 / 1102 loss=6.59, nll_loss=2.702, ppl=6.51, wps=33013.8, ups=9.29, wpb=3553.9, bsz=153.8, num_updates=299300, lr=5.78025e-05, gnorm=2.512, loss_scale=4, train_wall=11, gb_free=19.8, wall=20205
2022-12-10 01:37:55 | INFO | train_inner | epoch 272:    775 / 1102 loss=6.615, nll_loss=2.728, ppl=6.62, wps=33551.8, ups=9.28, wpb=3615.5, bsz=144.3, num_updates=299400, lr=5.77928e-05, gnorm=2.518, loss_scale=4, train_wall=11, gb_free=19.3, wall=20216
2022-12-10 01:38:06 | INFO | train_inner | epoch 272:    875 / 1102 loss=6.58, nll_loss=2.705, ppl=6.52, wps=33152.6, ups=9.21, wpb=3601.2, bsz=155, num_updates=299500, lr=5.77832e-05, gnorm=2.486, loss_scale=4, train_wall=11, gb_free=19.4, wall=20227
2022-12-10 01:38:17 | INFO | train_inner | epoch 272:    975 / 1102 loss=6.624, nll_loss=2.737, ppl=6.67, wps=33096.9, ups=9.2, wpb=3597.9, bsz=148.6, num_updates=299600, lr=5.77736e-05, gnorm=2.517, loss_scale=4, train_wall=11, gb_free=19.4, wall=20238
2022-12-10 01:38:28 | INFO | train_inner | epoch 272:   1075 / 1102 loss=6.669, nll_loss=2.774, ppl=6.84, wps=33245.9, ups=9.23, wpb=3602.7, bsz=140.2, num_updates=299700, lr=5.77639e-05, gnorm=2.577, loss_scale=4, train_wall=11, gb_free=19.5, wall=20249
2022-12-10 01:38:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:39:09 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 3.663 | nll_loss 2.125 | ppl 4.36 | bleu 37.54 | wps 4796.7 | wpb 2835.3 | bsz 115.6 | num_updates 299727 | best_bleu 37.72
2022-12-10 01:39:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 299727 updates
2022-12-10 01:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint272.pt
2022-12-10 01:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint272.pt (epoch 272 @ 299727 updates, score 37.54) (writing took 1.3216379899531603 seconds)
2022-12-10 01:39:10 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-12-10 01:39:10 | INFO | train | epoch 272 | loss 6.609 | nll_loss 2.719 | ppl 6.58 | wps 24927.7 | ups 6.96 | wpb 3583.6 | bsz 145.4 | num_updates 299727 | lr 5.77613e-05 | gnorm 2.544 | loss_scale 4 | train_wall 116 | gb_free 19.6 | wall 20291
2022-12-10 01:39:10 | INFO | fairseq.trainer | begin training epoch 273
2022-12-10 01:39:18 | INFO | train_inner | epoch 273:     73 / 1102 loss=6.641, nll_loss=2.733, ppl=6.65, wps=7030.5, ups=1.99, wpb=3525.6, bsz=131.5, num_updates=299800, lr=5.77543e-05, gnorm=2.569, loss_scale=4, train_wall=11, gb_free=19.4, wall=20299
2022-12-10 01:39:29 | INFO | train_inner | epoch 273:    173 / 1102 loss=6.618, nll_loss=2.714, ppl=6.56, wps=32479.6, ups=9.21, wpb=3527.4, bsz=136.4, num_updates=299900, lr=5.77447e-05, gnorm=2.575, loss_scale=4, train_wall=11, gb_free=19.8, wall=20310
2022-12-10 01:39:40 | INFO | train_inner | epoch 273:    273 / 1102 loss=6.547, nll_loss=2.669, ppl=6.36, wps=32531.9, ups=9.08, wpb=3581.7, bsz=158.8, num_updates=300000, lr=5.7735e-05, gnorm=2.532, loss_scale=4, train_wall=11, gb_free=19.9, wall=20321
2022-12-10 01:39:40 | INFO | fairseq_cli.train | Stopping training due to num_updates: 300000 >= max_update: 300000
2022-12-10 01:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-10 01:40:22 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 3.675 | nll_loss 2.132 | ppl 4.38 | bleu 37.46 | wps 4365.4 | wpb 2835.3 | bsz 115.6 | num_updates 300000 | best_bleu 37.72
2022-12-10 01:40:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 300000 updates
2022-12-10 01:40:23 | INFO | fairseq.trainer | Finished saving checkpoint to iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt
2022-12-10 01:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint iwslt14.rdrop.de-en-ckpt/checkpoint_last.pt (epoch 273 @ 300000 updates, score 37.46) (writing took 1.0704638250172138 seconds)
2022-12-10 01:40:23 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-12-10 01:40:23 | INFO | train | epoch 273 | loss 6.585 | nll_loss 2.69 | ppl 6.45 | wps 13307.4 | ups 3.76 | wpb 3540.8 | bsz 144.7 | num_updates 300000 | lr 5.7735e-05 | gnorm 2.55 | loss_scale 4 | train_wall 29 | gb_free 19.9 | wall 20363
2022-12-10 01:40:23 | INFO | fairseq_cli.train | done training in 20362.3 seconds
